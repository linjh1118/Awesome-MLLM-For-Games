{"title":"Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks","authors":"Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie","summary":"Building a general-purpose agent is a long-standing vision in the field of\nartificial intelligence. Existing agents have made remarkable progress in many\ndomains, yet they still struggle to complete long-horizon tasks in an open\nworld. We attribute this to the lack of necessary world knowledge and\nmultimodal experience that can guide agents through a variety of long-horizon\ntasks. In this paper, we propose a Hybrid Multimodal Memory module to address\nthe above challenges. It 1) transforms knowledge into Hierarchical Directed\nKnowledge Graph that allows agents to explicitly represent and learn world\nknowledge, and 2) summarises historical information into Abstracted Multimodal\nExperience Pool that provide agents with rich references for in-context\nlearning. On top of the Hybrid Multimodal Memory module, a multimodal agent,\nOptimus-1, is constructed with dedicated Knowledge-guided Planner and\nExperience-Driven Reflector, contributing to a better planning and reflection\nin the face of long-horizon tasks in Minecraft. Extensive experimental results\nshow that Optimus-1 significantly outperforms all existing agents on\nchallenging long-horizon task benchmarks, and exhibits near human-level\nperformance on many tasks. In addition, we introduce various Multimodal Large\nLanguage Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 exhibits strong generalization with the help of the Hybrid\nMultimodal Memory module, outperforming the GPT-4V baseline on many tasks.","url":"http:\/\/arxiv.org\/abs\/2408.03615v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2408.03615v2","published":1723018592000,"comment":"Accepted by NeurIPS 2024","pdf_text":"Optimus-1\n: Hybrid Multimodal Memory\nEmpowered Agents Excel in Long-Horizon Tasks\nZaijing Li1 2, Yuquan Xie1, Rui Shao1∗, Gongwei Chen1, Dongmei Jiang2, Liqiang Nie1∗\n1Harbin Institute of Technology, Shenzhen\n2Peng Cheng Laboratory\n{lzj14011,xieyuquan20016,rshaojimmy,nieliqiang}@gmail.com\nExperience-Driven Reflector\n \nAction Controller\n \nTask: Craft stone sword.\nHybrid Multimodal \nMemory\nHierarchical Directed \nKnowledge Graph\nAbstracted Multimodal \nExperience Pool\n                              \nKnowledge-Guided Planner\nMission Success!!!\nTask: Craft stone pickaxe.\nEnv: forest\nVisual Info: health: full...\nObs:\nRetrieve: Memory|Knowledge\nRetrieve: Memory|Experience\nReflection: Mine stone.\nGoal: Mine stone.\nFigure 1: An illustration of Optimus-1 performing long-horizon tasks in Minecraft. Given the task\n“Craft stone sword”, Knowledge-Guided Planner incorporates knowledge from Hierarchical Directed\nKnowledge Graph into planning, then Action Controller executes these planning sequences step-by-\nstep. During the execution of the task, the Experience-Driven Reflector is periodically activated and\nretrieve experience from Abstracted Multimodal Experience Pool to make reflection.\nAbstract\nBuilding a general-purpose agent is a long-standing vision in the field of artificial\nintelligence. Existing agents have made remarkable progress in many domains, yet\nthey still struggle to complete long-horizon tasks in an open world. We attribute\nthis to the lack of necessary world knowledge and multimodal experience that can\nguide agents through a variety of long-horizon tasks. In this paper, we propose\na Hybrid Multimodal Memory module to address the above challenges. It 1)\ntransforms knowledge into Hierarchical Directed Knowledge Graph that allows\nagents to explicitly represent and learn world knowledge, and 2) summarises histor-\nical information into Abstracted Multimodal Experience Pool that provide agents\n*Corresponding authors\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2408.03615v2  [cs.AI]  21 Oct 2024\nwith rich references for in-context learning. On top of the Hybrid Multimodal\nMemory module, a multimodal agent, Optimus-1, is constructed with dedicated\nKnowledge-guided Planner and Experience-Driven Reflector, contributing to a\nbetter planning and reflection in the face of long-horizon tasks in Minecraft. Exten-\nsive experimental results show that Optimus-1 significantly outperforms all existing\nagents on challenging long-horizon task benchmarks, and exhibits near human-\nlevel performance on many tasks. In addition, we introduce various Multimodal\nLarge Language Models (MLLMs) as the backbone of Optimus-1. Experimental\nresults show that Optimus-1 exhibits strong generalization with the help of the Hy-\nbrid Multimodal Memory module, outperforming the GPT-4V baseline on various\ntasks. Please see the project page at https:\/\/cybertronagent.github.io\/Optimus-\n1.github.io\/.\n1\nIntroduction\nOptimus Prime faces complex tasks alongside humans in Transformers to protect the peace of\nthe planet. Creating an agent [43, 13] like Optimus that can perceive, plan, reflect, and complete\nlong-horizon tasks in an open world has been a longstanding aspiration in the field of artificial\nintelligence [22, 35, 36]. Early research developed simple policy through reinforcement learning\n[7] or imitation learning [1, 25]. A lot of work [46, 49] have utilized Large Language Models\n(LLMs) as action planners for agents, generating executable sub-goal sequences for low-level action\ncontrollers. Further, recent studies [51, 32] employed Multimodal Large Language Models (MLLMs)\n[4, 38, 55] as planner and reflector. Leveraging the powerful instruction-following and logical\nreasoning capabilities of (Multimodal) LLMs [24], LLM-based agents have achieved remarkable\nsuccess across multiple domains [14, 9, 10, 54]. Nevertheless, the ability of these agents to complete\nlong-horizon tasks still falls significantly short of human-level performance.\nAccording to relevant studies [27, 41, 45], the human ability to complete long-horizon tasks in an\nopen world relies on long-term memory storage, which is divided into knowledge and experience. The\nstorage and utilization of knowledge and experience play a crucial role in guiding human behavior\nand enabling humans to adapt flexibly to their environments in order to accomplish long-horizon\ntasks. Inspired by this theory, we summarize the challenges faced by current agents as follows:\nInsufficient Exploration of Structured Knowledge: Structured knowledge, encompassing open\nworld rules, object relationships, and interaction methods with the environment, is essential for agents\nto complete complex tasks [33, 43]. However, MLLMs such as GPT-4V * lack sufficient knowledge\nin Minecraft. Existing agents [1, 25, 7] only learn dispersed knowledge from video data and are\nunable to efficiently represent and learn this structured knowledge, rendering them incapable of\nperforming complex tasks.\nLack of Multimodal Experience: Humans derive successful strategies and lessons from information\non historical experience [8, 31], which assists them in tackling current complex tasks. In a similar\nmanner, agents can benefit from in-context learning with experience demonstrations [42, 53]. How-\never, existing agents [46, 50, 32] only consider unimodal information, which prevents them from\nlearning from multimodal experience as humans do.\nTo address the aforementioned challenges, we propose Hybrid Multimodal Memory module\nthat consists of Hierarchical Directed Knowledge Graph (HDKG) and Abstracted Multimodal\nExperience Pool (AMEP). For HDKG, we map the logical relationships between objects into a\ndirected graph structure, thereby transforming knowledge into high-level semantic representations.\nHDKG efficiently provides the agent with the necessary knowledge for task execution, without\nrequiring any parameter updates. For AMEP, we dynamically summarize and store the multimodal\ninformation (e.g., environment, agent state, task plan, video frames, etc.) from the agent’s task\nexecution process, ensuring that historical information contains both a global overview and local\ndetails. Different from the method of directly storing successful cases as experience [51], AMEP\nconsiders both successful and failed cases as references. This innovative approach of incorporating\nfailure cases into in-context learning significantly enhances the performance of the agent.\n*https:\/\/openai.com\/index\/gpt-4v-system-card\/\n2\nOn top of the Hybrid Multimodal Memory module, we construct a multimodal composable agent,\nOptimus-1. As shown in Figure 1, Optimus-1 consists of Knowledge-Guided Planner, Experience-\nDriven Reflector, and Action Controller. To enhance the ability of agents to cope with complex\nenvironments and long-horizon tasks, Knowledge-Guided Planner incorporates visual observation\ninto the planning phase, leveraging HDKG to capture the knowledge needed. This allows the agent to\nefficiently transform tasks into executable sub-goals. Action Controller takes the sub-goal and the\ncurrent observation as inputs and generates low-level actions, interacting with the game environment\nto update the agent’s state. In open-world complex environments, agents are prone to be erroneous\nwhen performing long-horizon tasks. To address this, we propose Experience-Driven Reflector, which\nis periodically activated to retrieve relevant multimodal experiences from AMEP. This encourages\nthe agent to reflect on its current actions and refine the plan.\nWe validate the performance of Optimus-1 in Minecraft, a popular open-world game environment.\nExperimental results show that Optimus-1 exhibits remarkable performance on long-horizon tasks,\nrepresenting up to 30% improvement over existing agents. Moreover, we introduce various Multi-\nmodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 has a 2 to 6 times performance improvement with the help of Hybrid Multimodal\nMemory, outperforming powerful GPT-4V baseline on lots of long-horizon tasks. Additionally, we\nverified that the plug-and-play Hybrid Multimodal Memory can drive Optimus-1 to incrementally\nimprove its performance in a self-evolution manner. The extensive experimental results show that\nOptimus-1 makes a major step toward a general agent with a human-like level of performance. Main\ncontributions of our paper:\n• We propose Hybrid Multimodal Memory module which is composed of HDKG and AMEP.\nHDKG helps the agent make the planning of long-horizon tasks efficiently. AMEP provides refined\nhistorical experience and guides the agent to reason about the current situation state effectively.\n• On top of the Hybrid Multimodal Memory module, we construct Optimus-1, which consists of\nKnowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Optimus-1\noutperforms all baseline agents on long-horizon task benchmarks, and exhibits capabilities close to\nthe level of human players.\n• Driven by Hybrid Multimodal Memory, various MLLM-based Optimus-1 have demonstrated 2 to 6\ntimes performance improvement, demonstrating the generalization of Hybrid Multimodal Memory.\n2\nOptimus-1\nIn this section, we first elaborate on how to implement the Hybrid Multimodal Memory in Sec 2.1.\nAs a core innovation, it plays a crucial role in enabling Optimus-1 to execute long-horizon tasks.\nNext, we give an overview of Optimus-1 framework (Sec 2.2), which consists of Hybrid Multimodal\nMemory, Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Finally,\nwe introduce a non-parametric learning approach to expand the hybrid multimodal memory (Sec 2.3),\nthereby enhancing the success rate of task execution for Optimus-1.\n2.1\nHybrid Multimodal Memory\nIn order to endow agent with a long-term memory storage mechanism [27, 45], we propose the\nHybrid Multimodal Memory module, which consists of Abstracted Multimodal Experience Pool\n(AMEP) and Hierarchical Directed Knowledge Graph (HDKG).\n2.1.1\nAbstracted Multimodal Experience Pool\nRelevant studies [23, 28, 17, 15] highlight the importance of historical information for agents\ncompleting long-horizon tasks. Minedojo [7] and Voyager [46] employed unimodal storage of\nhistorical information. Jarvis-1 [51] used a multimodal experience mechanism that stores task\nplanning and visual information without summarization, posing challenges to storage capacity and\nretrieval speed. To address this issue, we propose AMEP, which aims to dynamically summarize all\nmultimodal information during task execution. It preserves the integrity of long-horizon data while\nenhancing storage and retrieval efficiency.\n3\nAbstracted Multimodal Experience Pool\nFilter\nHierarchical Directed Knowledge Graph\n(b)\nVideo \n...\nVideo Buffer\n...\n...\nImage Buffer\n0.3\n0.7\n0.6\n0.9\n0.8\nCraft stone \npickaxe\nO1\nO2\nO3\nO4\nupdate\nKnowledge\nvisual\ninfo\nTask: Craft stone pickaxe.\nEnv: forest\nPlan: \n(a)\nObs:                                                            ... \nVisual Info: health: full, food: full,hotbar: empty\nFigure 2: (a) Extraction process of multimodal experience. The frames are filtered through video\nbuffer and image buffer, then MineCLIP [7] is employed to compute the visual and sub-goal sim-\nilarities and finally they are stored in Abstracted Multimodal Experience Pool. (b) Overview of\nHierarchical Directed Knowledge Graph. Knowledge is stored as a directed graph, where its nodes\nrepresent objects, and directed edges point to materials that can be crafted by this object.\nSpecifically, as depicted in Figure 2, to conduct the static visual information abstraction, the video\nstream captured by Optimus-1 during task execution is first input to a video buffer, filtering the\nstream at a fixed frequency of 1 frame per second. Based on the filtered video frames, to further\nperform a dynamic visual information abstraction, these frames are then fed into an image buffer with\na window size of 16, where the image similarity is dynamically computed and final abstracted frames\nare adaptively updated. To align such abstracted visual information with the corresponding textual\nsub-goal, we then utilize MineCLIP [7], a pre-trained video-text alignment model, to calculate their\nmultimodal correlation. When this correlation exceeds a threshold, the corresponding image buffer\nand textual sub-goal are saved as multimodal experience into a pool. Finally, we further incorporate\nenvironment information, agent initial state, and plan generated by Knowledge-Guided Planner, into\nsuch a pool, which forms the AMEP. In this way, we consider the multimodal information of each\nsub-goal, and summarise it to finally compose the multimodal experience of the given task.\n2.1.2\nHierarchical Directed Knowledge Graph\nIn Minecraft, mining and crafting represent a complex knowledge network crucial for effective task\nplanning. For instance, crafting a diamond sword\nrequires two diamonds\nand one wooden stick\n, while mining diamonds requires an iron pickaxe\n, which involving further materials and steps.\nSuch knowledge is essential for an agent’s ability to perform long-horizon complex tasks. Instead of\nimplicit learning through fine-tuning [32, 58], we propose HDKG, which transforms knowledge into\na graph representation. It enables the agent to perform explicit learning by retrieving information\nfrom the knowledge graph.\nAs shown in the Figure 2, we transform knowledge into a graph D(V, E), where nodes set V represent\nobjects, and directed edges set E point to nodes that can be crafted by this object. An edge e ∈E in\nthe D can be represented as e = (u, v), where u, v ∈V. The directed graph efficiently stores and\nupdates knowledge. For a given object x, retrieving the corresponding node allows extraction of a\nsub-graph Dj(Vj, Ej) ∈D, where nodes set Vj and edges set Ej can be formulated as:\nVj = {v ∈V | x} ,\nEj = {e = (u, v) ∈V | u ∈Vj ∪v ∈Vj} ,\n(1)\nThen by topological sorting, we can get all the materials and their relationships needed to complete\nthe task. This knowledge is provided to the Knowledge-Guided Planner as a way to generate a more\n4\nHybrid Multimodal Memory\nAbstracted Multimodal Experience Pool\nHierarchical Directed Knowledge Graph\nEnvironment\n9\n27\n1\n7\n1\n1\n1\n1\n3\n11\n3\n1\nKnowledge-Guided \nPlanner\nSub-Goals\nExperience-Driven \nReflector\n    \n \n   \n   \n   \nReflection\n [replan]\n [continue]\n [complete]\nTask: Craft diamond.\nAction Controller\n...\nTask: Mine cobblestone.\nEnv: forest\nVisual Info: health: full, food: full,hotbar: empty.\nKnowledge：\nSub-Goal: Mine 11 cobblestone.\nCurrent Obs:                                            \nReplan: \nContinue: \nComplete: \nIn-context Cases\nFigure 3: Overview framework of our Optimus-1. Optimus-1 consists of Knowledge-Guided Planner,\nExperience-Driven Reflector, Action Controller, and Hybrid Multimodal Memory architecture. Given\nthe task “craft stone sword”, Optimus-1 incorporates the knowledge from HDKG into Knowledge-\nGuided Planning, then Action Controller generates low-level actions. Experience-Driven Reflector is\nperiodically activated to introduce multimodal experience from AMEP to determine if the current\ntask can be executed successfully. If not, it will ask the Knowledge-Guided Planner to refine the plan.\nreasonable sequence of sub-goals. With HDKG, we can significantly enhance the world knowledge\nof the agent in a train-free manner.\n2.2\nOptimus-1: Framework\nRelevant studies indicate that the human brain is essential for planning and reflection, while the\ncerebellum controls low-level actions, both crucial for complex tasks [39, 40]. Inspired by this, we\ndivide the structure of Optimus-1 into Knowledge-Guided Planner, Experience-Driven Reflector, and\nAction Controller. In a given game environment with a long-horizon task, the Knowledge-Guided\nPlanner senses the environment, retrieves knowledge from HDKG, and decomposes the task into\nexecutable sub-goals. The action controller then sequentially executes these sub-goals. During\nexecution, the Experience-Driven Reflector is activated periodically, leveraging historical experience\nfrom AMEP to assess whether Optimus-1 can complete the current sub-goal. If not, it instructs the\nKnowledge-Guided Planner to revise its plan. Through iterative interaction with the environment,\nOptimus-1 ultimately completes the task.\nKnowledge-Guided Planner. Open-world environments vary greatly, affecting task execution.\nPrevious approaches [50] using LLMs for task planning failed to consider the environment, leading\nto the failure of tasks. For example, an agent in a cave aims to catch fish. It lacks visual information\nto plan conditions on the current situation, such as “leave the cave and find a river”. Therefore, we\nintegrate environmental information into the planning stage. Unlike Jarvis-1 [51] and MP5 [32],\nwhich convert observation to textual descriptions, Optimus-1 directly employs observation as visual\nconditions to generate environment-related plans, i.e., sub-goal sequences. This results in more\ncomprehensive and reasonable planning. More importantly, Knowledge-Guided Planner retrieves\nthe knowledge needed to complete the task from HDKG, allowing task planning to be done once,\nrather than generating the next step in each iteration. Given the task t, observation o, the sub-goals\n5\nsequence g1, g2, g3, ..., gn can be formulated as:\ng1, g2, g3, ..., gn = pθ(o, t, pη(t)),\n(2)\nwhere n is the number of sub-goals, pη denotes sub-graph retrieved from HDKG, pθ denotes MLLM.\nIn this paper, we employ OpenAI’s GPT-4V as Knowledge-Guided Planner and Experience-Driven\nReflector. We also evaluate other alternatives of GPT-4V, such as open-source models like Deepseek-\nVL [26] and InternLM-XComposer2-VL [6] in Section 3.4.\nAction Controller. It takes the sub-goal and the current observation as inputs and then generates\nlow-level actions, which are control signals for the mouse and keyboard. Thus, it can interact with\nthe game environment to update the agent’s state and the observation. The formulation is as follows:\nak = pπ(o, gi),\n(3)\nwhere ak denotes low-level action at time k, pπ denotes action controller. Unlike generating code\n[46, 32, 49], generating control actions for the mouse and keyboard [1, 25, 51, 3] more closely\nresembles human behavior. In this paper, we employ STEVE-1 [25] as our Action Controller.\nExperience-Driven Reflector. The sub-goals generated by Knowledge-Guided Planner are interde-\npendent. The failure of any sub-goal halts the execution of subsequent ones, leading to overall task\nfailure. Therefore, a reflection module is essential to identify and rectify errors promptly. During\ntask execution, the Experience-Driven Reflector activates at regular intervals, retrieving historical\nexperience from AMEP, and then analyzing the current state of Optimus-1. The reflection results\nof Optimus-1 are categorized as COMPLETE, CONTINUE, or REPLAN. COMPLETE indicates successful\nexecution, prompting the action controller to proceed to the next sub-goal. CONTINUE signifies\nongoing execution without additional feedback. REPLAN denotes failure, requiring the Knowledge-\nGuided Planner to revise the plan. The reflection r generated by Experience-Driven Reflector can be\nformulated as:\nr = pθ(o, gi, pϵ(t)),\n(4)\nwhere pϵ denotes multimodal experience retrieved from AMEP. Experimental results in Section\n3.3 demonstrate that the Experience-Driven Reflector significantly enhances the success rate of\nlong-horizon tasks.\nDuring task execution, even in cases where task failure necessitates REPLAN, multimodal experiences\nare stored in AMEP. Thus, during the reflection phase, Optimus-1 can retrieve the most relevant\ncases from each of the three scenarios COMPLETE, CONTINUE, and REPLAN from AMEP as references.\nExperimental Results in Section 3.3 demonstrate the effectiveness of this innovative method of\nincorporating failure cases into in-context learning.\n2.3\nNon-parametric Learning of Hybrid Multimodal Memory\nTo implement the Hybrid Multimodal Memory and enhance Optimus-1’s capacity, we propose a non-\nparametric learning method named “free exploration-teacher guidance”. In the free exploration phase,\nOptimus-1’s equipment and tasks are randomly initialized, and it explores random environments,\nacquiring world knowledge through environmental feedback. For example, it learns that “a stone\nsword\ncan be crafted with a wooden stick\nand two cobblestones\n”, storing this in the HDKG.\nAdditionally, successful and failed cases are stored in the AMEP, providing reference experience for\nthe reflection phase. We initialize multiple Optimus-1, and they share the same HDKG and AMEP.\nThus the memory is filled up efficiently. After free exploration, Optimus-1 has basic world knowledge\nand multimodal experience. In the teacher guidance phase, Optimus-1 needs to learn a small number\nof long-horizon tasks based on extra knowledge. For example, it learns “a diamond sword\nis\nobtained by a stick\nand two diamonds\n” from the teacher, then perform the task “craft diamond\nsword”. During the teacher guidance phase, Optimus-1’s memory is further expanded and it gains the\nexperience of executing complete long-horizon tasks.\nUnlike fine-tuning, this method enhances Optimus-1 incrementally without updating parameters, in\na self-evolution manner. Starting with an empty Hybrid Multimodal Memory, Optimus-1 iterates\nbetween “free exploration-teacher guidance” learning and unseen task inference. With each iteration,\nits memory capacity grows, enabling mastery of tasks from easy to hard.\n6\nTable 1: Main Result of Optimus-1 on long-horizon tasks benchmark. We report the average success\nrate (SR), average number of steps (AS), and average time (AT) on each task group, the results of each\ntask can be found in the Appendix F. Lower AS and AT metrics mean that the agent is more efficient\nat completing the task, while +∞indicates that the agent is unable to complete the task. Overall\nrepresents the average result on the five groups of Iron, Gold, Diamond, Redstone, and Armor.\nGroup\nMetric\nGPT-3.5\nGPT-4V\nDEPS\nJarvis-1\nOptimus-1\nHuman-level\nWood\nSR ↑\n40.16\n41.42\n77.01\n93.76\n98.60\n100.00\nAT ↓\n56.39\n55.15\n85.53\n67.76\n47.09\n31.08\nAS ↓\n1127.78\n1103.04\n1710.61\n1355.25\n841.94\n621.59\nStone\nSR ↑\n20.40\n20.89\n48.52\n89.20\n92.35\n100.00\nAT ↓\n135.71\n132.77\n138.71\n141.50\n129.94\n80.85\nAS ↓\n2714.21\n2655.47\n2574.30\n2830.05\n2518.88\n1617.00\nIron\nSR ↑\n0.00\n0.00\n16.37\n36.15\n46.69\n86.00\nAT ↓\n+∞\n+∞\n944.61\n722.78\n651.33\n434.38\nAS ↓\n+∞\n+∞\n8892.24\n8455.51\n6017.85\n5687.60\nGold\nSR ↑\n0.00\n0.00\n0.00\n7.20\n8.51\n17.31\nAT ↓\n+∞\n+∞\n+∞\n787.37\n726.35\n557.08\nAS ↓\n+∞\n+∞\n+∞\n15747.13\n15527.07\n13141.60\nDiamond\nSR ↑\n0.00\n0.00\n0.60\n8.98\n11.61\n16.98\nAT ↓\n+∞\n+∞\n1296.96\n1255.06\n1150.98\n744.82\nAS ↓\n+∞\n+∞\n23939.30\n25101.25\n23019.64\n16237.54\nRedstone\nSR ↑\n0.00\n0.00\n0.00\n16.31\n25.02\n33.27\nAT ↓\n+∞\n+∞\n+∞\n1070.42\n932.50\n617.89\nAS ↓\n+∞\n+∞\n+∞\n17408.40\n12709.99\n12357.00\nArmor\nSR ↑\n0.00\n0.00\n9.98\n15.82\n19.47\n28.48\nAT ↓\n+∞\n+∞\n997.59\n924.60\n824.53\n551.30\nAS ↓\n+∞\n+∞\n17951.95\n16492.96\n16350.56\n11026.00\nOverall\nSR ↑\n0.00\n0.00\n5.39\n16.89\n22.26\n36.41\n3\nExperiments\n3.1\nExperiments Setting\nEnvironment. To ensure realistic gameplay like human players, we employ MineRL [11] with\nMinecraft 1.16.5 as our simulation environment. The agent operates at a fixed speed of 20 frames per\nsecond and only interacts with the environment via low-level action control signals of the mouse and\nkeyboard. For more information about the detailed descriptions of the observation and action spaces,\nplease refer to the Appendix B.\nBenchmark. We constructed a benchmark of 67 tasks to evaluate the Optimus-1’s ability to complete\nlong-horizon tasks. As illustrated in Table 5, we divide the 67 Minecraft tasks into 7 groups according\nto recommended categories in Minecraft. Please refer to Appendix D for more details.\nBaseline. We compare Optimus-1 with various agents, including GPT-3.5 2, GPT-4V, DEPS [50],\nand Jarvis-1 [51] on the challenging long-horizon tasks benchmark. In addition, we employed 10\nvolunteers to perform the same task on the benchmark, and their average performance served as a\nhuman-level baseline. Please refer to Appendix D.2 for more details about human-level baseline. For\na more comprehensive comparison, we also report Optimus-1’s performances on the benchmark used\nby Voyager [46], MP5 [32], and DEPS [50] in the Appendix F.2. Note that we initialize Optimus-1\nwith an empty inventory, while DEPS [50] and Jarvis-1 [51] have tools in their initial state. This\nmakes it more challenging for Optimus-1 to perform the same tasks.\n2https:\/\/openai.com\/research\/gpt-3.5\n7\nTable 2: Ablation study results. We report average\nsuccess rate (SR) on each task group. P., R., K.,\nE. represent Planning, Reflection, Knowledge, and\nExperience, respectively.\nAblation Setting\nTask Group\nP.\nR.\nK.\nE.\nWood\nStone\nIron\nGold\nDiamond\n14.29\n0.00\n0.00\n0.00\n0.00\n!\n42.95\n25.67\n0.00\n0.00\n0.00\n!\n!\n55.00\n47.37\n18.11\n2.08\n1.11\n!\n!\n!\n73.53\n64.20\n24.19\n3.08\n1.86\n!\n!\n!\n92.37\n69.63\n38.33\n3.49\n2.42\n!\n!\n!\n!\n97.49\n94.26\n53.33\n11.54\n9.59\nTable 3: Ablation study on AMEP. We report\nthe average success rate (SR) on each task group.\nZero, Suc., and Fail. represent retrieving from\nAMEP without getting the case, getting the success\ncase, and getting the failure case, respectively.\nAblation Setting\nTask Group\nZero\nSuc.\nFai.\nWood\nStone\nIron\nGold\nDiamond\n!\n92.00\n79.26\n36.32\n4.25\n3.25\n!\n95.00\n84.29\n46.98\n9.36\n7.89\n!\n95.00\n81.10\n45.47\n7.50\n6.39\n!\n!\n97.49\n94.26\n53.33\n11.54\n9.59\nAbstracted\nMultiModal\nExperience Pool\nfall into \nwater\ndrop in \ncave\nCurrent Goal: chop a tree\nCurrent Goal: go fishing\nAbstracted\nMultiModal\nExperience Pool\n...\nOurs\nSTEVE-1\nSTEVE-1\nOurs\nFigure 4: Illustration of the role of reflection mechanism. Without the help of reflective mechanisms,\nSTEVE-1 [25] often gets into trouble and fails to complete the task. While Optimus-1, with the\nhelp of the Experience-Driven Reflector, leverages the AMEP to retrieve relevant experience, reflect\ncurrent situation and correct errors. This improves Optimus-1’s success rate on long-horizon tasks.\nEvaluation Metrics. The agent always starts in survival mode, with an empty inventory. We\nconducted at least 30 times for each task using different world seeds and reported the average success\nrate to ensure fair and thorough evaluation. Additionally, we add the average steps and average time\nof completing the task as evaluation metrics.\n3.2\nExperimental Results\nThe overall experimental results on benchmark are shown in Table 1, see the accuracy for each task in\nAppendix F. Optimus-1 has a success rate near 100% on the Wood Group\n. Compared with Jarvis-1,\nOptimus-1 has 29.28% and 53.40% improvement on the Diamond Group\nand Redstone Group\n,\nrespectively. Optimus-1 achieves the best performance and the shortest elapsed time among all task\ngroups. It reveals the effectiveness and efficiency of our proposed Optimus-1 framework. Moreover,\ncompared with all baselines, Optimus-1 performance was closer (average 5.37% improvement) to\nhuman levels on long-horizon task groups.\n3.3\nAblation Study\nWe conduct extensive ablation experiments on 18 tasks, experiment setting can be found in Table 6.\nAs shown in Table 2, we first remove Knowledge-Driven Planner and Experience-Driven Reflector,\nthe performance of Optimus-1 on all task groups drops dramatically. It demonstrates the necessity of\nKnowledge-Guided Planner and Experience-Driven Reflector modules for performing long-horizon\ntasks. As for Hybrid Multimodal Memory, we remove HDKG from Optimus-1. Without the help\nof world knowledge, the performance of Optimus-1 decreased by an average of 20% across all task\ngroups. We then removed AMEP, this resulted in the performance of Optimus-1 decreased by an\n8\nSuccess Rate\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nWood Group\nStone Group\nIron Group\nGold Group\nDiamond Group\n97.5 92.4 92.6\n94.3\n79.2\n73.0\n53.3\n43.8\n48.8\n6.1\n4.0\n5.9\n9.6\n3.3 3.3\nGPT-4V w\/ Memory\nGPT-4V w\/o Memory\nDeepseek-VL w\/ Memory\nDeepseek-VL w\/o Memory\nXComposer2-VL w\/ Memory\nXComposer2-VL w\/o Memory\n(a) Generalisation of Hybrid Multimodal Memory\nSuccess Rate\nWood \nGroup\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nEpoch 1\nEpoch 2\nEpoch 3\nEpoch 4\nStone \nGroup\nIron \nGroup\nGold \nGroup\nDiamond \nGroup\n92.50\n83.07\n43.74\n5.75\n4.66\n(b) Self-Evolution\nFigure 5: (a) With the help of Hybrid Multimodal Memory, various MLLM-based Optimus-1 have\ndemonstrated 2 to 6 times performance improvement. (b) Illustration of the change in Optimus-1\nsuccess rate on the unseen task over 4 epochs.\naverage of 12%. Finally, we performed ablation experiments on the way of retrieving cases from\nAMEP. As shown in Table 3, without retrieving cases from AMEP, the success rate shows an average\nof 10% decrease across all groups. It reveals that this reflection mechanism, which considers both\nsuccess and failure cases, has a significant impact on the performance of Optimus-1. To illustrate the\nrole of the reflection mechanism, we have shown some cases in Figure 4.\n3.4\nGeneralization Ability\nIn this section, we explore an interesting issue: whether generic MLLMs can effectively perform\nvarious long-horizon complex tasks in Minecraft using Hybrid Multimodal Memory. As shown in\nFigure 5, We employ Deepseek-VL [26] and InternLM-XComposer2-VL [6] as Knowledge-Guided\nPlanner and Experience-Driven Reflector. The experimental results show that the original MLLM has\nlow performance on long-horizon tasks due to the lack of knowledge and experience of Minecraft.\nWith the assistance of Hybrid Multimodal Memory, the performance of MLLMs has improved by 2\nto 6 times across various task groups, outperforming the GPT-4V baseline. This encouraging result\ndemonstrates the generalization of the proposed Hybrid Multimodal Memory.\n3.5\nSelf-Evolution via Hybrid Multimodal Memory\nAs shown in Section 2.3, we randomly initialize the Hybrid Multimodal Memory of Optimus-1, then\nupdate it multiple times by using the “free exploration-teacher guidance” learning method. We set\nthe epoch to 4, and the number of learning tasks to 160. At each period, Optimus-1 performs free\nexploration on 150 tasks and teacher guidance learning on the remaining 10 tasks, we then evaluate\nOptimus-1’s learning ability on the task groups same as ablation study. Experimental results are\nshown in Figure 5. It reveals that Optimus-1 keeps getting stronger through the continuous expansion\nof memory during the learning process of multiple periods. Moreover, it demonstrates that MLLM\nwith Hybrid Multimodal Memory can incarnate an expert agent in a self-evolution manner [44].\n4\nRelated Work\n4.1\nAgents in Minecraft\nWe summarise the differences of existing Minecraft agents in the Appendix D.3. Earlier work\n[29, 56, 2, 3] introduced policy models for agents to perform simple tasks in Minecraft. MineCLIP\n[7] used text-video data to train a contrastive video-language model as a reward model for policy,\nwhile VPT [1] pre-trained on unlabelled videos but lacked instruction as input. Building on VPT\nand MineCLIP, STEVE-1 [25] added text input to generate low-level action sequences from human\ninstructions and images. However, these agents struggle with complex tasks due to limitations in\ninstruction comprehension and planning. Recent work [49, 46, 59] incorporated LLMs as planning\nand reflection modules, but lacked visual information integration for adaptive planning. MP5\n9\n[32], MineDreamer [58], and Jarvis-1 [51] enhanced situation-aware planning by obtaining textual\ndescriptions of visual information, yet lacked detailed visual data. Optimus-1 addresses these issues\nby directly using observation as situation-aware conditions in the planning phase, enabling more\nrational, visually informed planning. Additionally, unlike other agents requiring multiple queries\nfor task refinement, Optimus-1 generates a complete and effective plan in one step with the help of\nHDKG. This makes Optimus-1 planning more efficient.\n4.2\nMemory in Agents\nIn the agent-environment interaction process, memory is key to achieving experience accumulation\n[21], environment exploration [16], and knowledge abstraction [57]. There are two forms to represent\nmemory content in LLM-based agents: textual form [17, 15, 30] and parametric form [5, 28, 47, 20].\nIn textual form, the information is explicitly retained and recalled by natural languages. In parametric\nform, the memory information [37] is encoded into parameters and implicitly influences the agent’s\nactions. Recent work [48, 52, 12] has explored the long-term visual information storage [18, 19] and\nsummarisation in MLLM. Our proposed hybrid multimodal memory module is plug-and-play and\ncan provide world knowledge and multimodal experience for Optimus-1 efficiently.\n5\nConclusion\nIn this paper, we propose Hybrid Multimodal Memory module, which consists of two parts: HDKG\nand AMEP. HDKG provides the necessary world knowledge for the planning phase of the agent,\nand AMEP provides the refined historical experience for the reflection phase of the agent. On top\nof the Hybrid Multimodal Memory, we construct the multimodal composable agent, Optimus-1,\nin Minecraft. Extensive experimental results show that Optimus-1 outperforms all existing agents\non long-horizon tasks. Moreover, we validate that general-purpose MLLMs, based on Hybrid\nMultimodal Memory and without additional parameter updates, can exceed the powerful GPT-4V\nbaseline. The extensive experimental results show that Optimus-1 makes a major step toward a\ngeneral agent with a human-like level of performance.\n6\nLimitation and Future Work\nIn the framework of Optimus-1, we are dedicated to leverage proposed Hierarchical Directed Knowl-\nedge Graph and Abstracted Multimodal Experience Pool can be used to enhance the agent’s ability\nto plan and reflect. For Action Controller, we directly introduce STEVE-1 [25] as a generator of\nlow-level actions. However, limited by STEVE-1’s ability to follow instructions and execute complex\nactions, Optimus-1 is weak in completing challenging tasks such as “beat ender dragon” and “build a\nhouse”. Therefore, a potential future research direction is to enhance the instruction following and\naction generation capabilities of action controller.\nIn addition, most of the work, including Optimus-1, utilize a multimodal large language model for\nplanning and reflection, which then drives an action controller to perform the task. Building an\nend-to-end vision-language-action agent will be future work.\n7\nAcknowledgement\nThis study is supported by National Natural Science Foundation of China (Grant No. 62236003\nand 62306090), Shenzhen College Stability Support Plan (Grant No. GXWD20220817144428005),\nNatural Science Foundation of Guangdong Province of China (Grant No. 2024A1515010147), and\nMajor Key Project of Peng Cheng Laboratory (Grant No. PCL2023A08).\n10","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nOptimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks\n```\n#### 2. 论文摘要\n```\nBuilding a general-purpose agent is a long-standing vision in the field of\nartificial intelligence. Existing agents have made remarkable progress in many\ndomains, yet they still struggle to complete long-horizon tasks in an open\nworld. We attribute this to the lack of necessary world knowledge and\nmultimodal experience that can guide agents through a variety of long-horizon\ntasks. In this paper, we propose a Hybrid Multimodal Memory module to address\nthe above challenges. It 1) transforms knowledge into Hierarchical Directed\nKnowledge Graph that allows agents to explicitly represent and learn world\nknowledge, and 2) summarises historical information into Abstracted Multimodal\nExperience Pool that provide agents with rich references for in-context\nlearning. On top of the Hybrid Multimodal Memory module, a multimodal agent,\nOptimus-1, is constructed with dedicated Knowledge-guided Planner and\nExperience-Driven Reflector, contributing to a better planning and reflection\nin the face of long-horizon tasks in Minecraft. Extensive experimental results\nshow that Optimus-1 significantly outperforms all existing agents on\nchallenging long-horizon task benchmarks, and exhibits near human-level\nperformance on many tasks. In addition, we introduce various Multimodal Large\nLanguage Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 exhibits strong generalization with the help of the Hybrid\nMultimodal Memory module, outperforming the GPT-4V baseline on many tasks.\n```\n\n#### 3. 论文全文\n```\nOptimus-1\n: Hybrid Multimodal Memory\nEmpowered Agents Excel in Long-Horizon Tasks\nZaijing Li1 2, Yuquan Xie1, Rui Shao1∗, Gongwei Chen1, Dongmei Jiang2, Liqiang Nie1∗\n1Harbin Institute of Technology, Shenzhen\n2Peng Cheng Laboratory\n{lzj14011,xieyuquan20016,rshaojimmy,nieliqiang}@gmail.com\nExperience-Driven Reflector\n \nAction Controller\n \nTask: Craft stone sword.\nHybrid Multimodal \nMemory\nHierarchical Directed \nKnowledge Graph\nAbstracted Multimodal \nExperience Pool\n                              \nKnowledge-Guided Planner\nMission Success!!!\nTask: Craft stone pickaxe.\nEnv: forest\nVisual Info: health: full...\nObs:\nRetrieve: Memory|Knowledge\nRetrieve: Memory|Experience\nReflection: Mine stone.\nGoal: Mine stone.\nFigure 1: An illustration of Optimus-1 performing long-horizon tasks in Minecraft. Given the task\n“Craft stone sword”, Knowledge-Guided Planner incorporates knowledge from Hierarchical Directed\nKnowledge Graph into planning, then Action Controller executes these planning sequences step-by-\nstep. During the execution of the task, the Experience-Driven Reflector is periodically activated and\nretrieve experience from Abstracted Multimodal Experience Pool to make reflection.\nAbstract\nBuilding a general-purpose agent is a long-standing vision in the field of artificial\nintelligence. Existing agents have made remarkable progress in many domains, yet\nthey still struggle to complete long-horizon tasks in an open world. We attribute\nthis to the lack of necessary world knowledge and multimodal experience that can\nguide agents through a variety of long-horizon tasks. In this paper, we propose\na Hybrid Multimodal Memory module to address the above challenges. It 1)\ntransforms knowledge into Hierarchical Directed Knowledge Graph that allows\nagents to explicitly represent and learn world knowledge, and 2) summarises histor-\nical information into Abstracted Multimodal Experience Pool that provide agents\n*Corresponding authors\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2408.03615v2  [cs.AI]  21 Oct 2024\nwith rich references for in-context learning. On top of the Hybrid Multimodal\nMemory module, a multimodal agent, Optimus-1, is constructed with dedicated\nKnowledge-guided Planner and Experience-Driven Reflector, contributing to a\nbetter planning and reflection in the face of long-horizon tasks in Minecraft. Exten-\nsive experimental results show that Optimus-1 significantly outperforms all existing\nagents on challenging long-horizon task benchmarks, and exhibits near human-\nlevel performance on many tasks. In addition, we introduce various Multimodal\nLarge Language Models (MLLMs) as the backbone of Optimus-1. Experimental\nresults show that Optimus-1 exhibits strong generalization with the help of the Hy-\nbrid Multimodal Memory module, outperforming the GPT-4V baseline on various\ntasks. Please see the project page at https:\/\/cybertronagent.github.io\/Optimus-\n1.github.io\/.\n1\nIntroduction\nOptimus Prime faces complex tasks alongside humans in Transformers to protect the peace of\nthe planet. Creating an agent [43, 13] like Optimus that can perceive, plan, reflect, and complete\nlong-horizon tasks in an open world has been a longstanding aspiration in the field of artificial\nintelligence [22, 35, 36]. Early research developed simple policy through reinforcement learning\n[7] or imitation learning [1, 25]. A lot of work [46, 49] have utilized Large Language Models\n(LLMs) as action planners for agents, generating executable sub-goal sequences for low-level action\ncontrollers. Further, recent studies [51, 32] employed Multimodal Large Language Models (MLLMs)\n[4, 38, 55] as planner and reflector. Leveraging the powerful instruction-following and logical\nreasoning capabilities of (Multimodal) LLMs [24], LLM-based agents have achieved remarkable\nsuccess across multiple domains [14, 9, 10, 54]. Nevertheless, the ability of these agents to complete\nlong-horizon tasks still falls significantly short of human-level performance.\nAccording to relevant studies [27, 41, 45], the human ability to complete long-horizon tasks in an\nopen world relies on long-term memory storage, which is divided into knowledge and experience. The\nstorage and utilization of knowledge and experience play a crucial role in guiding human behavior\nand enabling humans to adapt flexibly to their environments in order to accomplish long-horizon\ntasks. Inspired by this theory, we summarize the challenges faced by current agents as follows:\nInsufficient Exploration of Structured Knowledge: Structured knowledge, encompassing open\nworld rules, object relationships, and interaction methods with the environment, is essential for agents\nto complete complex tasks [33, 43]. However, MLLMs such as GPT-4V * lack sufficient knowledge\nin Minecraft. Existing agents [1, 25, 7] only learn dispersed knowledge from video data and are\nunable to efficiently represent and learn this structured knowledge, rendering them incapable of\nperforming complex tasks.\nLack of Multimodal Experience: Humans derive successful strategies and lessons from information\non historical experience [8, 31], which assists them in tackling current complex tasks. In a similar\nmanner, agents can benefit from in-context learning with experience demonstrations [42, 53]. How-\never, existing agents [46, 50, 32] only consider unimodal information, which prevents them from\nlearning from multimodal experience as humans do.\nTo address the aforementioned challenges, we propose Hybrid Multimodal Memory module\nthat consists of Hierarchical Directed Knowledge Graph (HDKG) and Abstracted Multimodal\nExperience Pool (AMEP). For HDKG, we map the logical relationships between objects into a\ndirected graph structure, thereby transforming knowledge into high-level semantic representations.\nHDKG efficiently provides the agent with the necessary knowledge for task execution, without\nrequiring any parameter updates. For AMEP, we dynamically summarize and store the multimodal\ninformation (e.g., environment, agent state, task plan, video frames, etc.) from the agent’s task\nexecution process, ensuring that historical information contains both a global overview and local\ndetails. Different from the method of directly storing successful cases as experience [51], AMEP\nconsiders both successful and failed cases as references. This innovative approach of incorporating\nfailure cases into in-context learning significantly enhances the performance of the agent.\n*https:\/\/openai.com\/index\/gpt-4v-system-card\/\n2\nOn top of the Hybrid Multimodal Memory module, we construct a multimodal composable agent,\nOptimus-1. As shown in Figure 1, Optimus-1 consists of Knowledge-Guided Planner, Experience-\nDriven Reflector, and Action Controller. To enhance the ability of agents to cope with complex\nenvironments and long-horizon tasks, Knowledge-Guided Planner incorporates visual observation\ninto the planning phase, leveraging HDKG to capture the knowledge needed. This allows the agent to\nefficiently transform tasks into executable sub-goals. Action Controller takes the sub-goal and the\ncurrent observation as inputs and generates low-level actions, interacting with the game environment\nto update the agent’s state. In open-world complex environments, agents are prone to be erroneous\nwhen performing long-horizon tasks. To address this, we propose Experience-Driven Reflector, which\nis periodically activated to retrieve relevant multimodal experiences from AMEP. This encourages\nthe agent to reflect on its current actions and refine the plan.\nWe validate the performance of Optimus-1 in Minecraft, a popular open-world game environment.\nExperimental results show that Optimus-1 exhibits remarkable performance on long-horizon tasks,\nrepresenting up to 30% improvement over existing agents. Moreover, we introduce various Multi-\nmodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 has a 2 to 6 times performance improvement with the help of Hybrid Multimodal\nMemory, outperforming powerful GPT-4V baseline on lots of long-horizon tasks. Additionally, we\nverified that the plug-and-play Hybrid Multimodal Memory can drive Optimus-1 to incrementally\nimprove its performance in a self-evolution manner. The extensive experimental results show that\nOptimus-1 makes a major step toward a general agent with a human-like level of performance. Main\ncontributions of our paper:\n• We propose Hybrid Multimodal Memory module which is composed of HDKG and AMEP.\nHDKG helps the agent make the planning of long-horizon tasks efficiently. AMEP provides refined\nhistorical experience and guides the agent to reason about the current situation state effectively.\n• On top of the Hybrid Multimodal Memory module, we construct Optimus-1, which consists of\nKnowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Optimus-1\noutperforms all baseline agents on long-horizon task benchmarks, and exhibits capabilities close to\nthe level of human players.\n• Driven by Hybrid Multimodal Memory, various MLLM-based Optimus-1 have demonstrated 2 to 6\ntimes performance improvement, demonstrating the generalization of Hybrid Multimodal Memory.\n2\nOptimus-1\nIn this section, we first elaborate on how to implement the Hybrid Multimodal Memory in Sec 2.1.\nAs a core innovation, it plays a crucial role in enabling Optimus-1 to execute long-horizon tasks.\nNext, we give an overview of Optimus-1 framework (Sec 2.2), which consists of Hybrid Multimodal\nMemory, Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Finally,\nwe introduce a non-parametric learning approach to expand the hybrid multimodal memory (Sec 2.3),\nthereby enhancing the success rate of task execution for Optimus-1.\n2.1\nHybrid Multimodal Memory\nIn order to endow agent with a long-term memory storage mechanism [27, 45], we propose the\nHybrid Multimodal Memory module, which consists of Abstracted Multimodal Experience Pool\n(AMEP) and Hierarchical Directed Knowledge Graph (HDKG).\n2.1.1\nAbstracted Multimodal Experience Pool\nRelevant studies [23, 28, 17, 15] highlight the importance of historical information for agents\ncompleting long-horizon tasks. Minedojo [7] and Voyager [46] employed unimodal storage of\nhistorical information. Jarvis-1 [51] used a multimodal experience mechanism that stores task\nplanning and visual information without summarization, posing challenges to storage capacity and\nretrieval speed. To address this issue, we propose AMEP, which aims to dynamically summarize all\nmultimodal information during task execution. It preserves the integrity of long-horizon data while\nenhancing storage and retrieval efficiency.\n3\nAbstracted Multimodal Experience Pool\nFilter\nHierarchical Directed Knowledge Graph\n(b)\nVideo \n...\nVideo Buffer\n...\n...\nImage Buffer\n0.3\n0.7\n0.6\n0.9\n0.8\nCraft stone \npickaxe\nO1\nO2\nO3\nO4\nupdate\nKnowledge\nvisual\ninfo\nTask: Craft stone pickaxe.\nEnv: forest\nPlan: \n(a)\nObs:                                                            ... \nVisual Info: health: full, food: full,hotbar: empty\nFigure 2: (a) Extraction process of multimodal experience. The frames are filtered through video\nbuffer and image buffer, then MineCLIP [7] is employed to compute the visual and sub-goal sim-\nilarities and finally they are stored in Abstracted Multimodal Experience Pool. (b) Overview of\nHierarchical Directed Knowledge Graph. Knowledge is stored as a directed graph, where its nodes\nrepresent objects, and directed edges point to materials that can be crafted by this object.\nSpecifically, as depicted in Figure 2, to conduct the static visual information abstraction, the video\nstream captured by Optimus-1 during task execution is first input to a video buffer, filtering the\nstream at a fixed frequency of 1 frame per second. Based on the filtered video frames, to further\nperform a dynamic visual information abstraction, these frames are then fed into an image buffer with\na window size of 16, where the image similarity is dynamically computed and final abstracted frames\nare adaptively updated. To align such abstracted visual information with the corresponding textual\nsub-goal, we then utilize MineCLIP [7], a pre-trained video-text alignment model, to calculate their\nmultimodal correlation. When this correlation exceeds a threshold, the corresponding image buffer\nand textual sub-goal are saved as multimodal experience into a pool. Finally, we further incorporate\nenvironment information, agent initial state, and plan generated by Knowledge-Guided Planner, into\nsuch a pool, which forms the AMEP. In this way, we consider the multimodal information of each\nsub-goal, and summarise it to finally compose the multimodal experience of the given task.\n2.1.2\nHierarchical Directed Knowledge Graph\nIn Minecraft, mining and crafting represent a complex knowledge network crucial for effective task\nplanning. For instance, crafting a diamond sword\nrequires two diamonds\nand one wooden stick\n, while mining diamonds requires an iron pickaxe\n, which involving further materials and steps.\nSuch knowledge is essential for an agent’s ability to perform long-horizon complex tasks. Instead of\nimplicit learning through fine-tuning [32, 58], we propose HDKG, which transforms knowledge into\na graph representation. It enables the agent to perform explicit learning by retrieving information\nfrom the knowledge graph.\nAs shown in the Figure 2, we transform knowledge into a graph D(V, E), where nodes set V represent\nobjects, and directed edges set E point to nodes that can be crafted by this object. An edge e ∈E in\nthe D can be represented as e = (u, v), where u, v ∈V. The directed graph efficiently stores and\nupdates knowledge. For a given object x, retrieving the corresponding node allows extraction of a\nsub-graph Dj(Vj, Ej) ∈D, where nodes set Vj and edges set Ej can be formulated as:\nVj = {v ∈V | x} ,\nEj = {e = (u, v) ∈V | u ∈Vj ∪v ∈Vj} ,\n(1)\nThen by topological sorting, we can get all the materials and their relationships needed to complete\nthe task. This knowledge is provided to the Knowledge-Guided Planner as a way to generate a more\n4\nHybrid Multimodal Memory\nAbstracted Multimodal Experience Pool\nHierarchical Directed Knowledge Graph\nEnvironment\n9\n27\n1\n7\n1\n1\n1\n1\n3\n11\n3\n1\nKnowledge-Guided \nPlanner\nSub-Goals\nExperience-Driven \nReflector\n    \n \n   \n   \n   \nReflection\n [replan]\n [continue]\n [complete]\nTask: Craft diamond.\nAction Controller\n...\nTask: Mine cobblestone.\nEnv: forest\nVisual Info: health: full, food: full,hotbar: empty.\nKnowledge：\nSub-Goal: Mine 11 cobblestone.\nCurrent Obs:                                            \nReplan: \nContinue: \nComplete: \nIn-context Cases\nFigure 3: Overview framework of our Optimus-1. Optimus-1 consists of Knowledge-Guided Planner,\nExperience-Driven Reflector, Action Controller, and Hybrid Multimodal Memory architecture. Given\nthe task “craft stone sword”, Optimus-1 incorporates the knowledge from HDKG into Knowledge-\nGuided Planning, then Action Controller generates low-level actions. Experience-Driven Reflector is\nperiodically activated to introduce multimodal experience from AMEP to determine if the current\ntask can be executed successfully. If not, it will ask the Knowledge-Guided Planner to refine the plan.\nreasonable sequence of sub-goals. With HDKG, we can significantly enhance the world knowledge\nof the agent in a train-free manner.\n2.2\nOptimus-1: Framework\nRelevant studies indicate that the human brain is essential for planning and reflection, while the\ncerebellum controls low-level actions, both crucial for complex tasks [39, 40]. Inspired by this, we\ndivide the structure of Optimus-1 into Knowledge-Guided Planner, Experience-Driven Reflector, and\nAction Controller. In a given game environment with a long-horizon task, the Knowledge-Guided\nPlanner senses the environment, retrieves knowledge from HDKG, and decomposes the task into\nexecutable sub-goals. The action controller then sequentially executes these sub-goals. During\nexecution, the Experience-Driven Reflector is activated periodically, leveraging historical experience\nfrom AMEP to assess whether Optimus-1 can complete the current sub-goal. If not, it instructs the\nKnowledge-Guided Planner to revise its plan. Through iterative interaction with the environment,\nOptimus-1 ultimately completes the task.\nKnowledge-Guided Planner. Open-world environments vary greatly, affecting task execution.\nPrevious approaches [50] using LLMs for task planning failed to consider the environment, leading\nto the failure of tasks. For example, an agent in a cave aims to catch fish. It lacks visual information\nto plan conditions on the current situation, such as “leave the cave and find a river”. Therefore, we\nintegrate environmental information into the planning stage. Unlike Jarvis-1 [51] and MP5 [32],\nwhich convert observation to textual descriptions, Optimus-1 directly employs observation as visual\nconditions to generate environment-related plans, i.e., sub-goal sequences. This results in more\ncomprehensive and reasonable planning. More importantly, Knowledge-Guided Planner retrieves\nthe knowledge needed to complete the task from HDKG, allowing task planning to be done once,\nrather than generating the next step in each iteration. Given the task t, observation o, the sub-goals\n5\nsequence g1, g2, g3, ..., gn can be formulated as:\ng1, g2, g3, ..., gn = pθ(o, t, pη(t)),\n(2)\nwhere n is the number of sub-goals, pη denotes sub-graph retrieved from HDKG, pθ denotes MLLM.\nIn this paper, we employ OpenAI’s GPT-4V as Knowledge-Guided Planner and Experience-Driven\nReflector. We also evaluate other alternatives of GPT-4V, such as open-source models like Deepseek-\nVL [26] and InternLM-XComposer2-VL [6] in Section 3.4.\nAction Controller. It takes the sub-goal and the current observation as inputs and then generates\nlow-level actions, which are control signals for the mouse and keyboard. Thus, it can interact with\nthe game environment to update the agent’s state and the observation. The formulation is as follows:\nak = pπ(o, gi),\n(3)\nwhere ak denotes low-level action at time k, pπ denotes action controller. Unlike generating code\n[46, 32, 49], generating control actions for the mouse and keyboard [1, 25, 51, 3] more closely\nresembles human behavior. In this paper, we employ STEVE-1 [25] as our Action Controller.\nExperience-Driven Reflector. The sub-goals generated by Knowledge-Guided Planner are interde-\npendent. The failure of any sub-goal halts the execution of subsequent ones, leading to overall task\nfailure. Therefore, a reflection module is essential to identify and rectify errors promptly. During\ntask execution, the Experience-Driven Reflector activates at regular intervals, retrieving historical\nexperience from AMEP, and then analyzing the current state of Optimus-1. The reflection results\nof Optimus-1 are categorized as COMPLETE, CONTINUE, or REPLAN. COMPLETE indicates successful\nexecution, prompting the action controller to proceed to the next sub-goal. CONTINUE signifies\nongoing execution without additional feedback. REPLAN denotes failure, requiring the Knowledge-\nGuided Planner to revise the plan. The reflection r generated by Experience-Driven Reflector can be\nformulated as:\nr = pθ(o, gi, pϵ(t)),\n(4)\nwhere pϵ denotes multimodal experience retrieved from AMEP. Experimental results in Section\n3.3 demonstrate that the Experience-Driven Reflector significantly enhances the success rate of\nlong-horizon tasks.\nDuring task execution, even in cases where task failure necessitates REPLAN, multimodal experiences\nare stored in AMEP. Thus, during the reflection phase, Optimus-1 can retrieve the most relevant\ncases from each of the three scenarios COMPLETE, CONTINUE, and REPLAN from AMEP as references.\nExperimental Results in Section 3.3 demonstrate the effectiveness of this innovative method of\nincorporating failure cases into in-context learning.\n2.3\nNon-parametric Learning of Hybrid Multimodal Memory\nTo implement the Hybrid Multimodal Memory and enhance Optimus-1’s capacity, we propose a non-\nparametric learning method named “free exploration-teacher guidance”. In the free exploration phase,\nOptimus-1’s equipment and tasks are randomly initialized, and it explores random environments,\nacquiring world knowledge through environmental feedback. For example, it learns that “a stone\nsword\ncan be crafted with a wooden stick\nand two cobblestones\n”, storing this in the HDKG.\nAdditionally, successful and failed cases are stored in the AMEP, providing reference experience for\nthe reflection phase. We initialize multiple Optimus-1, and they share the same HDKG and AMEP.\nThus the memory is filled up efficiently. After free exploration, Optimus-1 has basic world knowledge\nand multimodal experience. In the teacher guidance phase, Optimus-1 needs to learn a small number\nof long-horizon tasks based on extra knowledge. For example, it learns “a diamond sword\nis\nobtained by a stick\nand two diamonds\n” from the teacher, then perform the task “craft diamond\nsword”. During the teacher guidance phase, Optimus-1’s memory is further expanded and it gains the\nexperience of executing complete long-horizon tasks.\nUnlike fine-tuning, this method enhances Optimus-1 incrementally without updating parameters, in\na self-evolution manner. Starting with an empty Hybrid Multimodal Memory, Optimus-1 iterates\nbetween “free exploration-teacher guidance” learning and unseen task inference. With each iteration,\nits memory capacity grows, enabling mastery of tasks from easy to hard.\n6\nTable 1: Main Result of Optimus-1 on long-horizon tasks benchmark. We report the average success\nrate (SR), average number of steps (AS), and average time (AT) on each task group, the results of each\ntask can be found in the Appendix F. Lower AS and AT metrics mean that the agent is more efficient\nat completing the task, while +∞indicates that the agent is unable to complete the task. Overall\nrepresents the average result on the five groups of Iron, Gold, Diamond, Redstone, and Armor.\nGroup\nMetric\nGPT-3.5\nGPT-4V\nDEPS\nJarvis-1\nOptimus-1\nHuman-level\nWood\nSR ↑\n40.16\n41.42\n77.01\n93.76\n98.60\n100.00\nAT ↓\n56.39\n55.15\n85.53\n67.76\n47.09\n31.08\nAS ↓\n1127.78\n1103.04\n1710.61\n1355.25\n841.94\n621.59\nStone\nSR ↑\n20.40\n20.89\n48.52\n89.20\n92.35\n100.00\nAT ↓\n135.71\n132.77\n138.71\n141.50\n129.94\n80.85\nAS ↓\n2714.21\n2655.47\n2574.30\n2830.05\n2518.88\n1617.00\nIron\nSR ↑\n0.00\n0.00\n16.37\n36.15\n46.69\n86.00\nAT ↓\n+∞\n+∞\n944.61\n722.78\n651.33\n434.38\nAS ↓\n+∞\n+∞\n8892.24\n8455.51\n6017.85\n5687.60\nGold\nSR ↑\n0.00\n0.00\n0.00\n7.20\n8.51\n17.31\nAT ↓\n+∞\n+∞\n+∞\n787.37\n726.35\n557.08\nAS ↓\n+∞\n+∞\n+∞\n15747.13\n15527.07\n13141.60\nDiamond\nSR ↑\n0.00\n0.00\n0.60\n8.98\n11.61\n16.98\nAT ↓\n+∞\n+∞\n1296.96\n1255.06\n1150.98\n744.82\nAS ↓\n+∞\n+∞\n23939.30\n25101.25\n23019.64\n16237.54\nRedstone\nSR ↑\n0.00\n0.00\n0.00\n16.31\n25.02\n33.27\nAT ↓\n+∞\n+∞\n+∞\n1070.42\n932.50\n617.89\nAS ↓\n+∞\n+∞\n+∞\n17408.40\n12709.99\n12357.00\nArmor\nSR ↑\n0.00\n0.00\n9.98\n15.82\n19.47\n28.48\nAT ↓\n+∞\n+∞\n997.59\n924.60\n824.53\n551.30\nAS ↓\n+∞\n+∞\n17951.95\n16492.96\n16350.56\n11026.00\nOverall\nSR ↑\n0.00\n0.00\n5.39\n16.89\n22.26\n36.41\n3\nExperiments\n3.1\nExperiments Setting\nEnvironment. To ensure realistic gameplay like human players, we employ MineRL [11] with\nMinecraft 1.16.5 as our simulation environment. The agent operates at a fixed speed of 20 frames per\nsecond and only interacts with the environment via low-level action control signals of the mouse and\nkeyboard. For more information about the detailed descriptions of the observation and action spaces,\nplease refer to the Appendix B.\nBenchmark. We constructed a benchmark of 67 tasks to evaluate the Optimus-1’s ability to complete\nlong-horizon tasks. As illustrated in Table 5, we divide the 67 Minecraft tasks into 7 groups according\nto recommended categories in Minecraft. Please refer to Appendix D for more details.\nBaseline. We compare Optimus-1 with various agents, including GPT-3.5 2, GPT-4V, DEPS [50],\nand Jarvis-1 [51] on the challenging long-horizon tasks benchmark. In addition, we employed 10\nvolunteers to perform the same task on the benchmark, and their average performance served as a\nhuman-level baseline. Please refer to Appendix D.2 for more details about human-level baseline. For\na more comprehensive comparison, we also report Optimus-1’s performances on the benchmark used\nby Voyager [46], MP5 [32], and DEPS [50] in the Appendix F.2. Note that we initialize Optimus-1\nwith an empty inventory, while DEPS [50] and Jarvis-1 [51] have tools in their initial state. This\nmakes it more challenging for Optimus-1 to perform the same tasks.\n2https:\/\/openai.com\/research\/gpt-3.5\n7\nTable 2: Ablation study results. We report average\nsuccess rate (SR) on each task group. P., R., K.,\nE. represent Planning, Reflection, Knowledge, and\nExperience, respectively.\nAblation Setting\nTask Group\nP.\nR.\nK.\nE.\nWood\nStone\nIron\nGold\nDiamond\n14.29\n0.00\n0.00\n0.00\n0.00\n!\n42.95\n25.67\n0.00\n0.00\n0.00\n!\n!\n55.00\n47.37\n18.11\n2.08\n1.11\n!\n!\n!\n73.53\n64.20\n24.19\n3.08\n1.86\n!\n!\n!\n92.37\n69.63\n38.33\n3.49\n2.42\n!\n!\n!\n!\n97.49\n94.26\n53.33\n11.54\n9.59\nTable 3: Ablation study on AMEP. We report\nthe average success rate (SR) on each task group.\nZero, Suc., and Fail. represent retrieving from\nAMEP without getting the case, getting the success\ncase, and getting the failure case, respectively.\nAblation Setting\nTask Group\nZero\nSuc.\nFai.\nWood\nStone\nIron\nGold\nDiamond\n!\n92.00\n79.26\n36.32\n4.25\n3.25\n!\n95.00\n84.29\n46.98\n9.36\n7.89\n!\n95.00\n81.10\n45.47\n7.50\n6.39\n!\n!\n97.49\n94.26\n53.33\n11.54\n9.59\nAbstracted\nMultiModal\nExperience Pool\nfall into \nwater\ndrop in \ncave\nCurrent Goal: chop a tree\nCurrent Goal: go fishing\nAbstracted\nMultiModal\nExperience Pool\n...\nOurs\nSTEVE-1\nSTEVE-1\nOurs\nFigure 4: Illustration of the role of reflection mechanism. Without the help of reflective mechanisms,\nSTEVE-1 [25] often gets into trouble and fails to complete the task. While Optimus-1, with the\nhelp of the Experience-Driven Reflector, leverages the AMEP to retrieve relevant experience, reflect\ncurrent situation and correct errors. This improves Optimus-1’s success rate on long-horizon tasks.\nEvaluation Metrics. The agent always starts in survival mode, with an empty inventory. We\nconducted at least 30 times for each task using different world seeds and reported the average success\nrate to ensure fair and thorough evaluation. Additionally, we add the average steps and average time\nof completing the task as evaluation metrics.\n3.2\nExperimental Results\nThe overall experimental results on benchmark are shown in Table 1, see the accuracy for each task in\nAppendix F. Optimus-1 has a success rate near 100% on the Wood Group\n. Compared with Jarvis-1,\nOptimus-1 has 29.28% and 53.40% improvement on the Diamond Group\nand Redstone Group\n,\nrespectively. Optimus-1 achieves the best performance and the shortest elapsed time among all task\ngroups. It reveals the effectiveness and efficiency of our proposed Optimus-1 framework. Moreover,\ncompared with all baselines, Optimus-1 performance was closer (average 5.37% improvement) to\nhuman levels on long-horizon task groups.\n3.3\nAblation Study\nWe conduct extensive ablation experiments on 18 tasks, experiment setting can be found in Table 6.\nAs shown in Table 2, we first remove Knowledge-Driven Planner and Experience-Driven Reflector,\nthe performance of Optimus-1 on all task groups drops dramatically. It demonstrates the necessity of\nKnowledge-Guided Planner and Experience-Driven Reflector modules for performing long-horizon\ntasks. As for Hybrid Multimodal Memory, we remove HDKG from Optimus-1. Without the help\nof world knowledge, the performance of Optimus-1 decreased by an average of 20% across all task\ngroups. We then removed AMEP, this resulted in the performance of Optimus-1 decreased by an\n8\nSuccess Rate\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nWood Group\nStone Group\nIron Group\nGold Group\nDiamond Group\n97.5 92.4 92.6\n94.3\n79.2\n73.0\n53.3\n43.8\n48.8\n6.1\n4.0\n5.9\n9.6\n3.3 3.3\nGPT-4V w\/ Memory\nGPT-4V w\/o Memory\nDeepseek-VL w\/ Memory\nDeepseek-VL w\/o Memory\nXComposer2-VL w\/ Memory\nXComposer2-VL w\/o Memory\n(a) Generalisation of Hybrid Multimodal Memory\nSuccess Rate\nWood \nGroup\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nEpoch 1\nEpoch 2\nEpoch 3\nEpoch 4\nStone \nGroup\nIron \nGroup\nGold \nGroup\nDiamond \nGroup\n92.50\n83.07\n43.74\n5.75\n4.66\n(b) Self-Evolution\nFigure 5: (a) With the help of Hybrid Multimodal Memory, various MLLM-based Optimus-1 have\ndemonstrated 2 to 6 times performance improvement. (b) Illustration of the change in Optimus-1\nsuccess rate on the unseen task over 4 epochs.\naverage of 12%. Finally, we performed ablation experiments on the way of retrieving cases from\nAMEP. As shown in Table 3, without retrieving cases from AMEP, the success rate shows an average\nof 10% decrease across all groups. It reveals that this reflection mechanism, which considers both\nsuccess and failure cases, has a significant impact on the performance of Optimus-1. To illustrate the\nrole of the reflection mechanism, we have shown some cases in Figure 4.\n3.4\nGeneralization Ability\nIn this section, we explore an interesting issue: whether generic MLLMs can effectively perform\nvarious long-horizon complex tasks in Minecraft using Hybrid Multimodal Memory. As shown in\nFigure 5, We employ Deepseek-VL [26] and InternLM-XComposer2-VL [6] as Knowledge-Guided\nPlanner and Experience-Driven Reflector. The experimental results show that the original MLLM has\nlow performance on long-horizon tasks due to the lack of knowledge and experience of Minecraft.\nWith the assistance of Hybrid Multimodal Memory, the performance of MLLMs has improved by 2\nto 6 times across various task groups, outperforming the GPT-4V baseline. This encouraging result\ndemonstrates the generalization of the proposed Hybrid Multimodal Memory.\n3.5\nSelf-Evolution via Hybrid Multimodal Memory\nAs shown in Section 2.3, we randomly initialize the Hybrid Multimodal Memory of Optimus-1, then\nupdate it multiple times by using the “free exploration-teacher guidance” learning method. We set\nthe epoch to 4, and the number of learning tasks to 160. At each period, Optimus-1 performs free\nexploration on 150 tasks and teacher guidance learning on the remaining 10 tasks, we then evaluate\nOptimus-1’s learning ability on the task groups same as ablation study. Experimental results are\nshown in Figure 5. It reveals that Optimus-1 keeps getting stronger through the continuous expansion\nof memory during the learning process of multiple periods. Moreover, it demonstrates that MLLM\nwith Hybrid Multimodal Memory can incarnate an expert agent in a self-evolution manner [44].\n4\nRelated Work\n4.1\nAgents in Minecraft\nWe summarise the differences of existing Minecraft agents in the Appendix D.3. Earlier work\n[29, 56, 2, 3] introduced policy models for agents to perform simple tasks in Minecraft. MineCLIP\n[7] used text-video data to train a contrastive video-language model as a reward model for policy,\nwhile VPT [1] pre-trained on unlabelled videos but lacked instruction as input. Building on VPT\nand MineCLIP, STEVE-1 [25] added text input to generate low-level action sequences from human\ninstructions and images. However, these agents struggle with complex tasks due to limitations in\ninstruction comprehension and planning. Recent work [49, 46, 59] incorporated LLMs as planning\nand reflection modules, but lacked visual information integration for adaptive planning. MP5\n9\n[32], MineDreamer [58], and Jarvis-1 [51] enhanced situation-aware planning by obtaining textual\ndescriptions of visual information, yet lacked detailed visual data. Optimus-1 addresses these issues\nby directly using observation as situation-aware conditions in the planning phase, enabling more\nrational, visually informed planning. Additionally, unlike other agents requiring multiple queries\nfor task refinement, Optimus-1 generates a complete and effective plan in one step with the help of\nHDKG. This makes Optimus-1 planning more efficient.\n4.2\nMemory in Agents\nIn the agent-environment interaction process, memory is key to achieving experience accumulation\n[21], environment exploration [16], and knowledge abstraction [57]. There are two forms to represent\nmemory content in LLM-based agents: textual form [17, 15, 30] and parametric form [5, 28, 47, 20].\nIn textual form, the information is explicitly retained and recalled by natural languages. In parametric\nform, the memory information [37] is encoded into parameters and implicitly influences the agent’s\nactions. Recent work [48, 52, 12] has explored the long-term visual information storage [18, 19] and\nsummarisation in MLLM. Our proposed hybrid multimodal memory module is plug-and-play and\ncan provide world knowledge and multimodal experience for Optimus-1 efficiently.\n5\nConclusion\nIn this paper, we propose Hybrid Multimodal Memory module, which consists of two parts: HDKG\nand AMEP. HDKG provides the necessary world knowledge for the planning phase of the agent,\nand AMEP provides the refined historical experience for the reflection phase of the agent. On top\nof the Hybrid Multimodal Memory, we construct the multimodal composable agent, Optimus-1,\nin Minecraft. Extensive experimental results show that Optimus-1 outperforms all existing agents\non long-horizon tasks. Moreover, we validate that general-purpose MLLMs, based on Hybrid\nMultimodal Memory and without additional parameter updates, can exceed the powerful GPT-4V\nbaseline. The extensive experimental results show that Optimus-1 makes a major step toward a\ngeneral agent with a human-like level of performance.\n6\nLimitation and Future Work\nIn the framework of Optimus-1, we are dedicated to leverage proposed Hierarchical Directed Knowl-\nedge Graph and Abstracted Multimodal Experience Pool can be used to enhance the agent’s ability\nto plan and reflect. For Action Controller, we directly introduce STEVE-1 [25] as a generator of\nlow-level actions. However, limited by STEVE-1’s ability to follow instructions and execute complex\nactions, Optimus-1 is weak in completing challenging tasks such as “beat ender dragon” and “build a\nhouse”. Therefore, a potential future research direction is to enhance the instruction following and\naction generation capabilities of action controller.\nIn addition, most of the work, including Optimus-1, utilize a multimodal large language model for\nplanning and reflection, which then drives an action controller to perform the task. Building an\nend-to-end vision-language-action agent will be future work.\n7\nAcknowledgement\nThis study is supported by National Natural Science Foundation of China (Grant No. 62236003\nand 62306090), Shenzhen College Stability Support Plan (Grant No. GXWD20220817144428005),\nNatural Science Foundation of Guangdong Province of China (Grant No. 2024A1515010147), and\nMajor Key Project of Peng Cheng Laboratory (Grant No. PCL2023A08).\n10\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Optimus-1：混合多模态记忆赋能代理在长时任务中表现出色\n\n## 📌 背景痛点\/本文动机\n构建一个通用代理是人工智能领域长期以来的愿景。尽管现有的代理在许多领域取得了显著进展，但它们仍然难以在开放世界中完成长时任务。本文认为，这主要是因为缺乏必要的世界知识和多模态经验，这些知识和经验可以指导代理完成各种长时任务。\n\n## 🚀 核心方法\n💡 创新点1：混合多模态记忆模块\n本文提出了一个混合多模态记忆模块，该模块由分层有向知识图（HDKG）和抽象多模态经验池（AMEP）组成。HDKG将知识转化为分层有向图结构，使代理能够显式地表示和学习世界知识。AMEP将历史信息总结为抽象多模态经验池，为代理提供丰富的参考，以便进行上下文学习。\n\n💡 创新点2：Optimus-1代理\n在混合多模态记忆模块的基础上，本文构建了一个多模态代理Optimus-1，该代理由知识引导规划器、经验驱动反思器和动作控制器组成。知识引导规划器利用HDKG来捕获完成任务所需的知识，并将其转化为可执行的子目标序列。动作控制器根据子目标和当前观察生成低级动作，与游戏环境交互以更新代理的状态。经验驱动反思器定期激活，从AMEP中检索相关的多模态经验，以评估当前子目标是否可以成功执行。如果不行，它会指示知识引导规划器修改其计划。\n\n## 📈 实验结果\n在Minecraft中进行的广泛实验结果表明，Optimus-1在具有挑战性的长时任务基准测试中显著优于所有现有代理，并在许多任务上表现出接近人类水平的性能。此外，本文引入了各种多模态大型语言模型（MLLMs）作为Optimus-1的骨干。实验结果表明，在混合多模态记忆模块的帮助下，Optimus-1在各种任务上表现出强大的泛化能力，优于GPT-4V基线。\n\n## 💬 可借鉴之处\n本文提出的混合多模态记忆模块和Optimus-1代理为构建能够完成长时任务的通用代理提供了新的思路和方法。此外，本文提出的非参数学习方法也为代理的学习和进化提供了新的思路。","llm_summary_res_status":200}
{"title":"Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy","authors":"Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie","summary":"Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.","url":"http:\/\/arxiv.org\/abs\/2502.19902v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2502.19902v2","published":1740647884000,"comment":"Accept to CVPR 2025, Project page:\n  https:\/\/cybertronagent.github.io\/Optimus-2.github.io\/","pdf_text":"Optimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nZaijing Li1 2, Yuquan Xie1, Rui Shao1*, Gongwei Chen1, Dongmei Jiang2, Liqiang Nie1*\n1Harbin Institute of Technology, Shenzhen\n2Peng Cheng Laboratory\n{lzj14011,xieyuquan20016}@gmail.com, {shaorui,nieliqiang}@hit.edu.cn\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/\nAbstract\nBuilding an agent that can mimic human behavior pat-\nterns to accomplish various open-world tasks is a long-\nterm goal.\nTo enable agents to effectively learn behav-\nioral patterns across diverse tasks, a key challenge lies\nin modeling the intricate relationships among observa-\ntions, actions, and language.\nTo this end, we propose\nOptimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-\nlevel planning, alongside a Goal-Observation-Action Con-\nditioned Policy (GOAP) for low-level control. GOAP con-\ntains (1) an Action-guided Behavior Encoder that models\ncausal relationships between observations and actions at\neach timestep, then dynamically interacts with the histori-\ncal observation-action sequence, consolidating it into fixed-\nlength behavior tokens, and (2) an MLLM that aligns be-\nhavior tokens with open-ended language instructions to pre-\ndict actions auto-regressively. Moreover, we introduce a\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset, which contains 25,000 videos across 8 atomic\ntasks, providing about 30M goal-observation-action pairs.\nThe automated construction method, along with the MGOA\ndataset, can contribute to the community’s efforts to train\nMinecraft agents. Extensive experimental results demon-\nstrate that Optimus-2 exhibits superior performance across\natomic tasks, long-horizon tasks, and open-ended instruc-\ntion tasks in Minecraft.\nPlease see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.\n1. Introduction\nEnabling agents to learn human behavioral patterns for\ncompleting complex tasks in open-world environments, is\na long-standing goal in the field of artificial intelligence\n[5, 23, 34, 47]. To effectively handle diverse tasks in an\n*Corresponding authors\nTransformer\nXL\nGoal \nEncoder\nkeyboard: W\nmouse: [0.0, 1.0]\n⊕\nLLM\nMLLM as Planner\nVision \nEncoder\nCausal \nPerceiver \nHistorical Sequence\nHistory\nAggregator \nAction-guided Behavior Encoder\nExsiting Goal-conditioned Policy\nOurs\n  1. chop a tree\n  2. craft four planks\n  3. craft two sticks\n  4. craft a wood sword\nSub-goals\nGoal-conditioned\nPolicy\nAction\nor\nsub-goal: chop a tree\nsub-goal: chop a tree\nI need a \nwooden \nsword\nVision \nEncoder\n...\nFigure 1.\nLeft: General agent framework.\nRight: Compari-\nson between existing goal-conditioned policies and ours. Existing\nTransformer-XL-based policies [3, 25] exhibit limited natural lan-\nguage understanding capabilities and rely solely on combining im-\nplicit goal embeddings with visual embeddings as inputs. In con-\ntrast, our GOAP achieves superior action prediction by 1) employ-\ning an Action-guided behavior encoder to strengthen causal mod-\neling between observations and actions, as well as to improve his-\ntorical sequence modeling capabilities, and 2) leveraging MLLM\nto enhance open-ended language comprehension.\nopen-world environment like Minecraft [20, 32], a promi-\nnent agent framework [24, 32, 41, 42] integrates a task\nplanner with a goal-conditioned policy. As illustrated in\nFigure 1 (left), this framework first utilizes the task plan-\nner’s language comprehension and visual perception abili-\nties to decompose complex task instructions into sequential\nsub-goals. These sub-goals are then processed by a goal-\nconditioned policy to generate actions.\nAlthough existing agents [24, 32, 42] have made promis-\ning progress by using Multimodal Large Language Models\n(MLLM) [4, 37, 45] as planners, the current performance\nbottleneck for agents lies in the improvement of the goal-\narXiv:2502.19902v2  [cs.AI]  11 Mar 2025\nconditioned policy [24]. As the sub-goal serves as a natu-\nral language description of an observation-action sequence,\nthe goal-conditioned policy needs to learn the crucial re-\nlationships among sub-goals, observations, and actions to\npredict actions. However, existing goal-conditioned poli-\ncies exhibit the following limitations: (1) Existing policies\nneglect the modeling of the relationship between observa-\ntions and actions. As shown in Figure 1, they only model\nthe relationship between the sub-goal and the current ob-\nservation by adding the sub-goal embedding to the obser-\nvation features [3, 25, 43]. However, the current observa-\ntion is generated by the previous action interacting with the\nenvironment. This implies a causal relationship between\naction and observation, which is neglected by current poli-\ncies; (2) Existing policies struggle to model the relation-\nship between open-ended sub-goals and observation-action\nsequences. As depicted in Figure 1, existing policies pri-\nmarily rely on either video encoders [3, 43] or conditional\nvariational autoencoders (CVAE) [25] as goal encoder to\nproduce implicit goal embeddings. Such embeddings have\nlimited representation ability [43]. Simply adding it to ob-\nservation features is sub-optimal and unable to handle the\ncomplex relationship between sub-goals and observation-\naction sequences.\nIn this paper, we propose Optimus-2, a novel agent that\nincorporates an MLLM for planning, alongside a Goal-\nObservation-Action Conditioned Policy (GOAP). To ad-\ndress the aforementioned challenges, we propose GOAP,\nwhich can better model the relationship among the obser-\nvations, actions, and sub-goals in two aspects.\nAn Action-guided Behavior Encoder for observation-\naction sequence modeling. To capture the relationship be-\ntween observations and actions, the Action-guided Behav-\nior Encoder first employs a Causal Perceiver to integrate ac-\ntion embeddings into observation features. It utilizes task-\nrelevant action information as guidance to adjust the obser-\nvation features, thereby providing fine-grained observation-\naction information for action prediction. Additionally, to\nmodel a long-term observation-action sequence without ex-\nceeding input length limitations, a History Aggregator is\nintroduced to dynamically integrate current observation-\naction information with the historical sequence into fixed-\nlength behavior tokens. Behavior tokens can capture the\nlong-term dependencies of the observation-action sequence\nwith a fixed and appropriate length. It enables the agent to\npredict actions that align with the logic of the observation-\naction sequence, rather than making isolated action predic-\ntions based solely on the current observation.\nAn MLLM to model the relationship between sub-\ngoal and observation-action sequence. To explicitly en-\ncode the semantics of sub-goals, we introduce an MLLM as\nthe backbone of GOAP. It aligns the sub-goal with behav-\nior tokens to predict subsequent actions auto-regressively.\nLeveraging the MLLM’s language comprehension and mul-\ntimodal perception capabilities, it can better integrate fea-\ntures from open-ended sub-goals and observation-action\nsequences, thereby enhancing the policy’s action predic-\ntion ability. To the best of our knowledge, GOAP is the\nfirst effort to employ MLLM as the core architecture of\na Minecraft policy, which demonstrates strong instruction\ncomprehension capabilities for open-ended sub-goals.\nMoreover, current Minecraft datasets either lack align-\nment among essential elements [10] or are not publicly\naccessible [1], resulting in a significant scarcity of high-\nquality observation-goal-action pairs necessary for policy\ntraining. To this end, we introduce an automated approach\nfor constructing the Minecraft Goal-Observation-Action\n(MGOA) dataset. The MGOA dataset comprises 25,000\nvideos across 8 atomic tasks, providing approximately 30\nmillion aligned observation-goal-action pairs.\nIt will be\nmade openly available to support advancements within\nthe research community.\nWe conducted comprehensive\nevaluations in the open-world environment of Minecraft,\nand the experimental results demonstrate that Optimus-\n2 achieves superior performance.\nCompared to previous\nSOTA, Optimus-2 achieves an average improvements of\n27%, 10%, and 18% on atomic tasks, long-horizon tasks,\nand open-ended sub-goal tasks, respectively.\nIn summary, our contributions are as follows:\n• We propose a novel agent Optimus-2, which consists of\nan MLLM for planning, and a policy for low-level con-\ntrol. The experimental results demonstrate that Optimus-\n2 exhibits superior performance on atomic tasks, long-\nhorizon tasks, and open-ended sub-goal tasks.\n• To better model the relationship among the observations,\nactions, and sub-goals, we propose Goal-Observation-\nAction Conditioned Policy, GOAP. It contains an Action-\nguided Behavior Encoder for observation-action se-\nquence modeling, and an MLLM to model the relation-\nship between sub-goal and observation-action sequence.\n• To address the scarcity of large-scale, high-quality\ndatasets, we introduce the MGOA dataset. It comprises\napproximately 30 million aligned observation-goal-action\npairs and is generated through an automated process with-\nout any manual annotations. The proposed dataset con-\nstruction method and the released MGOA dataset can\ncontribute to the community’s efforts to train agents.\n2. Related Work\nMinecraft Agents. Previous works [2, 8, 13, 31] have con-\nstructed policies in Minecraft using reinforcement learn-\ning or imitation learning. VPT [1] was training on large-\nscale video data recorded by human players, using behavior\ncloning to mimic human behavior patterns. GROOT [3] em-\nploys a video encoder as a goal encoder to learn semantic in-\nformation from videos. However, these policies rely solely\nLarge Language Model\nAction Head\n�t+1\n ViT\nLlama Tokenizer\nSub-goal: \nChop a tree.\nImage Token\nFFN\nCross-Attention\nV\nK\nQ\n...\nCross-Attention\nHistory-Attention\nV\nK\nQ\nV\nK\nQ\nText Token\nAction Token\nBehavior Token\nCausal Perceiver \nHistory\nAggregator \nAction-guided Behavior Encoder\nHistorical Memory Bank \n...\nAction-guided\nBehavior Encoder\n�t\n�t\n�1\n�t−3\n�t−2\n�t−1\n�t+1\n��\n...\n...\n�t\n...\n�t\n�1\n�2\n�3\n��−1\nLoRA\nMLLM-based\nPlanner\nTask: I need a \nwooden sword.\nFigure 2. Overview of Optimus-2. Given a task and the current observation, Optimus-2 first uses an MLLM-based Planner to generate a\nseries of sub-goals. Optimus-2 then sequentially executes these sub-goals through GOAP. GOAP obtains behavior tokens for the current\ntimestep via the Action-guided Behavior Encoder, and these behavior tokens, along with image and text tokens, are fed into the LLM to\npredict subsequent actions.\non visual observations as input and cannot follow human\ninstructions to accomplish specific tasks. MineCLIP [10]\nintroduces a video-text contrastive learning module as a re-\nward model for policy, and STEVE-1 [25] builds on VPT\n[1] by incorporating MineCLIP as goal encoder, enabling\npolicy to follow natural language instructions. Despite these\nadvancements, these policies are constrained by language\nunderstanding and reasoning capabilities. To address this,\ncurrent agents [20, 24, 32, 40, 42, 43] leverage MLLM’s\ninstruction following capabilities to decompose complex\ntasks into executable sub-goal sequences, which are then\nfed into a goal-conditioned policy [3, 25] or formed as exe-\ncutable code [26, 28, 51, 52]. Despite significant progress,\nthe performance of current policies remains constrained by\ntheir limited ability to understand sub-goals. In this paper,\nwe aim to develop an MLLM-based goal-conditioned pol-\nicy to enhance the policy’s comprehension of open-ended\nsub-goals, thereby improving overall performance.\nLong-term Video Modeling. Previous work [1, 3, 10, 25]\nhave segmented videos into multiple clips for training to\nalleviate the challenges posed by long-sequence video in-\nputs. However, this approach prevents the agent from learn-\ning comprehensive behavior representations from the entire\nvideo. To handle long-term video sequences [22, 48, 49],\nexisting studies employ temporal pooling [30], querying\ntransformers [14, 46], or token merging [16, 38, 50] to\nintegrate long-sequence visual tokens. Inspired by previ-\nous works [6, 18, 19, 44], we propose a Q-former [7, 21]\nstructure with a memory bank [14], enabling effective long-\nterm sequence modeling through interactions with histori-\ncal queries. Unlike existing methods that model only the\nobservation sequence, we focus on multimodal learning\n[33, 35, 36]. Moreover, different from previous work [14]\nthat primarily compress video features into fixed-length to-\nkens, our Action-guided Behavior Encoder dynamically in-\nteracts with the historical sequence at each timestep, pro-\nducing behavior tokens corresponding to the observation-\naction sequence from the start to the current timestep.\n3. Preliminaries and Problem Formulation\nIn Minecraft, agents [1, 3, 25] exhibit behavior patterns\nsimilar to humans: at each time step t, the agent receives\na visual observation ot and generates control actions at+1\nusing the mouse and keyboard.\nThese actions interact\nwith the environment, resulting in a new visual observa-\ntion ot+1.\nThrough continuous interactions, a trajectory\nJ = {(o1, a1), (o2, a2), (o3, a3), . . . , (oT , aT )} is formed,\nwhere T represents the length of the trajectory. Previous\nwork primarily trained Minecraft agents using reinforce-\nment learning [10] or behavior cloning [3, 25]. For exam-\nple, in behavior cloning, the goal of the policy pθ(at+1|o1:t)\nis to minimize the negative log-likelihood of the actions at\neach time step t given the trajectory J. Considering that\nsuch trajectories are typically generated under explicit or\nimplicit goals, many recent approaches condition the be-\nhavior on a (implicit or explicit) goal g and learn goal-\nconditioned policy pθ(at+1|o1:t, g) [3, 25]. Generally, for\nboth agents and humans, the explicit goal g is a natural lan-\nguage instruction.\nFormally, given a trajectory J with length T, standard\nbehavior cloning trains the policy pθ(·) with parameters θ\nby minimizing the negative log-likelihood of actions:\nmin\nθ\nT\nX\nt=1\n−log pθ(at+1|o1:t, g)\n(1)\n4. Optimus-2\nIn this section, we first give an overview of our proposed\nagent framework, Optimus-2. As shown in Figure 1 (left),\nit includes a planner for generating a series of executable\nsub-goals and a policy that sequentially executes these sub-\ngoals to complete the task.\nNext, we introduce how to implement Optimus-2’s plan-\nner (Sec. 4.1). Subsequently, we elaborate on how to imple-\nment the proposed GOAP (Sec. 4.2). Finally, in Sec 4.3, we\nintroduce an automated dataset generation method to obtain\na high-quality Minecraft Goal-Observation-Action dataset\n(MGOA) for training GOAP.\n4.1. MLLM-based Task Planner\nIn Minecraft, a complex task consists of multiple interme-\ndiate steps, i.e., sub-goals. For example, the task “I need\na wooden pickaxe” includes five sub-goals: ‘chop a tree to\nget logs\n’, ‘craft four planks\n’, ‘craft a crafting table\n’, ‘craft two sticks\n’, and ‘craft a wooden pickaxe\n’.\nTherefore, a planner is essential for the agent, as it needs to\ndecompose the given complex task into a sequence of exe-\ncutable sub-goals for the policy to execute sequentially. In\nthis paper, we follow Li et al. [24], employing an MLLM\nas the planner, which takes current observation and task in-\nstruction as input to generate sub-goals.\n4.2. Goal-Observation-Action Conditioned Policy\nAccording to Sec 3., a key insight into the relationship\namong observation o, action a, and sub-goal g is: that the\nobservation o and action a at the same time step have a\ncausal relationship; and the sub-goal g is a natural language\ndescription of the observation-action sequence over a cer-\ntain time. To better model the relationships among the three\nelements mentioned above, we propose first integrating the\nrepresentations of observation and action at each time step,\nthen modeling the observation-action sequences along the\ntemporal dimension, and finally aligning the observation-\naction sequences with the sub-goal for action prediction.\nMotivated\nby\nthis,\nwe\npropose\na\nnovel\nGoal-\nObservation-Action\nconditioned\nPolicy,\nGOAP.\nAs\nshown in Figure 2, our GOAP consists of an Action-guided\nBehavior Encoder that dynamically models observation-\naction sequences into fixed-length behavior tokens and an\nTable 1. Comparison of the MGOA dataset with existing datasets.\nO, G, and A represent observation, goal, and action. VPT† in-\ndicates the amount of data that is openly accessible. MineCLIP‡\ndenotes narrated Minecraft videos available on YouTube.\nFormat\nDataset\nO\nG\nA\n# Frames\nImage-Text Pairs\nMP5 [32]\n!\n!\n500K\nOmniJARVIS [43]\n!\n!\n!\n600K\nGameplay Video\nVPT† [1]\n!\n!\n6M\nMineCLIP‡ [10]\n!\n!\n20B\nSTEVE-1 [25]\n!\n!\n!\n32K\nMGOA (Ours)\n!\n!\n!\n30M\nMLLM that aligns such behavior tokens with sub-goal for\naction prediction.\n4.2.1. Action-guided Behavior Encoder\nPrevious policies often overlook the causal relationship be-\ntween observation and action at each timestep. Moreover,\nit remains a challenge to model the long-term observation-\naction sequence without exceeding input length constraints.\nTo this end, we propose an Action-guided Behavior En-\ncoder that integrates the representations of observation and\naction at each time step and then dynamically models the\nhistorical sequences into the fix-length behavior tokens.\nFirstly, for the timestep t, we pass observation ot into a\nvisual encoder VE to obtain the visual features:\nvt ←VE(ot)\n(2)\nwhere vt ∈RP ×d, P is the number of patches for each im-\nage, and d is the dimension of the extracted image feature.\nIn practice, we employ ViT [9] as our visual encoder.\nThen, we introduce a Causal Perceiver module to model\nthe relationship between observations and actions. It takes\nthe visual feature vt as query tokens and the action embed-\nding at as key and value. The module then constructs the\ninformation interaction between action at and vt through a\ncross-attention mechanism:\nQ = vtW Q\nv , K = atW K\na , V = atW V\na\n(3)\nˆvt = CrossAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(4)\nwhere W Q\nv , W K\na , and W V\na represent the weight matrices\nfor the query (Q), key (K), and value (V), respectively.\nCrossAttn(·) denotes the cross-attention layer, and d is\nthe dimension of the image features. In this way, it explic-\nitly assigns action information at at time step t to the visual\nfeatures ˆvt, enhancing the causal relationship between ob-\nservations and actions.\nSubsequently, we introduce a History Aggregator mod-\nule to capture the information of the observation-action se-\nquence along the temporal dimension, serving as the behav-\nior representation. At each timestep t, behavior tokens Bt\nserve as queries, while the sequence of historical behavior\ntokens Ht = [B1, B2, . . . , Bt−1] acts as keys and values.\nThe current behavior tokens interact with the historical se-\nquence through a history-attention layer HisAttn(·):\nˆBt = HisAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(5)\nwhere Q, K, and V are calculated similarly to Eq 3.\nFinally, another cross-attention layer is introduced, using\nthe behavior tokens ˆBt as queries, and the visual features ˆvt\nas keys and values. In this way, the behavior tokens incor-\nporate the current observation-action information. Follow-\ning the approach of He et al. [14], we construct a memory\nbank for historical behavior tokens Ht, utilizing the simi-\nlarity between adjacent features to aggregate and compress\nthe behavior tokens. This method not only preserves early\nhistorical information but also keeps the historical behav-\nior token sequence Ht at a fixed length to reduce compu-\ntational costs. Leveraging the Action-guided Behavior En-\ncoder, we obtain behavior tokens ˆBt, which correspond to\nthe observation-action sequence from the start to the current\ntime step t.\n4.2.2. MLLM Backbone\nTo model the relationship between the sub-goal and\nobservation-action sequence, we introduce an MLLM that\ntakes the sub-goal g, current observation features vt, and\nbehavior tokens Bt as input to predict subsequent actions\nauto-regressively. To enable the MLLM backbone MLLM\nto predict low-level actions, we employ VPT [1] as action\nhead AH to map output embeddings ¯at+1 of language model\ninto the action space.\n¯at+1 ←MLLM([g, vt, Bt])\n(6)\nat+1 ←AH(¯at+1)\n(7)\nFormally, given a dataset D = {(o1:T , a1:T )}M with M\ncomplete trajectories, we train GOAP to learn the behavior\ndistribution from D via behavioral cloning. Moreover, we\nintroduce a KL-divergence loss to measure the output dis-\ntribution similarity between GOAP and VPT [1]. This helps\nour model effectively learn the knowledge from the teacher\nmodel VPT. The training loss can be formulated as follows:\nLθ = λBC\nT\nX\nt=1\n−log pθ(at+1|o1:t, a1:t, g)\n+λKL\nT\nX\nt=1\nDKL(qϕ(at+1|o1:t) ∥pθ(at+1|o1:t, g))\n(8)\nwhere λBC and λKL are trade off coefficients, pθ is the\nGOAP, qϕ is the teacher model.\nTable 2. Main Result of GOAP on Atomic Tasks. We report the\naverage rewards of each task.\nPolicy\nLogs\nSeeds\nDirt\nStone\nVPT (text) [1]\n2.6\n0.8\n9.2\n0.0\nSTEVE-1 [25]\n11.0\n5.1\n10.0\n3.2\nGROOT [3]\n14.3\n7.3\n19.7\n19.0\nFSQ GROOT [43]\n10.8\n8.2\n20.3\n5.8\nGOAP [MLP ]\n7.2\n4.3\n14.4\n15.5\nGOAP [V P T ]\n15.0\n8.5\n26.7\n25.7\n4.3. MGOA Dataset\nIn Minecraft, there remains a significant lack of high-\nquality goal-observation-action pairs to support behavior\ncloning training.\nPrevious work has primarily relied on\ngameplay videos as training data. These datasets either lack\nnatural language instructions (explicit goals) [1, 3], or use\nactions predicted by IDM models [1] for each observation\nas pseudo-labels [1, 25], which leads to a risk of misalign-\nment between observations and actions. Inspired by Li et\nal. [24], we propose an automated data generation pipeline\nthat enables the creation of aligned goal-observation-action\npairs without the need for manual annotations or human\ncontractors. First, we utilize existing agents [25], provid-\ning them with clear natural language instructions to attempt\ntask completion in Minecraft. We then record the actions\nand corresponding observations during goal execution, gen-\nerating goal-observation-action pairs.\nTo ensure the quality of the generated data, we apply\nthe following filtering criteria: 1) only recording videos\nin which the task is successfully completed, and 2) dis-\ncarding videos where task execution takes an excessive\namount of time.\nFor more details, please refer to Sup.\nC. Through this automated approach, we obtained 25k\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset. A comparison of the MGOA dataset with the exist-\ning Minecraft datasets is shown in Table 1. Our automated\ndata generation pipeline offers several key advantages: 1)\nit enables the generation of aligned goal-observation-action\npairs without the need for manual annotation or pseudo-\nlabeling; 2) its construction process is parallelizable, allow-\ning for rapid dataset generation; and 3) it leverages local\nagents for data generation, resulting in low-cost production.\n5. Experiments\n5.1. Experiments Setting\nEnvironment. Following [1, 25], we conduct experiments\nin the complex, open-world environment of Minecraft on\nthe MineRL [12] platform. The agent interacts with the\nMineRL environment at 20 frames per second, generating\nlow-level control signals for the mouse and keyboard. For\neach task execution, the agent is initialized in a randomized\nTable 3. Main Result of Optimus-2 on Long-horizon Tasks. We report the average success rate (SR) on each task group, the results of each\ntask can be found in the Sup. F.1. Pure GPT-4V† denotes the use of GPT-4V in a zero-shot manner to generate executable sub-goals for\nthe policy. Human‡ denotes the human-level baseline, with results sourced from previous work [24].\nMethod\nPolicy\nWood\nStone\nIron\nGold\nDiamond\nRedStone\nArmor\nPure GPT-4V †\nVPT (text)\n0.22\n0.08\n0.00\n0.00\n0.00\n0.00\n0.00\nSTEVE-1\n0.41\n0.20\n0.00\n0.00\n0.00\n0.00\n0.00\nGOAP\n0.50\n0.31\n0.12\n0.02\n0.01\n0.03\n0.03\nDEPS [41]\nSTEVE-1\n0.77\n0.48\n0.16\n0.00\n0.01\n0.00\n0.10\nJarvis-1 [42]\nSTEVE-1\n0.93\n0.89\n0.36\n0.07\n0.08\n0.16\n0.15\nOptimus-1 [24]\nSTEVE-1\n0.98\n0.92\n0.46\n0.08\n0.11\n0.25\n0.19\nOptimus-2\nGOAP\n0.99\n0.93\n0.53\n0.09\n0.13\n0.28\n0.21\nHuman‡ [24]\n-\n1.00\n1.00\n0.86\n0.17\n0.16\n0.33\n0.28\nTable 4.\nMain Result of GOAP on Open-Ended Instruction\nTasks. We report the average success rate (SR) on Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nGROOT [3] and FSQ GROOT [43] were not included as base-\nlines, as they are unable to process language input.\nPlanner\nPolicy\nGLM-4V\nVPT (text)\n0.05\n0\n0\n0\n0\nSTEVE-1\n0.60\n0\n0\n0\n0\nGOAP\n0.71\n0.39\n0.11\n0.14\n0.13\nGPT-4V\nVPT (text)\n0.11\n0\n0\n0\n0\nSTEVE-1\n0.66\n0.10\n0\n0\n0\nGOAP\n0.75\n0.47\n0.13\n0.16\n0.17\nenvironment, allowing us to evaluate the agent’s generaliza-\ntion across diverse environments. Please refer to Sup. B for\nmore details about the Minecraft environment.\nImplementation details. For the planner, we follow Li et\nal. [24], using a hybrid multimodal memory empowered\nGPT-4V 1 as the agent’s planner. As for the policy, we ini-\ntialize GOAP with the weights of DeepSeek-VL-1.3B [29]\nas initialization. We train it on the MGOA dataset and the\npublicly available OpenAI Contractor Dataset [1] through\nbehavior cloning. All experiments were conducted on 8x\nNVIDIA L40 GPUs. Training details and hyperparameter\nsetting can be found in Sup. D.\nEvaluation Tasks & Metrics. Evaluation tasks are catego-\nrized into three types: Atomic Tasks, Long-Horizon Tasks,\nand Open-Ended Instruction Tasks. For each task, the en-\nvironment is randomly reinitialized on each attempt, with a\nminimum of 30 executions per task to ensure robustness.\n• Atomic Tasks represent short-term skills in Minecraft. We\nselect “chop a tree to get logs\n”, “collect seeds\n”,\n“collect dirt\n”, and “mine stone\nwith a pickaxe” as\nevaluation tasks. These tasks evaluate the policy’s basic\ncapabilities in Minecraft. We report the average rewards\n1https:\/\/openai.com\/index\/gpt-4v-system-card\n(number of items obtained) per task execution as an eval-\nuation metric.\n• Long-horizon Tasks consist of an interdependent atomic\ntasks sequence, where the failure of any single atomic\ntask results in the failure of the entire sequence. These\nlong-horizon tasks are designed to evaluate the agent’s ca-\npability to execute a series of diverse tasks continuously\nwithin a complex environment. We follow the setup of Li\net al. [24], conducting experiments on long-horizon tasks\ncomprising 67 tasks grouped into 7 categories. We report\nthe average Success Rate (SR) as an evaluation metric.\n• Open-Ended Instruction Tasks are not limited to prede-\nfined text formats; rather, they involve flexible language\ndirectives that prompt the agent to accomplish long-\nhorizon tasks. These tasks evaluate the agent’s capacity\nto interpret and execute instructions expressed in open-\nended natural language. We selected the Torch\nfrom\nthe Stone Group, Rail\nfrom the Iron Group, Golden\nShovel\nfrom the Gold Group, Diamond Pickaxe\nfrom the Diamond Group, and Compass\nfrom the Red-\nstone Group as evaluation tasks. Given a crafting rela-\ntionship graph, we instructed GPT-4V and GLM-4V [11]\nto generate five open-ended instructions for each task.\nThis allows us to evaluate the policies’ understanding and\nexecution capabilities regarding open-ended instructions.\nTask instructions are provided in the Sup. E.1.\nBaseline. For Atomic Tasks and Open-ended Instruction\nTasks, we compare GOAP with existing goal-conditioned\npolicies, including VPT [1], STEVE-1 [25], GROOT [3]\nand FSQ GROOT [43]. For Long-horizon Tasks, we employ\nGPT-4V, DEPS [41], Jarvis-1 [42], and Optimus-1 [24] as\nbaselines. We also introduce a human-level baseline [24] to\nevaluate the performance gap between existing agents and\nhuman capabilities.\n5.2. Experimental Results\nThe experimental results for Optimus-2 compared to the\nbaselines across Atomic Tasks, Long-horizon Tasks, and\nAgent\nInstruction: I need some iron ores, what should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 3. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need some iron ores,\nwhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nTable 5. Ablation study of Action-guided Behavior Encoder on\nAtomic Tasks. We report average rewards on each task. CP., HA.,\nand MB. represent the Causal Perceiver, History Aggregator, and\nMemory Bank, respectively.\nAblation Setting\nAtomic Task\nCP.\nHA.\nMB.\nLogs\nSeeds\nDirt\nStone\nAverage\n!\n!\n!\n15.0\n8.5\n26.7\n25.7\n19.0\n6.1\n5.4\n12.7\n15.7\n10.0 (↓47.4%)\n!\n10.2\n4.7\n12.8\n21.1\n12.2 (↓35.8%)\n!\n!\n7.4\n6.2\n13.1\n15.5\n10.6 (↓44.2%)\n!\n!\n12.0\n6.8\n22.7\n16.8\n14.6 (↓23.2%)\nOpen-ended Instruction Tasks are presented in Table 2, Ta-\nble 3, and Table 4, respectively.\nGOAP excels in Atomic Tasks. Table 2 shows that pro-\nposed GOAP achieves improvements of 5%, 4%, 31%, and\n35% over the current SOTA on the Logs\n, Seeds\n, Dirt\n, and Stone\n, respectively. These results demonstrate\nthat GOAP has successfully mastered a range of short-term\nskills across diverse environments, and can acquire items\nmore effectively than existing policies.\nOptimus-2 surpasses SOTA in Long-horizon Tasks.\nTable 3 shows that Optimus-2 achieved the highest success\nrates across all seven task groups, particularly excelling in\nthe challenging Diamond Group and Redstone Group with\nsuccess rates of 13% and 28%, respectively. This indicates\nthat Optimus-2 has effectively learned complex behavior\npatterns across atomic tasks, enabling it to sequentially ex-\necute multiple sub-goals and successfully complete long-\nhorizon tasks within complex environments.\nGOAP\noutperforms\nin\nOpen-ended\nInstruction\nTasks. As shown in Table 4, GOAP achieved significantly\nhigher success rates than existing agents across all tasks.\nNotably, on the challenging tasks of Golden Shovel\n, Di-\namond Pickaxe\n, and Compass\n, existing policies fail to\ncomplete these tasks, whereas GOAP achieves success rates\nof 13%, 16%, and 17%, respectively. This advantage stems\nSuccess Rate\n0.0\n5.0\n10.0\n15.0\nGolden \nShovel\nDiamond \nPickaxe\nCompass\nLLM \n20.0\n17.1\n13.1\n16.2\nTransformer-XL\n0.5\n0.5\n0.0\nFigure 4. Ablation of LLM backbone on Open-ended Instruction\nTasks, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nfrom GOAP’s superior comprehension of open-ended nat-\nural language instructions, whereas existing agents exhibit\nweaker instruction-following capabilities. Moreover, Fig-\nure 3 illustrates an example of different policies executing\nan open-ended goal. Due to the limited representation ca-\npability of their goal encoders, VPT [1] and STEVE-1 [25]\nfail to understand the goal, “I need some iron ores, what\nshould I do?” In contrast, GOAP leverages the MLLM’s\nunderstanding of open-ended instructions to effectively ac-\ncomplish the goal (obtaining iron ore\n).\n5.3. Ablation Study\nThere are many unexplored questions around best practices\nfor developing MLLM-based policy in Minecraft. In this\nsection, we conduct an extensive ablation study and sum-\nmarize our key findings.\nThe Action-guided Behavior Encoder plays a crucial\nrole in task execution. As shown in Table 5, the removal\nof the Causal Perceiver leads to an average performance de-\ncline of 42% across all tasks, highlighting the importance\nof capturing the causal relationship between observations\nand actions. Moreover, eliminating the History Aggregator\nAverage Rewards\n0.0\n5.0\n10.0\n15.0\n25.0\n30.0\nLogs\nSeeds\nDirt\nStone\nOCD_MGOA_Mix \n20.0\n26.7\n25.7\n6.0\n15.0\n8.5\n2.3\n14.3\n2.6\nOCD\n2.3\n1.4\n17.1\n22.1\nMGOA\nFigure 5. Ablation study on Training data. OCD refers to the\nOpenAI Contractor Dataset [1]. We report the average rewards on\neach Atomic Task.\nand Memory Bank also results in an average performance\ndecline of 36% across all tasks. This emphasizes the cru-\ncial role of the History Aggregator in modeling observation-\naction sequences and the Memory Bank in dynamically\nstoring long-sequence information.\nLLM significantly enhances policy’s ability to under-\nstand open-ended instructions. As shown in Figure 4, re-\nplacing the LLM backbone with a Transformer-XL leads to\na noticeable decline in performance. We attribute this to\nthe LLM’s pretraining on large-scale textual corpora, which\nendows it with a robust comprehension of open-ended lan-\nguage, a capability that Transformer-XL lacks.\nA pretrained action head improves performance in\nMinecraft. As shown in Table 2, replacing VPT with a\n2-layer MLP projector as the action head leads to a no-\nticeable decline in Optimus-2’s performance. While MLP-\nbased action heads have shown promising results in other\ndomains [17, 27], this substitution is less effective in the\nMinecraft environment. We attribute this to VPT’s exten-\nsive pretraining on large-scale gameplay data, which equips\nit with substantial domain-specific knowledge critical for\neffective task execution in Minecraft.\nThe MGOA datsaset is beneficial for training GOAP.\nWe conducted comparative experiments to evaluate the im-\npact of different training datasets on performance.\nAs\nshown in Figure 5, training only with the current most com-\nmonly used dataset, OpenAI Contractor Dataset (OCD), re-\nsults in suboptimal performance for GOAP on all Atomic\nTasks.\nFor example, compared to training with a mixed\ndataset, its performance on Stone\ndropped by 89%. We\nattribute this to the fact that OCD offers a wide variety of\ntasks but lacks high data quality.\nIn contrast, using our\nMGOA dataset, performance on the four atomic tasks im-\nproved by an average of 70% compared to using only the\nOCD data. We attribute this to the fact that MGOA contains\nhigh-quality aligned goal-observation-action pairs, which\nis beneficial for policy training. Further, we mix the two\ndatasets to train the policy in order to balance task diversity\nLog\nDirt\nSeed\nStone\n(a) ViT\n(c) Action-guided\n          Behavior Encoder\n(b) MineCLIP\nFigure 6. t-SNE visualization of representations extracted by (a)\nViT (b) MineCLIP and (c) Action-guided Behavior Encoder across\nAtomic Tasks. The visualization results show that the represen-\ntations in (a) and (b) cannot distinguish between different tasks,\nwhereas our Action-guided Behavior Encoder clearly differenti-\nates the behavior representations for the four tasks.\nand data quality, leading to improved performance.\n5.4. Visualization of Behavior Representation\nAs shown in Figure 6, we apply t-SNE [39] to visualize\nobservation features extracted by ViT [9], MineCLIP [10],\nand the Action-guided Behavior Encoder for four tasks.\nFrom (a) and (b) in Figure 6, it is evident that the behavior\nrepresentations extracted by ViT and MineCLIP are highly\nmixed, making it challenging to delineate the boundaries\nbetween different tasks. This lack of clear distinction be-\ntween task-specific behavior representations can hinder the\nmodel’s ability to understand the unique behavior patterns\nassociated with each task, potentially leading to task fail-\nure. In contrast, the visualization in (c) of Figure 6 reveals\nclear, distinct clusters for each task, demonstrating that the\nAction-guided Behavior Encoder effectively captures subtle\ndifferences in observation-action sequences, thereby learn-\ning robust behavior representations across tasks.\n6. Conclusion\nIn this paper, we propose a novel agent, Optimus-2, which\ncan excel in various tasks in the open-world environment of\nMinecraft. Optimus-2 integrates an MLLM for high-level\nplanning and a Goal-Observation-Action conditioned Pol-\nicy (GOAP) for low-level control. As a core contribution\nof this paper, GOAP includes an Action-guided Behavior\nEncoder to model the observation-action sequence and an\nMLLM to align the goal with the observation-action se-\nquence for predicting subsequent actions.\nExtensive ex-\nperimental results demonstrate that GOAP has mastered\nvarious atomic tasks and can comprehend open-ended lan-\nguage instructions. This enables Optimus-2 to achieve su-\nperior performance on long-horizon tasks, surpassing ex-\nisting SOTA. Moreover, we introduce a Minecraft Goal-\nObservation-Action dataset to provide the community with\nlarge-scale, high-quality data for training Minecraft agents.\nOptimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nSupplementary Material\nThe supplementary document is organized as follows:\n• Sec. A: Limitation and Future Work.\n• Sec. B: Minecraft Environment.\n• Sec. C: MGOA Dataset.\n• Sec. D: Training Details.\n• Sec. E: Evaluation Benchmark.\n• Sec. F: Experimental Results.\n• Sec. G: Case Study.\nA. Limitation and Future Work\nIn this paper, we aim to explore how agents can mimic\nhuman behavior patterns in Minecraft to accomplish vari-\nous tasks. Experimental results demonstrate that Optimus-\n2 performs exceptionally well in both atomic tasks and\nlong-horizon tasks. However, due to the lack of sufficient\nhigh-quality data for open-ended tasks (such as “building a\nhouse” and “defeating the Ender Dragon”), there remains\nsignificant room for improvement. Once such datasets are\navailable, the ability of Optimus-2 to complete open-ended\ntasks will be enhanced. Moreover, despite showing promis-\ning performance in Minecraft, we have not yet extended our\nexploration to other simulation platforms, which represents\na potential direction for future research.\nB. Minecraft\nMinecraft is an extremely popular sandbox video game de-\nveloped by Mojang Studios 2. It allows players to explore\na blockly, procedurally generated 3D world with infinite\nterrain, discover and extract raw materials, craft tools and\nitems, and build structures or earthworks. In this enviro-\nment, AI agents need to face situations that are highly sim-\nilar to the real world, making judgments and decisions to\ndeal with various environments and problems. As shown\nin Figure 7, both agents and humans are required to receive\nnatural language instructions and current observations as in-\nput, and then output low-level actions, such as mouse and\nkeyboard control commands. Therefore, Minecraft serves\n2https:\/\/www.minecraft.net\/en-us\/article\/meet-mojang-studios\n...\nObservation\n...\nAgent Action\nHuman Action\nspace\nW\nW\nKeyboard:\n {forward}\nMouse:{(2.7, 1.5)}\nKeyboard: \n{forward,jump}\nMouse: {(1.2, 0)}\nMouse: {attack}\nTask: chop a tree to get logs\nLanguage\nFigure 7.\nIllustration of behavior patterns of both human and\nagents in Minecraft.\nas an ideal open-world environment for training agent that\ncan learn human behavior patterns.\nB.1. Basic Rules\nBiomes. The Minecraft world is divided into different areas\ncalled “biomes”. Different biomes contain different blocks\nand plants and change how the land is shaped. There are 79\nbiomes in Minecraft 1.16.5, including ocean, plains, forest,\ndesert, etc. Diverse environments have high requirements\nfor the generalization of agents.\nItem. In Minecraft 1.16.5, there are 975 items can be ob-\ntained, such as wooden pickaxe\n, iron sword\n. Item can\nbe obtained by crafting or destroying blocks or attacking\nentities. For example, agent can attack cows\nto obtain\nleather\nand beef\n. Agent also can use 1 stick\nand 2\ndiamonds\nto craft diamond sword\n.\nTechnology Tree.\nIn Minecraft, the technology hierar-\nchy comprises six levels: wood\n, stone\n, iron\n, gold\n, diamond\n, and redstone\n.\nEach tool level corre-\nsponds to specific mining capabilities. Wooden tools can\nmine stone-level blocks but are incapable of mining iron-\nlevel or higher-level blocks.\nStone tools can mine iron-\nlevel blocks but cannot mine diamond-level or higher-level\nblocks.\nIron tools are capable of mining diamond-level\nblocks. Finally, diamond tools can mine blocks of any level,\nincluding redstone-level.\nGameplay progress. Progression in Minecraft primarily\ninvolves discovering and utilizing various materials and re-\nsources, each unlocking new capabilities and opportunities.\nFor instance, crafting a wooden pickaxe\nenables players\nTable 6. Action space of agent in Minecraft.\nIndex\nAgent Action\nHuman Action\nDescription\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove back.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nJump\nkey Space\nJump. When swimming, keeps the player afloat.\n6\nSneak\nkey left Shift\nSlowly move in the current direction of movement.\n7\nSprint\nkey left Ctrl\nMove quickly in the direction of current movement.\n8\nAttack\nleft Button\nDestroy blocks (hold down); Attack entity (click once).\n9\nUse\nright Button\nPlace blocks, entity, open items or other interact actions defined by game.\n10\nhotbar [1-9]\nkeys 1-9\nSelects the appropriate hotbar item.\n11\nOpen\/Close Inventory\nkey E\nOpens the Inventory. Close any open GUI.\n12\nYaw\nmove Mouse X\nTurning; aiming; camera movement.Ranging from -180 to +180.\n13\nPitch\nmove Mouse Y\nTurning; aiming; camera movement.Ranging from -180 to +180.\n14\nCraft\n-\nExecute a crafting recipe to obtain new item\n15\nSmelt\n-\nExecute a smelting recipe to obtain new item.\nto mine stone\n, which can then be used to create a stone\npickaxe\nand a furnace\n. These tools allow for the mining\nand smelting of iron ore\n. Subsequently, crafting an iron\npickaxe\nenables the extraction of diamonds\n, while a di-\namond pickaxe\ncan mine virtually any block in the game.\nSimilarly, cultivating crops facilitates breeding various an-\nimals, each providing unique resources beyond sustenance.\nDrops from enemies also serve specific purposes, with some\noffering greater utility than others. By integrating resources\nfrom mining, farming, and breeding, players can enchant\ntheir equipment, further enhancing their capabilities. Addi-\ntionally, collecting and crafting materials support construc-\ntion, enabling players to create diverse structures. Beyond\npractical functions, such as building secure bases or farms,\nconstructing personalized structures forms a significant as-\npect of the Minecraft experience. Figure 11 illustrates an\nexample of progression: crafting an iron sword\n.\nB.2. Observation and Action Spaces\nObservation. In this paper, observation space of agent is\ncompletely consistent with human players. The agent only\nreceives an RGB image with dimensions of 640 × 360 dur-\ning the gameplay process, including the hotbar, health in-\ndicators, food saturation, and animations of the player’s\nhands. It is worth helping the agent see more clearly in\nextremely dark environments, we have added a night vision\neffect for the agent, which increases the brightness of the\nenvironment during the night.\nAction Spaces. In MineRL [12] environment, agent’s ac-\ntion space is almost similar to human players. It consists of\ntwo parts: the mouse and the keyboard. The keypresses are\nLog\n48.1%\nSeed\n6.0%\nDirt\n6.0%\nStone\n8.2%\nIron\n8.1%\nGold\n8.0%\nDiamond\n7.9%\nRedstone\n7.7%\nFigure 8. Statistical information on MGOA dataset. It contains 8\nAtomic Tasks: ‘Log\n’, ‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’,\n‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’.\nresponsible for controlling the movement of agents, such as\njumping, forward, back, etc. The mouse movements are re-\nsponsible for controlling the perspective of agents and the\ncursor movements when the GUI is opened. The left and\nright buttons of the mouse are responsible for attacking and\nInstruction\nPool\nFiltering\ninteract\nfeedback\nGPT-4\nScript\nEnvironment\ncollect logs\nkeyboard: W\nmouse: [0.0, 1.0]\nMGOA Dataset\nInstruction\nItems\nFigure 9. The pipeline for generating the MGOA dataset. First, we extracted item names from the Minecraft Wiki and employed GPT-\n4 to generate corresponding instructions. These instructions were then provided as input to STEVE-1, enabling it to interact with the\nenvironment to accomplish the tasks. During task execution, each observation was paired with its corresponding action, resulting in the\ncreation of goal-observation-action pairs.\nusing or placing items. In Minecraft, precise mouse move-\nments are important when completing complex tasks that\nneed open inventory or crafting table. In order to achieve\nboth the same action space with MineDojo [10], we abstract\nthe craft and the smelt action into action space. The detailed\naction space is described in Table 6.\nC. MGOA Dataset\nIn Minecraft, there is still a lack of sufficient high-\nquality goal-observation-action pairs to support the train-\ning of Optimus-2.\nTo address this, we propose an au-\ntomated dataset construction process aimed at creating\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndatasets. Through this method. MGOA contains 25,000\nvideos, providing about 30M goal-observation-action pairs.\nIt contains 8 Atomic Tasks across 5 tech levels: ‘Log\n’,\n‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’, ‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’. Note that the Atomic Tasks in MGOA re-\nquire minimal steps and can typically be completed within\n2 ∼3 minutes. For instance, the task ‘Iron\n’ involves min-\ning iron with a stone pickaxe, without the need to gather raw\nmaterials to craft the stone pickaxe. The statistics for the\nMGOA dataset is shown in Figure 8. We provide several ex-\namples of the dataset in the MGOA samples folder within\nthe supplementary materials. We will release this dataset to\ncontribute to the development of open-world agents within\nthe community.\nC.1. Dataset Construction\nPipeline. Inspired by Li et al. [24], we employed a prior\npolicy (STEVE-1 [25] in our work) to perform specific tasks\nin Minecraft, and recorded the corresponding videos and\nactions to generate goal-observation-action pairs. As illus-\ntrated in Figure 9, we employed a custom script to extract\nitem names from the Minecraft Wiki3. Using these item\nnames, we queried GPT-44 with a predefined prompt tem-\nplate to generate task instructions, thereby constructing an\nInstruction Pool. The task instructions from the Instruc-\ntion Pool serve as input to STEVE-1 [25], enabling it to\ninteract with the environment to complete the tasks. During\ntask execution, each frame and corresponding action were\nrecorded and stored. To expedite data generation, we instan-\ntiated multiple policies and used parallelization to quickly\nproduce large amounts of data.\nData Filtering. We judged task success based on environ-\nmental feedback. For example, feedback like “obtained new\nitem, diamond axe” indicated that the task “craft a diamond\naxe” was successfully completed. During the dataset gen-\neration process, we observed a significant amount of low-\nquality video data due to limitations in the policy’s abil-\nity to follow instructions. Examples of low-quality data in-\ncluded task failures or task completion timeouts. To address\nthis issue, we established two filtering criteria to ensure data\nquality: (1) only retaining data from successfully completed\ntasks, and (2) removing data for tasks that lasted longer than\n2 minutes. These criteria allowed us to automatically filter\nout low-quality data, significantly reducing the cost of con-\nstructing the dataset. As a result, we obtained a high-quality\nMGOA dataset consisting of 25,000 samples.\n3https:\/\/minecraft.wiki\/\n4https:\/\/openai.com\/index\/gpt-4-research\/\nC.2. Comparison with Existing Datasets\nPrevious gameplay videos were primarily obtained through\ntwo methods below.\nVideo Platform: For example, MineDojo [10] collected\ngame videos uploaded by human players on platforms such\nas YouTube and Twitter, combining the video content with\ncorresponding titles or subtitles to form video-text pairs.\nHowever, this dataset lacked recorded actions. To address\nthis, VPT [1] used an Inverse Dynamics Model (IDM) to\ngenerate action sequences from the videos. However, the\nactions predicted by the IDM model are only approxima-\ntions, which introduces a potential risk of misalignment be-\ntween the frames and the corresponding actions.\nHuman Contractors: VPT [1] hired human players to\nfreely explore Minecraft and used the frames and actions\nto construct a video-action dataset. However, this dataset\nlacked explicit natural language instructions.\nTo create\ngoal-observation-action pairs, STEVE-1 [25] used GPT-3.5\nto generate specific task descriptions based on the game-\nplay, thereby integrating natural language instructions into\nthe dataset. However, they provide only approximately 32k\naligned goal-observation-action pairs, which remains a rel-\natively scarce amount of data.\nIn addition, some work [32, 43] have utilized GPT-4V\nto generate image captions, task planning, and reflections,\nthereby creating image-text pairs that form instruction-\nfollowing datasets.\nDistinct from the aforementioned datasets, the MGOA\ndataset directly captures agents performing specific tasks,\noffering clear natural language instructions with a one-to-\none correspondence between observations and actions. Fur-\nthermore, through rigorous data filtering, redundant action\nsequences that do not contribute to task completion are ex-\ncluded from MGOA. In addition, compared to the small-\nscale goal-observation-action datasets currently available,\nMGOA offers 25,000 videos, encompassing approximately\n30 million goal-observation-action pairs. This dataset is not\nonly significantly larger but also highly scalable in an auto-\nmated manner.\nD. Training Details\nD.1. Training Pipeline\nOne of the key factors in implementing our proposed\nmethod lies in the efficient alignment of language with the\nobservation-action sequence, and subsequently translating\nlanguage space into the action space. To tackle this prob-\nlem, we adopt a two-phase training approach. First, we\nalign language with the observation-action sequence via be-\nhavior pre-training. Then, we transform the language space\ninto the action space through action fine-tuning.\nBehavior Pre-training: During the pre-training phase, we\nintegrated the Vision-guided Behavior Encoder into the\nTable 7. Hyperparameter setting for pre-training and finetuning.\nHyperparameter\nPre-training\nFine-tuning\nOptimizer\nAdamW\nAdamW\nLearning Rate\n0.0001\n0.00004\nWarmup Steps\n0\n0\nEpochs\n5\n10\nBatch Size\n32\n2048\nNum. Frames\n5M\n80M\nLoRA r\n64\n64\nLoRA alpha\n128\n128\nmodel.\nWe used OpenAI Contractor Dataset [1] and a\nsubset of MGOA as training data, which comprised ap-\nproximately 5,000 videos. To balance efficiency and ef-\nfectiveness, we freeze the visual encoder, then tune the\nVision-guided Behavior Encoder along with a large lan-\nguage model (LoRA [15]). During pre-training, we set the\nlearning rate to 0.0001 and trained for 5 epochs. The hyper-\nparameter settings are shown in Table 7.\nAction Fine-tuning:\nDuring the fine-tuning phase, we\nadapted the general MLLM DeepSeek-VL-1.3B [29] to the\nMinecraft environment, transitioning the model’s output\nspace from language to low-level actions. We fine-tuned\nit using OpenAI Contractor Dataset [1] and MGOA, which\ncomprises approximately 20,000 videos. In this phase, we\nfreeze the Vision-guided Behavior Encoder, visual encoder,\nand large language model (LoRA), and only fine-tuned the\naction head. During fine-tuning, we set the learning rate to\n0.00004 and train for 10 epochs. The hyperparameter set-\ntings are shown in Table 7.\nD.2. Implementation Details\nFor the planner, we follow Li et al. [24], employing Mul-\ntimodal Hybrid Memory empowered GPT-4V for planning\nand reflection. For the policy, we train the GOAP through\nthe above pipeline. All experiments were conducted on 8x\nNVIDIA L40 GPUs. For the MGOA dataset, data collec-\ntion and filtering were conducted in parallel, taking approx-\nimately 7 days.\nTraining required around 2 days, while\ninference and evaluation on atomic tasks, long-horizon\ntasks, and open-ended instruction tasks took approximately\n4 days.\nE. Benchmark\nE.1. Evaluation Tasks\nThe evaluation tasks are divided into three categories:\nAtomic Tasks, Long-horizon Tasks, and Open-ended In-\nstruction Tasks. For each task, the agent’s environment is\n(a) chop a tree to get logs\n(b) mine dirt\n(c) collect seeds\n(d) dig down to mine stone\nFigure 10. Examples of Atomic Task. The agent must follow the instructions to collect resources. These four tasks represent the basic\ncapabilities of the agent. The more resources are collected, the stronger the basic capabilities of the agent will be.\nrandomly initialized each time, and every task is executed\nat least 30 times. For Atomic Tasks, we follow the setting of\nprior work [25, 43], which requires the agent to execute the\ntask within 2 minutes. We then report the average reward\nfor the task, defined as the number of items obtained. For\nOpen-ended Instruction Tasks and Long-horizon Tasks, we\nreport the average success rate (SR) for each task.\nAtomic Tasks. As shown in Figure 10, Atomic Tasks are\nshort-term skills in Minecraft, such as “chop a tree to get\nlogs\n”, “mine dirt\n”, “collect seeds\n”, and “dig down to\nmine stone\n”, etc.\nLong-horizon Tasks.\nAs shown in Figure 11, Long-\nHorizon Tasks are a sequence of Atomic Tasks. For exam-\nple, “craft an iron sword from scratch” requires complet-\ning the atomic tasks of “chop 7 logs”, “craft 21 planks”,\n“craft 5 sticks”, “craft 1 crafting table”, and so on. These\nAtomic Tasks are interdependent, meaning that the failure of\nany single atomic task will result in the failure of the entire\nLong-horizon Task.\nOpen-ended Instruction Tasks. Open-Ended Instruction\nTasks are not limited to predefined text formats; rather, they\ninvolve flexible language directives that prompt the agent\nto accomplish long-horizon tasks. These tasks evaluate the\nagent’s capacity to interpret and execute instructions ex-\npressed in open-ended natural language. We selected Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and\nCompass\nas evaluation tasks. Instruction for each task\nare shown in Table 8, Table 9, Table 10, Table 11 and Table\n12.\nE.2. Baselines\nIn this section, we provide a brief overview of existing\nMinecraft agents and compare them with our proposed\nOptimus-2. Current agents can be broadly categorized into\ntwo types: policy-based agents and planner-policy agents.\nPolicy-based Agents. Policy-based agents [1–3, 10, 25] re-\nfer to those trained through reinforcement learning or imi-\ntation learning, capable of completing atomic tasks within\nMinecraft. However, due to limitations in instruction un-\nderstanding and reasoning abilities, they struggle to accom-\nplish long-horizon tasks.\nPlanner-Policy Agents. Planner-policy agents [20, 24, 32,\n41–43] refer to non-end-to-end architectures that utilize a\nMLLM (Multi-Layered Language Model) as a planner to\ndecompose complex instructions into a sequence of sub-\ngoals executable by a policy. While significant progress has\n(a) Chop 7 logs\n(b) Craft 21 planks\n(c) Craft 5 sticks\n(d) Craft 1 crafting table\n(e) Craft 1 wooden pickaxe\n(f) Mine 11 cobblestone\n(g) Craft 1 furnace\n(h) Craft 1 stone pickaxe\n(i) Dig down more deeper to find iron ore\n(j) Mine 2 iron ores\n(k) Smelt 2 iron ingots\n(l) Craft 1 iron sword\nFigure 11. An example of long-horizon task “crafting an iron sword”. The agent must sequentially complete each atomic task in order to\nsuccessfully craft the iron sword. Failure in any of the atomic tasks will result in the failure of the entire long-horizon task.\nbeen made, the current performance bottleneck stems from\nthe policy’s ability to effectively understand and execute the\nsub-goals generated by the planner.\nComparison with Existing Agents. As a core contribu-\ntion of this work, we propose a novel Goal-Observation-\nAction Conditioned Policy, GOAP. It integrates two key\ncomponents: an Action-Guided Behavior Encoder for mod-\neling observation-action sequences, and an MLLM for\naligning sub-goals with these sequences. Leveraging the\nMLLM’s advanced understanding of open-ended instruc-\ntions, GOAP demonstrates superior instruction-following\ncapabilities compared to existing policies. On top of GOAP,\nthe proposed agent, Optimus-2, exhibits superior perfor-\nmance in long-horizon tasks, outperforming the current\nstate-of-the-art across all seven task groups.\nF. Experimental Results\nIn this section, we report the experimental results of\nOptimus-2 on each Long-horizon task.\nF.1. Results on Long-horizon Task\nIn this section, we report the results of Optimus-2 on each\nLong-horizon Task, with details including task name, num-\nbers of sub-goals, success rate (SR), and eval times. As\nshown in Tables 13 and 14, Optimus-2 demonstrates supe-\nrior performance across all 67 Long-horizon Tasks. Since\nOptimus-2 is randomly initialized in arbitrary environments\nfor each task execution, the experimental results also high-\nlight its generalization capability across diverse environ-\nments.\nG. Case Study\nIn this section, we provide additional cases to illustrate the\ndifferences in the ability of VPT (text) [1], STEVE-1 [25],\nand Optimus-2 to perform Open-ended Instruction Tasks.\nWe provide different open-ended instructions requiring the\nagent to perform tasks across various biomes. As shown in\nFigure 12, Figure 13, and Figure 14, Optimus-2 effectively\ncompletes all tasks, while VPT (text) and STEVE-1 fail due\nto limitations in language understanding and multimodal\nperception capabilities. Moreover, we provide several demo\nvideos of Optimus-2 performing long-horizon tasks in the\nOptimus2 videos folder within the supplementary ma-\nterials.\nTable 8. Open-ended instruction examples of “Craft a torch”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, I need to locate and chop down a couple of trees to collect 4 logs.\n2. Next, I should craft these logs into 13 planks. 3. Then, I need to use some of the\nplanks to craft 3 sticks. 4. I will use some of the planks to craft a crafting table. 5.\nNext, I should craft a wooden pickaxe using some of the planks and sticks. 6. With\nthe wooden pickaxe, I need to find and mine at least 1 coal from a nearby coal deposit\nor cave. 7. Finally, I can use one stick and one coal to craft a torch.\nPlan 2 1. First, I need to find a tree to gather 4 logs. 2. Next, I have to craft the logs\ninto planks. 3. From the planks, I will craft 3 sticks. 4. I need to craft a crafting table\nusing some of the planks. 5. I should then craft a wooden pickaxe to mine coal. 6. I\nwill locate and mine 1 piece of coal using the wooden pickaxe. 7. Finally, I will use\nthe coal and sticks to craft a torch.\nPlan 3 1. I have to find a tree to get 4 logs. 2. Next, I have to craft planks from the\nlogs. 3. Get sticks from some of the planks. 4. I need a crafting table, which requires\nplanks to craft. 5. I should craft a wooden pickaxe using the crafting table. 6. I need\nto find and mine some coal using the wooden pickaxe. 7. Finally, with the stick and\ncoal, I can craft a torch.\nGLM4V\nPlan 1 1. Locate and chop down a tree to gather 4 logs. 2. Convert the logs into 13\nplanks using the crafting menu. 3. Use the planks to craft a crafting table. 4. Craft 3\nsticks from the remaining planks. 5. With the crafting table and sticks, craft a wooden\npickaxe if one is not already available. 6. Use the wooden pickaxe to dig for coal\nor kill some Creepers or zombies for coal drops. 7. Once 1 coal is obtained, use the\ncrafting table to combine it with a stick to craft 1 torch.\nPlan 2 1. Locate and chop down a tree to gather the 4 logs required. 2. Convert the\nlogs into 13 planks using the crafting menu, as this will be the material needed for\ncrafting the crafting table and the wooden pickaxe. 3. Use some of the planks to craft\na crafting table, which is necessary for advanced crafting recipes. 4. Craft a wooden\npickaxe using some of the planks and sticks. The wooden pickaxe will be used to\nmine coal. 5. Gather 3 sticks, which can be obtained from the logs or by crafting them\nfrom planks. 6. Use the wooden pickaxe to mine coal. Only 1 coal is needed to craft\na torch. 7. Once all materials are gathered, open the crafting table and combine 1 coal\nand 1 stick to craft a torch.\nPlan 3 1. Locate and gather 4 logs from the nearest trees. 2. Convert the logs into\n13 planks using the crafting menu. 3. Use some planks to craft a crafting table for\nmore complex recipes. 4. Craft a wooden pickaxe using some planks and sticks to\ngather coal and other resources. 5. Convert some planks into 3 sticks required for\ncrafting torches and the wooden pickaxe. 6. Use the wooden pickaxe to mine coal\nore, ensuring to gather at least 1 coal. 7. With all materials gathered, open the crafting\ntable and craft 1 torch using 1 coal and 1 stick.\nTable 9. Open-ended instruction examples of “Craft a rail”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and punch down a tree to acquire 7 logs. 2. Convert these logs\ninto planks to obtain a total of 25 planks. 3. Use some of these planks to craft 5 sticks.\n4. Create a crafting table using 4 planks. 5. Craft a wooden pickaxe using 3 planks\nand 2 sticks. 6. Use the wooden pickaxe to mine 11 cobblestones. 7. Craft a furnace\nusing 8 cobblestones. 8. Craft a stone pickaxe using 3 cobblestones and 2 sticks. 9.\nUse the stone pickaxe to mine 6 iron ores. 10. Smelt the iron ores in the furnace to\ncreate 6 iron ingots. 11. Finally, craft a rail using the iron ingots and sticks.\nPlan 2 1. I have to find a tree to get 7 logs. 2. Next, I have to craft the logs into 25\nplanks. 3. Craft 5 sticks from the planks. 4. I need to create a crafting table. 5. Use\nthe crafting table to craft a wooden pickaxe. 6. Use the wooden pickaxe to mine 11\npieces of cobblestone. 7. I’ll then craft a furnace using the cobblestone. 8. Craft a\nstone pickaxe with the cobblestone and sticks. 9. Use the stone pickaxe to mine 6\npieces of iron ore. 10. Smelt the iron ore in the furnace to obtain 6 iron ingots. 11.\nFinally, use the crafting table to craft a rail using the iron ingots and sticks.\nPlan 3 1. I have to find a tree to get 7 logs. 2. Next, I need to craft planks from\nthe logs. 3. Get sticks from the planks. 4. Craft a crafting table using some of the\nplanks. 5. Craft a wooden pickaxe with sticks and planks to mine stone. 6. Mine 11\ncobblestone with the wooden pickaxe. 7. Craft a furnace using the cobblestone. 8.\nCraft a stone pickaxe using cobblestone and sticks. 9. Use the stone pickaxe to mine\n6 iron ore. 10. Smelt the iron ore in the furnace to get 6 iron ingots. 11. Finally, use\nthe iron ingots and sticks to craft a rail.\nGLM4V\nPlan 1 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down\na crafting table to access more crafting recipes. 5. Use the crafting table to craft\na wooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones from the\nground or from stone blocks. 7. Craft a furnace using the cobblestones. 8. Upgrade\nthe wooden pickaxe to a stone pickaxe by crafting it at the crafting table with the\ncobblestones. 9. Use the stone pickaxe to mine 6 iron ore from the appropriate rock\nlayers. 10. Smelt the 6 iron ore in the furnace to produce 6 iron ingots. 11. Finally,\nuse the crafting table to craft 1 rail using the 6 iron ingots and 1 stick.\nPlan 2 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu\nto convert the logs into 25 planks. 3. Craft 5 sticks using the planks in the crafting\nmenu. 4. Place a crafting table to enable the crafting of more complex items. 5.\nCraft a wooden pickaxe using some of the planks and sticks. 6. Use the wooden\npickaxe to mine 11 cobblestones from stone outcrops. 7. Construct a furnace using\nthe cobblestones. 8. Upgrade to a stone pickaxe by crafting it with the cobblestones\nand sticks. 9. Use the stone pickaxe to mine 6 iron ore from iron veins. 10. Smelt the\n6 iron ore in the furnace to produce 6 iron ingots. 11. Finally, use the crafting table to\ncraft 1 rail using the iron ingots and sticks.\nPlan 3 1. Find and chop down a tree to gather 7 logs. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down a\ncrafting table to access more crafting options. 5. Craft a wooden pickaxe using some\nof the planks and sticks. 6. Use the wooden pickaxe to mine 11 cobblestones from\nstone. 7. Construct a furnace using some of the cobblestones. 8. Upgrade the wooden\npickaxe to a stone pickaxe by crafting it with the cobblestones. 9. Search for and\nmine 6 iron ore blocks with the stone pickaxe. 10. Smelt the iron ore in the furnace to\nproduce 6 iron ingots. 11. Finally, use the crafting table to craft 1 rail using the iron\ningots and sticks.\nTable 10. Open-ended instruction examples of “Craft a golden shovel”\nPlanner Instruction\nGPT4V\nPlan 1 1. Start by finding and chopping down trees to collect 9 logs. 2. Convert the logs into 35 planks\nusing the crafting interface. 3. Use some of the planks to craft 8 sticks. 4. Create a crafting table using\nsome of the planks. 5. Craft a wooden pickaxe. 6. Mine 19 cobblestone. 7. Craft a furnace using 8 of the\ncobblestones. 8. Craft a stone pickaxe. 9. Mine 3 iron ores using the stone pickaxe. 10. Smelt the iron ore\nin the furnace. 11. Craft an iron pickaxe using sticks and iron ingots. 12. Locate and mine a gold block\nusing the iron pickaxe. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Craft a golden\nshovel using the gold ingot and sticks.\nPlan 2 1. Start by finding and chopping down trees to collect 9 logs. 2. Use the logs to craft planks. 3.\nCraft sticks. 4. Use some of the planks to craft a crafting table. 5. Use the crafting table to craft a wooden\npickaxe. 6. Use the wooden pickaxe to mine 19 pieces of cobblestone. 7. Use some of the cobblestone to\ncraft a furnace. 8. Use the remaining cobblestone to craft a stone pickaxe. 9. Mine iron ore with the stone\npickaxe. 10. Smelt the iron ore in the furnace to obtain 3 iron ingots. 11. Use 2 of the iron ingots and sticks\nto craft an iron pickaxe. 12. Use the iron pickaxe to mine gold ore or find a gold block; you need at least 1\ngold ingot. 13. If mining gold ore, smelt it in the furnace to obtain gold ingots. 14. Use the gold ingot and\nstick to craft a golden shovel.\nPlan 3 1. Find and cut down trees to collect 9 logs. 2. Use the logs to craft 35 planks. 3. Craft 8 sticks\nusing the planks. 4. Use some of the planks to craft a crafting table. 5. Craft a wooden pickaxe using planks\nand sticks. 6. Mine cobblestone until you have 19 pieces. 7. Use some cobblestone and the crafting table to\ncraft a furnace. 8. Craft a stone pickaxe using cobblestone and remaining sticks. 9. Use the stone pickaxe\nto mine 3 pieces of iron ore. 10. Smelt the iron ore in the furnace to get 3 iron ingots. 11. Craft an iron\npickaxe. 12. Use the iron pickaxe to mine gold ore to obtain gold blocks. 13. Use 1 gold ingot to craft a\ngolden shovel. 14. use a gold ingot and sticks to craft a golden shovel using the crafting table.\nGLM4V\nPlan 1 1. Find and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to expand the crafting options. 5. Craft\na wooden pickaxe with the planks and sticks. 6. Cine cobblestone until you have a total of 19. 7. Craft a\nfurnace using 8 cobblestone. 8. Use the furnace to smelt the cobblestone into 19 stone, then use the crafting\ntable to craft a stone pickaxe with the stone and sticks. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks at the crafting table. 12. Use the iron pickaxe to mine a gold block. 13. Smelt the gold block in\nthe furnace to get 1 gold ingot. 14. Finally, use the crafting table to craft the golden shovel using 1 gold\ningot and 2 sticks.\nPlan 2 1. Find and gather 9 logs from nearby trees. 2. Convert the logs into 35 planks using a crafting\ntable. 3. Craft 8 sticks from the planks. 4. Build a crafting table. 5. Use the crafting table to craft a wooden\npickaxe with the planks and sticks. 6. Use the wooden pickaxe to mine 19 cobblestones from stone. 7.\nConstruct a furnace to smelt materials. 8. Craft a stone pickaxe using the cobblestones and sticks. 9. Use\nthe stone pickaxe to mine 3 iron ore from iron deposits. 10. Smelt the 3 iron ore in the furnace to produce\n3 iron ingots. 11. Craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine a\ngold block. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Finally, craft the golden shovel\nusing the gold ingot and stick in the crafting table.\nPlan 3 1. Find and gather 9 logs from nearby trees. 2. Use the crafting menu to convert the logs into 35\nplanks. 3. Craft 8 sticks from the planks. 4. Craft a crafting table using some of the planks. 5. Use the\ncrafting table to craft a wooden pickaxe with some of the sticks and planks. 6. Use the wooden pickaxe to\nmine 19 cobblestones from stone. 7. Craft a furnace using some of the cobblestones. 8. Use the furnace to\nsmelt the cobblestones into 19 stone, then craft a stone pickaxe with the sticks and stone. 9. Mine 3 iron\nore from the appropriate rock faces. 10. Smelt the iron ore in the furnace to produce 3 iron ingots. 11. Use\nthe crafting table to craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine\na gold block. 13. Smelt the gold block in the furnace to produce 1 gold ingot. 14. Finally, use the crafting\ntable to craft the golden shovel with the gold ingot and 2 sticks.\nTable 11. Open-ended instruction examples of “Craft a diamond pickaxe”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some\nof the planks to craft 8 sticks. 4. Create a crafting table using 4 planks. 5. With the crafting table, craft\na wooden pickaxe using planks and sticks. 6. Use the wooden pickaxe to collect at least 11 cobblestones\nfrom stone. 7. With the cobblestones, craft a furnace. 8. Use more cobblestones to craft a stone pickaxe. 9.\nUse the stone pickaxe to mine 3 iron ores. 10. Smelt the iron ores into 3 iron ingots using the furnace. 11.\nCraft an iron pickaxe with iron ingots and sticks. 12. Use the iron pickaxe to mine 3 diamonds. 13. Finally,\ncraft the diamond pickaxe using the diamonds and sticks.\nPlan 2 1. Begin by locating a tree to gather 9 logs. 2. Convert the logs into planks. 3. Use some of the\nplanks to craft sticks. 4. Craft a crafting table to facilitate further crafting. 5. Create a wooden pickaxe to\nstart mining stone. 6. Mine 11 cobblestones. 7. Use the cobblestones to craft a furnace. 8. Craft a stone\npickaxe to advance to mining iron ore. 9. Locate and mine 3 iron ores using the stone pickaxe. 10. Smelt\nthe iron ores in the furnace to produce 3 iron ingots. 11. Use two of the iron ingots and sticks to craft an\niron pickaxe. 12. Locate a diamond ore vein and mine 3 diamonds using the iron pickaxe. 13. Finally, craft\nthe diamond pickaxe with the 3 diamonds and sticks.\nPlan 3 1. I need to find a tree to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some planks to\ncraft 8 sticks. 4. Craft a crafting table using some of the planks. 5. With the crafting table, make a wooden\npickaxe to mine stone. 6. Mine 11 cobblestones using the wooden pickaxe. 7. Craft a furnace using the\ncollected cobblestones. 8. Make a stone pickaxe to efficiently mine iron ore. 9. Find and mine 3 iron ores\nusing the stone pickaxe. 10. Smelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron\npickaxe using some planks, sticks, and the iron ingots. 12. Locate a source of diamonds and use the iron\npickaxe to mine 3 diamonds. 13. Finally, use the crafting table along with the sticks and diamonds to craft\na diamond pickaxe.\nGLM4V\nPlan 1 1. Start by finding trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to make a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ores. 10.\nSmelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine 3 diamonds, which may require exploring caves or mining deep underground with the\niron pickaxe. 13. Finally, use the crafting table to combine the diamonds and the sticks to craft a diamond\npickaxe.\nPlan 2 1. Locate and chop down trees to gather a total of 9 logs. 2. Use the logs to craft 35 planks, as this\nwill be the base material for several other items. 3. Craft 8 sticks from the planks, which will be needed to\ncreate various tools. 4. Craft a crafting table using some of the planks, as this is essential for crafting more\ncomplex items. 5. Use the crafting table to craft a wooden pickaxe with the planks and sticks, which will be\nused to mine cobblestone. 6. Mine 11 cobblestone using the wooden pickaxe. 7. Craft a stone pickaxe with\nthe cobblestone and sticks, as this is more durable and efficient for mining. 8. Construct a furnace using the\ncobblestone, which will be required to smelt ores into ingots. 9. Mine 3 iron ore with the stone pickaxe. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks, as this is necessary to mine diamonds. 12. Find and mine diamond ore with the iron pickaxe\nto gather 3 diamonds. 13. Finally, use the crafting table to craft a diamond pickaxe with the diamonds and\nsticks, completing the task.\nPlan 3 1. Begin by locating trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine diamond ore with the iron pickaxe to obtain 3 diamonds. 13. Finally, use the crafting\ntable to craft a diamond pickaxe with the diamonds.\nTable 12. Open-ended instruction examples of “Craft a compass”\nPlanner Instruction\nGPT4V\nPlan 1 1. Locate a tree and gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks from the\nplanks. 4. Use some planks to craft a crafting table. 5. With the crafting table, craft a wooden pickaxe. 6.\nMine cobblestone until you have 12 pieces. 7. Upgrade your tools by crafting a stone pickaxe. 8. Construct\na furnace using some of the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron\nore in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots. 12. Find and\nmine redstone using the iron pickaxe to obtain 1 redstone dust. 13. Finally, use the remaining iron ingots\nand redstone dust at the crafting table to craft a compass.\nPlan 2 1. Start by finding and chopping down trees to collect 10 logs. 2. Use the logs to craft 38 planks. 3.\nCraft sticks using the planks to get 8 sticks. 4. Craft a crafting table from the planks. 5. Use the crafting\ntable to create a wooden pickaxe. 6. Locate and mine 12 pieces of cobblestone using the wooden pickaxe.\n7. Craft a stone pickaxe from cobblestone and sticks. 8. Use the cobblestone to craft a furnace. 9. Mine\n7 iron ores using the stone pickaxe. 10. Smelt the iron ore in the furnace to get 7 iron ingots. 11. Craft\nan iron pickaxe using iron ingots and sticks. 12. Search for and mine redstone using the iron pickaxe. 13.\nFinally, use the iron ingots and redstone to craft a compass.\nPlan 3 1. First, I need to find a tree and collect 10 logs. 2. Convert the logs into planks, ensuring I get at\nleast 38 planks. 3. Use some of the planks to craft sticks, collecting at least 8 sticks. 4. Use some of the\nplanks to create a crafting table. 5. Craft a wooden pickaxe. 6. Locate cobblestone and mine at least 12\npieces using the wooden pickaxe. 7. Use some cobblestone to craft a furnace. 8. Craft a stone pickaxe for\nimproved mining. 9. Search for iron ore and mine at least 7 using the stone pickaxe. 10. Smelt the iron ore\nin the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using some of the iron ingots and sticks.\n12. Locate a redstone ore and mine it using the iron pickaxe to obtain redstone. 13. Finally, use 4 of the\niron ingots and the redstone to craft a compass at the crafting table.\nGLM4V\nPlan 1 1. Locate and gather 10 logs from nearby trees. 2. Convert the logs into 38 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to access more crafting options. 5. Use\nthe crafting table to make a wooden pickaxe. 6. With the wooden pickaxe, mine 12 cobblestones from\nstone. 7. Craft a stone pickaxe using the cobblestones and the crafting table. 8. Construct a furnace to smelt\nmaterials. 9. Use the stone pickaxe to mine 7 iron ores from the appropriate rock formations. 10. Smelt\nthe 7 iron ores in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe with the iron ingots at the\ncrafting table. 12. Mine for redstone, which can sometimes be found in the ground. 13. Use the crafting\ntable to craft a compass with the iron ingot and redstone.\nPlan 2 1. Locate and chop down trees to gather the required 10 logs. 2. Convert the logs into 38 planks\nusing the crafting menu. 3. Craft 8 sticks from the planks. 4. Use some planks to craft a crafting table.\n5. With the crafting table and the sticks and planks crafted, make a wooden pickaxe. 6. Use the wooden\npickaxe to mine 12 cobblestone. 7. Craft a stone pickaxe using the cobblestone. 8. Construct a furnace\nwith the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron ore in the furnace to\nproduce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots and the crafting table. 12. Search for\nredstone; gather at least 1 redstone. 13. Finally, use the crafting table to combine the redstone with the iron\ningots to craft a compass.\nPlan 3 1. Find and chop down trees to gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks\nfrom the planks. 4. Craft a crafting table using some of the planks. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine cobblestone until you have at least 12. 7. Upgrade the\nwooden pickaxe to a stone pickaxe using the cobblestone. 8. Construct a furnace out of the cobblestone. 9.\nUse the stone pickaxe to mine 7 iron ore. 10. Smelt the 7 iron ore in the furnace to produce 7 iron ingots.\n11. Craft an iron pickaxe using the iron ingots and sticks at the crafting table. 12. Search for redstone; you\nwill need at least 1 piece. 13. Finally, use the crafting table to combine the redstone with the iron ingots to\ncraft a compass.\nTable 13. The results of Optimus-2 on the Wood Group, Stone Group, and Iron Group. SR denotes success rate.\nGroup\nTask\nSub-Goal Num.\nSR\nEval Times\nWood\nCraft a wooden shovel\n6\n100.00\n40\nCraft a wooden pickaxe\n5\n100.00\n30\nCraft a wooden axe\n5\n97.37\n38\nCraft a wooden hoe\n5\n100.00\n30\nCraft a stick\n4\n100\n30\nCraft a crafting table\n3\n93.02\n43\nCraft a wooden sword\n5\n100.00\n30\nCraft a chest\n4\n100.00\n30\nCraft a bowl\n4\n100.00\n30\nCraft a ladder\n4\n100.00\n30\nStone\nCraft a stone shovel\n8\n89.47\n57\nCraft a stone pickaxe\n10\n98.00\n50\nCraft a stone axe\n10\n94.44\n54\nCraft a stone hoe\n8\n95.74\n47\nCraft a charcoal\n9\n85.71\n42\nCraft a smoker\n9\n90.00\n40\nCraft a stone sword\n8\n95.45\n44\nCraft a furnace\n9\n94.44\n36\nCraft a torch\n8\n89.36\n47\nIron\nCraft an iron shovel\n13\n52.08\n48\nCraft an iron pickaxe\n13\n56.00\n50\nCraft an iron axe\n13\n48.15\n54\nCraft an iron hoe\n13\n56.60\n53\nCraft a bucket\n13\n45.10\n51\nCraft a hopper\n14\n54.90\n51\nCraft a rail\n13\n51.02\n49\nCraft an iron sword\n12\n56.52\n46\nCraft a shears\n12\n48.28\n58\nCraft a smithing table\n12\n53.33\n45\nCraft a tripwire hook\n13\n55.56\n45\nCraft a chain\n13\n52.17\n46\nCraft an iron bars\n12\n51.06\n47\nCraft an iron nugget\n12\n54.55\n44\nCraft a blast furnace\n14\n52.27\n44\nCraft a stonecutter\n13\n52.27\n44\nTable 14. The results of Optimus-2 on the Gold group, Diamond Group, Redstone Group, and Armor Group. SR denotes success rate.\nGroup\nTask\nSub Goal Num.\nSR\nEval Times\nGold\nCraft a golden shovel\n16\n8.93\n56\nCraft a golden pickaxe\n16\n11.29\n62\nCraft a golden axe\n16\n8.93\n56\nCraft a golden hoe\n16\n8.96\n67\nCraft a golden sword\n16\n8.20\n61\nSmelt and craft an golden ingot\n15\n9.68\n62\nDiamond\nCraft a diamond shovel\n15\n15.91\n44\nCraft a diamond pickaxe\n15\n11.76\n34\nCraft a diamond axe\n16\n11.00\n36\nCraft a diamond hoe\n15\n15.91\n44\nCraft a diamond sword\n15\n11.11\n36\nDig down and mine a diamond\n15\n11.42\n35\nCraft a jukebox\n15\n13.15\n38\nRedstone\nCraft a piston\n16\n28.33\n60\nCraft a redstone torch\n16\n27.69\n65\nCraft an activator rail\n18\n25.81\n62\nCraft a compass\n23\n28.36\n67\nCraft a dropper\n16\n30.30\n66\nCraft a note block\n16\n25.40\n63\nArmor\nCraft shield\n14\n45.16\n62\nCraft iron chestplate\n14\n43.86\n57\nCraft iron boots\n14\n40.35\n57\nCraft iron leggings\n14\n8.57\n35\nCraft iron helmet\n14\n47.46\n56\nCraft diamond helmet\n17\n9.09\n33\nCraft diamond chestplate\n17\n7.89\n38\nCraft diamond leggings\n17\n5.41\n37\nCraft diamond boots\n17\n12.50\n40\nCraft golden helmet\n17\n13.89\n36\nCraft golden leggings\n17\n12.20\n41\nCraft golden boots\n17\n10.26\n39\nCraft golden chestplate\n17\n10.00\n40\nAgent\nInstruction: I want to get some logs to craft wooden sword, what should I do first?\nSuccess\n❌\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n✔\nFigure 12. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to get some logs\nto craft wooden sword, what should I do first?”. Existing policies are limited by their instruction comprehension abilities and thus fail to\ncomplete the task, whereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I need coal for heating. What should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 13. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need coal for heating.\nWhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I want to collect some seeds, Can you help me?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 14. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to collect some\nseeds, Can you help me?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task,\nwhereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nOptimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy\n```\n#### 2. 论文摘要\n```\nBuilding an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.\n```\n\n#### 3. 论文全文\n```\nOptimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nZaijing Li1 2, Yuquan Xie1, Rui Shao1*, Gongwei Chen1, Dongmei Jiang2, Liqiang Nie1*\n1Harbin Institute of Technology, Shenzhen\n2Peng Cheng Laboratory\n{lzj14011,xieyuquan20016}@gmail.com, {shaorui,nieliqiang}@hit.edu.cn\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/\nAbstract\nBuilding an agent that can mimic human behavior pat-\nterns to accomplish various open-world tasks is a long-\nterm goal.\nTo enable agents to effectively learn behav-\nioral patterns across diverse tasks, a key challenge lies\nin modeling the intricate relationships among observa-\ntions, actions, and language.\nTo this end, we propose\nOptimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-\nlevel planning, alongside a Goal-Observation-Action Con-\nditioned Policy (GOAP) for low-level control. GOAP con-\ntains (1) an Action-guided Behavior Encoder that models\ncausal relationships between observations and actions at\neach timestep, then dynamically interacts with the histori-\ncal observation-action sequence, consolidating it into fixed-\nlength behavior tokens, and (2) an MLLM that aligns be-\nhavior tokens with open-ended language instructions to pre-\ndict actions auto-regressively. Moreover, we introduce a\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset, which contains 25,000 videos across 8 atomic\ntasks, providing about 30M goal-observation-action pairs.\nThe automated construction method, along with the MGOA\ndataset, can contribute to the community’s efforts to train\nMinecraft agents. Extensive experimental results demon-\nstrate that Optimus-2 exhibits superior performance across\natomic tasks, long-horizon tasks, and open-ended instruc-\ntion tasks in Minecraft.\nPlease see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.\n1. Introduction\nEnabling agents to learn human behavioral patterns for\ncompleting complex tasks in open-world environments, is\na long-standing goal in the field of artificial intelligence\n[5, 23, 34, 47]. To effectively handle diverse tasks in an\n*Corresponding authors\nTransformer\nXL\nGoal \nEncoder\nkeyboard: W\nmouse: [0.0, 1.0]\n⊕\nLLM\nMLLM as Planner\nVision \nEncoder\nCausal \nPerceiver \nHistorical Sequence\nHistory\nAggregator \nAction-guided Behavior Encoder\nExsiting Goal-conditioned Policy\nOurs\n  1. chop a tree\n  2. craft four planks\n  3. craft two sticks\n  4. craft a wood sword\nSub-goals\nGoal-conditioned\nPolicy\nAction\nor\nsub-goal: chop a tree\nsub-goal: chop a tree\nI need a \nwooden \nsword\nVision \nEncoder\n...\nFigure 1.\nLeft: General agent framework.\nRight: Compari-\nson between existing goal-conditioned policies and ours. Existing\nTransformer-XL-based policies [3, 25] exhibit limited natural lan-\nguage understanding capabilities and rely solely on combining im-\nplicit goal embeddings with visual embeddings as inputs. In con-\ntrast, our GOAP achieves superior action prediction by 1) employ-\ning an Action-guided behavior encoder to strengthen causal mod-\neling between observations and actions, as well as to improve his-\ntorical sequence modeling capabilities, and 2) leveraging MLLM\nto enhance open-ended language comprehension.\nopen-world environment like Minecraft [20, 32], a promi-\nnent agent framework [24, 32, 41, 42] integrates a task\nplanner with a goal-conditioned policy. As illustrated in\nFigure 1 (left), this framework first utilizes the task plan-\nner’s language comprehension and visual perception abili-\nties to decompose complex task instructions into sequential\nsub-goals. These sub-goals are then processed by a goal-\nconditioned policy to generate actions.\nAlthough existing agents [24, 32, 42] have made promis-\ning progress by using Multimodal Large Language Models\n(MLLM) [4, 37, 45] as planners, the current performance\nbottleneck for agents lies in the improvement of the goal-\narXiv:2502.19902v2  [cs.AI]  11 Mar 2025\nconditioned policy [24]. As the sub-goal serves as a natu-\nral language description of an observation-action sequence,\nthe goal-conditioned policy needs to learn the crucial re-\nlationships among sub-goals, observations, and actions to\npredict actions. However, existing goal-conditioned poli-\ncies exhibit the following limitations: (1) Existing policies\nneglect the modeling of the relationship between observa-\ntions and actions. As shown in Figure 1, they only model\nthe relationship between the sub-goal and the current ob-\nservation by adding the sub-goal embedding to the obser-\nvation features [3, 25, 43]. However, the current observa-\ntion is generated by the previous action interacting with the\nenvironment. This implies a causal relationship between\naction and observation, which is neglected by current poli-\ncies; (2) Existing policies struggle to model the relation-\nship between open-ended sub-goals and observation-action\nsequences. As depicted in Figure 1, existing policies pri-\nmarily rely on either video encoders [3, 43] or conditional\nvariational autoencoders (CVAE) [25] as goal encoder to\nproduce implicit goal embeddings. Such embeddings have\nlimited representation ability [43]. Simply adding it to ob-\nservation features is sub-optimal and unable to handle the\ncomplex relationship between sub-goals and observation-\naction sequences.\nIn this paper, we propose Optimus-2, a novel agent that\nincorporates an MLLM for planning, alongside a Goal-\nObservation-Action Conditioned Policy (GOAP). To ad-\ndress the aforementioned challenges, we propose GOAP,\nwhich can better model the relationship among the obser-\nvations, actions, and sub-goals in two aspects.\nAn Action-guided Behavior Encoder for observation-\naction sequence modeling. To capture the relationship be-\ntween observations and actions, the Action-guided Behav-\nior Encoder first employs a Causal Perceiver to integrate ac-\ntion embeddings into observation features. It utilizes task-\nrelevant action information as guidance to adjust the obser-\nvation features, thereby providing fine-grained observation-\naction information for action prediction. Additionally, to\nmodel a long-term observation-action sequence without ex-\nceeding input length limitations, a History Aggregator is\nintroduced to dynamically integrate current observation-\naction information with the historical sequence into fixed-\nlength behavior tokens. Behavior tokens can capture the\nlong-term dependencies of the observation-action sequence\nwith a fixed and appropriate length. It enables the agent to\npredict actions that align with the logic of the observation-\naction sequence, rather than making isolated action predic-\ntions based solely on the current observation.\nAn MLLM to model the relationship between sub-\ngoal and observation-action sequence. To explicitly en-\ncode the semantics of sub-goals, we introduce an MLLM as\nthe backbone of GOAP. It aligns the sub-goal with behav-\nior tokens to predict subsequent actions auto-regressively.\nLeveraging the MLLM’s language comprehension and mul-\ntimodal perception capabilities, it can better integrate fea-\ntures from open-ended sub-goals and observation-action\nsequences, thereby enhancing the policy’s action predic-\ntion ability. To the best of our knowledge, GOAP is the\nfirst effort to employ MLLM as the core architecture of\na Minecraft policy, which demonstrates strong instruction\ncomprehension capabilities for open-ended sub-goals.\nMoreover, current Minecraft datasets either lack align-\nment among essential elements [10] or are not publicly\naccessible [1], resulting in a significant scarcity of high-\nquality observation-goal-action pairs necessary for policy\ntraining. To this end, we introduce an automated approach\nfor constructing the Minecraft Goal-Observation-Action\n(MGOA) dataset. The MGOA dataset comprises 25,000\nvideos across 8 atomic tasks, providing approximately 30\nmillion aligned observation-goal-action pairs.\nIt will be\nmade openly available to support advancements within\nthe research community.\nWe conducted comprehensive\nevaluations in the open-world environment of Minecraft,\nand the experimental results demonstrate that Optimus-\n2 achieves superior performance.\nCompared to previous\nSOTA, Optimus-2 achieves an average improvements of\n27%, 10%, and 18% on atomic tasks, long-horizon tasks,\nand open-ended sub-goal tasks, respectively.\nIn summary, our contributions are as follows:\n• We propose a novel agent Optimus-2, which consists of\nan MLLM for planning, and a policy for low-level con-\ntrol. The experimental results demonstrate that Optimus-\n2 exhibits superior performance on atomic tasks, long-\nhorizon tasks, and open-ended sub-goal tasks.\n• To better model the relationship among the observations,\nactions, and sub-goals, we propose Goal-Observation-\nAction Conditioned Policy, GOAP. It contains an Action-\nguided Behavior Encoder for observation-action se-\nquence modeling, and an MLLM to model the relation-\nship between sub-goal and observation-action sequence.\n• To address the scarcity of large-scale, high-quality\ndatasets, we introduce the MGOA dataset. It comprises\napproximately 30 million aligned observation-goal-action\npairs and is generated through an automated process with-\nout any manual annotations. The proposed dataset con-\nstruction method and the released MGOA dataset can\ncontribute to the community’s efforts to train agents.\n2. Related Work\nMinecraft Agents. Previous works [2, 8, 13, 31] have con-\nstructed policies in Minecraft using reinforcement learn-\ning or imitation learning. VPT [1] was training on large-\nscale video data recorded by human players, using behavior\ncloning to mimic human behavior patterns. GROOT [3] em-\nploys a video encoder as a goal encoder to learn semantic in-\nformation from videos. However, these policies rely solely\nLarge Language Model\nAction Head\n�t+1\n ViT\nLlama Tokenizer\nSub-goal: \nChop a tree.\nImage Token\nFFN\nCross-Attention\nV\nK\nQ\n...\nCross-Attention\nHistory-Attention\nV\nK\nQ\nV\nK\nQ\nText Token\nAction Token\nBehavior Token\nCausal Perceiver \nHistory\nAggregator \nAction-guided Behavior Encoder\nHistorical Memory Bank \n...\nAction-guided\nBehavior Encoder\n�t\n�t\n�1\n�t−3\n�t−2\n�t−1\n�t+1\n��\n...\n...\n�t\n...\n�t\n�1\n�2\n�3\n��−1\nLoRA\nMLLM-based\nPlanner\nTask: I need a \nwooden sword.\nFigure 2. Overview of Optimus-2. Given a task and the current observation, Optimus-2 first uses an MLLM-based Planner to generate a\nseries of sub-goals. Optimus-2 then sequentially executes these sub-goals through GOAP. GOAP obtains behavior tokens for the current\ntimestep via the Action-guided Behavior Encoder, and these behavior tokens, along with image and text tokens, are fed into the LLM to\npredict subsequent actions.\non visual observations as input and cannot follow human\ninstructions to accomplish specific tasks. MineCLIP [10]\nintroduces a video-text contrastive learning module as a re-\nward model for policy, and STEVE-1 [25] builds on VPT\n[1] by incorporating MineCLIP as goal encoder, enabling\npolicy to follow natural language instructions. Despite these\nadvancements, these policies are constrained by language\nunderstanding and reasoning capabilities. To address this,\ncurrent agents [20, 24, 32, 40, 42, 43] leverage MLLM’s\ninstruction following capabilities to decompose complex\ntasks into executable sub-goal sequences, which are then\nfed into a goal-conditioned policy [3, 25] or formed as exe-\ncutable code [26, 28, 51, 52]. Despite significant progress,\nthe performance of current policies remains constrained by\ntheir limited ability to understand sub-goals. In this paper,\nwe aim to develop an MLLM-based goal-conditioned pol-\nicy to enhance the policy’s comprehension of open-ended\nsub-goals, thereby improving overall performance.\nLong-term Video Modeling. Previous work [1, 3, 10, 25]\nhave segmented videos into multiple clips for training to\nalleviate the challenges posed by long-sequence video in-\nputs. However, this approach prevents the agent from learn-\ning comprehensive behavior representations from the entire\nvideo. To handle long-term video sequences [22, 48, 49],\nexisting studies employ temporal pooling [30], querying\ntransformers [14, 46], or token merging [16, 38, 50] to\nintegrate long-sequence visual tokens. Inspired by previ-\nous works [6, 18, 19, 44], we propose a Q-former [7, 21]\nstructure with a memory bank [14], enabling effective long-\nterm sequence modeling through interactions with histori-\ncal queries. Unlike existing methods that model only the\nobservation sequence, we focus on multimodal learning\n[33, 35, 36]. Moreover, different from previous work [14]\nthat primarily compress video features into fixed-length to-\nkens, our Action-guided Behavior Encoder dynamically in-\nteracts with the historical sequence at each timestep, pro-\nducing behavior tokens corresponding to the observation-\naction sequence from the start to the current timestep.\n3. Preliminaries and Problem Formulation\nIn Minecraft, agents [1, 3, 25] exhibit behavior patterns\nsimilar to humans: at each time step t, the agent receives\na visual observation ot and generates control actions at+1\nusing the mouse and keyboard.\nThese actions interact\nwith the environment, resulting in a new visual observa-\ntion ot+1.\nThrough continuous interactions, a trajectory\nJ = {(o1, a1), (o2, a2), (o3, a3), . . . , (oT , aT )} is formed,\nwhere T represents the length of the trajectory. Previous\nwork primarily trained Minecraft agents using reinforce-\nment learning [10] or behavior cloning [3, 25]. For exam-\nple, in behavior cloning, the goal of the policy pθ(at+1|o1:t)\nis to minimize the negative log-likelihood of the actions at\neach time step t given the trajectory J. Considering that\nsuch trajectories are typically generated under explicit or\nimplicit goals, many recent approaches condition the be-\nhavior on a (implicit or explicit) goal g and learn goal-\nconditioned policy pθ(at+1|o1:t, g) [3, 25]. Generally, for\nboth agents and humans, the explicit goal g is a natural lan-\nguage instruction.\nFormally, given a trajectory J with length T, standard\nbehavior cloning trains the policy pθ(·) with parameters θ\nby minimizing the negative log-likelihood of actions:\nmin\nθ\nT\nX\nt=1\n−log pθ(at+1|o1:t, g)\n(1)\n4. Optimus-2\nIn this section, we first give an overview of our proposed\nagent framework, Optimus-2. As shown in Figure 1 (left),\nit includes a planner for generating a series of executable\nsub-goals and a policy that sequentially executes these sub-\ngoals to complete the task.\nNext, we introduce how to implement Optimus-2’s plan-\nner (Sec. 4.1). Subsequently, we elaborate on how to imple-\nment the proposed GOAP (Sec. 4.2). Finally, in Sec 4.3, we\nintroduce an automated dataset generation method to obtain\na high-quality Minecraft Goal-Observation-Action dataset\n(MGOA) for training GOAP.\n4.1. MLLM-based Task Planner\nIn Minecraft, a complex task consists of multiple interme-\ndiate steps, i.e., sub-goals. For example, the task “I need\na wooden pickaxe” includes five sub-goals: ‘chop a tree to\nget logs\n’, ‘craft four planks\n’, ‘craft a crafting table\n’, ‘craft two sticks\n’, and ‘craft a wooden pickaxe\n’.\nTherefore, a planner is essential for the agent, as it needs to\ndecompose the given complex task into a sequence of exe-\ncutable sub-goals for the policy to execute sequentially. In\nthis paper, we follow Li et al. [24], employing an MLLM\nas the planner, which takes current observation and task in-\nstruction as input to generate sub-goals.\n4.2. Goal-Observation-Action Conditioned Policy\nAccording to Sec 3., a key insight into the relationship\namong observation o, action a, and sub-goal g is: that the\nobservation o and action a at the same time step have a\ncausal relationship; and the sub-goal g is a natural language\ndescription of the observation-action sequence over a cer-\ntain time. To better model the relationships among the three\nelements mentioned above, we propose first integrating the\nrepresentations of observation and action at each time step,\nthen modeling the observation-action sequences along the\ntemporal dimension, and finally aligning the observation-\naction sequences with the sub-goal for action prediction.\nMotivated\nby\nthis,\nwe\npropose\na\nnovel\nGoal-\nObservation-Action\nconditioned\nPolicy,\nGOAP.\nAs\nshown in Figure 2, our GOAP consists of an Action-guided\nBehavior Encoder that dynamically models observation-\naction sequences into fixed-length behavior tokens and an\nTable 1. Comparison of the MGOA dataset with existing datasets.\nO, G, and A represent observation, goal, and action. VPT† in-\ndicates the amount of data that is openly accessible. MineCLIP‡\ndenotes narrated Minecraft videos available on YouTube.\nFormat\nDataset\nO\nG\nA\n# Frames\nImage-Text Pairs\nMP5 [32]\n!\n!\n500K\nOmniJARVIS [43]\n!\n!\n!\n600K\nGameplay Video\nVPT† [1]\n!\n!\n6M\nMineCLIP‡ [10]\n!\n!\n20B\nSTEVE-1 [25]\n!\n!\n!\n32K\nMGOA (Ours)\n!\n!\n!\n30M\nMLLM that aligns such behavior tokens with sub-goal for\naction prediction.\n4.2.1. Action-guided Behavior Encoder\nPrevious policies often overlook the causal relationship be-\ntween observation and action at each timestep. Moreover,\nit remains a challenge to model the long-term observation-\naction sequence without exceeding input length constraints.\nTo this end, we propose an Action-guided Behavior En-\ncoder that integrates the representations of observation and\naction at each time step and then dynamically models the\nhistorical sequences into the fix-length behavior tokens.\nFirstly, for the timestep t, we pass observation ot into a\nvisual encoder VE to obtain the visual features:\nvt ←VE(ot)\n(2)\nwhere vt ∈RP ×d, P is the number of patches for each im-\nage, and d is the dimension of the extracted image feature.\nIn practice, we employ ViT [9] as our visual encoder.\nThen, we introduce a Causal Perceiver module to model\nthe relationship between observations and actions. It takes\nthe visual feature vt as query tokens and the action embed-\nding at as key and value. The module then constructs the\ninformation interaction between action at and vt through a\ncross-attention mechanism:\nQ = vtW Q\nv , K = atW K\na , V = atW V\na\n(3)\nˆvt = CrossAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(4)\nwhere W Q\nv , W K\na , and W V\na represent the weight matrices\nfor the query (Q), key (K), and value (V), respectively.\nCrossAttn(·) denotes the cross-attention layer, and d is\nthe dimension of the image features. In this way, it explic-\nitly assigns action information at at time step t to the visual\nfeatures ˆvt, enhancing the causal relationship between ob-\nservations and actions.\nSubsequently, we introduce a History Aggregator mod-\nule to capture the information of the observation-action se-\nquence along the temporal dimension, serving as the behav-\nior representation. At each timestep t, behavior tokens Bt\nserve as queries, while the sequence of historical behavior\ntokens Ht = [B1, B2, . . . , Bt−1] acts as keys and values.\nThe current behavior tokens interact with the historical se-\nquence through a history-attention layer HisAttn(·):\nˆBt = HisAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(5)\nwhere Q, K, and V are calculated similarly to Eq 3.\nFinally, another cross-attention layer is introduced, using\nthe behavior tokens ˆBt as queries, and the visual features ˆvt\nas keys and values. In this way, the behavior tokens incor-\nporate the current observation-action information. Follow-\ning the approach of He et al. [14], we construct a memory\nbank for historical behavior tokens Ht, utilizing the simi-\nlarity between adjacent features to aggregate and compress\nthe behavior tokens. This method not only preserves early\nhistorical information but also keeps the historical behav-\nior token sequence Ht at a fixed length to reduce compu-\ntational costs. Leveraging the Action-guided Behavior En-\ncoder, we obtain behavior tokens ˆBt, which correspond to\nthe observation-action sequence from the start to the current\ntime step t.\n4.2.2. MLLM Backbone\nTo model the relationship between the sub-goal and\nobservation-action sequence, we introduce an MLLM that\ntakes the sub-goal g, current observation features vt, and\nbehavior tokens Bt as input to predict subsequent actions\nauto-regressively. To enable the MLLM backbone MLLM\nto predict low-level actions, we employ VPT [1] as action\nhead AH to map output embeddings ¯at+1 of language model\ninto the action space.\n¯at+1 ←MLLM([g, vt, Bt])\n(6)\nat+1 ←AH(¯at+1)\n(7)\nFormally, given a dataset D = {(o1:T , a1:T )}M with M\ncomplete trajectories, we train GOAP to learn the behavior\ndistribution from D via behavioral cloning. Moreover, we\nintroduce a KL-divergence loss to measure the output dis-\ntribution similarity between GOAP and VPT [1]. This helps\nour model effectively learn the knowledge from the teacher\nmodel VPT. The training loss can be formulated as follows:\nLθ = λBC\nT\nX\nt=1\n−log pθ(at+1|o1:t, a1:t, g)\n+λKL\nT\nX\nt=1\nDKL(qϕ(at+1|o1:t) ∥pθ(at+1|o1:t, g))\n(8)\nwhere λBC and λKL are trade off coefficients, pθ is the\nGOAP, qϕ is the teacher model.\nTable 2. Main Result of GOAP on Atomic Tasks. We report the\naverage rewards of each task.\nPolicy\nLogs\nSeeds\nDirt\nStone\nVPT (text) [1]\n2.6\n0.8\n9.2\n0.0\nSTEVE-1 [25]\n11.0\n5.1\n10.0\n3.2\nGROOT [3]\n14.3\n7.3\n19.7\n19.0\nFSQ GROOT [43]\n10.8\n8.2\n20.3\n5.8\nGOAP [MLP ]\n7.2\n4.3\n14.4\n15.5\nGOAP [V P T ]\n15.0\n8.5\n26.7\n25.7\n4.3. MGOA Dataset\nIn Minecraft, there remains a significant lack of high-\nquality goal-observation-action pairs to support behavior\ncloning training.\nPrevious work has primarily relied on\ngameplay videos as training data. These datasets either lack\nnatural language instructions (explicit goals) [1, 3], or use\nactions predicted by IDM models [1] for each observation\nas pseudo-labels [1, 25], which leads to a risk of misalign-\nment between observations and actions. Inspired by Li et\nal. [24], we propose an automated data generation pipeline\nthat enables the creation of aligned goal-observation-action\npairs without the need for manual annotations or human\ncontractors. First, we utilize existing agents [25], provid-\ning them with clear natural language instructions to attempt\ntask completion in Minecraft. We then record the actions\nand corresponding observations during goal execution, gen-\nerating goal-observation-action pairs.\nTo ensure the quality of the generated data, we apply\nthe following filtering criteria: 1) only recording videos\nin which the task is successfully completed, and 2) dis-\ncarding videos where task execution takes an excessive\namount of time.\nFor more details, please refer to Sup.\nC. Through this automated approach, we obtained 25k\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset. A comparison of the MGOA dataset with the exist-\ning Minecraft datasets is shown in Table 1. Our automated\ndata generation pipeline offers several key advantages: 1)\nit enables the generation of aligned goal-observation-action\npairs without the need for manual annotation or pseudo-\nlabeling; 2) its construction process is parallelizable, allow-\ning for rapid dataset generation; and 3) it leverages local\nagents for data generation, resulting in low-cost production.\n5. Experiments\n5.1. Experiments Setting\nEnvironment. Following [1, 25], we conduct experiments\nin the complex, open-world environment of Minecraft on\nthe MineRL [12] platform. The agent interacts with the\nMineRL environment at 20 frames per second, generating\nlow-level control signals for the mouse and keyboard. For\neach task execution, the agent is initialized in a randomized\nTable 3. Main Result of Optimus-2 on Long-horizon Tasks. We report the average success rate (SR) on each task group, the results of each\ntask can be found in the Sup. F.1. Pure GPT-4V† denotes the use of GPT-4V in a zero-shot manner to generate executable sub-goals for\nthe policy. Human‡ denotes the human-level baseline, with results sourced from previous work [24].\nMethod\nPolicy\nWood\nStone\nIron\nGold\nDiamond\nRedStone\nArmor\nPure GPT-4V †\nVPT (text)\n0.22\n0.08\n0.00\n0.00\n0.00\n0.00\n0.00\nSTEVE-1\n0.41\n0.20\n0.00\n0.00\n0.00\n0.00\n0.00\nGOAP\n0.50\n0.31\n0.12\n0.02\n0.01\n0.03\n0.03\nDEPS [41]\nSTEVE-1\n0.77\n0.48\n0.16\n0.00\n0.01\n0.00\n0.10\nJarvis-1 [42]\nSTEVE-1\n0.93\n0.89\n0.36\n0.07\n0.08\n0.16\n0.15\nOptimus-1 [24]\nSTEVE-1\n0.98\n0.92\n0.46\n0.08\n0.11\n0.25\n0.19\nOptimus-2\nGOAP\n0.99\n0.93\n0.53\n0.09\n0.13\n0.28\n0.21\nHuman‡ [24]\n-\n1.00\n1.00\n0.86\n0.17\n0.16\n0.33\n0.28\nTable 4.\nMain Result of GOAP on Open-Ended Instruction\nTasks. We report the average success rate (SR) on Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nGROOT [3] and FSQ GROOT [43] were not included as base-\nlines, as they are unable to process language input.\nPlanner\nPolicy\nGLM-4V\nVPT (text)\n0.05\n0\n0\n0\n0\nSTEVE-1\n0.60\n0\n0\n0\n0\nGOAP\n0.71\n0.39\n0.11\n0.14\n0.13\nGPT-4V\nVPT (text)\n0.11\n0\n0\n0\n0\nSTEVE-1\n0.66\n0.10\n0\n0\n0\nGOAP\n0.75\n0.47\n0.13\n0.16\n0.17\nenvironment, allowing us to evaluate the agent’s generaliza-\ntion across diverse environments. Please refer to Sup. B for\nmore details about the Minecraft environment.\nImplementation details. For the planner, we follow Li et\nal. [24], using a hybrid multimodal memory empowered\nGPT-4V 1 as the agent’s planner. As for the policy, we ini-\ntialize GOAP with the weights of DeepSeek-VL-1.3B [29]\nas initialization. We train it on the MGOA dataset and the\npublicly available OpenAI Contractor Dataset [1] through\nbehavior cloning. All experiments were conducted on 8x\nNVIDIA L40 GPUs. Training details and hyperparameter\nsetting can be found in Sup. D.\nEvaluation Tasks & Metrics. Evaluation tasks are catego-\nrized into three types: Atomic Tasks, Long-Horizon Tasks,\nand Open-Ended Instruction Tasks. For each task, the en-\nvironment is randomly reinitialized on each attempt, with a\nminimum of 30 executions per task to ensure robustness.\n• Atomic Tasks represent short-term skills in Minecraft. We\nselect “chop a tree to get logs\n”, “collect seeds\n”,\n“collect dirt\n”, and “mine stone\nwith a pickaxe” as\nevaluation tasks. These tasks evaluate the policy’s basic\ncapabilities in Minecraft. We report the average rewards\n1https:\/\/openai.com\/index\/gpt-4v-system-card\n(number of items obtained) per task execution as an eval-\nuation metric.\n• Long-horizon Tasks consist of an interdependent atomic\ntasks sequence, where the failure of any single atomic\ntask results in the failure of the entire sequence. These\nlong-horizon tasks are designed to evaluate the agent’s ca-\npability to execute a series of diverse tasks continuously\nwithin a complex environment. We follow the setup of Li\net al. [24], conducting experiments on long-horizon tasks\ncomprising 67 tasks grouped into 7 categories. We report\nthe average Success Rate (SR) as an evaluation metric.\n• Open-Ended Instruction Tasks are not limited to prede-\nfined text formats; rather, they involve flexible language\ndirectives that prompt the agent to accomplish long-\nhorizon tasks. These tasks evaluate the agent’s capacity\nto interpret and execute instructions expressed in open-\nended natural language. We selected the Torch\nfrom\nthe Stone Group, Rail\nfrom the Iron Group, Golden\nShovel\nfrom the Gold Group, Diamond Pickaxe\nfrom the Diamond Group, and Compass\nfrom the Red-\nstone Group as evaluation tasks. Given a crafting rela-\ntionship graph, we instructed GPT-4V and GLM-4V [11]\nto generate five open-ended instructions for each task.\nThis allows us to evaluate the policies’ understanding and\nexecution capabilities regarding open-ended instructions.\nTask instructions are provided in the Sup. E.1.\nBaseline. For Atomic Tasks and Open-ended Instruction\nTasks, we compare GOAP with existing goal-conditioned\npolicies, including VPT [1], STEVE-1 [25], GROOT [3]\nand FSQ GROOT [43]. For Long-horizon Tasks, we employ\nGPT-4V, DEPS [41], Jarvis-1 [42], and Optimus-1 [24] as\nbaselines. We also introduce a human-level baseline [24] to\nevaluate the performance gap between existing agents and\nhuman capabilities.\n5.2. Experimental Results\nThe experimental results for Optimus-2 compared to the\nbaselines across Atomic Tasks, Long-horizon Tasks, and\nAgent\nInstruction: I need some iron ores, what should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 3. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need some iron ores,\nwhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nTable 5. Ablation study of Action-guided Behavior Encoder on\nAtomic Tasks. We report average rewards on each task. CP., HA.,\nand MB. represent the Causal Perceiver, History Aggregator, and\nMemory Bank, respectively.\nAblation Setting\nAtomic Task\nCP.\nHA.\nMB.\nLogs\nSeeds\nDirt\nStone\nAverage\n!\n!\n!\n15.0\n8.5\n26.7\n25.7\n19.0\n6.1\n5.4\n12.7\n15.7\n10.0 (↓47.4%)\n!\n10.2\n4.7\n12.8\n21.1\n12.2 (↓35.8%)\n!\n!\n7.4\n6.2\n13.1\n15.5\n10.6 (↓44.2%)\n!\n!\n12.0\n6.8\n22.7\n16.8\n14.6 (↓23.2%)\nOpen-ended Instruction Tasks are presented in Table 2, Ta-\nble 3, and Table 4, respectively.\nGOAP excels in Atomic Tasks. Table 2 shows that pro-\nposed GOAP achieves improvements of 5%, 4%, 31%, and\n35% over the current SOTA on the Logs\n, Seeds\n, Dirt\n, and Stone\n, respectively. These results demonstrate\nthat GOAP has successfully mastered a range of short-term\nskills across diverse environments, and can acquire items\nmore effectively than existing policies.\nOptimus-2 surpasses SOTA in Long-horizon Tasks.\nTable 3 shows that Optimus-2 achieved the highest success\nrates across all seven task groups, particularly excelling in\nthe challenging Diamond Group and Redstone Group with\nsuccess rates of 13% and 28%, respectively. This indicates\nthat Optimus-2 has effectively learned complex behavior\npatterns across atomic tasks, enabling it to sequentially ex-\necute multiple sub-goals and successfully complete long-\nhorizon tasks within complex environments.\nGOAP\noutperforms\nin\nOpen-ended\nInstruction\nTasks. As shown in Table 4, GOAP achieved significantly\nhigher success rates than existing agents across all tasks.\nNotably, on the challenging tasks of Golden Shovel\n, Di-\namond Pickaxe\n, and Compass\n, existing policies fail to\ncomplete these tasks, whereas GOAP achieves success rates\nof 13%, 16%, and 17%, respectively. This advantage stems\nSuccess Rate\n0.0\n5.0\n10.0\n15.0\nGolden \nShovel\nDiamond \nPickaxe\nCompass\nLLM \n20.0\n17.1\n13.1\n16.2\nTransformer-XL\n0.5\n0.5\n0.0\nFigure 4. Ablation of LLM backbone on Open-ended Instruction\nTasks, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nfrom GOAP’s superior comprehension of open-ended nat-\nural language instructions, whereas existing agents exhibit\nweaker instruction-following capabilities. Moreover, Fig-\nure 3 illustrates an example of different policies executing\nan open-ended goal. Due to the limited representation ca-\npability of their goal encoders, VPT [1] and STEVE-1 [25]\nfail to understand the goal, “I need some iron ores, what\nshould I do?” In contrast, GOAP leverages the MLLM’s\nunderstanding of open-ended instructions to effectively ac-\ncomplish the goal (obtaining iron ore\n).\n5.3. Ablation Study\nThere are many unexplored questions around best practices\nfor developing MLLM-based policy in Minecraft. In this\nsection, we conduct an extensive ablation study and sum-\nmarize our key findings.\nThe Action-guided Behavior Encoder plays a crucial\nrole in task execution. As shown in Table 5, the removal\nof the Causal Perceiver leads to an average performance de-\ncline of 42% across all tasks, highlighting the importance\nof capturing the causal relationship between observations\nand actions. Moreover, eliminating the History Aggregator\nAverage Rewards\n0.0\n5.0\n10.0\n15.0\n25.0\n30.0\nLogs\nSeeds\nDirt\nStone\nOCD_MGOA_Mix \n20.0\n26.7\n25.7\n6.0\n15.0\n8.5\n2.3\n14.3\n2.6\nOCD\n2.3\n1.4\n17.1\n22.1\nMGOA\nFigure 5. Ablation study on Training data. OCD refers to the\nOpenAI Contractor Dataset [1]. We report the average rewards on\neach Atomic Task.\nand Memory Bank also results in an average performance\ndecline of 36% across all tasks. This emphasizes the cru-\ncial role of the History Aggregator in modeling observation-\naction sequences and the Memory Bank in dynamically\nstoring long-sequence information.\nLLM significantly enhances policy’s ability to under-\nstand open-ended instructions. As shown in Figure 4, re-\nplacing the LLM backbone with a Transformer-XL leads to\na noticeable decline in performance. We attribute this to\nthe LLM’s pretraining on large-scale textual corpora, which\nendows it with a robust comprehension of open-ended lan-\nguage, a capability that Transformer-XL lacks.\nA pretrained action head improves performance in\nMinecraft. As shown in Table 2, replacing VPT with a\n2-layer MLP projector as the action head leads to a no-\nticeable decline in Optimus-2’s performance. While MLP-\nbased action heads have shown promising results in other\ndomains [17, 27], this substitution is less effective in the\nMinecraft environment. We attribute this to VPT’s exten-\nsive pretraining on large-scale gameplay data, which equips\nit with substantial domain-specific knowledge critical for\neffective task execution in Minecraft.\nThe MGOA datsaset is beneficial for training GOAP.\nWe conducted comparative experiments to evaluate the im-\npact of different training datasets on performance.\nAs\nshown in Figure 5, training only with the current most com-\nmonly used dataset, OpenAI Contractor Dataset (OCD), re-\nsults in suboptimal performance for GOAP on all Atomic\nTasks.\nFor example, compared to training with a mixed\ndataset, its performance on Stone\ndropped by 89%. We\nattribute this to the fact that OCD offers a wide variety of\ntasks but lacks high data quality.\nIn contrast, using our\nMGOA dataset, performance on the four atomic tasks im-\nproved by an average of 70% compared to using only the\nOCD data. We attribute this to the fact that MGOA contains\nhigh-quality aligned goal-observation-action pairs, which\nis beneficial for policy training. Further, we mix the two\ndatasets to train the policy in order to balance task diversity\nLog\nDirt\nSeed\nStone\n(a) ViT\n(c) Action-guided\n          Behavior Encoder\n(b) MineCLIP\nFigure 6. t-SNE visualization of representations extracted by (a)\nViT (b) MineCLIP and (c) Action-guided Behavior Encoder across\nAtomic Tasks. The visualization results show that the represen-\ntations in (a) and (b) cannot distinguish between different tasks,\nwhereas our Action-guided Behavior Encoder clearly differenti-\nates the behavior representations for the four tasks.\nand data quality, leading to improved performance.\n5.4. Visualization of Behavior Representation\nAs shown in Figure 6, we apply t-SNE [39] to visualize\nobservation features extracted by ViT [9], MineCLIP [10],\nand the Action-guided Behavior Encoder for four tasks.\nFrom (a) and (b) in Figure 6, it is evident that the behavior\nrepresentations extracted by ViT and MineCLIP are highly\nmixed, making it challenging to delineate the boundaries\nbetween different tasks. This lack of clear distinction be-\ntween task-specific behavior representations can hinder the\nmodel’s ability to understand the unique behavior patterns\nassociated with each task, potentially leading to task fail-\nure. In contrast, the visualization in (c) of Figure 6 reveals\nclear, distinct clusters for each task, demonstrating that the\nAction-guided Behavior Encoder effectively captures subtle\ndifferences in observation-action sequences, thereby learn-\ning robust behavior representations across tasks.\n6. Conclusion\nIn this paper, we propose a novel agent, Optimus-2, which\ncan excel in various tasks in the open-world environment of\nMinecraft. Optimus-2 integrates an MLLM for high-level\nplanning and a Goal-Observation-Action conditioned Pol-\nicy (GOAP) for low-level control. As a core contribution\nof this paper, GOAP includes an Action-guided Behavior\nEncoder to model the observation-action sequence and an\nMLLM to align the goal with the observation-action se-\nquence for predicting subsequent actions.\nExtensive ex-\nperimental results demonstrate that GOAP has mastered\nvarious atomic tasks and can comprehend open-ended lan-\nguage instructions. This enables Optimus-2 to achieve su-\nperior performance on long-horizon tasks, surpassing ex-\nisting SOTA. Moreover, we introduce a Minecraft Goal-\nObservation-Action dataset to provide the community with\nlarge-scale, high-quality data for training Minecraft agents.\nOptimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nSupplementary Material\nThe supplementary document is organized as follows:\n• Sec. A: Limitation and Future Work.\n• Sec. B: Minecraft Environment.\n• Sec. C: MGOA Dataset.\n• Sec. D: Training Details.\n• Sec. E: Evaluation Benchmark.\n• Sec. F: Experimental Results.\n• Sec. G: Case Study.\nA. Limitation and Future Work\nIn this paper, we aim to explore how agents can mimic\nhuman behavior patterns in Minecraft to accomplish vari-\nous tasks. Experimental results demonstrate that Optimus-\n2 performs exceptionally well in both atomic tasks and\nlong-horizon tasks. However, due to the lack of sufficient\nhigh-quality data for open-ended tasks (such as “building a\nhouse” and “defeating the Ender Dragon”), there remains\nsignificant room for improvement. Once such datasets are\navailable, the ability of Optimus-2 to complete open-ended\ntasks will be enhanced. Moreover, despite showing promis-\ning performance in Minecraft, we have not yet extended our\nexploration to other simulation platforms, which represents\na potential direction for future research.\nB. Minecraft\nMinecraft is an extremely popular sandbox video game de-\nveloped by Mojang Studios 2. It allows players to explore\na blockly, procedurally generated 3D world with infinite\nterrain, discover and extract raw materials, craft tools and\nitems, and build structures or earthworks. In this enviro-\nment, AI agents need to face situations that are highly sim-\nilar to the real world, making judgments and decisions to\ndeal with various environments and problems. As shown\nin Figure 7, both agents and humans are required to receive\nnatural language instructions and current observations as in-\nput, and then output low-level actions, such as mouse and\nkeyboard control commands. Therefore, Minecraft serves\n2https:\/\/www.minecraft.net\/en-us\/article\/meet-mojang-studios\n...\nObservation\n...\nAgent Action\nHuman Action\nspace\nW\nW\nKeyboard:\n {forward}\nMouse:{(2.7, 1.5)}\nKeyboard: \n{forward,jump}\nMouse: {(1.2, 0)}\nMouse: {attack}\nTask: chop a tree to get logs\nLanguage\nFigure 7.\nIllustration of behavior patterns of both human and\nagents in Minecraft.\nas an ideal open-world environment for training agent that\ncan learn human behavior patterns.\nB.1. Basic Rules\nBiomes. The Minecraft world is divided into different areas\ncalled “biomes”. Different biomes contain different blocks\nand plants and change how the land is shaped. There are 79\nbiomes in Minecraft 1.16.5, including ocean, plains, forest,\ndesert, etc. Diverse environments have high requirements\nfor the generalization of agents.\nItem. In Minecraft 1.16.5, there are 975 items can be ob-\ntained, such as wooden pickaxe\n, iron sword\n. Item can\nbe obtained by crafting or destroying blocks or attacking\nentities. For example, agent can attack cows\nto obtain\nleather\nand beef\n. Agent also can use 1 stick\nand 2\ndiamonds\nto craft diamond sword\n.\nTechnology Tree.\nIn Minecraft, the technology hierar-\nchy comprises six levels: wood\n, stone\n, iron\n, gold\n, diamond\n, and redstone\n.\nEach tool level corre-\nsponds to specific mining capabilities. Wooden tools can\nmine stone-level blocks but are incapable of mining iron-\nlevel or higher-level blocks.\nStone tools can mine iron-\nlevel blocks but cannot mine diamond-level or higher-level\nblocks.\nIron tools are capable of mining diamond-level\nblocks. Finally, diamond tools can mine blocks of any level,\nincluding redstone-level.\nGameplay progress. Progression in Minecraft primarily\ninvolves discovering and utilizing various materials and re-\nsources, each unlocking new capabilities and opportunities.\nFor instance, crafting a wooden pickaxe\nenables players\nTable 6. Action space of agent in Minecraft.\nIndex\nAgent Action\nHuman Action\nDescription\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove back.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nJump\nkey Space\nJump. When swimming, keeps the player afloat.\n6\nSneak\nkey left Shift\nSlowly move in the current direction of movement.\n7\nSprint\nkey left Ctrl\nMove quickly in the direction of current movement.\n8\nAttack\nleft Button\nDestroy blocks (hold down); Attack entity (click once).\n9\nUse\nright Button\nPlace blocks, entity, open items or other interact actions defined by game.\n10\nhotbar [1-9]\nkeys 1-9\nSelects the appropriate hotbar item.\n11\nOpen\/Close Inventory\nkey E\nOpens the Inventory. Close any open GUI.\n12\nYaw\nmove Mouse X\nTurning; aiming; camera movement.Ranging from -180 to +180.\n13\nPitch\nmove Mouse Y\nTurning; aiming; camera movement.Ranging from -180 to +180.\n14\nCraft\n-\nExecute a crafting recipe to obtain new item\n15\nSmelt\n-\nExecute a smelting recipe to obtain new item.\nto mine stone\n, which can then be used to create a stone\npickaxe\nand a furnace\n. These tools allow for the mining\nand smelting of iron ore\n. Subsequently, crafting an iron\npickaxe\nenables the extraction of diamonds\n, while a di-\namond pickaxe\ncan mine virtually any block in the game.\nSimilarly, cultivating crops facilitates breeding various an-\nimals, each providing unique resources beyond sustenance.\nDrops from enemies also serve specific purposes, with some\noffering greater utility than others. By integrating resources\nfrom mining, farming, and breeding, players can enchant\ntheir equipment, further enhancing their capabilities. Addi-\ntionally, collecting and crafting materials support construc-\ntion, enabling players to create diverse structures. Beyond\npractical functions, such as building secure bases or farms,\nconstructing personalized structures forms a significant as-\npect of the Minecraft experience. Figure 11 illustrates an\nexample of progression: crafting an iron sword\n.\nB.2. Observation and Action Spaces\nObservation. In this paper, observation space of agent is\ncompletely consistent with human players. The agent only\nreceives an RGB image with dimensions of 640 × 360 dur-\ning the gameplay process, including the hotbar, health in-\ndicators, food saturation, and animations of the player’s\nhands. It is worth helping the agent see more clearly in\nextremely dark environments, we have added a night vision\neffect for the agent, which increases the brightness of the\nenvironment during the night.\nAction Spaces. In MineRL [12] environment, agent’s ac-\ntion space is almost similar to human players. It consists of\ntwo parts: the mouse and the keyboard. The keypresses are\nLog\n48.1%\nSeed\n6.0%\nDirt\n6.0%\nStone\n8.2%\nIron\n8.1%\nGold\n8.0%\nDiamond\n7.9%\nRedstone\n7.7%\nFigure 8. Statistical information on MGOA dataset. It contains 8\nAtomic Tasks: ‘Log\n’, ‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’,\n‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’.\nresponsible for controlling the movement of agents, such as\njumping, forward, back, etc. The mouse movements are re-\nsponsible for controlling the perspective of agents and the\ncursor movements when the GUI is opened. The left and\nright buttons of the mouse are responsible for attacking and\nInstruction\nPool\nFiltering\ninteract\nfeedback\nGPT-4\nScript\nEnvironment\ncollect logs\nkeyboard: W\nmouse: [0.0, 1.0]\nMGOA Dataset\nInstruction\nItems\nFigure 9. The pipeline for generating the MGOA dataset. First, we extracted item names from the Minecraft Wiki and employed GPT-\n4 to generate corresponding instructions. These instructions were then provided as input to STEVE-1, enabling it to interact with the\nenvironment to accomplish the tasks. During task execution, each observation was paired with its corresponding action, resulting in the\ncreation of goal-observation-action pairs.\nusing or placing items. In Minecraft, precise mouse move-\nments are important when completing complex tasks that\nneed open inventory or crafting table. In order to achieve\nboth the same action space with MineDojo [10], we abstract\nthe craft and the smelt action into action space. The detailed\naction space is described in Table 6.\nC. MGOA Dataset\nIn Minecraft, there is still a lack of sufficient high-\nquality goal-observation-action pairs to support the train-\ning of Optimus-2.\nTo address this, we propose an au-\ntomated dataset construction process aimed at creating\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndatasets. Through this method. MGOA contains 25,000\nvideos, providing about 30M goal-observation-action pairs.\nIt contains 8 Atomic Tasks across 5 tech levels: ‘Log\n’,\n‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’, ‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’. Note that the Atomic Tasks in MGOA re-\nquire minimal steps and can typically be completed within\n2 ∼3 minutes. For instance, the task ‘Iron\n’ involves min-\ning iron with a stone pickaxe, without the need to gather raw\nmaterials to craft the stone pickaxe. The statistics for the\nMGOA dataset is shown in Figure 8. We provide several ex-\namples of the dataset in the MGOA samples folder within\nthe supplementary materials. We will release this dataset to\ncontribute to the development of open-world agents within\nthe community.\nC.1. Dataset Construction\nPipeline. Inspired by Li et al. [24], we employed a prior\npolicy (STEVE-1 [25] in our work) to perform specific tasks\nin Minecraft, and recorded the corresponding videos and\nactions to generate goal-observation-action pairs. As illus-\ntrated in Figure 9, we employed a custom script to extract\nitem names from the Minecraft Wiki3. Using these item\nnames, we queried GPT-44 with a predefined prompt tem-\nplate to generate task instructions, thereby constructing an\nInstruction Pool. The task instructions from the Instruc-\ntion Pool serve as input to STEVE-1 [25], enabling it to\ninteract with the environment to complete the tasks. During\ntask execution, each frame and corresponding action were\nrecorded and stored. To expedite data generation, we instan-\ntiated multiple policies and used parallelization to quickly\nproduce large amounts of data.\nData Filtering. We judged task success based on environ-\nmental feedback. For example, feedback like “obtained new\nitem, diamond axe” indicated that the task “craft a diamond\naxe” was successfully completed. During the dataset gen-\neration process, we observed a significant amount of low-\nquality video data due to limitations in the policy’s abil-\nity to follow instructions. Examples of low-quality data in-\ncluded task failures or task completion timeouts. To address\nthis issue, we established two filtering criteria to ensure data\nquality: (1) only retaining data from successfully completed\ntasks, and (2) removing data for tasks that lasted longer than\n2 minutes. These criteria allowed us to automatically filter\nout low-quality data, significantly reducing the cost of con-\nstructing the dataset. As a result, we obtained a high-quality\nMGOA dataset consisting of 25,000 samples.\n3https:\/\/minecraft.wiki\/\n4https:\/\/openai.com\/index\/gpt-4-research\/\nC.2. Comparison with Existing Datasets\nPrevious gameplay videos were primarily obtained through\ntwo methods below.\nVideo Platform: For example, MineDojo [10] collected\ngame videos uploaded by human players on platforms such\nas YouTube and Twitter, combining the video content with\ncorresponding titles or subtitles to form video-text pairs.\nHowever, this dataset lacked recorded actions. To address\nthis, VPT [1] used an Inverse Dynamics Model (IDM) to\ngenerate action sequences from the videos. However, the\nactions predicted by the IDM model are only approxima-\ntions, which introduces a potential risk of misalignment be-\ntween the frames and the corresponding actions.\nHuman Contractors: VPT [1] hired human players to\nfreely explore Minecraft and used the frames and actions\nto construct a video-action dataset. However, this dataset\nlacked explicit natural language instructions.\nTo create\ngoal-observation-action pairs, STEVE-1 [25] used GPT-3.5\nto generate specific task descriptions based on the game-\nplay, thereby integrating natural language instructions into\nthe dataset. However, they provide only approximately 32k\naligned goal-observation-action pairs, which remains a rel-\natively scarce amount of data.\nIn addition, some work [32, 43] have utilized GPT-4V\nto generate image captions, task planning, and reflections,\nthereby creating image-text pairs that form instruction-\nfollowing datasets.\nDistinct from the aforementioned datasets, the MGOA\ndataset directly captures agents performing specific tasks,\noffering clear natural language instructions with a one-to-\none correspondence between observations and actions. Fur-\nthermore, through rigorous data filtering, redundant action\nsequences that do not contribute to task completion are ex-\ncluded from MGOA. In addition, compared to the small-\nscale goal-observation-action datasets currently available,\nMGOA offers 25,000 videos, encompassing approximately\n30 million goal-observation-action pairs. This dataset is not\nonly significantly larger but also highly scalable in an auto-\nmated manner.\nD. Training Details\nD.1. Training Pipeline\nOne of the key factors in implementing our proposed\nmethod lies in the efficient alignment of language with the\nobservation-action sequence, and subsequently translating\nlanguage space into the action space. To tackle this prob-\nlem, we adopt a two-phase training approach. First, we\nalign language with the observation-action sequence via be-\nhavior pre-training. Then, we transform the language space\ninto the action space through action fine-tuning.\nBehavior Pre-training: During the pre-training phase, we\nintegrated the Vision-guided Behavior Encoder into the\nTable 7. Hyperparameter setting for pre-training and finetuning.\nHyperparameter\nPre-training\nFine-tuning\nOptimizer\nAdamW\nAdamW\nLearning Rate\n0.0001\n0.00004\nWarmup Steps\n0\n0\nEpochs\n5\n10\nBatch Size\n32\n2048\nNum. Frames\n5M\n80M\nLoRA r\n64\n64\nLoRA alpha\n128\n128\nmodel.\nWe used OpenAI Contractor Dataset [1] and a\nsubset of MGOA as training data, which comprised ap-\nproximately 5,000 videos. To balance efficiency and ef-\nfectiveness, we freeze the visual encoder, then tune the\nVision-guided Behavior Encoder along with a large lan-\nguage model (LoRA [15]). During pre-training, we set the\nlearning rate to 0.0001 and trained for 5 epochs. The hyper-\nparameter settings are shown in Table 7.\nAction Fine-tuning:\nDuring the fine-tuning phase, we\nadapted the general MLLM DeepSeek-VL-1.3B [29] to the\nMinecraft environment, transitioning the model’s output\nspace from language to low-level actions. We fine-tuned\nit using OpenAI Contractor Dataset [1] and MGOA, which\ncomprises approximately 20,000 videos. In this phase, we\nfreeze the Vision-guided Behavior Encoder, visual encoder,\nand large language model (LoRA), and only fine-tuned the\naction head. During fine-tuning, we set the learning rate to\n0.00004 and train for 10 epochs. The hyperparameter set-\ntings are shown in Table 7.\nD.2. Implementation Details\nFor the planner, we follow Li et al. [24], employing Mul-\ntimodal Hybrid Memory empowered GPT-4V for planning\nand reflection. For the policy, we train the GOAP through\nthe above pipeline. All experiments were conducted on 8x\nNVIDIA L40 GPUs. For the MGOA dataset, data collec-\ntion and filtering were conducted in parallel, taking approx-\nimately 7 days.\nTraining required around 2 days, while\ninference and evaluation on atomic tasks, long-horizon\ntasks, and open-ended instruction tasks took approximately\n4 days.\nE. Benchmark\nE.1. Evaluation Tasks\nThe evaluation tasks are divided into three categories:\nAtomic Tasks, Long-horizon Tasks, and Open-ended In-\nstruction Tasks. For each task, the agent’s environment is\n(a) chop a tree to get logs\n(b) mine dirt\n(c) collect seeds\n(d) dig down to mine stone\nFigure 10. Examples of Atomic Task. The agent must follow the instructions to collect resources. These four tasks represent the basic\ncapabilities of the agent. The more resources are collected, the stronger the basic capabilities of the agent will be.\nrandomly initialized each time, and every task is executed\nat least 30 times. For Atomic Tasks, we follow the setting of\nprior work [25, 43], which requires the agent to execute the\ntask within 2 minutes. We then report the average reward\nfor the task, defined as the number of items obtained. For\nOpen-ended Instruction Tasks and Long-horizon Tasks, we\nreport the average success rate (SR) for each task.\nAtomic Tasks. As shown in Figure 10, Atomic Tasks are\nshort-term skills in Minecraft, such as “chop a tree to get\nlogs\n”, “mine dirt\n”, “collect seeds\n”, and “dig down to\nmine stone\n”, etc.\nLong-horizon Tasks.\nAs shown in Figure 11, Long-\nHorizon Tasks are a sequence of Atomic Tasks. For exam-\nple, “craft an iron sword from scratch” requires complet-\ning the atomic tasks of “chop 7 logs”, “craft 21 planks”,\n“craft 5 sticks”, “craft 1 crafting table”, and so on. These\nAtomic Tasks are interdependent, meaning that the failure of\nany single atomic task will result in the failure of the entire\nLong-horizon Task.\nOpen-ended Instruction Tasks. Open-Ended Instruction\nTasks are not limited to predefined text formats; rather, they\ninvolve flexible language directives that prompt the agent\nto accomplish long-horizon tasks. These tasks evaluate the\nagent’s capacity to interpret and execute instructions ex-\npressed in open-ended natural language. We selected Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and\nCompass\nas evaluation tasks. Instruction for each task\nare shown in Table 8, Table 9, Table 10, Table 11 and Table\n12.\nE.2. Baselines\nIn this section, we provide a brief overview of existing\nMinecraft agents and compare them with our proposed\nOptimus-2. Current agents can be broadly categorized into\ntwo types: policy-based agents and planner-policy agents.\nPolicy-based Agents. Policy-based agents [1–3, 10, 25] re-\nfer to those trained through reinforcement learning or imi-\ntation learning, capable of completing atomic tasks within\nMinecraft. However, due to limitations in instruction un-\nderstanding and reasoning abilities, they struggle to accom-\nplish long-horizon tasks.\nPlanner-Policy Agents. Planner-policy agents [20, 24, 32,\n41–43] refer to non-end-to-end architectures that utilize a\nMLLM (Multi-Layered Language Model) as a planner to\ndecompose complex instructions into a sequence of sub-\ngoals executable by a policy. While significant progress has\n(a) Chop 7 logs\n(b) Craft 21 planks\n(c) Craft 5 sticks\n(d) Craft 1 crafting table\n(e) Craft 1 wooden pickaxe\n(f) Mine 11 cobblestone\n(g) Craft 1 furnace\n(h) Craft 1 stone pickaxe\n(i) Dig down more deeper to find iron ore\n(j) Mine 2 iron ores\n(k) Smelt 2 iron ingots\n(l) Craft 1 iron sword\nFigure 11. An example of long-horizon task “crafting an iron sword”. The agent must sequentially complete each atomic task in order to\nsuccessfully craft the iron sword. Failure in any of the atomic tasks will result in the failure of the entire long-horizon task.\nbeen made, the current performance bottleneck stems from\nthe policy’s ability to effectively understand and execute the\nsub-goals generated by the planner.\nComparison with Existing Agents. As a core contribu-\ntion of this work, we propose a novel Goal-Observation-\nAction Conditioned Policy, GOAP. It integrates two key\ncomponents: an Action-Guided Behavior Encoder for mod-\neling observation-action sequences, and an MLLM for\naligning sub-goals with these sequences. Leveraging the\nMLLM’s advanced understanding of open-ended instruc-\ntions, GOAP demonstrates superior instruction-following\ncapabilities compared to existing policies. On top of GOAP,\nthe proposed agent, Optimus-2, exhibits superior perfor-\nmance in long-horizon tasks, outperforming the current\nstate-of-the-art across all seven task groups.\nF. Experimental Results\nIn this section, we report the experimental results of\nOptimus-2 on each Long-horizon task.\nF.1. Results on Long-horizon Task\nIn this section, we report the results of Optimus-2 on each\nLong-horizon Task, with details including task name, num-\nbers of sub-goals, success rate (SR), and eval times. As\nshown in Tables 13 and 14, Optimus-2 demonstrates supe-\nrior performance across all 67 Long-horizon Tasks. Since\nOptimus-2 is randomly initialized in arbitrary environments\nfor each task execution, the experimental results also high-\nlight its generalization capability across diverse environ-\nments.\nG. Case Study\nIn this section, we provide additional cases to illustrate the\ndifferences in the ability of VPT (text) [1], STEVE-1 [25],\nand Optimus-2 to perform Open-ended Instruction Tasks.\nWe provide different open-ended instructions requiring the\nagent to perform tasks across various biomes. As shown in\nFigure 12, Figure 13, and Figure 14, Optimus-2 effectively\ncompletes all tasks, while VPT (text) and STEVE-1 fail due\nto limitations in language understanding and multimodal\nperception capabilities. Moreover, we provide several demo\nvideos of Optimus-2 performing long-horizon tasks in the\nOptimus2 videos folder within the supplementary ma-\nterials.\nTable 8. Open-ended instruction examples of “Craft a torch”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, I need to locate and chop down a couple of trees to collect 4 logs.\n2. Next, I should craft these logs into 13 planks. 3. Then, I need to use some of the\nplanks to craft 3 sticks. 4. I will use some of the planks to craft a crafting table. 5.\nNext, I should craft a wooden pickaxe using some of the planks and sticks. 6. With\nthe wooden pickaxe, I need to find and mine at least 1 coal from a nearby coal deposit\nor cave. 7. Finally, I can use one stick and one coal to craft a torch.\nPlan 2 1. First, I need to find a tree to gather 4 logs. 2. Next, I have to craft the logs\ninto planks. 3. From the planks, I will craft 3 sticks. 4. I need to craft a crafting table\nusing some of the planks. 5. I should then craft a wooden pickaxe to mine coal. 6. I\nwill locate and mine 1 piece of coal using the wooden pickaxe. 7. Finally, I will use\nthe coal and sticks to craft a torch.\nPlan 3 1. I have to find a tree to get 4 logs. 2. Next, I have to craft planks from the\nlogs. 3. Get sticks from some of the planks. 4. I need a crafting table, which requires\nplanks to craft. 5. I should craft a wooden pickaxe using the crafting table. 6. I need\nto find and mine some coal using the wooden pickaxe. 7. Finally, with the stick and\ncoal, I can craft a torch.\nGLM4V\nPlan 1 1. Locate and chop down a tree to gather 4 logs. 2. Convert the logs into 13\nplanks using the crafting menu. 3. Use the planks to craft a crafting table. 4. Craft 3\nsticks from the remaining planks. 5. With the crafting table and sticks, craft a wooden\npickaxe if one is not already available. 6. Use the wooden pickaxe to dig for coal\nor kill some Creepers or zombies for coal drops. 7. Once 1 coal is obtained, use the\ncrafting table to combine it with a stick to craft 1 torch.\nPlan 2 1. Locate and chop down a tree to gather the 4 logs required. 2. Convert the\nlogs into 13 planks using the crafting menu, as this will be the material needed for\ncrafting the crafting table and the wooden pickaxe. 3. Use some of the planks to craft\na crafting table, which is necessary for advanced crafting recipes. 4. Craft a wooden\npickaxe using some of the planks and sticks. The wooden pickaxe will be used to\nmine coal. 5. Gather 3 sticks, which can be obtained from the logs or by crafting them\nfrom planks. 6. Use the wooden pickaxe to mine coal. Only 1 coal is needed to craft\na torch. 7. Once all materials are gathered, open the crafting table and combine 1 coal\nand 1 stick to craft a torch.\nPlan 3 1. Locate and gather 4 logs from the nearest trees. 2. Convert the logs into\n13 planks using the crafting menu. 3. Use some planks to craft a crafting table for\nmore complex recipes. 4. Craft a wooden pickaxe using some planks and sticks to\ngather coal and other resources. 5. Convert some planks into 3 sticks required for\ncrafting torches and the wooden pickaxe. 6. Use the wooden pickaxe to mine coal\nore, ensuring to gather at least 1 coal. 7. With all materials gathered, open the crafting\ntable and craft 1 torch using 1 coal and 1 stick.\nTable 9. Open-ended instruction examples of “Craft a rail”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and punch down a tree to acquire 7 logs. 2. Convert these logs\ninto planks to obtain a total of 25 planks. 3. Use some of these planks to craft 5 sticks.\n4. Create a crafting table using 4 planks. 5. Craft a wooden pickaxe using 3 planks\nand 2 sticks. 6. Use the wooden pickaxe to mine 11 cobblestones. 7. Craft a furnace\nusing 8 cobblestones. 8. Craft a stone pickaxe using 3 cobblestones and 2 sticks. 9.\nUse the stone pickaxe to mine 6 iron ores. 10. Smelt the iron ores in the furnace to\ncreate 6 iron ingots. 11. Finally, craft a rail using the iron ingots and sticks.\nPlan 2 1. I have to find a tree to get 7 logs. 2. Next, I have to craft the logs into 25\nplanks. 3. Craft 5 sticks from the planks. 4. I need to create a crafting table. 5. Use\nthe crafting table to craft a wooden pickaxe. 6. Use the wooden pickaxe to mine 11\npieces of cobblestone. 7. I’ll then craft a furnace using the cobblestone. 8. Craft a\nstone pickaxe with the cobblestone and sticks. 9. Use the stone pickaxe to mine 6\npieces of iron ore. 10. Smelt the iron ore in the furnace to obtain 6 iron ingots. 11.\nFinally, use the crafting table to craft a rail using the iron ingots and sticks.\nPlan 3 1. I have to find a tree to get 7 logs. 2. Next, I need to craft planks from\nthe logs. 3. Get sticks from the planks. 4. Craft a crafting table using some of the\nplanks. 5. Craft a wooden pickaxe with sticks and planks to mine stone. 6. Mine 11\ncobblestone with the wooden pickaxe. 7. Craft a furnace using the cobblestone. 8.\nCraft a stone pickaxe using cobblestone and sticks. 9. Use the stone pickaxe to mine\n6 iron ore. 10. Smelt the iron ore in the furnace to get 6 iron ingots. 11. Finally, use\nthe iron ingots and sticks to craft a rail.\nGLM4V\nPlan 1 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down\na crafting table to access more crafting recipes. 5. Use the crafting table to craft\na wooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones from the\nground or from stone blocks. 7. Craft a furnace using the cobblestones. 8. Upgrade\nthe wooden pickaxe to a stone pickaxe by crafting it at the crafting table with the\ncobblestones. 9. Use the stone pickaxe to mine 6 iron ore from the appropriate rock\nlayers. 10. Smelt the 6 iron ore in the furnace to produce 6 iron ingots. 11. Finally,\nuse the crafting table to craft 1 rail using the 6 iron ingots and 1 stick.\nPlan 2 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu\nto convert the logs into 25 planks. 3. Craft 5 sticks using the planks in the crafting\nmenu. 4. Place a crafting table to enable the crafting of more complex items. 5.\nCraft a wooden pickaxe using some of the planks and sticks. 6. Use the wooden\npickaxe to mine 11 cobblestones from stone outcrops. 7. Construct a furnace using\nthe cobblestones. 8. Upgrade to a stone pickaxe by crafting it with the cobblestones\nand sticks. 9. Use the stone pickaxe to mine 6 iron ore from iron veins. 10. Smelt the\n6 iron ore in the furnace to produce 6 iron ingots. 11. Finally, use the crafting table to\ncraft 1 rail using the iron ingots and sticks.\nPlan 3 1. Find and chop down a tree to gather 7 logs. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down a\ncrafting table to access more crafting options. 5. Craft a wooden pickaxe using some\nof the planks and sticks. 6. Use the wooden pickaxe to mine 11 cobblestones from\nstone. 7. Construct a furnace using some of the cobblestones. 8. Upgrade the wooden\npickaxe to a stone pickaxe by crafting it with the cobblestones. 9. Search for and\nmine 6 iron ore blocks with the stone pickaxe. 10. Smelt the iron ore in the furnace to\nproduce 6 iron ingots. 11. Finally, use the crafting table to craft 1 rail using the iron\ningots and sticks.\nTable 10. Open-ended instruction examples of “Craft a golden shovel”\nPlanner Instruction\nGPT4V\nPlan 1 1. Start by finding and chopping down trees to collect 9 logs. 2. Convert the logs into 35 planks\nusing the crafting interface. 3. Use some of the planks to craft 8 sticks. 4. Create a crafting table using\nsome of the planks. 5. Craft a wooden pickaxe. 6. Mine 19 cobblestone. 7. Craft a furnace using 8 of the\ncobblestones. 8. Craft a stone pickaxe. 9. Mine 3 iron ores using the stone pickaxe. 10. Smelt the iron ore\nin the furnace. 11. Craft an iron pickaxe using sticks and iron ingots. 12. Locate and mine a gold block\nusing the iron pickaxe. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Craft a golden\nshovel using the gold ingot and sticks.\nPlan 2 1. Start by finding and chopping down trees to collect 9 logs. 2. Use the logs to craft planks. 3.\nCraft sticks. 4. Use some of the planks to craft a crafting table. 5. Use the crafting table to craft a wooden\npickaxe. 6. Use the wooden pickaxe to mine 19 pieces of cobblestone. 7. Use some of the cobblestone to\ncraft a furnace. 8. Use the remaining cobblestone to craft a stone pickaxe. 9. Mine iron ore with the stone\npickaxe. 10. Smelt the iron ore in the furnace to obtain 3 iron ingots. 11. Use 2 of the iron ingots and sticks\nto craft an iron pickaxe. 12. Use the iron pickaxe to mine gold ore or find a gold block; you need at least 1\ngold ingot. 13. If mining gold ore, smelt it in the furnace to obtain gold ingots. 14. Use the gold ingot and\nstick to craft a golden shovel.\nPlan 3 1. Find and cut down trees to collect 9 logs. 2. Use the logs to craft 35 planks. 3. Craft 8 sticks\nusing the planks. 4. Use some of the planks to craft a crafting table. 5. Craft a wooden pickaxe using planks\nand sticks. 6. Mine cobblestone until you have 19 pieces. 7. Use some cobblestone and the crafting table to\ncraft a furnace. 8. Craft a stone pickaxe using cobblestone and remaining sticks. 9. Use the stone pickaxe\nto mine 3 pieces of iron ore. 10. Smelt the iron ore in the furnace to get 3 iron ingots. 11. Craft an iron\npickaxe. 12. Use the iron pickaxe to mine gold ore to obtain gold blocks. 13. Use 1 gold ingot to craft a\ngolden shovel. 14. use a gold ingot and sticks to craft a golden shovel using the crafting table.\nGLM4V\nPlan 1 1. Find and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to expand the crafting options. 5. Craft\na wooden pickaxe with the planks and sticks. 6. Cine cobblestone until you have a total of 19. 7. Craft a\nfurnace using 8 cobblestone. 8. Use the furnace to smelt the cobblestone into 19 stone, then use the crafting\ntable to craft a stone pickaxe with the stone and sticks. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks at the crafting table. 12. Use the iron pickaxe to mine a gold block. 13. Smelt the gold block in\nthe furnace to get 1 gold ingot. 14. Finally, use the crafting table to craft the golden shovel using 1 gold\ningot and 2 sticks.\nPlan 2 1. Find and gather 9 logs from nearby trees. 2. Convert the logs into 35 planks using a crafting\ntable. 3. Craft 8 sticks from the planks. 4. Build a crafting table. 5. Use the crafting table to craft a wooden\npickaxe with the planks and sticks. 6. Use the wooden pickaxe to mine 19 cobblestones from stone. 7.\nConstruct a furnace to smelt materials. 8. Craft a stone pickaxe using the cobblestones and sticks. 9. Use\nthe stone pickaxe to mine 3 iron ore from iron deposits. 10. Smelt the 3 iron ore in the furnace to produce\n3 iron ingots. 11. Craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine a\ngold block. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Finally, craft the golden shovel\nusing the gold ingot and stick in the crafting table.\nPlan 3 1. Find and gather 9 logs from nearby trees. 2. Use the crafting menu to convert the logs into 35\nplanks. 3. Craft 8 sticks from the planks. 4. Craft a crafting table using some of the planks. 5. Use the\ncrafting table to craft a wooden pickaxe with some of the sticks and planks. 6. Use the wooden pickaxe to\nmine 19 cobblestones from stone. 7. Craft a furnace using some of the cobblestones. 8. Use the furnace to\nsmelt the cobblestones into 19 stone, then craft a stone pickaxe with the sticks and stone. 9. Mine 3 iron\nore from the appropriate rock faces. 10. Smelt the iron ore in the furnace to produce 3 iron ingots. 11. Use\nthe crafting table to craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine\na gold block. 13. Smelt the gold block in the furnace to produce 1 gold ingot. 14. Finally, use the crafting\ntable to craft the golden shovel with the gold ingot and 2 sticks.\nTable 11. Open-ended instruction examples of “Craft a diamond pickaxe”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some\nof the planks to craft 8 sticks. 4. Create a crafting table using 4 planks. 5. With the crafting table, craft\na wooden pickaxe using planks and sticks. 6. Use the wooden pickaxe to collect at least 11 cobblestones\nfrom stone. 7. With the cobblestones, craft a furnace. 8. Use more cobblestones to craft a stone pickaxe. 9.\nUse the stone pickaxe to mine 3 iron ores. 10. Smelt the iron ores into 3 iron ingots using the furnace. 11.\nCraft an iron pickaxe with iron ingots and sticks. 12. Use the iron pickaxe to mine 3 diamonds. 13. Finally,\ncraft the diamond pickaxe using the diamonds and sticks.\nPlan 2 1. Begin by locating a tree to gather 9 logs. 2. Convert the logs into planks. 3. Use some of the\nplanks to craft sticks. 4. Craft a crafting table to facilitate further crafting. 5. Create a wooden pickaxe to\nstart mining stone. 6. Mine 11 cobblestones. 7. Use the cobblestones to craft a furnace. 8. Craft a stone\npickaxe to advance to mining iron ore. 9. Locate and mine 3 iron ores using the stone pickaxe. 10. Smelt\nthe iron ores in the furnace to produce 3 iron ingots. 11. Use two of the iron ingots and sticks to craft an\niron pickaxe. 12. Locate a diamond ore vein and mine 3 diamonds using the iron pickaxe. 13. Finally, craft\nthe diamond pickaxe with the 3 diamonds and sticks.\nPlan 3 1. I need to find a tree to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some planks to\ncraft 8 sticks. 4. Craft a crafting table using some of the planks. 5. With the crafting table, make a wooden\npickaxe to mine stone. 6. Mine 11 cobblestones using the wooden pickaxe. 7. Craft a furnace using the\ncollected cobblestones. 8. Make a stone pickaxe to efficiently mine iron ore. 9. Find and mine 3 iron ores\nusing the stone pickaxe. 10. Smelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron\npickaxe using some planks, sticks, and the iron ingots. 12. Locate a source of diamonds and use the iron\npickaxe to mine 3 diamonds. 13. Finally, use the crafting table along with the sticks and diamonds to craft\na diamond pickaxe.\nGLM4V\nPlan 1 1. Start by finding trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to make a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ores. 10.\nSmelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine 3 diamonds, which may require exploring caves or mining deep underground with the\niron pickaxe. 13. Finally, use the crafting table to combine the diamonds and the sticks to craft a diamond\npickaxe.\nPlan 2 1. Locate and chop down trees to gather a total of 9 logs. 2. Use the logs to craft 35 planks, as this\nwill be the base material for several other items. 3. Craft 8 sticks from the planks, which will be needed to\ncreate various tools. 4. Craft a crafting table using some of the planks, as this is essential for crafting more\ncomplex items. 5. Use the crafting table to craft a wooden pickaxe with the planks and sticks, which will be\nused to mine cobblestone. 6. Mine 11 cobblestone using the wooden pickaxe. 7. Craft a stone pickaxe with\nthe cobblestone and sticks, as this is more durable and efficient for mining. 8. Construct a furnace using the\ncobblestone, which will be required to smelt ores into ingots. 9. Mine 3 iron ore with the stone pickaxe. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks, as this is necessary to mine diamonds. 12. Find and mine diamond ore with the iron pickaxe\nto gather 3 diamonds. 13. Finally, use the crafting table to craft a diamond pickaxe with the diamonds and\nsticks, completing the task.\nPlan 3 1. Begin by locating trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine diamond ore with the iron pickaxe to obtain 3 diamonds. 13. Finally, use the crafting\ntable to craft a diamond pickaxe with the diamonds.\nTable 12. Open-ended instruction examples of “Craft a compass”\nPlanner Instruction\nGPT4V\nPlan 1 1. Locate a tree and gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks from the\nplanks. 4. Use some planks to craft a crafting table. 5. With the crafting table, craft a wooden pickaxe. 6.\nMine cobblestone until you have 12 pieces. 7. Upgrade your tools by crafting a stone pickaxe. 8. Construct\na furnace using some of the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron\nore in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots. 12. Find and\nmine redstone using the iron pickaxe to obtain 1 redstone dust. 13. Finally, use the remaining iron ingots\nand redstone dust at the crafting table to craft a compass.\nPlan 2 1. Start by finding and chopping down trees to collect 10 logs. 2. Use the logs to craft 38 planks. 3.\nCraft sticks using the planks to get 8 sticks. 4. Craft a crafting table from the planks. 5. Use the crafting\ntable to create a wooden pickaxe. 6. Locate and mine 12 pieces of cobblestone using the wooden pickaxe.\n7. Craft a stone pickaxe from cobblestone and sticks. 8. Use the cobblestone to craft a furnace. 9. Mine\n7 iron ores using the stone pickaxe. 10. Smelt the iron ore in the furnace to get 7 iron ingots. 11. Craft\nan iron pickaxe using iron ingots and sticks. 12. Search for and mine redstone using the iron pickaxe. 13.\nFinally, use the iron ingots and redstone to craft a compass.\nPlan 3 1. First, I need to find a tree and collect 10 logs. 2. Convert the logs into planks, ensuring I get at\nleast 38 planks. 3. Use some of the planks to craft sticks, collecting at least 8 sticks. 4. Use some of the\nplanks to create a crafting table. 5. Craft a wooden pickaxe. 6. Locate cobblestone and mine at least 12\npieces using the wooden pickaxe. 7. Use some cobblestone to craft a furnace. 8. Craft a stone pickaxe for\nimproved mining. 9. Search for iron ore and mine at least 7 using the stone pickaxe. 10. Smelt the iron ore\nin the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using some of the iron ingots and sticks.\n12. Locate a redstone ore and mine it using the iron pickaxe to obtain redstone. 13. Finally, use 4 of the\niron ingots and the redstone to craft a compass at the crafting table.\nGLM4V\nPlan 1 1. Locate and gather 10 logs from nearby trees. 2. Convert the logs into 38 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to access more crafting options. 5. Use\nthe crafting table to make a wooden pickaxe. 6. With the wooden pickaxe, mine 12 cobblestones from\nstone. 7. Craft a stone pickaxe using the cobblestones and the crafting table. 8. Construct a furnace to smelt\nmaterials. 9. Use the stone pickaxe to mine 7 iron ores from the appropriate rock formations. 10. Smelt\nthe 7 iron ores in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe with the iron ingots at the\ncrafting table. 12. Mine for redstone, which can sometimes be found in the ground. 13. Use the crafting\ntable to craft a compass with the iron ingot and redstone.\nPlan 2 1. Locate and chop down trees to gather the required 10 logs. 2. Convert the logs into 38 planks\nusing the crafting menu. 3. Craft 8 sticks from the planks. 4. Use some planks to craft a crafting table.\n5. With the crafting table and the sticks and planks crafted, make a wooden pickaxe. 6. Use the wooden\npickaxe to mine 12 cobblestone. 7. Craft a stone pickaxe using the cobblestone. 8. Construct a furnace\nwith the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron ore in the furnace to\nproduce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots and the crafting table. 12. Search for\nredstone; gather at least 1 redstone. 13. Finally, use the crafting table to combine the redstone with the iron\ningots to craft a compass.\nPlan 3 1. Find and chop down trees to gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks\nfrom the planks. 4. Craft a crafting table using some of the planks. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine cobblestone until you have at least 12. 7. Upgrade the\nwooden pickaxe to a stone pickaxe using the cobblestone. 8. Construct a furnace out of the cobblestone. 9.\nUse the stone pickaxe to mine 7 iron ore. 10. Smelt the 7 iron ore in the furnace to produce 7 iron ingots.\n11. Craft an iron pickaxe using the iron ingots and sticks at the crafting table. 12. Search for redstone; you\nwill need at least 1 piece. 13. Finally, use the crafting table to combine the redstone with the iron ingots to\ncraft a compass.\nTable 13. The results of Optimus-2 on the Wood Group, Stone Group, and Iron Group. SR denotes success rate.\nGroup\nTask\nSub-Goal Num.\nSR\nEval Times\nWood\nCraft a wooden shovel\n6\n100.00\n40\nCraft a wooden pickaxe\n5\n100.00\n30\nCraft a wooden axe\n5\n97.37\n38\nCraft a wooden hoe\n5\n100.00\n30\nCraft a stick\n4\n100\n30\nCraft a crafting table\n3\n93.02\n43\nCraft a wooden sword\n5\n100.00\n30\nCraft a chest\n4\n100.00\n30\nCraft a bowl\n4\n100.00\n30\nCraft a ladder\n4\n100.00\n30\nStone\nCraft a stone shovel\n8\n89.47\n57\nCraft a stone pickaxe\n10\n98.00\n50\nCraft a stone axe\n10\n94.44\n54\nCraft a stone hoe\n8\n95.74\n47\nCraft a charcoal\n9\n85.71\n42\nCraft a smoker\n9\n90.00\n40\nCraft a stone sword\n8\n95.45\n44\nCraft a furnace\n9\n94.44\n36\nCraft a torch\n8\n89.36\n47\nIron\nCraft an iron shovel\n13\n52.08\n48\nCraft an iron pickaxe\n13\n56.00\n50\nCraft an iron axe\n13\n48.15\n54\nCraft an iron hoe\n13\n56.60\n53\nCraft a bucket\n13\n45.10\n51\nCraft a hopper\n14\n54.90\n51\nCraft a rail\n13\n51.02\n49\nCraft an iron sword\n12\n56.52\n46\nCraft a shears\n12\n48.28\n58\nCraft a smithing table\n12\n53.33\n45\nCraft a tripwire hook\n13\n55.56\n45\nCraft a chain\n13\n52.17\n46\nCraft an iron bars\n12\n51.06\n47\nCraft an iron nugget\n12\n54.55\n44\nCraft a blast furnace\n14\n52.27\n44\nCraft a stonecutter\n13\n52.27\n44\nTable 14. The results of Optimus-2 on the Gold group, Diamond Group, Redstone Group, and Armor Group. SR denotes success rate.\nGroup\nTask\nSub Goal Num.\nSR\nEval Times\nGold\nCraft a golden shovel\n16\n8.93\n56\nCraft a golden pickaxe\n16\n11.29\n62\nCraft a golden axe\n16\n8.93\n56\nCraft a golden hoe\n16\n8.96\n67\nCraft a golden sword\n16\n8.20\n61\nSmelt and craft an golden ingot\n15\n9.68\n62\nDiamond\nCraft a diamond shovel\n15\n15.91\n44\nCraft a diamond pickaxe\n15\n11.76\n34\nCraft a diamond axe\n16\n11.00\n36\nCraft a diamond hoe\n15\n15.91\n44\nCraft a diamond sword\n15\n11.11\n36\nDig down and mine a diamond\n15\n11.42\n35\nCraft a jukebox\n15\n13.15\n38\nRedstone\nCraft a piston\n16\n28.33\n60\nCraft a redstone torch\n16\n27.69\n65\nCraft an activator rail\n18\n25.81\n62\nCraft a compass\n23\n28.36\n67\nCraft a dropper\n16\n30.30\n66\nCraft a note block\n16\n25.40\n63\nArmor\nCraft shield\n14\n45.16\n62\nCraft iron chestplate\n14\n43.86\n57\nCraft iron boots\n14\n40.35\n57\nCraft iron leggings\n14\n8.57\n35\nCraft iron helmet\n14\n47.46\n56\nCraft diamond helmet\n17\n9.09\n33\nCraft diamond chestplate\n17\n7.89\n38\nCraft diamond leggings\n17\n5.41\n37\nCraft diamond boots\n17\n12.50\n40\nCraft golden helmet\n17\n13.89\n36\nCraft golden leggings\n17\n12.20\n41\nCraft golden boots\n17\n10.26\n39\nCraft golden chestplate\n17\n10.00\n40\nAgent\nInstruction: I want to get some logs to craft wooden sword, what should I do first?\nSuccess\n❌\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n✔\nFigure 12. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to get some logs\nto craft wooden sword, what should I do first?”. Existing policies are limited by their instruction comprehension abilities and thus fail to\ncomplete the task, whereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I need coal for heating. What should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 13. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need coal for heating.\nWhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I want to collect some seeds, Can you help me?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 14. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to collect some\nseeds, Can you help me?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task,\nwhereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Optimus-2：基于多模态大语言模型的Minecraft智能体\n\n## 📌 背景痛点\/本文动机\n在开放世界环境中，构建能够模仿人类行为模式并完成各种任务的智能体一直是人工智能领域的长期目标。然而，要使智能体有效地学习跨任务的行为模式，关键挑战在于建模观察、动作和语言之间的复杂关系。现有的智能体在处理开放世界环境中的多样化任务时，通常采用任务规划器和目标条件策略的框架。尽管现有的智能体在利用多模态大语言模型（MLLM）作为规划器方面取得了进展，但目标条件策略的性能瓶颈仍然存在。现有的策略通常忽略了观察和动作之间的关系，并且难以建模开放式的子目标和观察-动作序列之间的关系。\n\n## 🚀 核心方法\n为了解决上述挑战，本文提出了Optimus-2，一个新颖的Minecraft智能体，它结合了MLLM进行高级规划，并采用目标-观察-动作条件策略（GOAP）进行低级控制。GOAP包含两个关键组件：\n\n💡 创新点1：动作引导的行为编码器\n动作引导的行为编码器用于建模观察-动作序列。它首先使用因果感知器将动作嵌入到观察特征中，利用任务相关的动作信息作为指导来调整观察特征，从而为动作预测提供细粒度的观察-动作信息。此外，为了在不超出输入长度限制的情况下对长期观察-动作序列进行建模，引入了历史聚合器，将当前观察-动作信息与历史序列动态地整合成固定长度的行为标记。行为标记可以以固定且适当的长度捕获观察-动作序列的长期依赖关系，使智能体能够预测与观察-动作序列逻辑一致的动作，而不是仅基于当前观察进行孤立的动作预测。\n\n💡 创新点2：多模态大语言模型\n为了明确编码子目标的语义，引入了MLLM作为GOAP的骨干网络。它将子目标与行为标记对齐，以自回归方式预测后续动作。利用MLLM的语言理解和多模态感知能力，它可以更好地整合开放式子目标和观察-动作序列的特征，从而增强策略的动作预测能力。\n\n## 📈 实验结果\n在Minecraft的开放世界环境中进行了广泛的评估，实验结果表明Optimus-2在原子任务、长期任务和开放式指令任务中表现出优异的性能。与之前的SOTA相比，Optimus-2在原子任务、长期任务和开放式子目标任务上分别实现了平均27%、10%和18%的提升。\n\n## 💬 可借鉴之处\n本文提出的Optimus-2智能体及其GOAP策略为开放世界环境中的智能体设计提供了新的思路。动作引导的行为编码器和MLLM的引入有效地解决了观察、动作和语言之间的复杂关系建模问题，使得智能体能够更好地理解和执行开放式指令。此外，本文提出的MGOA数据集为训练Minecraft智能体提供了高质量的数据资源，有助于推动相关研究的发展。","llm_summary_res_status":200}
{"title":"Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos","authors":"Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune","summary":"Pretraining on noisy, internet-scale datasets has been heavily studied as a\ntechnique for training models with broad, general capabilities for text,\nimages, and other modalities. However, for many sequential decision domains\nsuch as robotics, video games, and computer use, publicly available data does\nnot contain the labels required to train behavioral priors in the same way. We\nextend the internet-scale pretraining paradigm to sequential decision domains\nthrough semi-supervised imitation learning wherein agents learn to act by\nwatching online unlabeled videos. Specifically, we show that with a small\namount of labeled data we can train an inverse dynamics model accurate enough\nto label a huge unlabeled source of online data -- here, online videos of\npeople playing Minecraft -- from which we can then train a general behavioral\nprior. Despite using the native human interface (mouse and keyboard at 20Hz),\nwe show that this behavioral prior has nontrivial zero-shot capabilities and\nthat it can be fine-tuned, with both imitation learning and reinforcement\nlearning, to hard-exploration tasks that are impossible to learn from scratch\nvia reinforcement learning. For many tasks our models exhibit human-level\nperformance, and we are the first to report computer agents that can craft\ndiamond tools, which can take proficient humans upwards of 20 minutes (24,000\nenvironment actions) of gameplay to accomplish.","url":"http:\/\/arxiv.org\/abs\/2206.11795v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2206.11795v1","published":1656000071000,"comment":null,"pdf_text":"Video PreTraining (VPT): Learning to Act by\nWatching Unlabeled Online Videos\nBowen Baker∗†\nbowen@openai.com\nIlge Akkaya∗†\nilge@openai.com\nPeter Zhokhov∗†\npeterz@openai.com\nJoost Huizinga∗†\njoost@openai.com\nJie Tang∗†\njietang@openai.com\nAdrien Ecoffet∗†\nadrien@openai.com\nBrandon Houghton∗†\nbrandon@openai.com\nRaul Sampedro∗†\nraulsamg@gmail.com\nJeff Clune∗†‡\njclune@gmail.com\nAbstract\nPretraining on noisy, internet-scale datasets has been heavily studied as a technique\nfor training models with broad, general capabilities for text, images, and other\nmodalities.1–6 However, for many sequential decision domains such as robotics,\nvideo games, and computer use, publicly available data does not contain the labels\nrequired to train behavioral priors in the same way. We extend the internet-scale\npretraining paradigm to sequential decision domains through semi-supervised\nimitation learning wherein agents learn to act by watching online unlabeled videos.\nSpeciﬁcally, we show that with a small amount of labeled data we can train an\ninverse dynamics model accurate enough to label a huge unlabeled source of online\ndata – here, online videos of people playing Minecraft – from which we can then\ntrain a general behavioral prior. Despite using the native human interface (mouse\nand keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-\nshot capabilities and that it can be ﬁne-tuned, with both imitation learning and\nreinforcement learning, to hard-exploration tasks that are impossible to learn from\nscratch via reinforcement learning. For many tasks our models exhibit human-\nlevel performance, and we are the ﬁrst to report computer agents that can craft\ndiamond tools, which can take proﬁcient humans upwards of 20 minutes (24,000\nenvironment actions) of gameplay to accomplish.\n1\nIntroduction\nWork in recent years has demonstrated the efﬁcacy of pretraining large and general foundation\nmodels7 on noisy internet-scale datasets for use in downstream tasks in natural language1–4 and\ncomputer vision.5,6,8 For sequential decision domains (e.g. robotics, game playing, and computer\nusage) where agents must repeatedly act within an environment, a wealth of data also exists on the\nweb; however, most of this data is in the form of unlabeled video (i.e. without the actions taken\nat each frame), making it much less straightforward to train a behavioral prior in these domains\nthan it is in e.g. natural language. In a few rare settings, such as Chess, Go, and StarCraft, there\n∗This was a large effort by a dedicated team. Each author made huge contributions on many fronts over long\ntime periods. All members were full time on the project for over six months. BB, IA, PZ, and JC were on the\noriginal VPT project team and were thus involved for even longer (over a year). Aside from those original team\nmembers, author order is random. It was also randomized between IA and PZ.\n†OpenAI\n‡University of British Columbia\narXiv:2206.11795v1  [cs.LG]  23 Jun 2022\nalready exist large datasets with action labels from various online platforms that researchers have\nused for imitation learning.9,10 When large labeled datasets do not exist, the canonical strategy\nfor training capable agents is reinforcement learning (RL),11 which can be sample inefﬁcient and\nexpensive for hard-exploration problems.12–18 Many virtual tasks, e.g. navigating websites, using\nPhotoshop, booking ﬂights, etc., can be very hard to learn with RL and do not have large, commonly\navailable sources of labeled data.19,20 In this paper, we seek to extend the paradigm of training\nlarge, general-purpose foundation models to sequential decision domains by utilizing freely available\ninternet-scale unlabeled video datasets with a simple semi-supervised imitation learning method. We\ncall this method Video PreTraining (VPT) and demonstrate its efﬁcacy in the domain of Minecraft.\nExisting semi-supervised imitation learning methods aim to learn with few or no explicit action labels;\nhowever, they generally rely on the policy’s ability to explore the environment throughout training,\nmaking them susceptible to exploration bottlenecks.21–25 Furthermore, most prior semi-supervised\nimitation learning work was tested in the relatively low data regime; because we experiment with far\nmore data (∼70k hours of unlabeled video), we hypothesize that we can achieve good performance\nwith a much simpler method, a trend that has proven true for pretraining in other modalities such\nas text.1 In particular, given a large but unlabeled dataset, we propose generating pseudo-labels by\ngathering a small amount of labeled data to train an inverse dynamics model (IDM) that predicts\nthe action taken at each timestep in a video. Behavioral cloning (BC) can require a large amount\nof data because the model must learn to infer intent and the distribution over future behaviors from\nonly past observations. In contrast, the inverse dynamics modeling task is simpler because it is\nnon-causal, meaning it can look at both past and future frames to infer actions. In most settings,\nenvironment mechanics are far simpler than the breadth of human behavior that can take place within\nthe environment, suggesting that non-causal IDMs could require far less data to train than causal BC\nmodels. Using pseudo-labels generated from the IDM, we then train a model to mimic the distribution\nof behavior in the previously unlabeled dataset with standard behavioral cloning at scale, which does\nnot require any model rollouts and thus does not suffer from any potential exploration bottlenecks\nin the environment. Finally, we show we can ﬁne-tune this model to downstream tasks with either\nbehavioral cloning or reinforcement learning.\nFigure 1:\nExample Minecraft\ncrafting GUI. Agents use the\nmouse and keyboard to navigate\nmenus and drag and drop items.\nWe chose to test our method in Minecraft because (a) it is one\nof the most actively played games in the world26 and thus has\na wealth of commonly available video data online, (b) it is a\nfairly open-ended sandbox game with an extremely wide variety\nof potential things to do, build, and collect, making our results\nmore applicable to real-world applications such as computer us-\nage, which also tends to be varied and open-ended, and (c) it\nhas already garnered interest by the RL community as a research\ndomain due to its complexity and correspondingly difﬁcult ex-\nploration challenges.27–31 In this work we use the native human\ninterface for Minecraft so that we can (1) most accurately model\nthe human behavior distribution and reduce domain shift between\nvideo data and the environment, (2) make data collection easier by allowing our human contractors to\nplay the game without modiﬁcation, and (3) eliminate the need to hand-engineer a custom interface\nfor models to interact with the environment. This choice means that our models play at 20 frames\nper second and must use a mouse and keyboard interface to interact with human GUIs for crafting,\nsmelting, trading, etc., including dragging items to speciﬁc slots or navigating the recipe book with\nthe mouse cursor (Fig. 1). Compared to prior work in Minecraft that uses a lower frame rate and\nconstructs crafting and attacking macros,30,32–34 using the native human interface drastically increases\nthe environment’s exploration difﬁculty, making most simple tasks near impossible with RL from\nscratch. Even the simple task of gathering a single wooden log while already facing a tree takes 60\nconsecutive attack actions with the human interface, meaning the chance for a naive random policy to\nsucceed is 1\n2\n60. While this paper shows results in Minecraft only, the VPT method is general and\ncould be applied to any domain.\nIn Section 4 we show that the VPT foundation model has nontrivial zero-shot performance, accom-\nplishing tasks impossible to learn with RL alone, such as crafting planks and crafting tables (tasks\nrequiring a human proﬁcient in Minecraft a median of 50 seconds or ∼970 consecutive actions).\nThrough ﬁne-tuning with behavioral cloning to smaller datasets that target more speciﬁc behavior\ndistributions, our agent is able to push even further into the technology tree, crafting stone tools\n2\n(taking a human a median of 2.3 minutes or ∼2790 actions). Finally, ﬁne-tuning via RL produces\nthe most dramatic improvements: our agent is able to craft diamond tools, an unprecedented result\nin Minecraft made even more challenging by using the native human interface. This task requires\na proﬁcient human a median upwards of 20 minutes or ∼24000 actions. The main contributions\nof this work are (1) we are the ﬁrst to show promising results applying semi-supervised imitation\nlearning to extremely large, noisy, and freely available video datasets for sequential decision domains,\n(2) we show that such pretraining plus ﬁne-tuning enables agents to solve tasks that were otherwise\nimpossible to learn, (3) we show that labeled contractor data is far more efﬁciently used within\nthe VPT method than it would be by directly training a foundation model from it and (4) we open\nsource our contractor data, trained model weights, and Minecraft environment for future research\ninto learning to act via semi-supervised imitation learning at scale.\n2\nPreliminaries and Related Work\nImitation learning methods35–38 seek to construct a policy that accurately models the distribution of\nbehavior in some dataset D = {(oi, ai)}, i ∈{1...N} of action-observation pairs. In order to roll\nout these policies in an environment, they must be causal, meaning they condition on observations\nfrom the current timestep t and past timesteps only, i.e. π ∼p(at|o1...ot). Imitation learning is\nsimplest when demonstrations are labeled with corresponding actions. Imitating labeled trajectories\nhas seen success in aerial vehicles,39,40 self-driving cars,41,42 board games,9,43 and video games.10,44\nWhen labeled demonstrations are not available, standard behavioral cloning will not work; however,\nthere is a large body of work in imitating behavior from unlabeled demonstrations.22 For instance,\nGAIL23 constructs an adversarial objective incentivizing the trained policy to exhibit behaviors\nindistinguishable from those in the target dataset. Edwards et al. 45 propose to ﬁrst learn a latent\npolicy using unlabeled demonstrations and then map the learned latent actions to real actions with\na small amount of environment interaction. Peng et al. 46 ﬁrst use motion-capture methods to track\nagent positions in videos and then train RL agents to match these waypoints. Similarly, Behbahani\net al. 47 and Aytar et al. 48 task a RL agent to match waypoints; however, they construct waypoints that\nare embeddings from unsupervised feature learning models. Pathak et al. 49 and Nair et al. 50 train\ngoal conditioned policies to take actions that advance the current state towards expert-provided goal\nstates expressed as high dimensional visual waypoints. Most similar to our own work, Torabi et al. 24\nsimultaneously train (1) an inverse dynamics model (IDM),51 which aims to uncover the underlying\naction between timesteps given observations of past and future timesteps, e.g. pIDM(at|ot, ot+1), and\n(2) a behavioral cloning (BC) model on trajectories of observations labeled with the IDM. Data to\ntrain the IDM is collected by rolling out the BC model in the target environment such that both\nmodels improve in tandem. However, at any point in training if there are sequences in the dataset that\nthe IDM performs poorly on, it requires that the BC model perform those or similar sequences in\norder for the IDM to improve and correctly label them. Therefore, if the BC model does not explore\nefﬁciently, it could severely slow down learning. In order to avoid this potential issue we opted for a\nsimpler two-stage approach: we ﬁrst train an IDM on a small number of labeled trajectories collected\nfrom human contractors (they play the game as would normally as we record their keypresses and\nmouse movements). Because human contractors reach most relevant parts of the state space, we can\nhold the IDM ﬁxed throughout BC training.\nCompared to most previous work in semi-supervised imitation learning, we experiment in the much\nmore complex and open-ended environment of Minecraft. Minecraft is a voxel-based 3D video\ngame that, due its popularity and wide variety of mechanics, has attracted a vast amount of RL\nresearch.27,28,30–34,52–60 A large body of work focuses on small, custom-made Minecraft worlds\nwith tasks such as navigation,53,60 block placing,54,55 instruction following,58,59 combat,56 and\nothers.28,31,57 Work operating in the massive, randomly generated environments of Minecraft itself\nhas included hill climbing,52 automated curriculum learning30 and, most closely related to the RL\nexperiments presented in Sec. 4.4, diamond mining.27,32–34 However, to the best of our knowledge,\nthere is no published work that operates in the full, unmodiﬁed human action space, which includes\ndrag-and-drop inventory management and item crafting.\n3\nCollecting “Clean” Data\nTraining the VPT Foundation Model\nvia Behavioral Cloning\nTraining the Inverse Dynamics Model (IDM)\n~270k hours\nunlabeled\nvideo\n~70k hours\nunlabeled\nvideo\n~2k hours\nvideo\nlabeled with\nactions\nFilter for “clean”\nvideo segments\nSearch for relevant\nMinecraft videos\nvia keywords\nContractors\ncollect data \nLabel videos\nwith IDM \n~70k hours\nvideo\nIDM-labeled\nwith actions\nTrain non-causal IDM\nTrain causal\nVPT Foundation Model\na\nd\nspace\nw\na\nd\nspace\nw\nFigure 2: Video Pretraining (VPT) Method Overview.\n3\nMethods\nInverse Dynamics Models (IDM)\nVPT, illustrated in Figure 2, requires we ﬁrst collect a small\namount of labeled contractor data with which to train an inverse dynamics model pIDM(at|o1...T ),\nwhich seeks to minimize the negative log-likelihood of an action at timestep t given a trajectory of T\nobservations ot : t ∈[1...T]. In contrast to an imitation learning policy, the IDM can be non-causal,\nmeaning its prediction for at can be a function of both past and future events, i.e. ot′>t. Compared to\nthe behavioral cloning objective of modeling the distribution of human intent given past frames only,\nwe hypothesize that inverting environment dynamics is easier and more data efﬁcient to learn. Indeed,\nSec. 4.1 will show that the IDM objective is much easier to learn, and furthermore Sec. 4.6 will show\nthat with very little labeled data (as few as 100 hours) we can train a fairly accurate IDM. This IDM\ncan be used to label online videos, providing the large amount of data required for the harder task of\nbehavioral cloning. See appendices D and B for IDM training and data collection details.\nData Filtering\nWe gather a large dataset of Minecraft videos by searching the web for related\nkeywords (Appendix A). Online videos often (1) include overlaid artifacts, such as a video feed\nof the player’s face, channel logos, watermarks, etc., (2) are collected from platforms other than\na computer with different gameplay, or (3) are from different game modes, e.g. in Minecraft we\nonly want \"survival mode\" where players start from scratch and must gather or craft all their items.\nWe call data “clean” if it does not contain visual artifacts and is from survival mode, and call all\nother data “unclean.” With enough data, a large enough model, and enough training compute, a BC\nmodel trained on both unclean and clean videos would likely still perform well in a clean Minecraft\nenvironment. However, for simplicity and training compute efﬁciency, we choose to ﬁlter out unclean\nsegments of video (note that a video may contain both clean and unclean segments). We do this by\ntraining a model to ﬁlter out unclean segments using a small dataset (8800) of images sampled from\nonline videos labeled by contractors as clean or unclean (Appendix A.2).\nVPT Foundation Model\nWe train a foundation model with standard behavioral cloning, i.e. mini-\nmizing the negative log-likelihood of actions predicted by the IDM on clean data. For a particular\ntrajectory of length T we minimize\nmin\nθ\nX\nt∈[1...T ]\n−log πθ(at|o1, . . . , ot), where at ∼pIDM(at|o1, . . . , ot, . . . , oT )\n(1)\nAs we will see in the following sections, this model exhibits nontrivial zero-shot behavior and can be\nﬁne-tuned with both imitation learning and RL to perform even more complex skills.\n4\nResults\n4.1\nPerformance of the Inverse Dynamics Model\nThe IDM architecture is comprised primarily of a temporal convolution layer, a ResNet62 image\nprocessing stack, and residual unmasked attention layers, from which the IDM simultaneously\npredicts keypresses and mouse movements (see Appendix D for IDM architecture and training\ndetails). A key hypothesis behind our work is that IDMs can be trained with a relatively small amount\nof labeled data. While more data improves both mouse movement and keypress predictions, our best\n4\nFigure 3: (Left) IDM keypress accuracy and mouse movement R2 (explained variance61) as a\nfunction of dataset size. (Right) IDM vs. behavioral cloning data efﬁciency.\nIDM trains on only 1962 hours of data (compared to the ∼70k hours of clean data we collected from\nthe internet) and achieves 90.6% keypress accuracy and a 0.97 R2 for mouse movements evaluated\non a held-out validation set of contractor-labeled data (Figure 3 left).\nFigure 3 (right) validates our hypothesis that IDMs are far more data efﬁcient than BC models, likely\nbecause inverting environment mechanics is far easier than modeling the entire distribution of human\nbehavior. The IDM is two orders of magnitude more data efﬁcient than a BC model trained on the\nsame data and improves more quickly with more data. This evidence supports the hypothesis that it is\nmore effective to use contractor data within the VPT pipeline by training an IDM than it is to train a\nfoundation model from contractor data directly (Sections 4.5 and 4.6 provide additional evidence).\n4.2\nVPT Foundation Model Training and Zero-Shot Performance\nFigure 4: (Left) Training and validation loss on the web_clean internet dataset with IDM pseudo-\nlabels, and loss on the main IDM contractor dataset, which has ground-truth labels but is out-of-\ndistribution (see text). (Right) Amount a given item was collected per episode averaged over 2500\n60-minute survival episodes as a function of training epoch, shaded with the standard error of the\nmean. Basic mining refers to collection of dirt, gravel, or sand (all materials that can be gathered\nwithout tools). Logs are obtained by repeatedly hitting trees for three seconds, a difﬁcult feat for an\nRL agent to achieve as we show in Sec. 4.4. Planks can be crafted from logs, and crafting tables\ncrafted from planks. Crafting requires using in-game crafting GUIs, and proﬁcient humans take a\nmedian of 50 seconds (970 consecutive actions) to make a crafting table.\nWe now explore the emergent behavior learned by a behavioral cloning policy trained on an extremely\nlarge, but noisy, internet dataset labeled with our IDM. To collect the unlabeled internet dataset,\nwe searched for publicly available videos of Minecraft play with search terms such as “minecraft\nsurvival for beginners.” These searches resulted in ∼270k hours of video, which we ﬁltered down to\n“clean” video segments yielding an unlabeled dataset of ∼70k hours, which we refer to as web_clean\n(Appendix A has further details on data scraping and ﬁltering). We then generated pseudo-labels\nfor web_clean with our best IDM (Section 3) and then trained the VPT foundation model with\nbehavioral cloning. Preliminary experiments suggested that our model could beneﬁt from 30 epochs\nof training and that a 0.5 billion parameter model was required to stay in the efﬁcient learning\nregime63 for that training duration (Appendix H), which took ∼9 days on 720 V100 GPUs.\nWe evaluate our models by measuring validation loss (Fig. 4, left) and rolling them out in the\nMinecraft environment. Unless otherwise noted, in all environment evaluations we spawn agents in a\nstandard survival mode game where they play for 60 minutes, i.e. 72000 consecutive actions, and we\nplot the mean and shade the standard error of the mean for various game statistics such as crafting\nand collection rates (Fig. 4, right). The VPT foundation model quickly learns to chop down trees\nto collect logs, a task we found near impossible for an RL agent to achieve with the native human\ninterface (Sec. 4.4). It also learns to craft those logs into wooden planks and then use those planks\n5\nto craft a crafting table, which are required to unlock most other technology in the game and take a\nhuman proﬁcient in Minecraft approximately 50 seconds (970 consecutive actions) to collect. While\nthese behaviors are fairly complex in the native human action space, the VPT foundation model crafts\nthese items at a rate far below that of our proﬁcient contractors, e.g. on average our contractors craft\n5.44 crafting tables in 60 minutes of play versus 0.19 for the foundation model. The model also crafts\na non-negligible amount of wooden sticks, which are required to make wooden tools; collects various\nﬂowers and crafts dyes from them; kills zombies that appear during the night; hunts wild animals;\ncollects various berries and mushrooms and eats them; and ﬁnds game-generated villages from which\nto collect various rare items from chests. The model also learned to navigate uneven terrain, swim,\nand pillar jump, which involves the agent repeatedly jumping and quickly placing a block below itself\nsuch that it climbs upward by making a pillar.(iv)\nWhile training and validation loss decrease healthily over training (Fig. 4, left), loss on our contractor\ndataset (which the VPT model does not train on) begins increasing after 7 epochs. Contractor data\ncould be out-of-distribution because our contractors may have a different distribution of play or\nbecause there is some impactful visual domain shift compared to videos from the web. While one\ncould have expected this would be predictive of declining evaluation performance, we do not see\nnotable game statistics from the VPT foundation model rollouts (Figure 4, right) decrease over\ntraining, and in the next section we show that BC ﬁne-tuning performance continually improves as the\nVPT foundation model trains. We provide more insight into this curious phenomenon in Appendix H.\n4.3\nFine-Tuning with Behavioral Cloning\nFoundation models are designed to have a broad behavior proﬁle and be generally capable across a\nwide variety of tasks. To incorporate new knowledge or allow them to specialize on a narrower task\ndistribution, it is common practice to ﬁne-tune these models to smaller, more speciﬁc datasets.1 The\nVPT foundation model trained on the broad web_clean dataset had nontrivial zero-shot performance;\nit was able to craft a crafting table yet unable to go past this in the technology tree. As a case\nstudy into BC ﬁne-tuning, we attempt to improve the VPT foundation model’s ability to collect\nand craft these “early game” items by ﬁne-tuning to two narrower datasets targeted at Minecraft\nbehavior within the ﬁrst few minutes of players starting in a fresh world. In the ﬁrst dataset,\ncontractor_house, contractors have 10 minutes to build a basic house from scratch using primarily\nwood, sand, and dirt. Collecting contractor data can be difﬁcult and expensive, so we also construct a\ndataset earlygame_keyword by searching for videos online with descriptions that match keywords\nsuch as “new world”, “let’s play episode 1”, etc.; this is a subset of web_clean and is labeled with\nthe IDM. See Appendix B.4 and A.3 for full descriptions of both datasets.\nEffect of Foundation Model Quality on BC Fine-Tuning\n59x\n213x\n59x\nFigure 5:\n(Left) Collection and crafting rates for three policies:\nthe zero-shot VPT foun-\ndation model, and the VPT foundation model BC ﬁne-tuned to the earlygame_keyword or\ncontractor_house datasets. BC ﬁne-tuning to either dataset improves performance, including (for\nthe contractor_house dataset) yielding wooden and stone tools. Proﬁcient Minecraft players take\na median of 1.2 minutes (1390 actions) to construct wooden tools and 2.3 minutes (2790 actions)\nto construct stone tools. (Right) Collection and crafting rates for VPT foundation model snapshots\nthroughout training after they are BC ﬁne-tuned to the contractor_house dataset. In general,\ncrafting-related behaviors increase throughout foundation model training. Fig. 4 deﬁnes the other\ntask terms (logs, planks, crafting tables, and total crafting).\n(iv)Sample videos: https:\/\/www.youtube.com\/playlist?list=PLNAOIb_agjf3U3rSvG_BCWqJ869NdBhcP\n6\nFine-tuning to earlygame_keyword results in a large boost compared to the zero-shot foundation\nmodel: 2.5x more crafting tables, 6.1x more planks, 4.3x more logs, and 5.5x more crafting overall\n(Fig. 5). However, when ﬁne-tuning to this dataset we did not see any new behaviors emerge,\nonly a reﬁnement of existing skills. We saw an even bigger improvement when ﬁne-tuning to the\ncontractor_house dataset: 213x more crafting tables, 59x more wooden planks, 7x more logs,\nand 59x more crafting over all. In addition, we saw the emergence of crafting wooden tools, which\nrequires placing a crafting table on the ground, opening it to reveal a new crafting interface, and then\nusing it to craft wooden tools. This entire sequence takes a proﬁcient human player a median of 1.2\nminutes (1390 consecutive actions) to accomplish. The model goes further and collects cobblestone,\nwhich requires a wooden pickaxe to mine, and crafts stone tools, requiring it to again use a crafting\ntable; this takes a proﬁcient human player a median of 2.3 minutes (2790 consecutive actions). We\nalso saw this model more frequently raiding villages that randomly spawn in the game, hunting\nanimals for food, in addition to many behaviors we saw performed by the foundation model.(v)\nDespite the foundation model’s zero-shot rollout performance plateauing 1\/3 into training (Fig. 4,\nright), ﬁne-tuning performance does continue to increase throughout foundation model training\n(Fig. 5, right). Additionally, there is a stark difference in performance when training from scratch vs.\nﬁne-tuning from the VPT foundation model (Fig. 5 right, comparing the left and rightmost points).\n4.4\nFine-Tuning with Reinforcement Learning\nFigure 6: Typical sequence of items for obtaining a diamond pickaxe. Below each item is the median\ntime and number of actions contractors required to obtain that item and the percentage of contractors\nthat got the item within 10 minutes. The median time to obtain a diamond pickaxe is unknown (except\nthat it is > 20m) because contractors obtained this item in less than 50% of 20-minute episodes.\nTo demonstrate the efﬁcacy of RL ﬁne-tuning, we chose the challenging goal of obtaining a diamond\npickaxe within 10 minutes starting from a fresh Minecraft survival world. Doing so involves acquiring\na sequence of difﬁcult-to-obtain items that require complex skills like mining, inventory management,\ncrafting with and without a crafting table, tool use, operating a furnace, and mining at the lowest\ndepths, where many hazards like enemies and lava exist (Fig. 6). Adding to the difﬁculty, progress\ncan be easily lost by dropping items, destroying items, or dying. Obtaining a diamond pickaxe more\noften than not takes a proﬁcient human over 20 minutes (24,000 actions).\nAgents are rewarded for each item obtained in the sequence, with lower rewards for items that have to\nbe collected in bulk and higher rewards for items near the end of the sequence. Agents are optimized\nwith the phasic policy gradient64 RL algorithm for ∼1.3 million episodes (roughly 1.4×1010 frames).\nEpisodes last for 10 minutes. See Appendix G.1 for reward function and RL training details. Due to\ncomputational constraints, RL experiments use a ∼248 million parameter VPT model (Appendix H).\nA major problem when ﬁne-tuning with RL is catastrophic forgetting65,66 because previously learned\nskills can be lost before their value is realized. For instance, while our VPT foundation model never\nexhibits the entire sequence of behaviors required to smelt iron zero-shot, it did train on examples of\nplayers smelting with furnaces. It therefore may have some latent ability to smelt iron once the many\nprerequisites to do so have been performed. To combat the catastrophic forgetting of latent skills\nsuch that they can continually improve exploration throughout RL ﬁne-tuning, we add an auxiliary\nKullback-Leibler (KL) divergence loss between the RL model and the frozen pretrained policy.10\nTraining from a randomly initialized policy fails to achieve almost any reward, underscoring how\nhard an exploration challenge the diamond pickaxe task is for RL in the native human action space\n(Fig. 7a). The model never learns to reliably collect logs, typically the ﬁrst of many steps to obtaining\na diamond pickaxe (Fig. 7b). RL ﬁne-tuning from the VPT foundation model does substantially\nbetter (Fig. 7a), learning everything up to mining iron ore and crafting furnaces. (Fig. 7c). However,\nthis agent fails at smelting an iron ingot, the next item required to get further into the tech tree, likely\n(v)Sample Videos: https:\/\/www.youtube.com\/playlist?list=PLNAOIb_agjf2yDSs4AqcoyPv4z_eWUiKm\n7\n0\n5\n10\n15\n20\n25\nReward\nReward over episodes\nRL from Rand. Init. model\nRL from VPT Found. model\nRL from Early-Game model\nNo KL-loss\n(a)\n0\n20\n40\n60\n80\n100\n% episodes\nRL from Rand. Init. model\nLogs\nPlanks\nSticks\nCrafting Tables\nWooden Pickaxe\nCobblestone\nStone Pickaxe\nCoal\nTorch\nFurnace\nIron Ore\nIron Ingot\nIron Pickaxe\nDiamonds\nDiamond Pickaxe\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nEpisodes\n1e6\n0\n20\n40\n60\n80\n100\n% episodes\nRL from VPT Found. model\n(c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nEpisodes\n1e6\n0\n20\n40\n60\n80\n100\n% episodes\n2.5%\nRL from Early-Game model\n(d)\nFigure 7: RL Fine-tuning results. (a) RL from a randomly initialized model fails to get almost\nany reward, RL ﬁne-tuning from the VPT foundation model performs substantially better with a\nreward near 13, and RL ﬁne-tuning from the early-game model performs best with a reward of 25.\nWhen training the early-game model without a KL loss to the original policy (No KL-loss) progress\nstalls after 100,000 episodes, suggesting that the skills necessary to make further progress have been\ncatastrophically forgotten. (b) RL from a randomly initialized model occasionally collects sticks by\nbreaking leaves (an easy but inefﬁcient method of getting sticks that does not require logs or planks)\nand never learns to reliably collect logs. (c) RL ﬁne-tuning from the VPT Foundation model learns\neverything in the curriculum up to iron ore and making furnaces, but fails to learn to use the furnace to\nsmelt iron ingots. (d) RL ﬁne-tuning from the early-game model learns to obtain (at human-level) all\nitems in the sequence towards a diamond pickaxe and crafts a diamond pickaxe in 2.5% of episodes.\nbecause the zero-shot probability that the VPT foundation model smelts an iron ingot is too low, even\nwhen given the prerequisite materials.\nResults further improve by ﬁrst BC ﬁne-tuning the VPT Foundation Model to the\nearlygame_keyword dataset (the early-game model, Sec. 4.3) and then ﬁne-tuning with RL\n(Fig. 7a), which in preliminary experiments we found to perform better than ﬁrst ﬁne-tuning to\ncontractor_house followed by ﬁne-tuning with RL (Appendix G.2). The three-phase training\n(pretraining, BC ﬁne-tuning, and then RL ﬁne-tuning) succeeds in learning extremely difﬁcult tasks:\nit achieves over 80% reliability on iron pickaxes, almost 20% reliability on collecting diamonds, and\n2.5% reliability on obtaining a diamond pickaxe (Fig. 7d). For comparison, human players given\nthe objective of obtaining a diamond pickaxe collect these items in 57%, 15%, and 12% of episodes,\nrespectively, meaning our model is human-level for crafting iron pickaxes and mining diamonds.\nOthers have managed to obtain diamonds with ∼0.1% reliability in 15 minutes32,33 but always with a\nsimpliﬁed action space designed to ease exploration. To the best of our knowledge, we are the ﬁrst to\nreport non-zero success rates on crafting a diamond pickaxe. Qualitatively, the model developed\nuseful skills for diamond mining, such as efﬁcient mining patterns, cave exploration, returning to\npreviously placed objects like crafting tables, and advanced techniques like using wooden pickaxes\nas fuel when moving on to iron tools.(vi)\nFinally, we validated the importance of the KL loss to the pretrained model during RL ﬁne-tuning.\nThe treatment without a KL loss obtains only items early in the sequence (logs, planks, sticks, and\ncrafting tables) limiting its reward (Fig. 7a). This failure to progress further into the sequence is\nlikely because, while the initial skills of chopping logs and crafting planks are being learned with RL,\nsubsequent skills like crafting a wooden pickaxe are lost due to catastrophic forgetting.\n4.5\nData Scaling Properties of the Foundation Model\nIn this section we validate a core hypothesis behind this work: that it is far more effective to use\nlabeled contractor data to train an IDM within the VPT method than it is to directly train a BC\nfoundation model from that same small contractor dataset. If we could cheaply collect a labeled\ncontractor dataset of a similar order of magnitude as web_clean, then this would not be important;\nhowever, collecting that scale of data would have cost millions of dollars. Figure 8 compares\nfoundation models trained on increasing orders of magnitude of data from 1 hour up to the full ∼70k\nweb_clean dataset. Foundation models trained up to and including 1k hours are trained on the IDM\n(vi)Videos found at https:\/\/www.youtube.com\/playlist?list=PLNAOIb_agjf3e_UKweM5pQUSfTw8r-Wfc\n8\nTrained on\nContractor Data\nTrained on IDM\nLabeled Web Data\nFigure 8: (Left) Zero-shot rollout performance of foundation models trained on varying amounts\nof data. Models to the left of the dashed black line (points ≤1k hours) were trained on contractor\ndata (ground-truth labels), and models to the right were trained on IDM pseudo-labeled subsets\nof web_clean. Due to compute limitations, this analysis was performed with smaller (71 million\nparameter) models except for the ﬁnal point, which is the 0.5 billion parameter VPT foundation\nmodel. (Right) The corresponding performance of each model after BC ﬁne-tuning each model to\nthe contractor_house dataset.\ncontractor data, and those trained on 5k hours and above are trained on subsets of web_clean, which\ndoes not contain any IDM contractor data. Scaling training data increases log collection, mining, and\ncrafting capabilities. The zero-shot model only begins to start crafting crafting tables at over 5000\nhours of training data. When ﬁne-tuning each foundation model to contractor_house, we see that\ncrafting rates for crafting tables and wooden tools increase by orders of magnitude when using the\nentire ∼70k hour web_clean dataset. We furthermore only see the emergence of crafting stone tools\nat the largest data scale.\n4.6\nEffect of Inverse Dynamics Model Quality on Behavioral Cloning\nFigure 9: Zero-shot performance of BC models\ntrained from scratch on the earlygame_keyword\ndataset labeled with IDMs that were trained on\nincreasing amounts of contractor data.\nThis section investigates how downstream\nBC performance is affected by IDM qual-\nity.\nWe train IDMs on increasingly larger\ndatasets and use each to independently label\nthe earlygame_keyword dataset (this smaller\ndataset was chosen due to a limited compute bud-\nget). We then train a BC model from scratch on\neach dataset and report game statistics for each\nmodel as a function of IDM contractor dataset\nsize (Fig. 9).\nIDMs trained on at least 10 hours of data are\nrequired for any crafting, and the crafting rate\nincreases quickly up until 100 hours of data, after which there are few to no gains and differences are\nlikely due to noise. Similarly, crafting tables are only crafted after 50 or more hours of IDM data, and\nagain gains plateau after 100 hours. While in all previous experiments we use our best IDM trained\non 1962 hours of data, these results suggest we could reduce that number to as low as 100 hours.\n5\nDiscussion and Conclusion\nThe results presented in this paper help pave the path to utilizing the wealth of unlabeled data on the\nweb for sequential decision domains. Compared to generative video modeling or contrastive methods\nthat would only yield representational priors, VPT offers the exciting possibility of directly learning\nto act during pretraining and using these learned behavioral priors as extremely effective exploration\npriors for RL. VPT could even be a better general representation learning method even when the\ndownstream task is not learning to act in that domain—for example, ﬁne-tuning to explain what is\nhappening in a video—because arguably the most important information in any given scene would be\npresent in features trained to correctly predict the distribution over future human actions. We leave\nthis intriguing direction to future work.\nFuture work could improve results with more data (we estimate we could collect >1M hours) and\nlarger, better-tuned models. Furthermore, all the models in this work condition on past observations\nonly; we cannot ask the model to perform speciﬁc tasks. Appendix I presents preliminary experiments\non conditioning our models on closed captions (text transcripts of speech in videos), showing they\n9\nbecome weakly steerable; we believe this a rich direction for future research. Also, loss was not\nconsistently correlated with downstream evaluation metrics (Sec. 4.2), which often made progress\nslow and hard-won. Another fruitful future direction would be to investigate the correlation between\nvarious training metrics and downstream evaluations. Finally, while we do not anticipate any direct\nnegative societal impacts from the models trained in this work, as VPT improves and expands to other\ndomains it will be important to assess and mitigate harms that emerge with other forms of pretraining\non internet datasets, such as emulating inappropriate behavior.67\nIn conclusion, VPT extends the paradigm of training large and general purpose behavioral priors from\nfreely available internet-scale data to sequential decision domains. Our models exhibited impressive\nzero-shot behavior and, when ﬁne-tuned with RL, achieved an unprecedented result of crafting a\ndiamond pickaxe in Minecraft (all the more difﬁcult given the human interface). We further showed\nthat contractor data is far better used within the VPT pipeline than to train a foundation model directly\nand that only a small amount of contractor data (about $2000 USD) was required to unlock massive\namounts of unlabeled online data for use in BC. Finally, learning with the human keyboard and mouse\ninterface is highly general and allows losslessly modeling the entire distribution of human behavior.\nWhile we only experiment in Minecraft, we believe that VPT provides a general recipe for training\nbehavioral priors in hard, yet generic, action spaces in any domain that has a large amount of freely\navailable unlabeled data, such as computer usage.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nVideo PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos\n```\n#### 2. 论文摘要\n```\nPretraining on noisy, internet-scale datasets has been heavily studied as a\ntechnique for training models with broad, general capabilities for text,\nimages, and other modalities. However, for many sequential decision domains\nsuch as robotics, video games, and computer use, publicly available data does\nnot contain the labels required to train behavioral priors in the same way. We\nextend the internet-scale pretraining paradigm to sequential decision domains\nthrough semi-supervised imitation learning wherein agents learn to act by\nwatching online unlabeled videos. Specifically, we show that with a small\namount of labeled data we can train an inverse dynamics model accurate enough\nto label a huge unlabeled source of online data -- here, online videos of\npeople playing Minecraft -- from which we can then train a general behavioral\nprior. Despite using the native human interface (mouse and keyboard at 20Hz),\nwe show that this behavioral prior has nontrivial zero-shot capabilities and\nthat it can be fine-tuned, with both imitation learning and reinforcement\nlearning, to hard-exploration tasks that are impossible to learn from scratch\nvia reinforcement learning. For many tasks our models exhibit human-level\nperformance, and we are the first to report computer agents that can craft\ndiamond tools, which can take proficient humans upwards of 20 minutes (24,000\nenvironment actions) of gameplay to accomplish.\n```\n\n#### 3. 论文全文\n```\nVideo PreTraining (VPT): Learning to Act by\nWatching Unlabeled Online Videos\nBowen Baker∗†\nbowen@openai.com\nIlge Akkaya∗†\nilge@openai.com\nPeter Zhokhov∗†\npeterz@openai.com\nJoost Huizinga∗†\njoost@openai.com\nJie Tang∗†\njietang@openai.com\nAdrien Ecoffet∗†\nadrien@openai.com\nBrandon Houghton∗†\nbrandon@openai.com\nRaul Sampedro∗†\nraulsamg@gmail.com\nJeff Clune∗†‡\njclune@gmail.com\nAbstract\nPretraining on noisy, internet-scale datasets has been heavily studied as a technique\nfor training models with broad, general capabilities for text, images, and other\nmodalities.1–6 However, for many sequential decision domains such as robotics,\nvideo games, and computer use, publicly available data does not contain the labels\nrequired to train behavioral priors in the same way. We extend the internet-scale\npretraining paradigm to sequential decision domains through semi-supervised\nimitation learning wherein agents learn to act by watching online unlabeled videos.\nSpeciﬁcally, we show that with a small amount of labeled data we can train an\ninverse dynamics model accurate enough to label a huge unlabeled source of online\ndata – here, online videos of people playing Minecraft – from which we can then\ntrain a general behavioral prior. Despite using the native human interface (mouse\nand keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-\nshot capabilities and that it can be ﬁne-tuned, with both imitation learning and\nreinforcement learning, to hard-exploration tasks that are impossible to learn from\nscratch via reinforcement learning. For many tasks our models exhibit human-\nlevel performance, and we are the ﬁrst to report computer agents that can craft\ndiamond tools, which can take proﬁcient humans upwards of 20 minutes (24,000\nenvironment actions) of gameplay to accomplish.\n1\nIntroduction\nWork in recent years has demonstrated the efﬁcacy of pretraining large and general foundation\nmodels7 on noisy internet-scale datasets for use in downstream tasks in natural language1–4 and\ncomputer vision.5,6,8 For sequential decision domains (e.g. robotics, game playing, and computer\nusage) where agents must repeatedly act within an environment, a wealth of data also exists on the\nweb; however, most of this data is in the form of unlabeled video (i.e. without the actions taken\nat each frame), making it much less straightforward to train a behavioral prior in these domains\nthan it is in e.g. natural language. In a few rare settings, such as Chess, Go, and StarCraft, there\n∗This was a large effort by a dedicated team. Each author made huge contributions on many fronts over long\ntime periods. All members were full time on the project for over six months. BB, IA, PZ, and JC were on the\noriginal VPT project team and were thus involved for even longer (over a year). Aside from those original team\nmembers, author order is random. It was also randomized between IA and PZ.\n†OpenAI\n‡University of British Columbia\narXiv:2206.11795v1  [cs.LG]  23 Jun 2022\nalready exist large datasets with action labels from various online platforms that researchers have\nused for imitation learning.9,10 When large labeled datasets do not exist, the canonical strategy\nfor training capable agents is reinforcement learning (RL),11 which can be sample inefﬁcient and\nexpensive for hard-exploration problems.12–18 Many virtual tasks, e.g. navigating websites, using\nPhotoshop, booking ﬂights, etc., can be very hard to learn with RL and do not have large, commonly\navailable sources of labeled data.19,20 In this paper, we seek to extend the paradigm of training\nlarge, general-purpose foundation models to sequential decision domains by utilizing freely available\ninternet-scale unlabeled video datasets with a simple semi-supervised imitation learning method. We\ncall this method Video PreTraining (VPT) and demonstrate its efﬁcacy in the domain of Minecraft.\nExisting semi-supervised imitation learning methods aim to learn with few or no explicit action labels;\nhowever, they generally rely on the policy’s ability to explore the environment throughout training,\nmaking them susceptible to exploration bottlenecks.21–25 Furthermore, most prior semi-supervised\nimitation learning work was tested in the relatively low data regime; because we experiment with far\nmore data (∼70k hours of unlabeled video), we hypothesize that we can achieve good performance\nwith a much simpler method, a trend that has proven true for pretraining in other modalities such\nas text.1 In particular, given a large but unlabeled dataset, we propose generating pseudo-labels by\ngathering a small amount of labeled data to train an inverse dynamics model (IDM) that predicts\nthe action taken at each timestep in a video. Behavioral cloning (BC) can require a large amount\nof data because the model must learn to infer intent and the distribution over future behaviors from\nonly past observations. In contrast, the inverse dynamics modeling task is simpler because it is\nnon-causal, meaning it can look at both past and future frames to infer actions. In most settings,\nenvironment mechanics are far simpler than the breadth of human behavior that can take place within\nthe environment, suggesting that non-causal IDMs could require far less data to train than causal BC\nmodels. Using pseudo-labels generated from the IDM, we then train a model to mimic the distribution\nof behavior in the previously unlabeled dataset with standard behavioral cloning at scale, which does\nnot require any model rollouts and thus does not suffer from any potential exploration bottlenecks\nin the environment. Finally, we show we can ﬁne-tune this model to downstream tasks with either\nbehavioral cloning or reinforcement learning.\nFigure 1:\nExample Minecraft\ncrafting GUI. Agents use the\nmouse and keyboard to navigate\nmenus and drag and drop items.\nWe chose to test our method in Minecraft because (a) it is one\nof the most actively played games in the world26 and thus has\na wealth of commonly available video data online, (b) it is a\nfairly open-ended sandbox game with an extremely wide variety\nof potential things to do, build, and collect, making our results\nmore applicable to real-world applications such as computer us-\nage, which also tends to be varied and open-ended, and (c) it\nhas already garnered interest by the RL community as a research\ndomain due to its complexity and correspondingly difﬁcult ex-\nploration challenges.27–31 In this work we use the native human\ninterface for Minecraft so that we can (1) most accurately model\nthe human behavior distribution and reduce domain shift between\nvideo data and the environment, (2) make data collection easier by allowing our human contractors to\nplay the game without modiﬁcation, and (3) eliminate the need to hand-engineer a custom interface\nfor models to interact with the environment. This choice means that our models play at 20 frames\nper second and must use a mouse and keyboard interface to interact with human GUIs for crafting,\nsmelting, trading, etc., including dragging items to speciﬁc slots or navigating the recipe book with\nthe mouse cursor (Fig. 1). Compared to prior work in Minecraft that uses a lower frame rate and\nconstructs crafting and attacking macros,30,32–34 using the native human interface drastically increases\nthe environment’s exploration difﬁculty, making most simple tasks near impossible with RL from\nscratch. Even the simple task of gathering a single wooden log while already facing a tree takes 60\nconsecutive attack actions with the human interface, meaning the chance for a naive random policy to\nsucceed is 1\n2\n60. While this paper shows results in Minecraft only, the VPT method is general and\ncould be applied to any domain.\nIn Section 4 we show that the VPT foundation model has nontrivial zero-shot performance, accom-\nplishing tasks impossible to learn with RL alone, such as crafting planks and crafting tables (tasks\nrequiring a human proﬁcient in Minecraft a median of 50 seconds or ∼970 consecutive actions).\nThrough ﬁne-tuning with behavioral cloning to smaller datasets that target more speciﬁc behavior\ndistributions, our agent is able to push even further into the technology tree, crafting stone tools\n2\n(taking a human a median of 2.3 minutes or ∼2790 actions). Finally, ﬁne-tuning via RL produces\nthe most dramatic improvements: our agent is able to craft diamond tools, an unprecedented result\nin Minecraft made even more challenging by using the native human interface. This task requires\na proﬁcient human a median upwards of 20 minutes or ∼24000 actions. The main contributions\nof this work are (1) we are the ﬁrst to show promising results applying semi-supervised imitation\nlearning to extremely large, noisy, and freely available video datasets for sequential decision domains,\n(2) we show that such pretraining plus ﬁne-tuning enables agents to solve tasks that were otherwise\nimpossible to learn, (3) we show that labeled contractor data is far more efﬁciently used within\nthe VPT method than it would be by directly training a foundation model from it and (4) we open\nsource our contractor data, trained model weights, and Minecraft environment for future research\ninto learning to act via semi-supervised imitation learning at scale.\n2\nPreliminaries and Related Work\nImitation learning methods35–38 seek to construct a policy that accurately models the distribution of\nbehavior in some dataset D = {(oi, ai)}, i ∈{1...N} of action-observation pairs. In order to roll\nout these policies in an environment, they must be causal, meaning they condition on observations\nfrom the current timestep t and past timesteps only, i.e. π ∼p(at|o1...ot). Imitation learning is\nsimplest when demonstrations are labeled with corresponding actions. Imitating labeled trajectories\nhas seen success in aerial vehicles,39,40 self-driving cars,41,42 board games,9,43 and video games.10,44\nWhen labeled demonstrations are not available, standard behavioral cloning will not work; however,\nthere is a large body of work in imitating behavior from unlabeled demonstrations.22 For instance,\nGAIL23 constructs an adversarial objective incentivizing the trained policy to exhibit behaviors\nindistinguishable from those in the target dataset. Edwards et al. 45 propose to ﬁrst learn a latent\npolicy using unlabeled demonstrations and then map the learned latent actions to real actions with\na small amount of environment interaction. Peng et al. 46 ﬁrst use motion-capture methods to track\nagent positions in videos and then train RL agents to match these waypoints. Similarly, Behbahani\net al. 47 and Aytar et al. 48 task a RL agent to match waypoints; however, they construct waypoints that\nare embeddings from unsupervised feature learning models. Pathak et al. 49 and Nair et al. 50 train\ngoal conditioned policies to take actions that advance the current state towards expert-provided goal\nstates expressed as high dimensional visual waypoints. Most similar to our own work, Torabi et al. 24\nsimultaneously train (1) an inverse dynamics model (IDM),51 which aims to uncover the underlying\naction between timesteps given observations of past and future timesteps, e.g. pIDM(at|ot, ot+1), and\n(2) a behavioral cloning (BC) model on trajectories of observations labeled with the IDM. Data to\ntrain the IDM is collected by rolling out the BC model in the target environment such that both\nmodels improve in tandem. However, at any point in training if there are sequences in the dataset that\nthe IDM performs poorly on, it requires that the BC model perform those or similar sequences in\norder for the IDM to improve and correctly label them. Therefore, if the BC model does not explore\nefﬁciently, it could severely slow down learning. In order to avoid this potential issue we opted for a\nsimpler two-stage approach: we ﬁrst train an IDM on a small number of labeled trajectories collected\nfrom human contractors (they play the game as would normally as we record their keypresses and\nmouse movements). Because human contractors reach most relevant parts of the state space, we can\nhold the IDM ﬁxed throughout BC training.\nCompared to most previous work in semi-supervised imitation learning, we experiment in the much\nmore complex and open-ended environment of Minecraft. Minecraft is a voxel-based 3D video\ngame that, due its popularity and wide variety of mechanics, has attracted a vast amount of RL\nresearch.27,28,30–34,52–60 A large body of work focuses on small, custom-made Minecraft worlds\nwith tasks such as navigation,53,60 block placing,54,55 instruction following,58,59 combat,56 and\nothers.28,31,57 Work operating in the massive, randomly generated environments of Minecraft itself\nhas included hill climbing,52 automated curriculum learning30 and, most closely related to the RL\nexperiments presented in Sec. 4.4, diamond mining.27,32–34 However, to the best of our knowledge,\nthere is no published work that operates in the full, unmodiﬁed human action space, which includes\ndrag-and-drop inventory management and item crafting.\n3\nCollecting “Clean” Data\nTraining the VPT Foundation Model\nvia Behavioral Cloning\nTraining the Inverse Dynamics Model (IDM)\n~270k hours\nunlabeled\nvideo\n~70k hours\nunlabeled\nvideo\n~2k hours\nvideo\nlabeled with\nactions\nFilter for “clean”\nvideo segments\nSearch for relevant\nMinecraft videos\nvia keywords\nContractors\ncollect data \nLabel videos\nwith IDM \n~70k hours\nvideo\nIDM-labeled\nwith actions\nTrain non-causal IDM\nTrain causal\nVPT Foundation Model\na\nd\nspace\nw\na\nd\nspace\nw\nFigure 2: Video Pretraining (VPT) Method Overview.\n3\nMethods\nInverse Dynamics Models (IDM)\nVPT, illustrated in Figure 2, requires we ﬁrst collect a small\namount of labeled contractor data with which to train an inverse dynamics model pIDM(at|o1...T ),\nwhich seeks to minimize the negative log-likelihood of an action at timestep t given a trajectory of T\nobservations ot : t ∈[1...T]. In contrast to an imitation learning policy, the IDM can be non-causal,\nmeaning its prediction for at can be a function of both past and future events, i.e. ot′>t. Compared to\nthe behavioral cloning objective of modeling the distribution of human intent given past frames only,\nwe hypothesize that inverting environment dynamics is easier and more data efﬁcient to learn. Indeed,\nSec. 4.1 will show that the IDM objective is much easier to learn, and furthermore Sec. 4.6 will show\nthat with very little labeled data (as few as 100 hours) we can train a fairly accurate IDM. This IDM\ncan be used to label online videos, providing the large amount of data required for the harder task of\nbehavioral cloning. See appendices D and B for IDM training and data collection details.\nData Filtering\nWe gather a large dataset of Minecraft videos by searching the web for related\nkeywords (Appendix A). Online videos often (1) include overlaid artifacts, such as a video feed\nof the player’s face, channel logos, watermarks, etc., (2) are collected from platforms other than\na computer with different gameplay, or (3) are from different game modes, e.g. in Minecraft we\nonly want \"survival mode\" where players start from scratch and must gather or craft all their items.\nWe call data “clean” if it does not contain visual artifacts and is from survival mode, and call all\nother data “unclean.” With enough data, a large enough model, and enough training compute, a BC\nmodel trained on both unclean and clean videos would likely still perform well in a clean Minecraft\nenvironment. However, for simplicity and training compute efﬁciency, we choose to ﬁlter out unclean\nsegments of video (note that a video may contain both clean and unclean segments). We do this by\ntraining a model to ﬁlter out unclean segments using a small dataset (8800) of images sampled from\nonline videos labeled by contractors as clean or unclean (Appendix A.2).\nVPT Foundation Model\nWe train a foundation model with standard behavioral cloning, i.e. mini-\nmizing the negative log-likelihood of actions predicted by the IDM on clean data. For a particular\ntrajectory of length T we minimize\nmin\nθ\nX\nt∈[1...T ]\n−log πθ(at|o1, . . . , ot), where at ∼pIDM(at|o1, . . . , ot, . . . , oT )\n(1)\nAs we will see in the following sections, this model exhibits nontrivial zero-shot behavior and can be\nﬁne-tuned with both imitation learning and RL to perform even more complex skills.\n4\nResults\n4.1\nPerformance of the Inverse Dynamics Model\nThe IDM architecture is comprised primarily of a temporal convolution layer, a ResNet62 image\nprocessing stack, and residual unmasked attention layers, from which the IDM simultaneously\npredicts keypresses and mouse movements (see Appendix D for IDM architecture and training\ndetails). A key hypothesis behind our work is that IDMs can be trained with a relatively small amount\nof labeled data. While more data improves both mouse movement and keypress predictions, our best\n4\nFigure 3: (Left) IDM keypress accuracy and mouse movement R2 (explained variance61) as a\nfunction of dataset size. (Right) IDM vs. behavioral cloning data efﬁciency.\nIDM trains on only 1962 hours of data (compared to the ∼70k hours of clean data we collected from\nthe internet) and achieves 90.6% keypress accuracy and a 0.97 R2 for mouse movements evaluated\non a held-out validation set of contractor-labeled data (Figure 3 left).\nFigure 3 (right) validates our hypothesis that IDMs are far more data efﬁcient than BC models, likely\nbecause inverting environment mechanics is far easier than modeling the entire distribution of human\nbehavior. The IDM is two orders of magnitude more data efﬁcient than a BC model trained on the\nsame data and improves more quickly with more data. This evidence supports the hypothesis that it is\nmore effective to use contractor data within the VPT pipeline by training an IDM than it is to train a\nfoundation model from contractor data directly (Sections 4.5 and 4.6 provide additional evidence).\n4.2\nVPT Foundation Model Training and Zero-Shot Performance\nFigure 4: (Left) Training and validation loss on the web_clean internet dataset with IDM pseudo-\nlabels, and loss on the main IDM contractor dataset, which has ground-truth labels but is out-of-\ndistribution (see text). (Right) Amount a given item was collected per episode averaged over 2500\n60-minute survival episodes as a function of training epoch, shaded with the standard error of the\nmean. Basic mining refers to collection of dirt, gravel, or sand (all materials that can be gathered\nwithout tools). Logs are obtained by repeatedly hitting trees for three seconds, a difﬁcult feat for an\nRL agent to achieve as we show in Sec. 4.4. Planks can be crafted from logs, and crafting tables\ncrafted from planks. Crafting requires using in-game crafting GUIs, and proﬁcient humans take a\nmedian of 50 seconds (970 consecutive actions) to make a crafting table.\nWe now explore the emergent behavior learned by a behavioral cloning policy trained on an extremely\nlarge, but noisy, internet dataset labeled with our IDM. To collect the unlabeled internet dataset,\nwe searched for publicly available videos of Minecraft play with search terms such as “minecraft\nsurvival for beginners.” These searches resulted in ∼270k hours of video, which we ﬁltered down to\n“clean” video segments yielding an unlabeled dataset of ∼70k hours, which we refer to as web_clean\n(Appendix A has further details on data scraping and ﬁltering). We then generated pseudo-labels\nfor web_clean with our best IDM (Section 3) and then trained the VPT foundation model with\nbehavioral cloning. Preliminary experiments suggested that our model could beneﬁt from 30 epochs\nof training and that a 0.5 billion parameter model was required to stay in the efﬁcient learning\nregime63 for that training duration (Appendix H), which took ∼9 days on 720 V100 GPUs.\nWe evaluate our models by measuring validation loss (Fig. 4, left) and rolling them out in the\nMinecraft environment. Unless otherwise noted, in all environment evaluations we spawn agents in a\nstandard survival mode game where they play for 60 minutes, i.e. 72000 consecutive actions, and we\nplot the mean and shade the standard error of the mean for various game statistics such as crafting\nand collection rates (Fig. 4, right). The VPT foundation model quickly learns to chop down trees\nto collect logs, a task we found near impossible for an RL agent to achieve with the native human\ninterface (Sec. 4.4). It also learns to craft those logs into wooden planks and then use those planks\n5\nto craft a crafting table, which are required to unlock most other technology in the game and take a\nhuman proﬁcient in Minecraft approximately 50 seconds (970 consecutive actions) to collect. While\nthese behaviors are fairly complex in the native human action space, the VPT foundation model crafts\nthese items at a rate far below that of our proﬁcient contractors, e.g. on average our contractors craft\n5.44 crafting tables in 60 minutes of play versus 0.19 for the foundation model. The model also crafts\na non-negligible amount of wooden sticks, which are required to make wooden tools; collects various\nﬂowers and crafts dyes from them; kills zombies that appear during the night; hunts wild animals;\ncollects various berries and mushrooms and eats them; and ﬁnds game-generated villages from which\nto collect various rare items from chests. The model also learned to navigate uneven terrain, swim,\nand pillar jump, which involves the agent repeatedly jumping and quickly placing a block below itself\nsuch that it climbs upward by making a pillar.(iv)\nWhile training and validation loss decrease healthily over training (Fig. 4, left), loss on our contractor\ndataset (which the VPT model does not train on) begins increasing after 7 epochs. Contractor data\ncould be out-of-distribution because our contractors may have a different distribution of play or\nbecause there is some impactful visual domain shift compared to videos from the web. While one\ncould have expected this would be predictive of declining evaluation performance, we do not see\nnotable game statistics from the VPT foundation model rollouts (Figure 4, right) decrease over\ntraining, and in the next section we show that BC ﬁne-tuning performance continually improves as the\nVPT foundation model trains. We provide more insight into this curious phenomenon in Appendix H.\n4.3\nFine-Tuning with Behavioral Cloning\nFoundation models are designed to have a broad behavior proﬁle and be generally capable across a\nwide variety of tasks. To incorporate new knowledge or allow them to specialize on a narrower task\ndistribution, it is common practice to ﬁne-tune these models to smaller, more speciﬁc datasets.1 The\nVPT foundation model trained on the broad web_clean dataset had nontrivial zero-shot performance;\nit was able to craft a crafting table yet unable to go past this in the technology tree. As a case\nstudy into BC ﬁne-tuning, we attempt to improve the VPT foundation model’s ability to collect\nand craft these “early game” items by ﬁne-tuning to two narrower datasets targeted at Minecraft\nbehavior within the ﬁrst few minutes of players starting in a fresh world. In the ﬁrst dataset,\ncontractor_house, contractors have 10 minutes to build a basic house from scratch using primarily\nwood, sand, and dirt. Collecting contractor data can be difﬁcult and expensive, so we also construct a\ndataset earlygame_keyword by searching for videos online with descriptions that match keywords\nsuch as “new world”, “let’s play episode 1”, etc.; this is a subset of web_clean and is labeled with\nthe IDM. See Appendix B.4 and A.3 for full descriptions of both datasets.\nEffect of Foundation Model Quality on BC Fine-Tuning\n59x\n213x\n59x\nFigure 5:\n(Left) Collection and crafting rates for three policies:\nthe zero-shot VPT foun-\ndation model, and the VPT foundation model BC ﬁne-tuned to the earlygame_keyword or\ncontractor_house datasets. BC ﬁne-tuning to either dataset improves performance, including (for\nthe contractor_house dataset) yielding wooden and stone tools. Proﬁcient Minecraft players take\na median of 1.2 minutes (1390 actions) to construct wooden tools and 2.3 minutes (2790 actions)\nto construct stone tools. (Right) Collection and crafting rates for VPT foundation model snapshots\nthroughout training after they are BC ﬁne-tuned to the contractor_house dataset. In general,\ncrafting-related behaviors increase throughout foundation model training. Fig. 4 deﬁnes the other\ntask terms (logs, planks, crafting tables, and total crafting).\n(iv)Sample videos: https:\/\/www.youtube.com\/playlist?list=PLNAOIb_agjf3U3rSvG_BCWqJ869NdBhcP\n6\nFine-tuning to earlygame_keyword results in a large boost compared to the zero-shot foundation\nmodel: 2.5x more crafting tables, 6.1x more planks, 4.3x more logs, and 5.5x more crafting overall\n(Fig. 5). However, when ﬁne-tuning to this dataset we did not see any new behaviors emerge,\nonly a reﬁnement of existing skills. We saw an even bigger improvement when ﬁne-tuning to the\ncontractor_house dataset: 213x more crafting tables, 59x more wooden planks, 7x more logs,\nand 59x more crafting over all. In addition, we saw the emergence of crafting wooden tools, which\nrequires placing a crafting table on the ground, opening it to reveal a new crafting interface, and then\nusing it to craft wooden tools. This entire sequence takes a proﬁcient human player a median of 1.2\nminutes (1390 consecutive actions) to accomplish. The model goes further and collects cobblestone,\nwhich requires a wooden pickaxe to mine, and crafts stone tools, requiring it to again use a crafting\ntable; this takes a proﬁcient human player a median of 2.3 minutes (2790 consecutive actions). We\nalso saw this model more frequently raiding villages that randomly spawn in the game, hunting\nanimals for food, in addition to many behaviors we saw performed by the foundation model.(v)\nDespite the foundation model’s zero-shot rollout performance plateauing 1\/3 into training (Fig. 4,\nright), ﬁne-tuning performance does continue to increase throughout foundation model training\n(Fig. 5, right). Additionally, there is a stark difference in performance when training from scratch vs.\nﬁne-tuning from the VPT foundation model (Fig. 5 right, comparing the left and rightmost points).\n4.4\nFine-Tuning with Reinforcement Learning\nFigure 6: Typical sequence of items for obtaining a diamond pickaxe. Below each item is the median\ntime and number of actions contractors required to obtain that item and the percentage of contractors\nthat got the item within 10 minutes. The median time to obtain a diamond pickaxe is unknown (except\nthat it is > 20m) because contractors obtained this item in less than 50% of 20-minute episodes.\nTo demonstrate the efﬁcacy of RL ﬁne-tuning, we chose the challenging goal of obtaining a diamond\npickaxe within 10 minutes starting from a fresh Minecraft survival world. Doing so involves acquiring\na sequence of difﬁcult-to-obtain items that require complex skills like mining, inventory management,\ncrafting with and without a crafting table, tool use, operating a furnace, and mining at the lowest\ndepths, where many hazards like enemies and lava exist (Fig. 6). Adding to the difﬁculty, progress\ncan be easily lost by dropping items, destroying items, or dying. Obtaining a diamond pickaxe more\noften than not takes a proﬁcient human over 20 minutes (24,000 actions).\nAgents are rewarded for each item obtained in the sequence, with lower rewards for items that have to\nbe collected in bulk and higher rewards for items near the end of the sequence. Agents are optimized\nwith the phasic policy gradient64 RL algorithm for ∼1.3 million episodes (roughly 1.4×1010 frames).\nEpisodes last for 10 minutes. See Appendix G.1 for reward function and RL training details. Due to\ncomputational constraints, RL experiments use a ∼248 million parameter VPT model (Appendix H).\nA major problem when ﬁne-tuning with RL is catastrophic forgetting65,66 because previously learned\nskills can be lost before their value is realized. For instance, while our VPT foundation model never\nexhibits the entire sequence of behaviors required to smelt iron zero-shot, it did train on examples of\nplayers smelting with furnaces. It therefore may have some latent ability to smelt iron once the many\nprerequisites to do so have been performed. To combat the catastrophic forgetting of latent skills\nsuch that they can continually improve exploration throughout RL ﬁne-tuning, we add an auxiliary\nKullback-Leibler (KL) divergence loss between the RL model and the frozen pretrained policy.10\nTraining from a randomly initialized policy fails to achieve almost any reward, underscoring how\nhard an exploration challenge the diamond pickaxe task is for RL in the native human action space\n(Fig. 7a). The model never learns to reliably collect logs, typically the ﬁrst of many steps to obtaining\na diamond pickaxe (Fig. 7b). RL ﬁne-tuning from the VPT foundation model does substantially\nbetter (Fig. 7a), learning everything up to mining iron ore and crafting furnaces. (Fig. 7c). However,\nthis agent fails at smelting an iron ingot, the next item required to get further into the tech tree, likely\n(v)Sample Videos: https:\/\/www.youtube.com\/playlist?list=PLNAOIb_agjf2yDSs4AqcoyPv4z_eWUiKm\n7\n0\n5\n10\n15\n20\n25\nReward\nReward over episodes\nRL from Rand. Init. model\nRL from VPT Found. model\nRL from Early-Game model\nNo KL-loss\n(a)\n0\n20\n40\n60\n80\n100\n% episodes\nRL from Rand. Init. model\nLogs\nPlanks\nSticks\nCrafting Tables\nWooden Pickaxe\nCobblestone\nStone Pickaxe\nCoal\nTorch\nFurnace\nIron Ore\nIron Ingot\nIron Pickaxe\nDiamonds\nDiamond Pickaxe\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nEpisodes\n1e6\n0\n20\n40\n60\n80\n100\n% episodes\nRL from VPT Found. model\n(c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nEpisodes\n1e6\n0\n20\n40\n60\n80\n100\n% episodes\n2.5%\nRL from Early-Game model\n(d)\nFigure 7: RL Fine-tuning results. (a) RL from a randomly initialized model fails to get almost\nany reward, RL ﬁne-tuning from the VPT foundation model performs substantially better with a\nreward near 13, and RL ﬁne-tuning from the early-game model performs best with a reward of 25.\nWhen training the early-game model without a KL loss to the original policy (No KL-loss) progress\nstalls after 100,000 episodes, suggesting that the skills necessary to make further progress have been\ncatastrophically forgotten. (b) RL from a randomly initialized model occasionally collects sticks by\nbreaking leaves (an easy but inefﬁcient method of getting sticks that does not require logs or planks)\nand never learns to reliably collect logs. (c) RL ﬁne-tuning from the VPT Foundation model learns\neverything in the curriculum up to iron ore and making furnaces, but fails to learn to use the furnace to\nsmelt iron ingots. (d) RL ﬁne-tuning from the early-game model learns to obtain (at human-level) all\nitems in the sequence towards a diamond pickaxe and crafts a diamond pickaxe in 2.5% of episodes.\nbecause the zero-shot probability that the VPT foundation model smelts an iron ingot is too low, even\nwhen given the prerequisite materials.\nResults further improve by ﬁrst BC ﬁne-tuning the VPT Foundation Model to the\nearlygame_keyword dataset (the early-game model, Sec. 4.3) and then ﬁne-tuning with RL\n(Fig. 7a), which in preliminary experiments we found to perform better than ﬁrst ﬁne-tuning to\ncontractor_house followed by ﬁne-tuning with RL (Appendix G.2). The three-phase training\n(pretraining, BC ﬁne-tuning, and then RL ﬁne-tuning) succeeds in learning extremely difﬁcult tasks:\nit achieves over 80% reliability on iron pickaxes, almost 20% reliability on collecting diamonds, and\n2.5% reliability on obtaining a diamond pickaxe (Fig. 7d). For comparison, human players given\nthe objective of obtaining a diamond pickaxe collect these items in 57%, 15%, and 12% of episodes,\nrespectively, meaning our model is human-level for crafting iron pickaxes and mining diamonds.\nOthers have managed to obtain diamonds with ∼0.1% reliability in 15 minutes32,33 but always with a\nsimpliﬁed action space designed to ease exploration. To the best of our knowledge, we are the ﬁrst to\nreport non-zero success rates on crafting a diamond pickaxe. Qualitatively, the model developed\nuseful skills for diamond mining, such as efﬁcient mining patterns, cave exploration, returning to\npreviously placed objects like crafting tables, and advanced techniques like using wooden pickaxes\nas fuel when moving on to iron tools.(vi)\nFinally, we validated the importance of the KL loss to the pretrained model during RL ﬁne-tuning.\nThe treatment without a KL loss obtains only items early in the sequence (logs, planks, sticks, and\ncrafting tables) limiting its reward (Fig. 7a). This failure to progress further into the sequence is\nlikely because, while the initial skills of chopping logs and crafting planks are being learned with RL,\nsubsequent skills like crafting a wooden pickaxe are lost due to catastrophic forgetting.\n4.5\nData Scaling Properties of the Foundation Model\nIn this section we validate a core hypothesis behind this work: that it is far more effective to use\nlabeled contractor data to train an IDM within the VPT method than it is to directly train a BC\nfoundation model from that same small contractor dataset. If we could cheaply collect a labeled\ncontractor dataset of a similar order of magnitude as web_clean, then this would not be important;\nhowever, collecting that scale of data would have cost millions of dollars. Figure 8 compares\nfoundation models trained on increasing orders of magnitude of data from 1 hour up to the full ∼70k\nweb_clean dataset. Foundation models trained up to and including 1k hours are trained on the IDM\n(vi)Videos found at https:\/\/www.youtube.com\/playlist?list=PLNAOIb_agjf3e_UKweM5pQUSfTw8r-Wfc\n8\nTrained on\nContractor Data\nTrained on IDM\nLabeled Web Data\nFigure 8: (Left) Zero-shot rollout performance of foundation models trained on varying amounts\nof data. Models to the left of the dashed black line (points ≤1k hours) were trained on contractor\ndata (ground-truth labels), and models to the right were trained on IDM pseudo-labeled subsets\nof web_clean. Due to compute limitations, this analysis was performed with smaller (71 million\nparameter) models except for the ﬁnal point, which is the 0.5 billion parameter VPT foundation\nmodel. (Right) The corresponding performance of each model after BC ﬁne-tuning each model to\nthe contractor_house dataset.\ncontractor data, and those trained on 5k hours and above are trained on subsets of web_clean, which\ndoes not contain any IDM contractor data. Scaling training data increases log collection, mining, and\ncrafting capabilities. The zero-shot model only begins to start crafting crafting tables at over 5000\nhours of training data. When ﬁne-tuning each foundation model to contractor_house, we see that\ncrafting rates for crafting tables and wooden tools increase by orders of magnitude when using the\nentire ∼70k hour web_clean dataset. We furthermore only see the emergence of crafting stone tools\nat the largest data scale.\n4.6\nEffect of Inverse Dynamics Model Quality on Behavioral Cloning\nFigure 9: Zero-shot performance of BC models\ntrained from scratch on the earlygame_keyword\ndataset labeled with IDMs that were trained on\nincreasing amounts of contractor data.\nThis section investigates how downstream\nBC performance is affected by IDM qual-\nity.\nWe train IDMs on increasingly larger\ndatasets and use each to independently label\nthe earlygame_keyword dataset (this smaller\ndataset was chosen due to a limited compute bud-\nget). We then train a BC model from scratch on\neach dataset and report game statistics for each\nmodel as a function of IDM contractor dataset\nsize (Fig. 9).\nIDMs trained on at least 10 hours of data are\nrequired for any crafting, and the crafting rate\nincreases quickly up until 100 hours of data, after which there are few to no gains and differences are\nlikely due to noise. Similarly, crafting tables are only crafted after 50 or more hours of IDM data, and\nagain gains plateau after 100 hours. While in all previous experiments we use our best IDM trained\non 1962 hours of data, these results suggest we could reduce that number to as low as 100 hours.\n5\nDiscussion and Conclusion\nThe results presented in this paper help pave the path to utilizing the wealth of unlabeled data on the\nweb for sequential decision domains. Compared to generative video modeling or contrastive methods\nthat would only yield representational priors, VPT offers the exciting possibility of directly learning\nto act during pretraining and using these learned behavioral priors as extremely effective exploration\npriors for RL. VPT could even be a better general representation learning method even when the\ndownstream task is not learning to act in that domain—for example, ﬁne-tuning to explain what is\nhappening in a video—because arguably the most important information in any given scene would be\npresent in features trained to correctly predict the distribution over future human actions. We leave\nthis intriguing direction to future work.\nFuture work could improve results with more data (we estimate we could collect >1M hours) and\nlarger, better-tuned models. Furthermore, all the models in this work condition on past observations\nonly; we cannot ask the model to perform speciﬁc tasks. Appendix I presents preliminary experiments\non conditioning our models on closed captions (text transcripts of speech in videos), showing they\n9\nbecome weakly steerable; we believe this a rich direction for future research. Also, loss was not\nconsistently correlated with downstream evaluation metrics (Sec. 4.2), which often made progress\nslow and hard-won. Another fruitful future direction would be to investigate the correlation between\nvarious training metrics and downstream evaluations. Finally, while we do not anticipate any direct\nnegative societal impacts from the models trained in this work, as VPT improves and expands to other\ndomains it will be important to assess and mitigate harms that emerge with other forms of pretraining\non internet datasets, such as emulating inappropriate behavior.67\nIn conclusion, VPT extends the paradigm of training large and general purpose behavioral priors from\nfreely available internet-scale data to sequential decision domains. Our models exhibited impressive\nzero-shot behavior and, when ﬁne-tuned with RL, achieved an unprecedented result of crafting a\ndiamond pickaxe in Minecraft (all the more difﬁcult given the human interface). We further showed\nthat contractor data is far better used within the VPT pipeline than to train a foundation model directly\nand that only a small amount of contractor data (about $2000 USD) was required to unlock massive\namounts of unlabeled online data for use in BC. Finally, learning with the human keyboard and mouse\ninterface is highly general and allows losslessly modeling the entire distribution of human behavior.\nWhile we only experiment in Minecraft, we believe that VPT provides a general recipe for training\nbehavioral priors in hard, yet generic, action spaces in any domain that has a large amount of freely\navailable unlabeled data, such as computer usage.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 视频预训练（VPT）：通过观看无标签在线视频学习行动\n\n## 📌 背景痛点\/本文动机\n近年来，在自然语言处理和计算机视觉等领域，通过在大型互联网数据集上进行预训练，已经证明了训练大型通用基础模型的有效性。然而，对于许多序列决策领域，如机器人、视频游戏和计算机使用，公开可用的数据并不包含训练行为先验所需的标签。本文旨在通过利用互联网上大量未标记的视频数据，将这些预训练范式扩展到序列决策领域。\n\n## 🚀 核心方法\n💡 创新点1：半监督模仿学习\n本文提出了一种半监督模仿学习方法，通过观看在线未标记的视频，使智能体学会行动。具体来说，使用少量标记数据训练一个逆动力学模型，该模型足够准确，可以标记大量未标记的在线数据（例如，人们玩Minecraft的视频），然后从中训练一个通用的行为先验。\n\n💡 创新点2：逆动力学模型\n与行为克隆相比，逆动力学建模任务更简单，因为它是非因果的，这意味着它可以查看过去和未来的帧来推断动作。在大多数情况下，环境机制比环境中可能发生的人类行为的广度要简单得多，这表明非因果逆动力学模型可能需要比因果行为克隆模型少得多的数据来训练。\n\n## 📈 实验结果\n实验结果表明，尽管使用了原生人类界面（20Hz的鼠标和键盘），但这种方法的行为先验具有非平凡的零样本能力，并且可以通过模仿学习和强化学习进行微调，以解决强化学习无法从头学习的困难探索任务。对于许多任务，模型表现出人类水平的性能，并且是第一个报告能够制作钻石工具的计算机代理，这需要熟练的人类玩家超过20分钟（24,000个环境动作）的游戏时间才能完成。\n\n## 💬 可借鉴之处\n本文提出的视频预训练（VPT）方法为利用互联网上大量未标记的数据进行序列决策领域的预训练提供了一种新的思路。该方法不仅能够有效地利用少量标记数据，而且能够通过模仿学习和强化学习进行微调，以解决强化学习无法从头学习的困难探索任务。此外，该方法还可以应用于任何具有大量未标记数据的领域，具有广泛的应用前景。","llm_summary_res_status":200}
{"title":"GROOT: Learning to Follow Instructions by Watching Gameplay Videos","authors":"Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, Yitao Liang","summary":"We study the problem of building a controller that can follow open-ended\ninstructions in open-world environments. We propose to follow reference videos\nas instructions, which offer expressive goal specifications while eliminating\nthe need for expensive text-gameplay annotations. A new learning framework is\nderived to allow learning such instruction-following controllers from gameplay\nvideos while producing a video instruction encoder that induces a structured\ngoal space. We implement our agent GROOT in a simple yet effective\nencoder-decoder architecture based on causal transformers. We evaluate GROOT\nagainst open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the\nhuman-machine gap as well as exhibiting a 70% winning rate over the best\ngeneralist agent baseline. Qualitative analysis of the induced goal space\nfurther demonstrates some interesting emergent properties, including the goal\ncomposition and complex gameplay behavior synthesis. The project page is\navailable at https:\/\/craftjarvis-groot.github.io.","url":"http:\/\/arxiv.org\/abs\/2310.08235v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.08235v2","published":1697110261000,"comment":null,"pdf_text":"GROOT: Learning to Follow Instructions by\nWatching Gameplay Videos\nShaofei Cai1, Bowei Zhang1, Zihao Wang1, Xiaojian Ma3, Anji Liu2 and Yitao Liang1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis,\nWe study the problem of building a controller that can follow open-ended instructions in open-world\nenvironments. We propose to follow reference videos as instructions, which offer expressive goal\nspecifications while eliminating the need for expensive text-gameplay annotations. A new learning\nframework is derived to allow learning such instruction-following controllers from gameplay videos\nwhile producing a video instruction encoder that induces a structured goal space. We implement our\nagent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers.\nWe evaluate GROOT against open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap\nas well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis. The project page is available at https:\n\/\/craftjarvis-groot.github.io.\nFigure 1 | Through the cultivation of extensive gameplay videos, GROOT has grown a rich set of skill fruits\n(number denotes success rate; skills shown above do not mean to be exhaustive; kudos to our artist Haowei).\n1. Introduction\nDeveloping human-level embodied agents that can solve endless tasks in open-world environments,\nsuch as Minecraft (Fan et al., 2022; Johnson et al., 2016), has always been a long-term goal pursued\nin AI. Recent works have explored using Large Language Models (LLMs) to generate high-level plans,\nwhich guide the agent to accomplish challenging long-horizon tasks (Wang et al., 2023a,b; Zhu\net al., 2023). However, a major gap between these LLM-based agents and generalist agents that can\ncomplete endless amounts of tasks is the capability of their low-level controllers, which map the plans\nto motor commands. Recently developed controllers are only capable of completing a predefined and\nnarrow set of programmatic tasks (Baker et al., 2022; Cai et al., 2023; Lin et al., 2021), which hinders\nLLM-based planning agents from unleashing their full potential. We attribute the limitation of these\nlow-level controllers to how the goal is specified. Specifically, existing controllers use task indicator\nCorresponding author(s): Yitao Liang\nShaofei Cai<caishaofei@stu.pku.edu.cn>, Bowei Zhang<zhangbowei@stu.pku.edu.cn>,Zihao Wang<zhwang@stu.pku.edu.cn>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Anji Liu<liuanji@cs.ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2310.08235v2  [cs.AI]  29 Nov 2023\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(Yu et al., 2019), future outcome (Chen et al., 2021; Lifshitz et al., 2023), and language (Brohan\net al., 2022) to represent the goal. While it is easy to learn a controller with some of these goal\nspecifications, they may not be expressive enough for diverse tasks. Taking future outcome goals as an\nexample, an image of a desired house clearly lacks procedural information on how the house was built.\nOne exception is language, but learning a controller that can receive language goals is prohibitively\nexpensive as it requires a huge number of trajectory-text pairs with text that precisely depicts the full\ndetails of the gameplay, therefore preventing them from scaling up to more open-ended tasks.\nHaving observed the limitations of goal specification in the prior works, this paper seeks to find\na balance between the capacity of goal specification and the cost of controller learning. Concretely,\nwe propose to specify the goal as a reference gameplay video clip. While such video instruction is\nindeed expressive, there are two challenges: 1) How can the controller understand the actual goal\nbeing specified as the video itself can be ambiguous, i.e. a goal space or video instruction encoder\nhas to be learned; 2) How to ultimately map such goal to actual motor commands? To this end, we\nintroduce a learning framework that simultaneously produces a goal space and a video instruction\nfollowing controller from gameplay videos. The fundamental idea is casting the problem as future\nstate prediction based on past observations:\n• The predicting model needs to identify which goal is being pursued from the past observations,\nwhich requires a good goal space (induced by a video instruction encoder);\n• Since the transition dynamics model is fixed, a policy that maps both the state and the recognized\ngoal to action is also needed by the predicting model when rolling the future state predictions.\nEffectively, this results in the goal space and control policy we need. We introduce a variational learning\nobjective for this problem, which leads to a combination of a cloning loss and a KL regularization loss.\nBased on this framework, we implement GROOT, an agent with an encoder-decoder architecture to\nsolve open-ended Minecraft tasks by following video instructions. The video encoder is a non-causal\ntransformer that extracts the semantic information expressed in the video and maps it to the latent\ngoal space. The controller policy is a decoder module implemented by a causal transformer, which\ndecodes the goal information in the latent space and translates it into a sequence of actions in the\ngiven environment states in an autoregressive manner.\nTo comprehensively evaluate an agent’s mastery of skills, we designed a benchmark called\nMinecraft SkillForge. The benchmark covers six common Minecraft task groups: collect, build,\nsurvive, explore, tool, and craft, testing the agent’s abilities in resource collection, structure\nbuilding, environmental understanding, and tool usage, in a total of 30 tasks. We calculate Elo\nratings among GROOT, several counterparts, and human players based on human evaluations. Our\nexperiments showed that GROOT is closing the human-machine gap and outperforms the best\nbaseline by 150 points (or 70% winning rate) in an Elo tournament system. Our qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis.\nTo sum up, our main contributions are as follows:\n• Start by maximizing the log-likelihood of future states given past ones, we have discovered the\nlearning objectives that lead to a good goal space and ultimately the instruction-following controller\nfrom gameplay videos. It provides theoretical guidance for the agent architecture design and model\noptimization.\n• Based on our proposed learning framework, we implemented a simple yet efficient encoder-\ndecoder agent based on causal transformers. The encoder is responsible for understanding the goal\ninformation in the video instruction while the decoder as the policy emits motor commands.\n• On our newly introduced benchmark, Minecraft SkillForge, GROOT is closing the human-machine\n2\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\ngap and surpassing the state-of-the-art baselines by a large margin in the overall Elo rating com-\nparison. GROOT also exhibits several interesting emergent properties, including goal composition\nand complex gameplay behavior synthesis.\n2. Preliminaries and Problem Formulation\nReinforcement Learning (RL) concerns the problem in which an agent interacts with an environment\nat discrete time steps, aiming to maximize its expected cumulative reward (Espeholt et al., 2018; Mnih\net al., 2015; Schulman et al., 2017). Specifically, the environment is defined as a Markov Decision\nProcess (MDP) ⟨S, A, R, P, 𝑑0⟩, where S is the state space, A is the action space, R : S × A →ℝis\nthe reward function, P : S × A →S is the transition dynamics, and 𝑑0 is the initial state distribution.\nOur goal is to learn a policy 𝜋(𝑎|𝑠) that maximizes the expected cumulative reward 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑡], where\n𝛾∈(0, 1] is a discount factor.\nIn goal-conditioned RL (GCRL) tasks, we are additionally provided with a goal 𝑔∈G (Andrychowicz\net al., 2017; Cai et al., 2023; Ding et al., 2019; Jing et al., 2020, 2021; Liu et al., 2022; Yang et al.,\n2019). And the task becomes learning a goal-conditioned policy 𝜋(𝑎|𝑠, 𝑔) that maximizes the expected\nreturn 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑔\n𝑡], where 𝑟𝑔\n𝑡is the goal-specific reward achieved at time step 𝑡. Apart from being\na new type of RL task, GCRL has been widely studied as a pre-training stage toward conquering\nmore challenging environments\/tasks (Aytar et al., 2018b; Baker et al., 2022; Zhang et al., 2022).\nSpecifically, suppose we are provided with a good goal-condition policy, the goal can be viewed as a\nmeta-action that drives the agent to accomplish various sub-tasks, which significantly simplifies tasks\nthat require an extended horizon to accomplish. Further, when equipped with goal planners, we can\nachieve zero- or few-shot learning on compositional tasks that are beyond the reach of RL algorithms\n(Gong et al., 2023; Huang et al., 2022; Wang et al., 2023a,b; Zhu et al., 2023).\nAt the heart of leveraging such benefits, a key requirement is to have a properly-defined goal\nspace that (i) has a wide coverage of common tasks\/behaviors, and (ii) succinctly describes the task\nwithout including unnecessary information about the state. Many prior works establish goal spaces\nusing guidance from other modalities such as language (Cai et al., 2023; Hong et al., 2020; Stone\net al., 2023) or code (Huang et al., 2023; Wang et al., 2023a). While effective, the requirement on\nlarge-scale trajectory data paired with this auxiliary information could be hard to fulfill in practice.\nInstead, this paper studies the problem of simultaneously learning a rich and coherent goal space\nand the corresponding goal-conditioned policy, given a pre-trained inverse dynamic model and raw\ngameplay videos, i.e. sequences of states {𝑠(𝑖)\n0:𝑇}𝑖collected using unknown policies.\n3. Goal Space Discovery via Future State Prediction\nThis section explains our learning framework: discovering a “good” goal space as well as a video\ninstruction following the controller through the task of predicting future states given previous ones.\nWe start with an illustrative example in Minecraft (Johnson et al., 2016). Imagine that an agent is\nstanding inside a grassland holding an axe that can be used to chop the tree in front of them. Suppose\nin the gameplay video, players either go straight to chop the tree or bypass it to explore the territory.\nIn order to predict future frames, it is sufficient to know (i) which goal (chop tree or bypass tree) is\nbeing pursued by the agent, and (ii) what will happen if the agent chooses a particular option (i.e.\ntransition dynamics). Apart from the latter information that is irrelevant to the past observations, we\nonly need to capture the goal information, i.e. whether the agent decides to chop the tree or bypass\nthe tree. Therefore, the task of establishing a comprehensive yet succinct goal space can be interpreted\nas predicting future states while conditioning on the transition dynamics of the environment.\n3\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nFormally, our learning objective is to maximize the log-likelihood of future states given past\nones: log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡). Define 𝑔as a latent variable conditioned on past states (think of it as the\npotential goals the agent is pursuing given past states), the evidence lower-bound of the objective\ngiven variational posterior 𝑞𝜙(𝑔|𝑠0:𝑇) is the following (see Appendix A for the derivation of this and\nthe following equations):\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡) = log\n∑︁\n𝑔\n𝑝𝜃(𝑠𝑡+1:𝑇, 𝑔|𝑠0:𝑡)\n≥𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇) [log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔)] −𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n,\nwhere 𝐷KL(·∥·) denotes the KL-divergence. Next, we break down the first term (i.e. 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔))\ninto components contributed by the (unknown) goal-conditioned policy 𝜋𝜃(𝑎|𝑠, 𝑔) and the transition\ndynamics 𝑝𝜃(𝑠𝑡+1|𝑠0:𝑡, 𝑎𝑡) :\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔) =\n𝑇\n∑︁\n𝜏=𝑡\nlog\n∑︁\n𝑎𝜏\n𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) · 𝑝𝜃(𝑠𝜏+1|𝑠0:𝜏, 𝑎𝜏)\n≥\n𝑇\n∑︁\n𝜏=𝑡\n𝔼𝑎𝜏∼𝑝𝜃(𝑎𝜏|𝑠0:𝜏+1)\n\u0002\nlog 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) + 𝐶\n\u0003\n,\nwhere the constant 𝐶contains terms that depend solely on the environment dynamics and are\nirrelevant to what we want to learn (i.e. the goal space and the goal-conditioned policy). Bring it\nback to the original objective, we have\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) ≥\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇),𝑎𝜏∼𝑝𝜃(·|𝑠0:𝜏+1) [log 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔)]\n|                                                         {z                                                         }\nbehaviour cloning\n−\n𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n|                               {z                               }\ngoal space constraint (KL regularization)\n,\nwhere 𝑞𝜙(·|𝑠0:𝑇) is implemented as a video encoder that maps the whole state sequence into the latent\ngoal space. 𝑝𝜃(·|𝑠0:𝜏+1) is the inverse dynamic model (IDM) that predicts actions required to achieve\na desired change in the states, which is usually a pre-trained model, details are in Appendix C. Thus,\nthe objective can be explained as jointly learning a video encoder and a goal-controller policy through\nbehavior cloning under succinct goal space constraints.\n4. GROOT Architecture Design and Training Strategy\nThis section illustrates how to create an agent (we call it GROOT) that can understand the semantic\nmeaning of a reference video and interact with the environment based on the aforementioned learning\nframework. According to the discussion in Section 3, the learnable parts of GROOT include the\nvideo encoder and the goal-conditioned policy. Recently, Transformer (Vaswani et al., 2017) has\ndemonstrated effectiveness in solving sequential decision-making problems (Brohan et al., 2022; Chen\net al., 2021; Parisotto et al., 2019). Motivated by this, we implement GROOT with transformer-based\nencoder-decoder architecture, as shown in Figure 2. The video encoder is a non-causal transformer\nthat extracts semantic information and generates goal embeddings. The policy is a causal transformer\ndecoder that receives the goal embeddings as the instruction and autoregressively translates the state\nsequence into a sequence of actions. Next, we describe how each module is constructed together with\nthe training strategy.\n4\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nVideo Encoder\nDecoder as Policy\n𝑠!\n𝑠\"\n𝑠#\n𝑠!\n𝑠\"\n𝑎#\n𝑎$\n𝑎\"\nVideo Encoder\nDecoder as Policy\n𝑠̂#\n𝑠̂%\n𝑠̂\"\n𝑠#\n𝑠%\n𝑠!\n𝑎#\n𝑎%\n𝑎!\n𝑠%\n𝑠!\nBehavior Cloning\nReference Video\nStates\nStates\nRollout Observations\n(a) Training\n(b) Inference\n𝑐̂#\n𝑐̂&\nLearnable Tokens\nLearned Tokens\n1.0\n0.5\n0.0\n𝜇\n𝒒𝒈𝟏:𝑵𝒔𝟏:𝑻)\nsample\n𝑠#\n𝑐#\n𝑐&\n𝑐#\n𝑐&\n𝑐̂#\n𝑐̂&\nEnvironment\n⋯\n⋯\n⋯\n⋯\n⋯\n1.0\n0.5\n0.0\n𝜇\n𝒑𝒈𝟏:𝑵𝒔𝟏:𝒕)\n𝑫𝑲𝑳\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n𝑠#\n⋯\n𝝁(⋅)\nFigure 2 | Our GROOT agent architecture. Left: In the training stage, a video encoder (non-causal\ntransformer) learns to extract the semantic meaning and transfer the video (state sequence) into the goal\nembedding space. A goal-conditioned policy (causal transformer) is learned to predict actions following the\ngiven instructions. We learn the agent using behavior cloning under a KL constraint. Right: During the\ninference, any reference video is passed into the video encoder to generate the goal embeddings that drive the\npolicy to interact with the environment.\n4.1. Video Encoder\nThe video encoder includes a Convolutional Neural Network (CNN) to extract spatial information\nfrom image states 𝑠1:𝑇and a non-causal transformer to capture temporal information from videos.\nSpecifically, we use a CNN backbone to extract visual embeddings {𝑥1:𝑇} for all frames. Additionally,\nmotivated by Devlin et al. (2019); Dosovitskiy et al. (2020), we construct a set of learnable embeddings\n(or summary tokens), represented as {𝑐1:𝑁}, to capture the semantic information present in the video.\nThe visual embeddings and summary tokens are passed to a non-causal transformer, resulting in the\noutput corresponding to the summary tokens as {ˆ𝑐1:𝑁}\n𝑥1:𝑇←Backbone(𝑠1:𝑇),\nˆ𝑐1:𝑁←Transformer([𝑥1:𝑇, 𝑐1:𝑁]).\n(1)\nSimilar to VAE (Kingma & Welling, 2013), we assume that the latent goal space follows a Gaussian\ndistribution, hence we use two fully connected layers, 𝜇(·) and 𝜎(·), to generate the mean and\nstandard deviation of the distribution, respectively. During training, we use the reparameterization\ntrick to sample a set of embeddings {𝑔1:𝑁} from the distribution, where 𝑔𝑡∼N (𝜇(ˆ𝑐𝑡), 𝜎(ˆ𝑐𝑡)). During\ninference, we use the mean of the distribution as the goal embeddings, i.e. 𝑔𝑡←𝜇(ˆ𝑐𝑡).\n4.2. Decoder as Policy\nTo introduce our policy module, we start with VPT (Baker et al., 2022), a Minecraft foundation model\ntrained with standard behavioral cloning. It is built on Transformer-XL (Dai et al., 2019) that can\nleverage long-horizon historical states and predict the next action seeing the current observation.\nHowever, the vanilla VPT architecture does not support instruction input. To condition the policy\non goal embeddings, we draw the inspiration from Flamingo (Alayrac et al., 2022), that is, to insert\ngated cross-attention dense layers into every Transformer-XL block. The keys and values in these layers\nare obtained from goal embeddings, while the queries are derived from the environment states\nˆ𝑥(𝑙)\n1:𝑡←GatedXATTN(kv = 𝑔1:𝑁, q = 𝑥(𝑙−1)\n1:𝑡\n; 𝜃𝑙),\n𝑥(𝑙)\n1:𝑡←TransformerXL(qkv = ˆ𝑥(𝑙)\n1:𝑡; 𝜃𝑙),\nˆ𝑎𝑡←FeedForward(𝑥(𝑀)\n𝑡\n),\n(2)\n5\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nwhere the policy reuses the visual embeddings extracted by the video encoder, i.e. 𝑥(0)\n1:𝑡= 𝑥1:𝑡, the\npolicy consists of 𝑀transformer blocks, 𝜃𝑙is the parameter of 𝑙-th block, ˆ𝑎𝑡is the predicted action.\nSince our goal space contains information about how to complete a task that is richer than previous\nlanguage-conditioned policy (Cai et al., 2023; Lifshitz et al., 2023), the cross-attention mechanism is\nnecessary. It allows the GROOT to query the task progress from instruction information based on\npast states, and then perform corresponding behaviors to complete the remaining progress.\n4.3. Training and Inference\nThe training dataset can be a mixture of Minecraft gameplay videos and offline trajectories. For\nthose videos without actions, an inverse dynamic model (Baker et al., 2022) can be used to generate\napproximate actions. Limited by the computation resources, we truncated all the trajectories into\nsegments with a fixed length of 𝑇without using any prior. We denote the final dataset as D =\n{(𝑥1:𝑇, 𝑎1:𝑇)}𝑀, where 𝑀is the number of trajectories. We train GROOT in a fully self-supervised\nmanner while the training process can be viewed as self-imitating, that is, training GROOT jointly\nusing behavioral cloning and KL divergence loss\nL(𝜃, 𝜙) = 𝔼(𝑠,𝑎)∼D\n\"\n𝜆𝐵𝐶\n∑︁\n𝑡\n−log 𝜋𝜃(𝑎𝑡|𝑠1:𝑡, 𝑔) + 𝜆𝐾𝐿\n∑︁\n𝜏\n𝐷𝐾𝐿\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝜏)\u0001\n#\n,\n(3)\nwhere 𝜆𝐵𝐶, 𝜆𝐾𝐿are tradeoff coefficients, 𝑞𝜙is a posterior visual encoder, 𝑝𝜃is a prior video encoder\nwith the same architecture, 𝑔∼𝑞𝜙(·|𝑠0:𝑇). More details are in the Appendix D.\n5. Result\n5.1. Performance on Mastering Minecraft Skills\nMinecraft SkillForge Benchmark. In order to comprehensively evaluate the mastery of tasks by\nagents in Minecraft, we created a diverse benchmark called Minecraft SkillForge. It covers 30 tasks\nfrom 6 major categories of representative skills in Minecraft, including collect, explore, craft,\ntool, survive, and build. For example, the task “dig three down and fill one up” in the build\ncategory asks the agent to first dig three blocks of dirt, then use the dirt to fill the space above; The\ntask of “building a snow golem” ( ) requires the agent to sequentially stack 2 snow blocks (\n) and\n1 carved pumpkin (\n). We put the details of this benchmark in the Appendix H. Apart from some\nrelatively simple or common tasks such as “collect wood” and “hunt animals”, other tasks require the\nagent to have the ability to perform multiple steps in succession.\nWe compare GROOT with the following baselines: (a) VPT (Baker et al., 2022), a foundation\nmodel pre-trained on large-scale YouTube data, with three variants: VPT (fd), VPT(bc), and VPT(rl),\nindicating vanilla foundation model, behavior cloning finetuned model, and RL finetuned model; (b)\nSTEVE-1 (Lifshitz et al., 2023), an instruction-following agent finetuned from VPT, with two variants:\nSTEVE-1 (visual) and STEVE-1 (text) that receives visual and test instructions. More details are in\nAppendix F.1. It is worth noting that GROOT was trained from scratch.\nHuman Evaluation with Elo Rating. We evaluated the relative strength of agents by running an\ninternal tournament and reporting their Elo ratings, as in Mnih et al. (2015). Before the tournament,\neach agent is required to generate 10 videos of length 600 on each task. Note that, all the reference\nvideos used by GROOT are generated from another biome to ensure generalization. Additionally,\nwe also invited 3 experienced players to do these tasks following the same settings. After the video\ncollection, we asked 10 players to judge the quality of each pair of sampled videos from different\n6\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Elo Rating Comparison\n(b) Winning Rate of GROOT vs. Baselines\n(c) Success Rate Comparison\nFigure 3 | Results on Minecraft SkillForge benchmark. Left: Tournament evaluation of GROOT assessed\nby human players. GROOT performs better than state-of-the-art Minecraft agent STEVE-1. A 150-score gap\ncorresponds to a 70% probability of winning. Middle: Winning rate of GROOT v.s. other agents on specific\ntask categories. Colors from red to blue denote a decrease in the winning rate. Apart from the human player,\nGROOT surpasses all other baselines. Right: Success rate on 9 representative tasks. GROOT champions\nprocess-oriented tasks, such as dig three and fill one (\n) and build snow golems ( ).\nagents. Considering the diversity of tasks, we designed specific evaluation criteria for every task to\nmeasure the quality of rollout trajectories. For example, in the task of “build snow golem”, we rank\nthe completion degree of the task in ascending order: no blocks placed, one type of block placed,\ntwo types of blocks placed, and snow golem built successfully. After 1500 comparisons, the Elo\nrating converged as in Figure 3 (left). Although there is a large performance gap compared with\nhuman players, GROOT has significantly surpassed the current state-of-the-art STEVE-1 series and\ncondition-free VPT series on the overall tasks. Additional details are in Appendix G.\nIn Figure 3 (middle), we compare GROOT with other baselines in winning rate on six task groups.\nWe found that except for the performance on craft tasks, where STEVE-1 (visual) outperforms our\nmodel, GROOT achieves state-of-the-art results. In particular, GROOT greatly outperforms other\nbaselines by a large margin on build and tool. For build, the goal space needs to contain more\ndetailed procedural information, which is the disadvantage of methods that use future outcomes as\nthe goal. Moreover, such tasks are distributed sparsely in the dataset, or even absent in the dataset,\nwhich requires the agent to have strong generalization ability. As for craft group, GROOT is not\nsuperior enough, especially on the “crafting table” task. We attribute this to the wide task distribution\nin the dataset. Thus the future outcomes can prompt STEVE-1 to achieve a high success rate.\nProgrammatic Evaluation. To quantitatively compare the performance of the agents, we selected\n9 representative tasks out of 30 and reported the success rate of GROOT, STEVE-1 (visual), and\nVPT (bc) on these tasks in Figure 3 (right). We found that, based on the success rate on tasks such\nas dye and shear sheep(\n), enchat sword (\n), smelt food (\n), use bow (\n), sleep\n(\n), and lead animals (\n), GROOT has already reached a level comparable to that of human\nplayers (100%). However, the success rates for build snow golems ( ) and build obsidian\n(\n) tasks are only 60% and 50%. By observing the generated videos, we found that GROOT cannot\nprecisely identify the items in Hotbar (such as buckets, lava buckets, snow blocks, and pumpkin\nheads), resulting in a low probability of switching to the correct item. STEVE-1 also has the same\nproblem. This may be due to the current training paradigm lacking strong supervisory signals at the\nimage level. Future work may introduce auxiliary tasks such as vision-question answering (VQA) to\nhelp alleviate this phenomenon. Details are in Appendix F.3.\n7\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Random Initialized\n(b) GROOT w\/o KL\n(c) GROOT w\/ KL\n(d) Synthesized Videos\nFigure 4 | t-SNE visualization of the goal space. Each color corresponds to a specific video category. (Left):\nSpace of randomly initialized video encoder. All the videos are entangled together. Middle: Space of GROOT\ntrained with self-supervised learning w\/ and w\/o KL regularization, respectively. The videos are clustered\nbased on their semantics. Visualization shows the subtle differences between the two. Right: Synthesized\nvideos using concatenation manner. The concatenated videos lay on the position between the source videos.\n5.2. Properties of Learned Goal Space\nThis section studies the properties of learned goal space. We used the t-SNE algorithm (van der Maaten\n& Hinton, 2008) to visualize the clustering effect of reference videos encoded in goal space, as in Figure\n4. We select 7 kinds of videos, including craft items, combat enemies, harvest crops,\nhunt animals, chop trees, trade with villagers, and mine ores. These videos are\nsampled from the contractor data (Baker et al., 2022) according to the meta information (details\nare in Appendix F.2). Each category contains 1k video segments. As a control group, in Figure 4\n(left), we showed the initial goal space of the video encoder (with a pre-trained EfficientNet-B0 (Tan\n& Le, 2019) as the backbone) before training. We found that the points are entangled together. After\nbeing trained on offline trajectories, as in Figure 4 (middle), it well understands reference videos and\nclusters them according to their semantics. This proves that it is efficient to learn behavior-relevant\ntask representations using our self-supervised training strategy. The clustering effect is slightly better\nwith KL regularization, though the difference is not very significant. Inevitably, there are still some\nvideos from different categories entangled together. We attribute this to the possibility of overlap\nin the performed behaviors of these videos. For example, chop trees and harvest crops both\nrely on a sequential of “attack” actions.\nCondition on Concatenated Videos. We also study the possibility of conditioning the policy\non concatenated videos. First, we collect 3 kinds of source videos, including chop trees, hunt\nanimals, and trade with villagers. We randomly sampled two videos from sources of chop\ntrees and hunt animals, downsampled and concatenated them into a synthetic video, denoted as\n[chop trees, hunt animals]. By the same token, we can obtain [hunt animals, trade\nwith villagers]. We visualize these videos together with the source videos in Figure 4 (right). We\nfound that the source videos lie far away from each other while the concatenated videos are distributed\nbetween their source videos. Based on this intriguing phenomenon, we infer that concatenated videos\nmay prompt GROOT to solve both tasks simultaneously. To verify this, we evaluate GROOT on\nthree kinds of reference videos, i.e. chop trees, hunt animals, and [chop trees, hunt\nanimals]. We launched GROOT in the forest and in the animal plains, respectively. The collected\nwood and killed mobs are reported in Figure 5. We found that although the concatenated video may\nnot be as effective as raw video in driving an agent to complete a single task (60% of the performance\nof raw video), it does possess the ability to drive the agent to perform multiple tasks. This is an\nimportant ability. As discussed in Wang et al. (2023b), sometimes the high-level planner will propose\nmultiple candidate goals, it will be efficient if the low-level controller can automatically determine\n8\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nchop\nhunt\nconcat\nWood Collected\n11.0\n1.5\n6.5\nchop\nhunt\nconcat\nMobs Killed\n0.5\n2.2\n1.5\nFigure 5 | Comparison on using raw and concate-\nnated reference videos as conditions. Left: Col-\nlected wood in the forest biome. Right: Killed mobs\nin the plains biome. “concat” denotes the reference\nvideo is [chop trees, hunt animals]. Statis-\ntics are measured over 10 episodes.\nw\/ KL\nw\/o KL\nSeagrass Collected\n3.7\n1.8\nw\/ KL\nw\/o KL\nWood Collected\n11.0\n7.3\nw\/ KL\nw\/o KL\nArrows Fired\n10.7\n9.3\nFigure 6 | Ablation study on KL loss.\nAfter being\njointly trained with KL loss, GROOT can collect 2×\nmore seagrass (\n) underwater and 1.5× wood (\n)\nin the forest while the difference is not as impressive\non the use bow (\n) task. Statistics are measured\nover 10 episodes.\ndiamonds distribute in this level\nGROOT\nSTEVE-1\ncondition changed\ncondition changed\ndiamonds distribute in this level\n~𝟏𝟔%\nstuck in the bedrock\nFigure 7 | Results on solving challenging obtain diamond task. The vertical dashed lines represent the\ntime when a certain item is first obtained. Left: GROOT first dug down to the depth of 12 and then mined\nhorizontally to obtain diamonds with an average success rate of 16%. Right: STEVE-1 quickly dug down to the\nspecific depth but struggled to maintain its height.\nwhich to accomplish based on the current observation.\nAblation on KL Divergence Loss. To investigate the role of KL loss in training, we evaluated\nGROOT (w\/ KL) and its variant (w\/o KL) on three tasks: collect seagrass (\n), collect\nwood (\n), and use bow (\n). As shown in Figure 6, we found that introducing the constraint of\nKL loss improved agent performance by 2× and 1.5× in the first two tasks, whereas there was no\nsignificant effect in the use bow task. This may be because the first two tasks require the agent to\ngeneralize the corresponding skills to different terrains (e.g. locating trees in the environment for\ncollecting wood and sinking to specific locations for collecting seagrass). Therefore, it puts higher\ndemands on the agent’s ability to generalize in the goal space, and this is exactly the role played by\nthe KL loss. The use bow task is relatively simple in comparison because it only requires charging\nand shooting the arrow, without considering environmental factors.\n5.3. Combining Skills for Long-horizon Tasks\nIn this section, we explore whether GROOT can combine skills to solve long-horizon tasks, which is\nkey to its integration with a high-level planner. Taking the task of mining diamonds as an example,\nprior knowledge is that diamond ores are generally distributed between the 7th and 14th floors\nunderground, and the probability of appearing in other depths is almost zero. Therefore, the agent\nneeds to first dig down to the specified depth (12) and then maintain horizontal mining. To achieve\nthis, we designed two reference videos, each 128 frames long. One describes the policy of starting\n9\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfrom the surface and digging down, and the other demonstrates the behaviors of horizontal mining.\nWe show an example in Figure 7 (left). In the beginning, GROOT quickly digs down to the specified\ndepth and then switches to horizontal mining mode. It maintains the same height for a long time and\nfound diamonds at 11k steps. In addition, we compared STEVE-1 (visual) under the same setting in\nFigure 7 (right). After switching to the horizontal mining prompt, STEVE-1 maintains its height for a\nshort time before it stuck in the bedrock layer (unbreakable in survival mode), greatly reducing the\nprobability of finding diamonds. This indicates that our goal space is expressive enough to instruct\nthe way of mining, and the policy can follow the instructions persistently and reliably. In contrast,\nSTEVE-1, which relies on future outcomes as a condition, was unable to maintain its depth, despite\nattempts at various visual prompts. We conducted 25 experiments each on GROOT and STEVE-1,\nwith success rates of 16% and 0% for finding diamonds. Additional details are in the Appendix F.4.\n6. Related Works\nPre-train Policy on Offline Data. Pre-training neural networks on web-scale data has been demon-\nstrated as an effective training paradigm in Nature Language Processing (Brown et al., 2020) and\nComputer Vision (Kirillov et al., 2023). Inspired by this, researchers tried to transfer the success\nto the field of decision-making from pre-training visual representations and directly distilling the\npolicy from offline data. As the former, Aytar et al. (2018a); Bruce et al. (2023) leveraged temporal\ninformation present in videos as the supervision signal to learn visual representations. The represen-\ntations are then used to generate intrinsic rewards for boosting downstream policy learning, which\nstill requires expensive online interactions with the environment. Chen et al. (2021); Schmidhuber\n(2019) leveraged scalable offline trajectories to train optimal policy by conditioning it on cumulated\nrewards. Laskin et al. (2022) proposed to learn an in-context policy improvement operator that\ncan distill an RL algorithm in high data efficiency. Reed et al. (2022) learned a multi-task agent\nGato by doing behavior cloning on a large-scale expert dataset. By serializing task data into a flat of\nsequence, they use the powerful transformer architecture to model the behavior distribution. However,\nthese methods either require elaborated reward functions or explicit task definitions. This makes\nit hard to be applied to open worlds, where tasks are infinite while rewards are lacking. Another\ninteresting direction is to use pre-trained language models for reasoning and vision language models\nfor discrimination, to guide the policy in life-long learning in the environment (Di Palo et al., 2023).\nCondition Policy on Goal Space. Researchers have explored many goal modalities, such as\nlanguage (Khandelwal et al., 2021), image (Du et al., 2021), and future video (Xie et al., 2023), to\nbuild a controllable policy. Brohan et al. (2022) collected a large-scale dataset of trajectory-text pairs\nand trained a transformer policy to follow language instructions. Despite the language being a natural\ninstruction interface, the cost of collecting paired training data is expensive. As a solution, Majumdar\net al. (2022) sorted to use hindsight relabeling to first train a policy conditioned on the target image,\nthen aligned text to latent image space, which greatly improves training efficiency. Lifshitz et al.\n(2023) moved a big step on this paradigm by replacing the target image with a 16-frame future video\nand reformulating the modality alignment problem into training a prior of latent goal given text.\nBuild Agents in Minecraft. As a challenging open-world environment, Minecraft is attracting an\nincreasing number of researchers to develop AI agents on it, which can be divided into plan-oriented\n(Wang et al., 2023a,b) and control-oriented methods (Baker et al., 2022; Cai et al., 2023; Lifshitz\net al., 2023) based on their emphasis. Plan-oriented agents aim to reason with Minecraft knowledge\nand decompose the long-horizon task into sub-tasks followed by calling a low-level controller. Control-\noriented works follow the given instructions and directly interact with the environments using low-level\nactions (mouse and keyboard). Baker et al. (2022) pre-trained the first foundation model VPT in\nMinecraft using internet-scale videos. Although it achieves the first obtaining diamond milestone by\n10\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfine-tuning with RL, it does not support instruction input. Lifshitz et al. (2023) created the first agent\nthat can solve open-ended tasks by bridging VPT and MineCLIP (Fan et al., 2022). However, its goal\nspace is not expressive enough and prevents it from solving multi-step tasks.\n7. Limitation and Conclusion\nAlthough GROOT has demonstrated powerful capabilities in expressing open-ended tasks in the\nform of video instructions, training such a goal space remains highly challenging. We found that\nGROOT is quite sensitive to the selection of reference videos, which we attribute to the fact that\nthe goal space trained from an unsupervised perspective may not be fully aligned with the human\nintention for understanding the semantics of the reference video. Therefore, it would be a promising\nresearch direction in the future to use SFT (supervised fine-tuning, Sanh et al. (2021)) and RLHF\n(Ziegler et al., 2019) to align the pre-trained goal space with human preference.\nIn conclusion, we propose a paradigm for learning to follow instructions by watching gameplay\nvideos. We prove that video instruction is a good form of goal space that not only expresses open-ended\ntasks but can be trained through self-imitation (once the IDM is available to label pseudo actions for raw\ngameplay videos). Based on this, we built an encoder-decoder transformer architecture agent named\nGROOT in Minecraft. Without collecting any text-video data, GROOT demonstrated extraordinary\ninstruction-following ability and crowned the Minecraft SkillForge benchmark. Additionally, we also\nshowed its potential as a planner downstream controller in the challenging obtain diamond task.\nWe believe that this training paradigm can be generalized in other complex open-world environments.\n11\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/GROOT: Learning to Follow Instructions by Watching Gameplay Videos.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n```\n#### 2. 论文摘要\n```\nWe study the problem of building a controller that can follow open-ended\ninstructions in open-world environments. We propose to follow reference videos\nas instructions, which offer expressive goal specifications while eliminating\nthe need for expensive text-gameplay annotations. A new learning framework is\nderived to allow learning such instruction-following controllers from gameplay\nvideos while producing a video instruction encoder that induces a structured\ngoal space. We implement our agent GROOT in a simple yet effective\nencoder-decoder architecture based on causal transformers. We evaluate GROOT\nagainst open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the\nhuman-machine gap as well as exhibiting a 70% winning rate over the best\ngeneralist agent baseline. Qualitative analysis of the induced goal space\nfurther demonstrates some interesting emergent properties, including the goal\ncomposition and complex gameplay behavior synthesis. The project page is\navailable at https:\/\/craftjarvis-groot.github.io.\n```\n\n#### 3. 论文全文\n```\nGROOT: Learning to Follow Instructions by\nWatching Gameplay Videos\nShaofei Cai1, Bowei Zhang1, Zihao Wang1, Xiaojian Ma3, Anji Liu2 and Yitao Liang1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis,\nWe study the problem of building a controller that can follow open-ended instructions in open-world\nenvironments. We propose to follow reference videos as instructions, which offer expressive goal\nspecifications while eliminating the need for expensive text-gameplay annotations. A new learning\nframework is derived to allow learning such instruction-following controllers from gameplay videos\nwhile producing a video instruction encoder that induces a structured goal space. We implement our\nagent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers.\nWe evaluate GROOT against open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap\nas well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis. The project page is available at https:\n\/\/craftjarvis-groot.github.io.\nFigure 1 | Through the cultivation of extensive gameplay videos, GROOT has grown a rich set of skill fruits\n(number denotes success rate; skills shown above do not mean to be exhaustive; kudos to our artist Haowei).\n1. Introduction\nDeveloping human-level embodied agents that can solve endless tasks in open-world environments,\nsuch as Minecraft (Fan et al., 2022; Johnson et al., 2016), has always been a long-term goal pursued\nin AI. Recent works have explored using Large Language Models (LLMs) to generate high-level plans,\nwhich guide the agent to accomplish challenging long-horizon tasks (Wang et al., 2023a,b; Zhu\net al., 2023). However, a major gap between these LLM-based agents and generalist agents that can\ncomplete endless amounts of tasks is the capability of their low-level controllers, which map the plans\nto motor commands. Recently developed controllers are only capable of completing a predefined and\nnarrow set of programmatic tasks (Baker et al., 2022; Cai et al., 2023; Lin et al., 2021), which hinders\nLLM-based planning agents from unleashing their full potential. We attribute the limitation of these\nlow-level controllers to how the goal is specified. Specifically, existing controllers use task indicator\nCorresponding author(s): Yitao Liang\nShaofei Cai<caishaofei@stu.pku.edu.cn>, Bowei Zhang<zhangbowei@stu.pku.edu.cn>,Zihao Wang<zhwang@stu.pku.edu.cn>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Anji Liu<liuanji@cs.ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2310.08235v2  [cs.AI]  29 Nov 2023\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(Yu et al., 2019), future outcome (Chen et al., 2021; Lifshitz et al., 2023), and language (Brohan\net al., 2022) to represent the goal. While it is easy to learn a controller with some of these goal\nspecifications, they may not be expressive enough for diverse tasks. Taking future outcome goals as an\nexample, an image of a desired house clearly lacks procedural information on how the house was built.\nOne exception is language, but learning a controller that can receive language goals is prohibitively\nexpensive as it requires a huge number of trajectory-text pairs with text that precisely depicts the full\ndetails of the gameplay, therefore preventing them from scaling up to more open-ended tasks.\nHaving observed the limitations of goal specification in the prior works, this paper seeks to find\na balance between the capacity of goal specification and the cost of controller learning. Concretely,\nwe propose to specify the goal as a reference gameplay video clip. While such video instruction is\nindeed expressive, there are two challenges: 1) How can the controller understand the actual goal\nbeing specified as the video itself can be ambiguous, i.e. a goal space or video instruction encoder\nhas to be learned; 2) How to ultimately map such goal to actual motor commands? To this end, we\nintroduce a learning framework that simultaneously produces a goal space and a video instruction\nfollowing controller from gameplay videos. The fundamental idea is casting the problem as future\nstate prediction based on past observations:\n• The predicting model needs to identify which goal is being pursued from the past observations,\nwhich requires a good goal space (induced by a video instruction encoder);\n• Since the transition dynamics model is fixed, a policy that maps both the state and the recognized\ngoal to action is also needed by the predicting model when rolling the future state predictions.\nEffectively, this results in the goal space and control policy we need. We introduce a variational learning\nobjective for this problem, which leads to a combination of a cloning loss and a KL regularization loss.\nBased on this framework, we implement GROOT, an agent with an encoder-decoder architecture to\nsolve open-ended Minecraft tasks by following video instructions. The video encoder is a non-causal\ntransformer that extracts the semantic information expressed in the video and maps it to the latent\ngoal space. The controller policy is a decoder module implemented by a causal transformer, which\ndecodes the goal information in the latent space and translates it into a sequence of actions in the\ngiven environment states in an autoregressive manner.\nTo comprehensively evaluate an agent’s mastery of skills, we designed a benchmark called\nMinecraft SkillForge. The benchmark covers six common Minecraft task groups: collect, build,\nsurvive, explore, tool, and craft, testing the agent’s abilities in resource collection, structure\nbuilding, environmental understanding, and tool usage, in a total of 30 tasks. We calculate Elo\nratings among GROOT, several counterparts, and human players based on human evaluations. Our\nexperiments showed that GROOT is closing the human-machine gap and outperforms the best\nbaseline by 150 points (or 70% winning rate) in an Elo tournament system. Our qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis.\nTo sum up, our main contributions are as follows:\n• Start by maximizing the log-likelihood of future states given past ones, we have discovered the\nlearning objectives that lead to a good goal space and ultimately the instruction-following controller\nfrom gameplay videos. It provides theoretical guidance for the agent architecture design and model\noptimization.\n• Based on our proposed learning framework, we implemented a simple yet efficient encoder-\ndecoder agent based on causal transformers. The encoder is responsible for understanding the goal\ninformation in the video instruction while the decoder as the policy emits motor commands.\n• On our newly introduced benchmark, Minecraft SkillForge, GROOT is closing the human-machine\n2\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\ngap and surpassing the state-of-the-art baselines by a large margin in the overall Elo rating com-\nparison. GROOT also exhibits several interesting emergent properties, including goal composition\nand complex gameplay behavior synthesis.\n2. Preliminaries and Problem Formulation\nReinforcement Learning (RL) concerns the problem in which an agent interacts with an environment\nat discrete time steps, aiming to maximize its expected cumulative reward (Espeholt et al., 2018; Mnih\net al., 2015; Schulman et al., 2017). Specifically, the environment is defined as a Markov Decision\nProcess (MDP) ⟨S, A, R, P, 𝑑0⟩, where S is the state space, A is the action space, R : S × A →ℝis\nthe reward function, P : S × A →S is the transition dynamics, and 𝑑0 is the initial state distribution.\nOur goal is to learn a policy 𝜋(𝑎|𝑠) that maximizes the expected cumulative reward 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑡], where\n𝛾∈(0, 1] is a discount factor.\nIn goal-conditioned RL (GCRL) tasks, we are additionally provided with a goal 𝑔∈G (Andrychowicz\net al., 2017; Cai et al., 2023; Ding et al., 2019; Jing et al., 2020, 2021; Liu et al., 2022; Yang et al.,\n2019). And the task becomes learning a goal-conditioned policy 𝜋(𝑎|𝑠, 𝑔) that maximizes the expected\nreturn 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑔\n𝑡], where 𝑟𝑔\n𝑡is the goal-specific reward achieved at time step 𝑡. Apart from being\na new type of RL task, GCRL has been widely studied as a pre-training stage toward conquering\nmore challenging environments\/tasks (Aytar et al., 2018b; Baker et al., 2022; Zhang et al., 2022).\nSpecifically, suppose we are provided with a good goal-condition policy, the goal can be viewed as a\nmeta-action that drives the agent to accomplish various sub-tasks, which significantly simplifies tasks\nthat require an extended horizon to accomplish. Further, when equipped with goal planners, we can\nachieve zero- or few-shot learning on compositional tasks that are beyond the reach of RL algorithms\n(Gong et al., 2023; Huang et al., 2022; Wang et al., 2023a,b; Zhu et al., 2023).\nAt the heart of leveraging such benefits, a key requirement is to have a properly-defined goal\nspace that (i) has a wide coverage of common tasks\/behaviors, and (ii) succinctly describes the task\nwithout including unnecessary information about the state. Many prior works establish goal spaces\nusing guidance from other modalities such as language (Cai et al., 2023; Hong et al., 2020; Stone\net al., 2023) or code (Huang et al., 2023; Wang et al., 2023a). While effective, the requirement on\nlarge-scale trajectory data paired with this auxiliary information could be hard to fulfill in practice.\nInstead, this paper studies the problem of simultaneously learning a rich and coherent goal space\nand the corresponding goal-conditioned policy, given a pre-trained inverse dynamic model and raw\ngameplay videos, i.e. sequences of states {𝑠(𝑖)\n0:𝑇}𝑖collected using unknown policies.\n3. Goal Space Discovery via Future State Prediction\nThis section explains our learning framework: discovering a “good” goal space as well as a video\ninstruction following the controller through the task of predicting future states given previous ones.\nWe start with an illustrative example in Minecraft (Johnson et al., 2016). Imagine that an agent is\nstanding inside a grassland holding an axe that can be used to chop the tree in front of them. Suppose\nin the gameplay video, players either go straight to chop the tree or bypass it to explore the territory.\nIn order to predict future frames, it is sufficient to know (i) which goal (chop tree or bypass tree) is\nbeing pursued by the agent, and (ii) what will happen if the agent chooses a particular option (i.e.\ntransition dynamics). Apart from the latter information that is irrelevant to the past observations, we\nonly need to capture the goal information, i.e. whether the agent decides to chop the tree or bypass\nthe tree. Therefore, the task of establishing a comprehensive yet succinct goal space can be interpreted\nas predicting future states while conditioning on the transition dynamics of the environment.\n3\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nFormally, our learning objective is to maximize the log-likelihood of future states given past\nones: log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡). Define 𝑔as a latent variable conditioned on past states (think of it as the\npotential goals the agent is pursuing given past states), the evidence lower-bound of the objective\ngiven variational posterior 𝑞𝜙(𝑔|𝑠0:𝑇) is the following (see Appendix A for the derivation of this and\nthe following equations):\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡) = log\n∑︁\n𝑔\n𝑝𝜃(𝑠𝑡+1:𝑇, 𝑔|𝑠0:𝑡)\n≥𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇) [log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔)] −𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n,\nwhere 𝐷KL(·∥·) denotes the KL-divergence. Next, we break down the first term (i.e. 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔))\ninto components contributed by the (unknown) goal-conditioned policy 𝜋𝜃(𝑎|𝑠, 𝑔) and the transition\ndynamics 𝑝𝜃(𝑠𝑡+1|𝑠0:𝑡, 𝑎𝑡) :\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔) =\n𝑇\n∑︁\n𝜏=𝑡\nlog\n∑︁\n𝑎𝜏\n𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) · 𝑝𝜃(𝑠𝜏+1|𝑠0:𝜏, 𝑎𝜏)\n≥\n𝑇\n∑︁\n𝜏=𝑡\n𝔼𝑎𝜏∼𝑝𝜃(𝑎𝜏|𝑠0:𝜏+1)\n\u0002\nlog 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) + 𝐶\n\u0003\n,\nwhere the constant 𝐶contains terms that depend solely on the environment dynamics and are\nirrelevant to what we want to learn (i.e. the goal space and the goal-conditioned policy). Bring it\nback to the original objective, we have\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) ≥\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇),𝑎𝜏∼𝑝𝜃(·|𝑠0:𝜏+1) [log 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔)]\n|                                                         {z                                                         }\nbehaviour cloning\n−\n𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n|                               {z                               }\ngoal space constraint (KL regularization)\n,\nwhere 𝑞𝜙(·|𝑠0:𝑇) is implemented as a video encoder that maps the whole state sequence into the latent\ngoal space. 𝑝𝜃(·|𝑠0:𝜏+1) is the inverse dynamic model (IDM) that predicts actions required to achieve\na desired change in the states, which is usually a pre-trained model, details are in Appendix C. Thus,\nthe objective can be explained as jointly learning a video encoder and a goal-controller policy through\nbehavior cloning under succinct goal space constraints.\n4. GROOT Architecture Design and Training Strategy\nThis section illustrates how to create an agent (we call it GROOT) that can understand the semantic\nmeaning of a reference video and interact with the environment based on the aforementioned learning\nframework. According to the discussion in Section 3, the learnable parts of GROOT include the\nvideo encoder and the goal-conditioned policy. Recently, Transformer (Vaswani et al., 2017) has\ndemonstrated effectiveness in solving sequential decision-making problems (Brohan et al., 2022; Chen\net al., 2021; Parisotto et al., 2019). Motivated by this, we implement GROOT with transformer-based\nencoder-decoder architecture, as shown in Figure 2. The video encoder is a non-causal transformer\nthat extracts semantic information and generates goal embeddings. The policy is a causal transformer\ndecoder that receives the goal embeddings as the instruction and autoregressively translates the state\nsequence into a sequence of actions. Next, we describe how each module is constructed together with\nthe training strategy.\n4\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nVideo Encoder\nDecoder as Policy\n𝑠!\n𝑠\"\n𝑠#\n𝑠!\n𝑠\"\n𝑎#\n𝑎$\n𝑎\"\nVideo Encoder\nDecoder as Policy\n𝑠̂#\n𝑠̂%\n𝑠̂\"\n𝑠#\n𝑠%\n𝑠!\n𝑎#\n𝑎%\n𝑎!\n𝑠%\n𝑠!\nBehavior Cloning\nReference Video\nStates\nStates\nRollout Observations\n(a) Training\n(b) Inference\n𝑐̂#\n𝑐̂&\nLearnable Tokens\nLearned Tokens\n1.0\n0.5\n0.0\n𝜇\n𝒒𝒈𝟏:𝑵𝒔𝟏:𝑻)\nsample\n𝑠#\n𝑐#\n𝑐&\n𝑐#\n𝑐&\n𝑐̂#\n𝑐̂&\nEnvironment\n⋯\n⋯\n⋯\n⋯\n⋯\n1.0\n0.5\n0.0\n𝜇\n𝒑𝒈𝟏:𝑵𝒔𝟏:𝒕)\n𝑫𝑲𝑳\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n𝑠#\n⋯\n𝝁(⋅)\nFigure 2 | Our GROOT agent architecture. Left: In the training stage, a video encoder (non-causal\ntransformer) learns to extract the semantic meaning and transfer the video (state sequence) into the goal\nembedding space. A goal-conditioned policy (causal transformer) is learned to predict actions following the\ngiven instructions. We learn the agent using behavior cloning under a KL constraint. Right: During the\ninference, any reference video is passed into the video encoder to generate the goal embeddings that drive the\npolicy to interact with the environment.\n4.1. Video Encoder\nThe video encoder includes a Convolutional Neural Network (CNN) to extract spatial information\nfrom image states 𝑠1:𝑇and a non-causal transformer to capture temporal information from videos.\nSpecifically, we use a CNN backbone to extract visual embeddings {𝑥1:𝑇} for all frames. Additionally,\nmotivated by Devlin et al. (2019); Dosovitskiy et al. (2020), we construct a set of learnable embeddings\n(or summary tokens), represented as {𝑐1:𝑁}, to capture the semantic information present in the video.\nThe visual embeddings and summary tokens are passed to a non-causal transformer, resulting in the\noutput corresponding to the summary tokens as {ˆ𝑐1:𝑁}\n𝑥1:𝑇←Backbone(𝑠1:𝑇),\nˆ𝑐1:𝑁←Transformer([𝑥1:𝑇, 𝑐1:𝑁]).\n(1)\nSimilar to VAE (Kingma & Welling, 2013), we assume that the latent goal space follows a Gaussian\ndistribution, hence we use two fully connected layers, 𝜇(·) and 𝜎(·), to generate the mean and\nstandard deviation of the distribution, respectively. During training, we use the reparameterization\ntrick to sample a set of embeddings {𝑔1:𝑁} from the distribution, where 𝑔𝑡∼N (𝜇(ˆ𝑐𝑡), 𝜎(ˆ𝑐𝑡)). During\ninference, we use the mean of the distribution as the goal embeddings, i.e. 𝑔𝑡←𝜇(ˆ𝑐𝑡).\n4.2. Decoder as Policy\nTo introduce our policy module, we start with VPT (Baker et al., 2022), a Minecraft foundation model\ntrained with standard behavioral cloning. It is built on Transformer-XL (Dai et al., 2019) that can\nleverage long-horizon historical states and predict the next action seeing the current observation.\nHowever, the vanilla VPT architecture does not support instruction input. To condition the policy\non goal embeddings, we draw the inspiration from Flamingo (Alayrac et al., 2022), that is, to insert\ngated cross-attention dense layers into every Transformer-XL block. The keys and values in these layers\nare obtained from goal embeddings, while the queries are derived from the environment states\nˆ𝑥(𝑙)\n1:𝑡←GatedXATTN(kv = 𝑔1:𝑁, q = 𝑥(𝑙−1)\n1:𝑡\n; 𝜃𝑙),\n𝑥(𝑙)\n1:𝑡←TransformerXL(qkv = ˆ𝑥(𝑙)\n1:𝑡; 𝜃𝑙),\nˆ𝑎𝑡←FeedForward(𝑥(𝑀)\n𝑡\n),\n(2)\n5\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nwhere the policy reuses the visual embeddings extracted by the video encoder, i.e. 𝑥(0)\n1:𝑡= 𝑥1:𝑡, the\npolicy consists of 𝑀transformer blocks, 𝜃𝑙is the parameter of 𝑙-th block, ˆ𝑎𝑡is the predicted action.\nSince our goal space contains information about how to complete a task that is richer than previous\nlanguage-conditioned policy (Cai et al., 2023; Lifshitz et al., 2023), the cross-attention mechanism is\nnecessary. It allows the GROOT to query the task progress from instruction information based on\npast states, and then perform corresponding behaviors to complete the remaining progress.\n4.3. Training and Inference\nThe training dataset can be a mixture of Minecraft gameplay videos and offline trajectories. For\nthose videos without actions, an inverse dynamic model (Baker et al., 2022) can be used to generate\napproximate actions. Limited by the computation resources, we truncated all the trajectories into\nsegments with a fixed length of 𝑇without using any prior. We denote the final dataset as D =\n{(𝑥1:𝑇, 𝑎1:𝑇)}𝑀, where 𝑀is the number of trajectories. We train GROOT in a fully self-supervised\nmanner while the training process can be viewed as self-imitating, that is, training GROOT jointly\nusing behavioral cloning and KL divergence loss\nL(𝜃, 𝜙) = 𝔼(𝑠,𝑎)∼D\n\"\n𝜆𝐵𝐶\n∑︁\n𝑡\n−log 𝜋𝜃(𝑎𝑡|𝑠1:𝑡, 𝑔) + 𝜆𝐾𝐿\n∑︁\n𝜏\n𝐷𝐾𝐿\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝜏)\u0001\n#\n,\n(3)\nwhere 𝜆𝐵𝐶, 𝜆𝐾𝐿are tradeoff coefficients, 𝑞𝜙is a posterior visual encoder, 𝑝𝜃is a prior video encoder\nwith the same architecture, 𝑔∼𝑞𝜙(·|𝑠0:𝑇). More details are in the Appendix D.\n5. Result\n5.1. Performance on Mastering Minecraft Skills\nMinecraft SkillForge Benchmark. In order to comprehensively evaluate the mastery of tasks by\nagents in Minecraft, we created a diverse benchmark called Minecraft SkillForge. It covers 30 tasks\nfrom 6 major categories of representative skills in Minecraft, including collect, explore, craft,\ntool, survive, and build. For example, the task “dig three down and fill one up” in the build\ncategory asks the agent to first dig three blocks of dirt, then use the dirt to fill the space above; The\ntask of “building a snow golem” ( ) requires the agent to sequentially stack 2 snow blocks (\n) and\n1 carved pumpkin (\n). We put the details of this benchmark in the Appendix H. Apart from some\nrelatively simple or common tasks such as “collect wood” and “hunt animals”, other tasks require the\nagent to have the ability to perform multiple steps in succession.\nWe compare GROOT with the following baselines: (a) VPT (Baker et al., 2022), a foundation\nmodel pre-trained on large-scale YouTube data, with three variants: VPT (fd), VPT(bc), and VPT(rl),\nindicating vanilla foundation model, behavior cloning finetuned model, and RL finetuned model; (b)\nSTEVE-1 (Lifshitz et al., 2023), an instruction-following agent finetuned from VPT, with two variants:\nSTEVE-1 (visual) and STEVE-1 (text) that receives visual and test instructions. More details are in\nAppendix F.1. It is worth noting that GROOT was trained from scratch.\nHuman Evaluation with Elo Rating. We evaluated the relative strength of agents by running an\ninternal tournament and reporting their Elo ratings, as in Mnih et al. (2015). Before the tournament,\neach agent is required to generate 10 videos of length 600 on each task. Note that, all the reference\nvideos used by GROOT are generated from another biome to ensure generalization. Additionally,\nwe also invited 3 experienced players to do these tasks following the same settings. After the video\ncollection, we asked 10 players to judge the quality of each pair of sampled videos from different\n6\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Elo Rating Comparison\n(b) Winning Rate of GROOT vs. Baselines\n(c) Success Rate Comparison\nFigure 3 | Results on Minecraft SkillForge benchmark. Left: Tournament evaluation of GROOT assessed\nby human players. GROOT performs better than state-of-the-art Minecraft agent STEVE-1. A 150-score gap\ncorresponds to a 70% probability of winning. Middle: Winning rate of GROOT v.s. other agents on specific\ntask categories. Colors from red to blue denote a decrease in the winning rate. Apart from the human player,\nGROOT surpasses all other baselines. Right: Success rate on 9 representative tasks. GROOT champions\nprocess-oriented tasks, such as dig three and fill one (\n) and build snow golems ( ).\nagents. Considering the diversity of tasks, we designed specific evaluation criteria for every task to\nmeasure the quality of rollout trajectories. For example, in the task of “build snow golem”, we rank\nthe completion degree of the task in ascending order: no blocks placed, one type of block placed,\ntwo types of blocks placed, and snow golem built successfully. After 1500 comparisons, the Elo\nrating converged as in Figure 3 (left). Although there is a large performance gap compared with\nhuman players, GROOT has significantly surpassed the current state-of-the-art STEVE-1 series and\ncondition-free VPT series on the overall tasks. Additional details are in Appendix G.\nIn Figure 3 (middle), we compare GROOT with other baselines in winning rate on six task groups.\nWe found that except for the performance on craft tasks, where STEVE-1 (visual) outperforms our\nmodel, GROOT achieves state-of-the-art results. In particular, GROOT greatly outperforms other\nbaselines by a large margin on build and tool. For build, the goal space needs to contain more\ndetailed procedural information, which is the disadvantage of methods that use future outcomes as\nthe goal. Moreover, such tasks are distributed sparsely in the dataset, or even absent in the dataset,\nwhich requires the agent to have strong generalization ability. As for craft group, GROOT is not\nsuperior enough, especially on the “crafting table” task. We attribute this to the wide task distribution\nin the dataset. Thus the future outcomes can prompt STEVE-1 to achieve a high success rate.\nProgrammatic Evaluation. To quantitatively compare the performance of the agents, we selected\n9 representative tasks out of 30 and reported the success rate of GROOT, STEVE-1 (visual), and\nVPT (bc) on these tasks in Figure 3 (right). We found that, based on the success rate on tasks such\nas dye and shear sheep(\n), enchat sword (\n), smelt food (\n), use bow (\n), sleep\n(\n), and lead animals (\n), GROOT has already reached a level comparable to that of human\nplayers (100%). However, the success rates for build snow golems ( ) and build obsidian\n(\n) tasks are only 60% and 50%. By observing the generated videos, we found that GROOT cannot\nprecisely identify the items in Hotbar (such as buckets, lava buckets, snow blocks, and pumpkin\nheads), resulting in a low probability of switching to the correct item. STEVE-1 also has the same\nproblem. This may be due to the current training paradigm lacking strong supervisory signals at the\nimage level. Future work may introduce auxiliary tasks such as vision-question answering (VQA) to\nhelp alleviate this phenomenon. Details are in Appendix F.3.\n7\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Random Initialized\n(b) GROOT w\/o KL\n(c) GROOT w\/ KL\n(d) Synthesized Videos\nFigure 4 | t-SNE visualization of the goal space. Each color corresponds to a specific video category. (Left):\nSpace of randomly initialized video encoder. All the videos are entangled together. Middle: Space of GROOT\ntrained with self-supervised learning w\/ and w\/o KL regularization, respectively. The videos are clustered\nbased on their semantics. Visualization shows the subtle differences between the two. Right: Synthesized\nvideos using concatenation manner. The concatenated videos lay on the position between the source videos.\n5.2. Properties of Learned Goal Space\nThis section studies the properties of learned goal space. We used the t-SNE algorithm (van der Maaten\n& Hinton, 2008) to visualize the clustering effect of reference videos encoded in goal space, as in Figure\n4. We select 7 kinds of videos, including craft items, combat enemies, harvest crops,\nhunt animals, chop trees, trade with villagers, and mine ores. These videos are\nsampled from the contractor data (Baker et al., 2022) according to the meta information (details\nare in Appendix F.2). Each category contains 1k video segments. As a control group, in Figure 4\n(left), we showed the initial goal space of the video encoder (with a pre-trained EfficientNet-B0 (Tan\n& Le, 2019) as the backbone) before training. We found that the points are entangled together. After\nbeing trained on offline trajectories, as in Figure 4 (middle), it well understands reference videos and\nclusters them according to their semantics. This proves that it is efficient to learn behavior-relevant\ntask representations using our self-supervised training strategy. The clustering effect is slightly better\nwith KL regularization, though the difference is not very significant. Inevitably, there are still some\nvideos from different categories entangled together. We attribute this to the possibility of overlap\nin the performed behaviors of these videos. For example, chop trees and harvest crops both\nrely on a sequential of “attack” actions.\nCondition on Concatenated Videos. We also study the possibility of conditioning the policy\non concatenated videos. First, we collect 3 kinds of source videos, including chop trees, hunt\nanimals, and trade with villagers. We randomly sampled two videos from sources of chop\ntrees and hunt animals, downsampled and concatenated them into a synthetic video, denoted as\n[chop trees, hunt animals]. By the same token, we can obtain [hunt animals, trade\nwith villagers]. We visualize these videos together with the source videos in Figure 4 (right). We\nfound that the source videos lie far away from each other while the concatenated videos are distributed\nbetween their source videos. Based on this intriguing phenomenon, we infer that concatenated videos\nmay prompt GROOT to solve both tasks simultaneously. To verify this, we evaluate GROOT on\nthree kinds of reference videos, i.e. chop trees, hunt animals, and [chop trees, hunt\nanimals]. We launched GROOT in the forest and in the animal plains, respectively. The collected\nwood and killed mobs are reported in Figure 5. We found that although the concatenated video may\nnot be as effective as raw video in driving an agent to complete a single task (60% of the performance\nof raw video), it does possess the ability to drive the agent to perform multiple tasks. This is an\nimportant ability. As discussed in Wang et al. (2023b), sometimes the high-level planner will propose\nmultiple candidate goals, it will be efficient if the low-level controller can automatically determine\n8\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nchop\nhunt\nconcat\nWood Collected\n11.0\n1.5\n6.5\nchop\nhunt\nconcat\nMobs Killed\n0.5\n2.2\n1.5\nFigure 5 | Comparison on using raw and concate-\nnated reference videos as conditions. Left: Col-\nlected wood in the forest biome. Right: Killed mobs\nin the plains biome. “concat” denotes the reference\nvideo is [chop trees, hunt animals]. Statis-\ntics are measured over 10 episodes.\nw\/ KL\nw\/o KL\nSeagrass Collected\n3.7\n1.8\nw\/ KL\nw\/o KL\nWood Collected\n11.0\n7.3\nw\/ KL\nw\/o KL\nArrows Fired\n10.7\n9.3\nFigure 6 | Ablation study on KL loss.\nAfter being\njointly trained with KL loss, GROOT can collect 2×\nmore seagrass (\n) underwater and 1.5× wood (\n)\nin the forest while the difference is not as impressive\non the use bow (\n) task. Statistics are measured\nover 10 episodes.\ndiamonds distribute in this level\nGROOT\nSTEVE-1\ncondition changed\ncondition changed\ndiamonds distribute in this level\n~𝟏𝟔%\nstuck in the bedrock\nFigure 7 | Results on solving challenging obtain diamond task. The vertical dashed lines represent the\ntime when a certain item is first obtained. Left: GROOT first dug down to the depth of 12 and then mined\nhorizontally to obtain diamonds with an average success rate of 16%. Right: STEVE-1 quickly dug down to the\nspecific depth but struggled to maintain its height.\nwhich to accomplish based on the current observation.\nAblation on KL Divergence Loss. To investigate the role of KL loss in training, we evaluated\nGROOT (w\/ KL) and its variant (w\/o KL) on three tasks: collect seagrass (\n), collect\nwood (\n), and use bow (\n). As shown in Figure 6, we found that introducing the constraint of\nKL loss improved agent performance by 2× and 1.5× in the first two tasks, whereas there was no\nsignificant effect in the use bow task. This may be because the first two tasks require the agent to\ngeneralize the corresponding skills to different terrains (e.g. locating trees in the environment for\ncollecting wood and sinking to specific locations for collecting seagrass). Therefore, it puts higher\ndemands on the agent’s ability to generalize in the goal space, and this is exactly the role played by\nthe KL loss. The use bow task is relatively simple in comparison because it only requires charging\nand shooting the arrow, without considering environmental factors.\n5.3. Combining Skills for Long-horizon Tasks\nIn this section, we explore whether GROOT can combine skills to solve long-horizon tasks, which is\nkey to its integration with a high-level planner. Taking the task of mining diamonds as an example,\nprior knowledge is that diamond ores are generally distributed between the 7th and 14th floors\nunderground, and the probability of appearing in other depths is almost zero. Therefore, the agent\nneeds to first dig down to the specified depth (12) and then maintain horizontal mining. To achieve\nthis, we designed two reference videos, each 128 frames long. One describes the policy of starting\n9\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfrom the surface and digging down, and the other demonstrates the behaviors of horizontal mining.\nWe show an example in Figure 7 (left). In the beginning, GROOT quickly digs down to the specified\ndepth and then switches to horizontal mining mode. It maintains the same height for a long time and\nfound diamonds at 11k steps. In addition, we compared STEVE-1 (visual) under the same setting in\nFigure 7 (right). After switching to the horizontal mining prompt, STEVE-1 maintains its height for a\nshort time before it stuck in the bedrock layer (unbreakable in survival mode), greatly reducing the\nprobability of finding diamonds. This indicates that our goal space is expressive enough to instruct\nthe way of mining, and the policy can follow the instructions persistently and reliably. In contrast,\nSTEVE-1, which relies on future outcomes as a condition, was unable to maintain its depth, despite\nattempts at various visual prompts. We conducted 25 experiments each on GROOT and STEVE-1,\nwith success rates of 16% and 0% for finding diamonds. Additional details are in the Appendix F.4.\n6. Related Works\nPre-train Policy on Offline Data. Pre-training neural networks on web-scale data has been demon-\nstrated as an effective training paradigm in Nature Language Processing (Brown et al., 2020) and\nComputer Vision (Kirillov et al., 2023). Inspired by this, researchers tried to transfer the success\nto the field of decision-making from pre-training visual representations and directly distilling the\npolicy from offline data. As the former, Aytar et al. (2018a); Bruce et al. (2023) leveraged temporal\ninformation present in videos as the supervision signal to learn visual representations. The represen-\ntations are then used to generate intrinsic rewards for boosting downstream policy learning, which\nstill requires expensive online interactions with the environment. Chen et al. (2021); Schmidhuber\n(2019) leveraged scalable offline trajectories to train optimal policy by conditioning it on cumulated\nrewards. Laskin et al. (2022) proposed to learn an in-context policy improvement operator that\ncan distill an RL algorithm in high data efficiency. Reed et al. (2022) learned a multi-task agent\nGato by doing behavior cloning on a large-scale expert dataset. By serializing task data into a flat of\nsequence, they use the powerful transformer architecture to model the behavior distribution. However,\nthese methods either require elaborated reward functions or explicit task definitions. This makes\nit hard to be applied to open worlds, where tasks are infinite while rewards are lacking. Another\ninteresting direction is to use pre-trained language models for reasoning and vision language models\nfor discrimination, to guide the policy in life-long learning in the environment (Di Palo et al., 2023).\nCondition Policy on Goal Space. Researchers have explored many goal modalities, such as\nlanguage (Khandelwal et al., 2021), image (Du et al., 2021), and future video (Xie et al., 2023), to\nbuild a controllable policy. Brohan et al. (2022) collected a large-scale dataset of trajectory-text pairs\nand trained a transformer policy to follow language instructions. Despite the language being a natural\ninstruction interface, the cost of collecting paired training data is expensive. As a solution, Majumdar\net al. (2022) sorted to use hindsight relabeling to first train a policy conditioned on the target image,\nthen aligned text to latent image space, which greatly improves training efficiency. Lifshitz et al.\n(2023) moved a big step on this paradigm by replacing the target image with a 16-frame future video\nand reformulating the modality alignment problem into training a prior of latent goal given text.\nBuild Agents in Minecraft. As a challenging open-world environment, Minecraft is attracting an\nincreasing number of researchers to develop AI agents on it, which can be divided into plan-oriented\n(Wang et al., 2023a,b) and control-oriented methods (Baker et al., 2022; Cai et al., 2023; Lifshitz\net al., 2023) based on their emphasis. Plan-oriented agents aim to reason with Minecraft knowledge\nand decompose the long-horizon task into sub-tasks followed by calling a low-level controller. Control-\noriented works follow the given instructions and directly interact with the environments using low-level\nactions (mouse and keyboard). Baker et al. (2022) pre-trained the first foundation model VPT in\nMinecraft using internet-scale videos. Although it achieves the first obtaining diamond milestone by\n10\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfine-tuning with RL, it does not support instruction input. Lifshitz et al. (2023) created the first agent\nthat can solve open-ended tasks by bridging VPT and MineCLIP (Fan et al., 2022). However, its goal\nspace is not expressive enough and prevents it from solving multi-step tasks.\n7. Limitation and Conclusion\nAlthough GROOT has demonstrated powerful capabilities in expressing open-ended tasks in the\nform of video instructions, training such a goal space remains highly challenging. We found that\nGROOT is quite sensitive to the selection of reference videos, which we attribute to the fact that\nthe goal space trained from an unsupervised perspective may not be fully aligned with the human\nintention for understanding the semantics of the reference video. Therefore, it would be a promising\nresearch direction in the future to use SFT (supervised fine-tuning, Sanh et al. (2021)) and RLHF\n(Ziegler et al., 2019) to align the pre-trained goal space with human preference.\nIn conclusion, we propose a paradigm for learning to follow instructions by watching gameplay\nvideos. We prove that video instruction is a good form of goal space that not only expresses open-ended\ntasks but can be trained through self-imitation (once the IDM is available to label pseudo actions for raw\ngameplay videos). Based on this, we built an encoder-decoder transformer architecture agent named\nGROOT in Minecraft. Without collecting any text-video data, GROOT demonstrated extraordinary\ninstruction-following ability and crowned the Minecraft SkillForge benchmark. Additionally, we also\nshowed its potential as a planner downstream controller in the challenging obtain diamond task.\nWe believe that this training paradigm can be generalized in other complex open-world environments.\n11\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | GROOT：通过观看游戏视频学习指令遵循\n\n## 📌 背景痛点\/本文动机\n在开放世界环境中，构建能够遵循开放指令的控制器一直是人工智能领域的长期目标。然而，现有的控制器通常只能完成预定义的、有限的程序性任务，这限制了它们在开放世界环境中的应用。本文旨在解决这个问题，提出了一种新的学习框架，通过观看游戏视频来学习指令遵循控制器。\n\n## 🚀 核心方法\n💡 创新点1：将目标指定为参考游戏视频片段，从而提供丰富的目标规范，同时消除对昂贵的文本-游戏注释的需求。\n💡 创新点2：引入了一种新的学习框架，该框架同时产生一个目标空间和一个视频指令遵循控制器，从而实现从游戏视频中学习指令遵循控制器。\n\n## 📈 实验结果\n在Minecraft SkillForge基准测试中，GROOT在整体Elo评分比较中超过了最先进的基线，并且在解决具有挑战性的获取钻石任务中表现出色。\n\n## 💬 可借鉴之处\n本文提出的学习框架和GROOT代理的架构设计为构建能够遵循开放指令的控制器提供了新的思路和方法。此外，本文还展示了目标空间和控制器策略的潜在应用，为解决开放世界环境中的复杂任务提供了新的可能性。","llm_summary_res_status":200}
{"title":"MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge","authors":"Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, Anima Anandkumar","summary":"Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https:\/\/minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.","url":"http:\/\/arxiv.org\/abs\/2206.08853v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2206.08853v2","published":1655481185000,"comment":"Outstanding Paper Award at NeurIPS 2022. Project website:\n  https:\/\/minedojo.org","pdf_text":"MINEDOJO: Building Open-Ended\nEmbodied Agents with Internet-Scale Knowledge\nLinxi Fan1, Guanzhi Wang2∗, Yunfan Jiang3∗, Ajay Mandlekar1, Yuncong Yang4,\nHaoyi Zhu5, Andrew Tang4, De-An Huang1, Yuke Zhu1 6†, Anima Anandkumar1 2†\n1NVIDIA, 2Caltech, 3Stanford, 4Columbia, 5SJTU, 6UT Austin\n∗Equal contribution\n†Equal advising\nhttps:\/\/minedojo.org\nAbstract\nAutonomous agents have made great strides in specialist domains like Atari games\nand Go. However, they typically learn tabula rasa in isolated environments with\nlimited and manually conceived objectives, thus failing to generalize across a wide\nspectrum of tasks and capabilities. Inspired by how humans continually learn\nand adapt in the open world, we advocate a trinity of ingredients for building\ngeneralist agents: 1) an environment that supports a multitude of tasks and goals,\n2) a large-scale database of multimodal knowledge, and 3) a ﬂexible and scalable\nagent architecture. We introduce MINEDOJO, a new framework built on the\npopular Minecraft game that features a simulation suite with thousands of diverse\nopen-ended tasks and an internet-scale knowledge base with Minecraft videos,\ntutorials, wiki pages, and forum discussions. Using MINEDOJO’s data, we propose\na novel agent learning algorithm that leverages large pre-trained video-language\nmodels as a learned reward function. Our agent is able to solve a variety of open-\nended tasks speciﬁed in free-form language without any manually designed dense\nshaping reward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https:\/\/minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.\n1\nIntroduction\nDeveloping autonomous embodied agents that can attain human-level performance across a wide\nspectrum of tasks has been a long-standing goal for AI research. There has been impressive progress\ntowards this goal, most notably in games [80, 85, 126] and robotics [68, 99, 146, 134, 107]. These\nembodied agents are typically trained tabula rasa in isolated worlds with limited complexity and\ndiversity. Although highly performant, they are specialist models that do not generalize beyond a\nnarrow set of tasks. In contrast, humans inhabit an inﬁnitely rich reality, continuously learn from and\nadapt to a wide variety of open-ended tasks, and are able to leverage large amount of prior knowledge\nfrom their own experiences as well as others.\nWe argue that three main pillars are necessary for generalist embodied agents to emerge. First, the\nenvironment in which the agent acts needs to enable an unlimited variety of open-ended goals\n[116, 71, 120, 117]. Natural evolution is able to nurture an ever-expanding tree of diverse life forms\nthanks to the inﬁnitely varied ecological settings that the Earth supports [117, 129]. This process has\nnot stagnated for billions of years. In contrast, today’s agent training algorithms cease to make new\nprogress after convergence in narrow environments [80, 146]. Second, a large-scale database of\nprior knowledge is necessary to facilitate learning in open-ended settings. Just as humans frequently\nlearn from the internet, agents should also be able to harvest practical knowledge encoded in large\namounts of video demos [42, 77], multimedia tutorials [79], and forum discussions [127, 65, 54]. In a\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\narXiv:2206.08853v2  [cs.LG]  22 Nov 2022\nAt\nCraft Glass Bridge\nCombat Zombie\nFish Squid\nBuild Oak House\nMake Ice Igloo\nFarm Sugar Cane\nFind Ocean \nMonument\nTreasure Hunt\nin End City\nExplore \nDesert Temple\nOpen-ended Environments\nGeneralist Agent\nInternet-scale Knowledge Base\nWiki\nYouTube\nReddit\nFigure 1: MINEDOJO is a novel framework for developing open-ended, generally capable agents\nthat can learn and adapt continually to new goals. MINEDOJO features a benchmarking suite with\nthousands of diverse open-ended tasks speciﬁed in natural language prompts, and also provides an\ninternet-scale, multimodal knowledge base of YouTube videos, Wiki pages, and Reddit posts. The\ndatabase captures the collective experience and wisdom of millions of Minecraft gamers for an AI\nagent to learn from. Best viewed zoomed in.\ncomplex world, it would be extremely inefﬁcient for an agent to learn everything from scratch through\ntrial and error. Third, the agent’s architecture needs to be ﬂexible enough to pursue any task in open-\nended environments, and scalable enough to convert large-scale knowledge sources into actionable\ninsights [19, 96]. This motivates the design of an agent that has a uniﬁed observation\/action space,\nconditions on natural language task prompts, and adopts the Transformer pre-training paradigm [27,\n91, 15] to internalize knowledge effectively.\nIn light of these three pillars, we introduce MINEDOJO, a new framework to help the community\ndevelop open-ended, generally-capable agents. It is built on the popular Minecraft game, where a\nplayer explores a procedurally generated 3D world with diverse types of terrains to roam, materials\nto mine, tools to craft, structures to build, and wonders to discover. Unlike most other games\n[80, 85, 126], Minecraft deﬁnes no speciﬁc reward to maximize and no ﬁxed storyline to follow,\nmaking it well suited for developing open-ended environments for embodied AI research. We make\nthe following three major contributions:\n1. Simulation platform with thousands of diverse open-ended tasks.\nMINEDOJO provides\nconvenient APIs on top of Minecraft that standardize task speciﬁcation, world settings, and agent’s\nobservation\/action spaces. We introduce a benchmark suite that consists of thousands of natural\nlanguage-prompted tasks, making it two orders of magnitude larger than prior Minecraft benchmarks\nlike the MineRL Challenge [48, 62]. The suite includes long-horizon, open-ended tasks that cannot\nbe easily evaluated through automated procedures, such as “build an epic modern house with two\nﬂoors and a swimming pool”. Inspired by the Inception score [98] and FID score [55] that are\ncommonly used to assess AI-generated image quality, we introduce a novel agent evaluation protocol\nusing a large video-language model pre-trained on Minecraft YouTube videos. This complements\nhuman scoring [104] that is precise but more expensive. Our learned evaluation metric has good\nagreement with human judgment in a subset of the full task suite considered in the experiments.\n2. Internet-scale multimodal Minecraft knowledge base.\nMinecraft has more than 100 million\nactive players [131], who have collectively generated an enormous wealth of data. They record\ntutorial videos, stream live play sessions, compile recipes, and discuss tips and tricks on forums.\nMINEDOJO features a massive collection of 730K+ YouTube videos with time-aligned transcripts,\n6K+ free-form Wiki pages, and 340K+ Reddit posts with multimedia contents (Fig. 3). We hope that\nthis enormous knowledge base can help the agent acquire diverse skills, develop complex strategies,\ndiscover interesting objectives, and learn actionable representations automatically.\n2\nshear a sheep\ncombat zombie \npigman\nfind a nether \nportal\nput carpets on \nthe floor\nFigure 2: Visualization of our agent’s learned behaviors on four selected tasks. Leftmost texts are the\ntask prompts used in training. Best viewed on a color display.\n3. Novel algorithm for embodied agents with large-scale pre-training.\nWe develop a new\nlearning algorithm for embodied agents that makes use of the internet-scale domain knowledge we\nhave collected from the web. Using the massive volume of YouTube videos from MINEDOJO, we\ntrain a video-text contrastive model in the spirit of CLIP [92], which associates natural language\nsubtitles with their time-aligned video segments. We demonstrate that this learned correlation score\ncan be used effectively as an open-vocabulary, massively multi-task reward function for RL training.\nOur agent solves the majority of 12 tasks in our experiment using the learned reward model (Fig. 2).\nIt achieves competitive performance to agents trained with meticulously engineered dense-shaping\nrewards, and in some cases outperforms them, with up to 73% improvement in success rates. For\nopen-ended tasks that do not have a simple success criterion, our agents also perform well without\nany special modiﬁcations.\nIn summary, this paper proposes an open-ended task suite, internet-scale domain knowledge, and agent\nlearning with recent advances on large pre-trained models [13]. We have open-sourced MINEDOJO’s\nsimulator, knowledge bases, algorithm implementations, pretrained model checkpoints, and task\ncuration tools at https:\/\/minedojo.org\/. We hope that MINEDOJO will serve as an effective\nstarter framework for the community to develop new algorithms and advance towards generally\ncapable embodied agents.\n2\nMINEDOJO Simulator & Benchmark Suite\nMINEDOJO offers a set of simulator APIs help researchers develop generally capable, open-ended\nagents in Minecraft. It builds upon the open-source MineRL codebase [48] and makes the following\nupgrades: 1) We provide uniﬁed observation and action spaces across all tasks, facilitating the\ndevelopment of multi-task and continually learning agents that can constantly adapt to new scenarios\nand novel tasks. This deviates from the MineRL Challenge design that tailors observation and action\nspaces to individual tasks; 2) Our simulation unlocks all three types of worlds in Minecraft, including\nthe Overworld, the Nether, and the End, which substantially expands the possible task space,\nwhile MineRL only supports the Overworld natively; and 3) We provide convenient APIs to conﬁgure\ninitial conditions and world settings to standardize our tasks.\nWith this MINEDOJO simulator, we deﬁne thousands of benchmarking tasks, which are divided into\ntwo categories: 1) Programmatic tasks that can be automatically assessed based on the ground-truth\nsimulator states; and 2) Creative tasks that do not have well-deﬁned or easily-automated success\ncriteria, which motivates our novel evaluation protocol using a learned model (Sec. 4). To scale up\nthe number of Creative tasks, we mine ideas from YouTube tutorials and use OpenAI’s GPT-3 [15]\n3\nservice to generate substantially more task deﬁnitions. Compared to Creative tasks, Programmatic\ntasks are simpler to get started, but tend to have restricted scope, limited language variations, and less\nopen-endedness in general.\n2.1\nTask Suite I: Programmatic Tasks\nWe formalize each programmatic task as a 5-tuple: T = (G, G, I, fS, fR). G is an English\ndescription of the task goal, such as “ﬁnd material and craft a gold pickaxe”. G is a natural\nlanguage guidance that provides helpful hints, recipes, or advice to the agent.\nWe leverage\nOpenAI’s GPT-3-davinci API to automatically generate detailed guidance for a subset of\nthe tasks.\nFor the example goal “bring a pig into Nether”, GPT-3 returns: 1) Find a pig\nin the overworld; 2) Right-click on the pig with a lead; 3) Right-click on\nthe Nether Portal with the lead and pig selected; 4) The pig will be pulled\nthrough the portal! I is the initial conditions of the agent and the world, such as the initial\ninventory, spawn terrain, and weather. fS: st →{0, 1} is the success criterion, a deterministic\nfunction that maps the current world state st to a Boolean success label. fR: st →R is an optional\ndense reward function. We only provide fR for a small subset of the tasks in MINEDOJO due\nto the high costs of meticulously crafting dense rewards. For our current agent implementation\n(Sec. 4.1), we do not use detailed guidance. Inspired by concurrent works SayCan [3] and Socratic\nModels [143], one potential idea is to feed each step in the guidance to our learned reward model\nsequentially so that it becomes a stagewise reward function for a complex multi-stage task.\nMINEDOJO provides 4 categories of programmatic tasks with 1,581 template-generated natural\nlanguage goals to evaluate the agent’s different capabilities systematically and comprehensively:\n1. Survival: surviving for a designated number of days.\n2. Harvest: ﬁnding, obtaining, cultivating, or manufacturing hundreds of materials and objects.\n3. Tech Tree: crafting and using a hierarchy of tools.\n4. Combat: ﬁghting various monsters and creatures that require fast reﬂex and martial skills.\nEach task template has a number of variations based on the terrain, initial inventory, quantity, etc.,\nwhich form a ﬂexible spectrum of difﬁculty. In comparison, the NeurIPS MineRL Diamond challenge\n[48] is a subset of our programmatic task suite, deﬁned by the task goal “obtain 1 diamond\" in\nMINEDOJO.\n2.2\nTask Suite II: Creative Tasks\nWe deﬁne each creative task as a 3-tuple, T = (G, G, I), which differs from programmatic tasks due\nto the lack of straightforward success criteria. Inspired by model-based metrics like the Inception\nscore [98] and FID score [55] for image generation, we design a novel task evaluation metric based\non a pre-trained contrastive video-language model (Sec. 4.1). In the experiments, we ﬁnd that the\nlearned metric exhibits a high level of agreement with human evaluations (see Table 2).\nWe brainstorm and author 216 Creative tasks, such as “build a haunted house with zombie inside” and\n“race by riding a pig”. Nonetheless, such a manual approach is not scalable. Therefore, we develop\ntwo systematic approaches to extend the total number of task deﬁnitions to 1,560. This makes our\nCreative tasks 3 orders of magnitude larger than Minecraft BASALT challenge [104], which has 4\nCreative tasks.\nApproach 1. Task Mining from YouTube Tutorial Videos.\nWe identify our YouTube dataset\nas a rich source of tasks, as many human players demonstrate and narrate creative missions in\nthe tutorial playlists. To collect high-quality tasks and accompanying videos, we design a 3-stage\npipeline that makes it easy to ﬁnd and annotate interesting tasks (see Sec. C.2 for details). Through\nthis pipeline, we extract 1,042 task ideas from the common wisdom of a huge number of veteran\nMinecraft gamers, such as “make an automated mining machine” and “grow cactus up to the sky”.\nApproach 2. Task Creation by GPT-3.\nWe leverage GPT-3’s few-shot capability to generate new\ntask ideas by seeding it with the tasks we manually author or mine from YouTube. The prompt tem-\nplate is: Here are some example creative tasks in Minecraft:\n{a few examples}.\n4\nYouTube\nWiki\nReddit\nFigure 3: MINEDOJO’s internet-scale, multimodal knowledge base.\nLeft, YouTube videos:\nMinecraft gamers showcase the impressive feats they are able to achieve. Clockwise order: an\narchery range, Hogwarts castle, Taj Mahal, a Nether homebase. Middle, Wiki: Wiki pages contain\nmultimodal knowledge in structured layouts, such as comprehensive catalogs of creatures and recipes\nfor crafting. More examples in Fig. A.4 and A.5. Right, Reddit: We create a word cloud from\nReddit posts and comment threads. Gamers ask questions, share achievements, and discuss strategies\nextensively. Sample posts in Fig. A.7. Best viewed zoomed in.\nLet’s brainstorm more detailed while reasonable creative tasks in Minecraft.\nGPT-3 contributes 302 creative tasks after de-duplication, and demonstrates a surprisingly proﬁcient\nunderstanding of Minecraft terminology.\n2.3\nCollection of Starter Tasks\nWe curate a set of 64 core tasks for future researchers to get started more easily. If their agent works\nwell on these tasks, they can more conﬁdently scale to the full benchmark.\n• 32 programmatic tasks: 16 “standard” and 16 “difﬁcult”, spanning all 4 categories (survival,\nharvesting, combat, and tech tree). We rely on our Minecraft knowledge to decide the difﬁculty\nlevel. “Standard” tasks require fewer steps and lower resource dependencies to complete.\n• 32 creative tasks: 16 “standard” and 16 “difﬁcult”. Similarly, tasks labeled with “standard” are\ntypically short-horizon tasks.\nWe recommend that researchers run 100 evaluation episodes for each task and report the percentage\nsuccess rate. The programmatic tasks have ground-truth success, while the creative tasks need our\nnovel evaluation protocol (Sec. 5).\n3\nInternet-scale Knowledge Base\nTwo commonly used approaches [112, 126, 85, 36] to train embodied agents include training agents\nfrom scratch using RL with well-tuned reward functions for each task, or using a large amount of\nhuman-demonstrations to bootstrap agent learning. However, crafting well-tuned reward functions is\nchallenging or infeasible for our task suite (Sec. 2.2), and employing expert gamers to provide large\namounts of demonstration data would also be costly and infeasible [126].\nInstead, we turn to the open web as an ever-growing, virtually unlimited source of learning material\nfor embodied agents. The internet provides a vast amount of domain knowledge about Minecraft,\nwhich we harvest by extensive web scraping and ﬁltering. We collect 33 years worth of YouTube\nvideos, 6K+ Wiki pages, and millions of Reddit comment threads. Instead of hiring a handful of\nhuman demonstrators, we capture the collective wisdom of millions of Minecraft gamers around the\nworld. Furthermore, language is a key and pervasive component of our database that takes the form\nof YouTube transcripts, textual descriptions in Wiki, and Reddit discussions. Language facilitates\nopen-vocabulary understanding, provides grounding for image and video modalities, and unlocks the\npower of large language models [27, 109, 15] for embodied agents. To ensure socially responsible\nmodel development, we take special measures to ﬁlter out low-quality and toxic contents [13, 51]\nfrom our databases, detailed in the Appendix (Sec. D).\n5\n“Shear sheep to \nobtain wool”\nMineCLIP\nCorrelation = 0.95\nRGB\nVoxel\nGPS\nInventory\nObservation space\nMove\nAttack\nCam\nEquip\nAction space\nMineDojo Sim\nR\nTcuK9hpoR1KJs20sZlkSDJCGfoPblwo4tb\/cefmD4EFT0kcDjnXu69J0oZVdpxPqzC2vrG5lZxu7Szu7d\/UD48CpTIJCZtLJiQ3QgpwignbU01I91UEpREjHSiydXc79wTqajgt3qakjBI05jipE2UtBPx3QDMoVx676NfOgY3ue59erhri1hu\nc3oGs7C1TACq1B+b0\/FDhLCNeYIaV6rpPqMEdSU8zIrNTPFEkRnqAR6RnKUJUmC+2ncEzowxhLKT5XMOF+r0jR4lS0yQylQnSY\/Xbm4t\/eb1MxdhTnmacLxclCcMagFnJ8Oh1QSrNnUEIQlNbtCPEYSYW0CKpkQvi6F\/5Ogaru+7d14leblKo4\niOAGn4By4oA6a4Bq0QBtgcAcewBN4toT1aL1Yr8vSgrXqOQY\/YL19AvPQj2g=<\/latexit>φV\nStack the last 16 RGB frames\nφI\nTime\nφI\nφI\nφI\nAggregate\nVideo\nFeature\nPer-frame\nFeature\nFigure 4: Algorithm design. MINECLIP is a contrastive video-language model pre-trained on\nMINEDOJO’s massive Youtube database. It computes the correlation between an open-vocabulary\nlanguage goal string and a 16-frame video snippet. The correlation score can be used as a learned\ndense reward function to train a strong multi-task RL agent.\nYouTube Videos and Transcripts.\nMinecraft is among the most streamed games on YouTube [41].\nHuman players have demonstrated a stunning range of creative activities and sophisticated missions\nthat take hours to complete (examples in Fig. 3). We collect 730K+ narrated Minecraft videos, which\nadd up to 33 years of duration and 2.2B words in English transcripts. In comparison, HowTo100M [77]\nis a large-scale human instructional video dataset that includes 15 years of experience in total – about\nhalf of our volume. The time-aligned transcripts enable the agent to ground free-form natural lan-\nguage in video pixels and learn the semantics of diverse activities without laborious human labeling.\nWe operationalize this insight in our pre-trained video-language model (Sec. 4.1).\nMinecraft Wiki.\nThe Wiki pages cover almost every aspect of the game mechanics, and supply\na rich source of unstructured knowledge in multimodal tables, recipes, illustrations, and step-by-step\ntutorials. We use Selenium [103] to scrape 6,735 pages that interleave text, images, tables, and\ndiagrams. The pages are highly unstructured and do not share any common schema, as the Wiki\nis meant for human consumption rather than AI training. To preserve the layout information, we\nadditionally save the screenshots of entire pages and extract 2.2M bounding boxes of the visual\nelements (visualization in Fig. A.4 and A.5). We do not use Wiki data in our current experiments.\nSince the Wiki contains detailed recipes for all crafted objects, they could be provided as input\nor training data for hierarchical planning methods and policy sketches [8]. Another promising\nfuture direction is to apply document understanding models such as LayoutLM [138, 137] and\nDocFormer [9] to learn actionable knowledge from these unstructured Wiki data.\nReddit.\nWe scrape 340K+ posts along with 6.6M comments under the “r\/Minecraft” subreddit.\nThese posts ask questions on how to solve certain tasks, showcase cool architectures and achievements\nin image\/video snippets, and discuss general tips and tricks for players of all expertise levels. We\ndo not use Reddit data for training in Sec. 5, but a potential idea is to ﬁnetune large language models\n[27, 91] on our Reddit corpus to generate instructions and execution plans that are better grounded\nin the Minecraft domain. Concurrent works [3, 56, 143] have explored similar ideas and showed\nexcellent results on robot learning, which is encouraging for more future research in MINEDOJO.\n4\nAgent Learning with Large-scale Pre-training\nOne of the grand challenges of embodied AI is to build a single agent that can complete a wide range\nof open-world tasks. The MINEDOJO framework aims to facilitate new techniques towards this goal\nby providing an open-ended task suite (Sec. 2) and large-scale internet knowledge base (Sec. 3).\nHere we take an initial step towards this goal by developing a proof of concept that demonstrates\nhow a single language-prompted agent can be trained in MINEDOJO to complete several complex\nMinecraft tasks. To this end, we propose a novel agent learning algorithm that takes advantage of the\nmassive YouTube data offered by MINEDOJO. We note that this is only one of the numerous possible\n6\nTable 1: Our novel MINECLIP reward model is able to achieve competitive performance with\nmanually written dense reward function for Programmatic tasks, and signiﬁcantly outperforms the\nCLIPOpenAI method across all Creative tasks. Entries represent percentage success rates averaged\nover 3 seeds, each tested for 200 episodes. Success conditions are precise in Programmatic tasks, but\nestimated by MineCLIP for Creative tasks.\nGroup\nTasks\nOurs (Attn)\nOurs (Avg)\nManual Reward\nSparse-only\nCLIPOpenAI\nMilk Cow\n64.5 ± 37.1\n6.5 ± 3.5\n62.8 ± 40.1\n0.0 ± 0.0\n0.0 ± 0.0\nHunt Cow\n83.5 ± 7.1\n0.0 ± 0.0\n48.3 ± 35.9\n0.3 ± 0.4\n0.0 ± 0.0\nShear Sheep\n12.1 ± 9.1\n0.6 ± 0.2\n52.3 ± 33.2\n0.0 ± 0.0\n0.0 ± 0.0\nHunt Sheep\n8.1 ± 4.1\n0.0 ± 0.0\n41.9 ± 33.0\n0.3 ± 0.4\n0.0 ± 0.0\nCombat Spider\n80.5 ± 13.0\n60.1 ± 42.5\n87.5 ± 4.6\n47.8 ± 33.8\n0.0 ± 0.0\nCombat Zombie\n47.3 ± 10.6\n72.3 ± 6.4\n49.8 ± 26.9\n8.8 ± 12.4\n0.0 ± 0.0\nCombat Pigman\n1.6 ± 2.3\n0.0 ± 0.0\n13.6 ± 9.8\n0.0 ± 0.0\n0.0 ± 0.0\nCombat Enderman\n0.0 ± 0.0\n0.0 ± 0.0\n0.3 ± 0.2\n0.0 ± 0.0\n0.0 ± 0.0\nFind Nether Portal\n37.4 ± 40.8\n89.8 ± 5.7\nN\/A\nN\/A\n26.3 ± 32.6\nFind Ocean\n33.4 ± 45.6\n54.3 ± 40.7\nN\/A\nN\/A\n9.9 ± 14.1\nDig Hole\n91.6 ± 5.9\n88.1 ± 13.3\nN\/A\nN\/A\n0.0 ± 0.0\nLay Carpet\n97.6 ± 1.9\n98.8 ± 1.0\nN\/A\nN\/A\n0.0 ± 0.0\nways to use MINEDOJO’s internet database — the Wiki and Reddit corpus also hold great potential\nto drive new algorithm discoveries for the community in future works.\nIn this paper, we consider a multi-task reinforcement learning (RL) setting, where an agent is tasked\nwith completing a collection of MINEDOJO tasks speciﬁed by language instructions (Sec. 2). Solving\nthese tasks often requires the agent to interact with the Minecraft world in a prolonged fashion.\nAgents developed in popular RL benchmarks [119, 146] often rely on meticulously crafted dense and\ntask-speciﬁc reward functions to guide random explorations. However, these rewards are hard or even\ninfeasible to deﬁne for our diverse and open-ended tasks in MINEDOJO. To address this challenge, our\nkey insight is to learn a dense, language-conditioned reward function from in-the-wild YouTube\nvideos and their transcripts. Therefore, we introduce MINECLIP, a contrastive video-language\nmodel that learns to correlate video snippets and natural language descriptions (Fig. 4). MINECLIP\nis multi-task by design, as it is trained on open-vocabulary and diverse English transcripts.\nDuring RL training, MINECLIP provides a high-quality reward signal without any domain adaptation\ntechniques, despite the domain gap between noisy YouTube videos and clean simulator-rendered\nframes. MINECLIP eliminates the need to manually engineer reward functions for each and every\nMINEDOJO task. For Creative tasks that lack a simple success criterion (Sec. 2.2), MINECLIP also\nserves the dual purpose of an automatic evaluation metric that agrees well with human judgement\non a subset of tasks we investigate (Sec. 4.2, Table 2). Because the learned reward model incurs\na non-trivial computational overhead, we introduce several techniques to signiﬁcantly improve RL\ntraining efﬁciency, making MINECLIP a practical module for open-ended agent learning in Minecraft\n(Sec. 4.2).\n4.1\nPre-Training MINECLIP on Large-scale Videos\nFormally, the learned reward function can be deﬁned as ΦR : (G, V ) →R that maps a language goal\nG and a video snippet V to a scalar reward. An ideal ΦR should return a high reward if the behavior\ndepicted in the video faithfully follows the language description, and a low reward otherwise. This\ncan be achieved by optimizing the InfoNCE objective [125, 52, 20], which learns to correlate positive\nvideo and text pairs [118, 6, 78, 4, 136].\nSimilar to the image-text CLIP model [92], MINECLIP is composed of a separate text encoder φG\nthat embeds a language goal and a video encoder φV that embeds a moving window of 16 consecutive\nframes with 160 × 256 resolution (Fig. 4). Our neural architecture has a similar design as CLIP4Clip\n[75], where φG reuses OpenAI CLIP’s pretrained text encoder, and φV is factorized into a frame-wise\nimage encoder φI and a temporal aggregator φa that summarizes the sequence of 16 image features\ninto a single video embedding. Unlike CLIP4Clip, we insert two extra layers of residual CLIP\nAdapter [38] after the aggregator φa to produce a better video feature, and ﬁnetune only the last two\nlayers of the pretrained φI and φG.\n7\nTable 2: MINECLIP agrees well with the ground-truth human judgment on the Creative tasks we\nconsider. Numbers are F1 scores between MINECLIP’s binary classiﬁcation of tasks success and\nhuman labels (scaled to the percentage for better readability).\nTasks\nFind Nether Portal\nFind Ocean\nDig Hole\nLay Carpet\nOurs (Attn)\n98.7\n100.0\n99.4\n97.4\nOurs (Avg)\n100.0\n100.0\n100.0\n98.4\nCLIPOpenAI\n48.7\n98.4\n80.6\n54.1\nFrom the MINEDOJO YouTube database, we follow the procedure in VideoCLIP [136] to sample\n640K pairs of 16-second video snippets and time-aligned English transcripts, after applying a keyword\nﬁlter. We train two MINECLIP variants with different types of aggregator φa: (1) MINECLIP[avg]\ndoes simple average pooling, which is fast but agnostic to the temporal ordering; (2) MINECLIP[attn]\nencodes the sequence by two transformer layers, which is relatively slower but captures more temporal\ninformation, and thus produces a better reward signal in general. Details of data preprocessing,\narchitecture, and hyperparameters are listed in the Appendix (Sec. E).\n4.2\nRL with MINECLIP Reward\nWe train a language-conditioned policy network that takes as input raw pixels and predicts discrete\ncontrol. The policy is trained with PPO [102] on the MINECLIP rewards. In each episode, the\nagent is prompted with a language goal and takes a sequence of actions to fulﬁll this goal. When\ncalculating the MINECLIP rewards, we concatenate the agent’s latest 16 egocentric RGB frames in a\ntemporal window to form a video snippet. MINECLIP handles all task prompts zero-shot without any\nfurther ﬁnetuning. In our experiments (Sec. 5), we show that MINECLIP provides effective dense\nrewards out of the box, despite the domain shift between in-the-wild YouTube frames and simulator\nframes. Besides regular video data augmentation, we do not employ any special domain adaptation\nmethods during pre-training. Our ﬁnding is consistent with CLIP’s strong zero-shot performances on\nrobustness benchmarks in object recognition [92].\nCompared to hard-coded reward functions in popular benchmarks [146, 119, 34], the MINECLIP\nmodel has 150M parameters and is thus much more expensive to query. We make several design\nchoices to greatly accelerate RL training with MINECLIP in the loop:\n1. The language goal G is ﬁxed for a speciﬁc task, so the text features φG can be precomputed\nto avoid invoking the text encoder repeatedly.\n2. Our agent’s RGB encoder reuses the pre-trained weights of φI from MINECLIP. We do\nnot ﬁnetune φI during RL training, which saves computation and endows the agent with good\nvisual representations from the beginning.\n3. MINECLIP’s video encoder φV is factorized into an image encoder φI and a light-weight\naggregator φa. This design choice enables efﬁcient image feature caching. Consider two\noverlapping video sequences of 8 frames, V[0:8] and V[1:9]. We can cache the image\nfeatures of the 7 overlapping frames V[1] to V[7] to maximize compute savings. If φV is\na monolithic model like S3D [135] in VideoCLIP [136], then the video features from every\nsliding window must be recomputed, which would incur a much higher cost per time step.\n4. We leverage Self-Imitation Learning [84] to store the trajectories with high MINECLIP\nreward values in a buffer, and alternate between PPO and self-imitation gradient steps. It\nfurther improves sample efﬁciency as shown in the Appendix (Fig. A.8).\n5\nExperiments\nWe evaluate our agent-learning approach (Section 4) on 8 Programmatic tasks and 4 Creative tasks\nfrom the MINEDOJO benchmarking suite. We select these 12 tasks due to the diversity of skills\nrequired to solve them (e.g., harvesting, combat, building, navigation) and domain-speciﬁc entities\n(e.g., animals, resources, monsters, terrains, and structures). We split the tasks into 3 groups\nand train one multi-task agent for each group: Animal-Zoo (4 Programmatic tasks on hunting or\n8\nTable 3: MINECLIP agents have stronger zero-shot visual generalization ability to unseen terrains,\nweathers, and lighting. Numbers outside parentheses are percentage success rates averaged over 3\nseeds (each tested for 200 episodes), while those inside parentheses are relative performance changes.\nTasks\nOurs (Attn), train\nOurs (Attn), unseen test\nCLIPOpenAI, train\nCLIPOpenAI, unseen test\nMilk Cow\n64.5 ± 37.1\n64.8 ± 31.3(+ 0.8%)\n90.0 ± 0.4\n29.2 ± 3.7 (−67.6%)\nHunt Cow\n83.5 ± 7.1\n55.9 ± 7.2 (−32.9%)\n72.7 ± 3.5\n16.7 ± 1.6 (−77.0%)\nCombat Spider\n80.5 ± 13.0\n62.1 ± 29.7(−22.9%)\n79.5 ± 2.5\n54.2 ± 9.6 (−31.8%)\nCombat Zombie\n47.3 ± 10.6\n39.9 ± 25.3(−15.4%)\n50.2 ± 7.5\n30.8 ± 14.4(−38.6%)\nharvesting resource from animals), Mob-Combat (Programmatic, ﬁght 4 types of hostile monsters),\nand Creative (4 tasks).\nIn the experiments, we empirically check the quality of MINECLIP against manually written reward\nfunctions, and quantify how different variants of our learned model affect the RL performance. Table 1\npresents our main results, and Fig. 2 visualizes our learned agent behavior in 4 of the considered tasks.\nPolicy networks of all methods share the same architecture and are trained by PPO + Self-Imitation\n(Sec. 4.2, training details in the Appendix, Sec. F). We compare the following methods:\n• Ours (Attn): our agent trained with the MINECLIP[attn] reward model. For Program-\nmatic tasks, we also add the ﬁnal success condition as a binary reward. For Creative tasks,\nMINECLIP is the only source of reward.\n• Ours (Avg): the average-pooling variant of our method.\n• Manual Reward: hand-engineered dense reward using ground-truth simulator states.\n• Sparse-only: the ﬁnal binary success as a single sparse reward. Note that neither sparse-only\nnor manual reward is available for Creative tasks.\n• CLIPOpenAI: pre-trained OpenAI CLIP model that has not been ﬁnetuned on any MINEDOJO\nvideos.\nMINECLIP is competitive with manual reward.\nFor Programmatic tasks (ﬁrst 8 rows), RL\nagents guided by MINECLIP achieve competitive performance as those trained by manual reward.\nIn three of the tasks, they even outperform the hand-engineered reward functions, which rely on privi-\nleged simulator states unavailable to MINECLIP. For a more statistically sound analysis, we conduct\nthe Paired Student’s t-test to compare the mean success rate of each task (pairing column 3 “Ours\n(Attn)” and column 5 “Manual Reward” in Table 1). The test yields p-value 0.3991 ≫0.05, which\nindicates that the difference between our method and manual reward is not considered statistically\nsigniﬁcant, and therefore they are comparable with each other. Because all tasks require nontrivial ex-\nploration, our approach also dominates the Sparse-only baseline. Note that the original OpenAI CLIP\nmodel fails to achieve any success. We hypothesize that the creatures in Minecraft look dramatically\ndifferent from their real-world counterparts, which causes CLIP to produce misleading signals worse\nthan no shaping reward at all. It implies the importance of ﬁnetuning on MINEDOJO’s YouTube data.\nMINECLIP provides automated evaluation.\nFor Creative tasks (last 4 rows), there are no\nprogrammatic success criteria available. We convert MINECLIP into a binary success classiﬁer\nby thresholding the reward value it outputs for an episode. To test the quality of MINECLIP as\nan automatic evaluation metric, we ask human judges to curate a dataset of 100 successful and\n100 failed trajectories for each task. We then run both MINECLIP variants and CLIPOpenAI on the\ndataset and report the binary F1 score of their judgement against human ground-truth in Table 2.\nThe results demonstrate that both MINECLIP[attn] and MINECLIP[avg] attain a very high degree of\nagreement with human evaluation results on this subset of the Creative task suite that we investigate.\nCLIPOpenAI baseline also achieves nontrivial agreement on Find Ocean and Dig Hole tasks, likely\nbecause real-world oceans and holes have similar texture. We use the attn variant as an automated\nsuccess criterion to score the 4 Creative task results in Table 1. Our proposed method consistently\nlearns better than CLIPOpenAI-guided agents. It shows that MINECLIP is an effective approach to\nsolving open-ended tasks when no straightforward reward signal is available. We provide further\nanalysis beyond these 4 tasks in the Appendix (Sec. G.4).\n9\nTable 4: We train a single multi-task agent for all 12 tasks. All numbers represent percentage success\nrates averaged over 3 seeds, each tested for 200 episodes.\nGroup\nTasks\nSingle Agent on All Tasks\nOriginal\nPerformance Change\nMilk Cow\n91.5 ± 0.7\n64.5 ± 37.1\n↑\nHunt Cow\n46.8 ± 3.7\n83.5 ± 7.1\n↓\nShear Sheep\n73.5 ± 0.8\n12.1 ± 9.1\n↑\nHunt Sheep\n27.0 ± 1.0\n8.1 ± 4.1\n↑\nCombat Spider\n72.1 ± 1.3\n80.5 ± 13.0\n↓\nCombat Zombie\n27.1 ± 2.7\n47.3 ± 10.6\n↓\nCombat Pigman\n6.5 ± 1.2\n1.6 ± 2.3\n↑\nCombat Enderman\n0.0 ± 0.0\n0.0 ± 0.0\n=\nFind Nether Portal\n99.1 ± 0.4\n37.4 ± 40.8\n↑\nFind Ocean\n95.1 ± 1.5\n33.4 ± 45.6\n↑\nDig Hole\n85.8 ± 1.2\n91.6 ± 5.9\n↓\nLay Carpet\n96.5 ± 0.8\n97.6 ± 1.9\n=\nTable 5: We test the open-vocabulary generalization ability to two unseen tasks. All numbers represent\npercentage success rates averaged over 3 seeds, each tested for 200 episodes.\nTasks\nOurs (zero-shot)\nOurs (after RL ﬁnetune)\nBaseline (RL from scratch)\nHunt Pig\n1.3 ± 0.6\n46.0 ± 15.3\n0.0 ± 0.0\nHarvest Spider String\n1.6 ± 1.3\n36.5 ± 16.9\n12.5 ± 12.7\nMINECLIP shows good zero-shot generalization to signiﬁcant visual distribution shift.\nWe\nevaluate the learned policy without ﬁnetuning on a combination of unseen weather, lighting\nconditions, and terrains — 27 scenarios in total. For the baseline, we train agents with the original\nCLIPOpenAI image encoder (not trained on our YouTube videos) by imitation learning. The robustness\nagainst visual shift can be quantitatively measured by the relative performance degradation on\nnovel test scenarios for each task. Table 3 shows that while all methods incur performance drops,\nagents with MINECLIP encoder is more robust to visual changes than the baseline across all\ntasks. Pre-training on diverse in-the-wild YouTube videos is important to boosting zero-shot visual\ngeneralization capability, a ﬁnding consistent with literature [92, 82].\nLearning a Single Agent for All 12 Tasks\nWe have also trained a single agent for all 12 tasks.\nTo reduce the computational burden without loss of generality, the agent is trained by self-imitating\nfrom successful trajectories generated from the self-imitation learning pipeline (Section F.3). We\nsummarize the result in Table 4. Similar to our main experiments, all numbers represent percentage\nsuccess rates averaged over 3 training seeds, each tested for 200 episodes. Compared to the original\nagents, the new 12-multitask agent sees a performance boost in 6 tasks, degradation in 4 tasks,\nand roughly the same success rates in 2 tasks. This result suggests that there are both positive and\nnegative task transfers happening. To improve the multi-task performance, more advanced algorithms\n[141, 133] can be employed to mitigate the negative transfer effects.\nWe also perform Paired Student’s t-test to statistically compare the performance of the 12-multitask\nagent and those separately trained on each task group. We obtain a p-value of 0.3720 ≫0.05, which\nsuggests that the difference between the two training settings is not statistically signiﬁcant.\nGeneralize to Novel Tasks\nTo test the ability to generalize to new open-vocabulary commands,\nwe evaluate the agent on two novel tasks: “harvest spider string” and “hunt pig”. Table 5 shows\nthat the agent struggles in the zero-shot setting because it has not interacted with pigs or spider\nstrings during training, and thus does not know how to interact with them effectively. However, the\nperformance improves substantially by ﬁnetuning with the MINECLIP reward. Here the baseline\nmethods are trained from scratch using RL with the MINECLIP encoders and reward. Therefore,\nthe only difference is whether the policy has been pre-trained on the 12 tasks or not. Given the\n10\nsame environment sampling budget (only around 5% of total samples), it signiﬁcantly outperforms\nbaselines. It suggests that the multitask agent has learned transferable knowledge on hunting and\nresource collection, which enables it to quickly adapt to novel tasks.\n6\nRelated work\nOpen-ended Environments for Decision-making Agents.\nThere are many environments\ndeveloped with the goal of open-ended agent learning. Prior works include maze-style worlds\n[121, 129, 61], purely text-based game [69], grid worlds [21, 16], browser\/GUI-based environments\n[108, 124], and indoor simulators for robotics [1, 107, 114, 34, 110, 99, 89]. Minecraft offers\nan exciting alternative for open-ended agent learning. It is a 3D visual world with procedurally\ngenerated landscapes and extremely ﬂexible game mechanics that support an enormous variety\nof activities. Prior methods in open-ended agent learning [30, 57, 130, 63, 26] do not make use of\nexternal knowledge, but our approach leverages internet-scale database to learn open-vocabulary\nreward models, thanks to Minecraft’s abundance of gameplay data online.\nMinecraft for AI Research.\nThe Malmo platform [60] is the ﬁrst comprehensive release of a\nGym-style agent API [14] for Minecraft. Based on Malmo, MineRL [48] provides a codebase and\nhuman play trajectories for the annual Diamond Challenge at NeurIPS [47, 49, 62]. MINEDOJO’s\nsimulator builds upon the pioneering work of MineRL, but greatly expands the API and benchmarking\ntask suite. Other Minecraft benchmarks exist with different focuses. For example, CraftAssist [44]\nand IGLU [66] study agents with interactive dialogues. BASALT [104] applies human evaluation to 4\nopen-ended tasks. EvoCraft [45] is designed for structure building, and Crafter [50] optimizes for fast\nexperimentation. Unlike prior works, MINEDOJO’s core mission is to facilitate the development of\ngenerally capable embodied agents using internet-scale knowledge. We include a feature comparison\ntable of different Minecraft platforms for AI research in Table A.1.\nInternet-scale Multimodal Knowledge Bases.\nBig dataset such as Common Crawl [24], the Pile\n[37], LAION [100], YouTube-8M [2] and HowTo100M [77] have been fueling the success of large pre-\ntrained language models [27, 91, 15] and multimodal models [118, 6, 78, 145, 7, 4, 136]. While gener-\nally useful for learning representations, these datasets are not speciﬁcally targeted at embodied agents.\nTo provide agent-centric training data, RoboNet [25] collects video frames from 7 robot platforms,\nand Ego4D [43] recruits volunteers to record egocentric videos of household activities. In comparison,\nMINEDOJO’s knowledge base is constructed without human curation efforts, much larger in volume,\nmore diverse in data modalities, and comprehensively covers all aspects of the Minecraft environment.\nEmbodied Agents with Large-scale Pre-training.\nInspired by the success in NLP, embodied\nagent research [29, 11, 94, 23] has seen a surge in adoption of the large-scale pre-training paradigm.\nThe recent advances can be roughly divided into 4 categories.\n1) Novel agent architecture:\nDecision Transformer [19, 58, 144] applies the powerful self-attention models to sequential decision\nmaking. GATO [95] and Uniﬁed-IO [74] learn a single model to solve various decision-making\ntasks with different control interfaces. VIMA [59] uniﬁes a wide range of robot manipulation\ntasks with multimodal prompting. 2) Pre-training for better representations: R3M [82] trains a\ngeneral-purpose visual encoder for robot perception on Ego4D videos [43]. CLIPort [111] leverages\nthe pre-trained CLIP model [92] to enable free-form language instructions for robot manipulation.\n3) Pre-training for better policies: AlphaStar [126] achieves champion-level performance on\nStarCraft by imitating from numerous human demos. SayCan [3] leverages large language models\n(LMs) to ground value functions in the physical world. [72] and [96] directly reuse pre-trained\nLMs as policy backbone. VPT [10] is a concurrent work that learns an inverse dynamics model from\nhuman contractors to pseudo-label YouTube videos for behavior cloning. VPT is complementary to\nour approach, and can be ﬁnetuned to solve language-conditioned open-ended tasks with our learned\nreward model.\n4) Data-driven reward functions: Concept2Robot [105] and DVD [18] learn a\nbinary classiﬁer to score behaviors from in-the-wild videos [42]. LOReL [81] crowd-sources humans\nlabels to train language-conditioned reward function for ofﬂine RL. AVID [113] and XIRL [142]\nextract reward signals via cycle consistency. MINEDOJO’s task benchmark and internet knowledge\nbase are generally useful for developing new algorithms in all the above categories. In Sec. 4, we\nalso propose an open-vocabulary, multi-task reward model using MINEDOJO YouTube videos.\n11\n7\nConclusion\nIn this work, we introduce the MINEDOJO framework for developing generally capable embodied\nagents. MINEDOJO features a benchmarking suite of thousands of Programmatic and Creative tasks,\nand an internet-scale multimodal knowledge base of videos, wiki, and forum discussions. As an\nexample of the novel research possibilities enabled by MINEDOJO, we propose MINECLIP as an\neffective language-conditioned reward function trained with in-the-wild YouTube videos. MINECLIP\nachieves strong performance empirically and agrees well with human evaluation results, making it a\ngood automatic metric for Creative tasks. We look forward to seeing how MINEDOJO empowers the\ncommunity to make progress on the important challenge of open-ended agent learning.\n8\nAcknowledgement\nWe are extremely grateful to Anssi Kanervisto, Shikun Liu, Zhiding Yu, Chaowei Xiao, Weili Nie,\nJean Kossaiﬁ, Jonathan Raiman, Neel Kant, Saad Godil, Jaakko Haapasalo, Bryan Catanzaro, John\nSpitzer, Zhiyuan “Jerry” Lin, Yingqi Zheng, Chen Tessler, Dieter Fox, Oli Wright, Jeff Clune, Jack\nParker-Holder, and many other colleagues and friends for their helpful feedback and insightful\ndiscussions. We also thank the anonymous reviewers for offering us highly constructive advice\nand kind encouragement during the review and rebuttal period. NVIDIA provides the necessary\ncomputing resource and infrastructure for this project. Guanzhi Wang is supported by the Kortschak\nfellowship in Computing and Mathematical Sciences at Caltech.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nMineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge\n```\n#### 2. 论文摘要\n```\nAutonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https:\/\/minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.\n```\n\n#### 3. 论文全文\n```\nMINEDOJO: Building Open-Ended\nEmbodied Agents with Internet-Scale Knowledge\nLinxi Fan1, Guanzhi Wang2∗, Yunfan Jiang3∗, Ajay Mandlekar1, Yuncong Yang4,\nHaoyi Zhu5, Andrew Tang4, De-An Huang1, Yuke Zhu1 6†, Anima Anandkumar1 2†\n1NVIDIA, 2Caltech, 3Stanford, 4Columbia, 5SJTU, 6UT Austin\n∗Equal contribution\n†Equal advising\nhttps:\/\/minedojo.org\nAbstract\nAutonomous agents have made great strides in specialist domains like Atari games\nand Go. However, they typically learn tabula rasa in isolated environments with\nlimited and manually conceived objectives, thus failing to generalize across a wide\nspectrum of tasks and capabilities. Inspired by how humans continually learn\nand adapt in the open world, we advocate a trinity of ingredients for building\ngeneralist agents: 1) an environment that supports a multitude of tasks and goals,\n2) a large-scale database of multimodal knowledge, and 3) a ﬂexible and scalable\nagent architecture. We introduce MINEDOJO, a new framework built on the\npopular Minecraft game that features a simulation suite with thousands of diverse\nopen-ended tasks and an internet-scale knowledge base with Minecraft videos,\ntutorials, wiki pages, and forum discussions. Using MINEDOJO’s data, we propose\na novel agent learning algorithm that leverages large pre-trained video-language\nmodels as a learned reward function. Our agent is able to solve a variety of open-\nended tasks speciﬁed in free-form language without any manually designed dense\nshaping reward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https:\/\/minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.\n1\nIntroduction\nDeveloping autonomous embodied agents that can attain human-level performance across a wide\nspectrum of tasks has been a long-standing goal for AI research. There has been impressive progress\ntowards this goal, most notably in games [80, 85, 126] and robotics [68, 99, 146, 134, 107]. These\nembodied agents are typically trained tabula rasa in isolated worlds with limited complexity and\ndiversity. Although highly performant, they are specialist models that do not generalize beyond a\nnarrow set of tasks. In contrast, humans inhabit an inﬁnitely rich reality, continuously learn from and\nadapt to a wide variety of open-ended tasks, and are able to leverage large amount of prior knowledge\nfrom their own experiences as well as others.\nWe argue that three main pillars are necessary for generalist embodied agents to emerge. First, the\nenvironment in which the agent acts needs to enable an unlimited variety of open-ended goals\n[116, 71, 120, 117]. Natural evolution is able to nurture an ever-expanding tree of diverse life forms\nthanks to the inﬁnitely varied ecological settings that the Earth supports [117, 129]. This process has\nnot stagnated for billions of years. In contrast, today’s agent training algorithms cease to make new\nprogress after convergence in narrow environments [80, 146]. Second, a large-scale database of\nprior knowledge is necessary to facilitate learning in open-ended settings. Just as humans frequently\nlearn from the internet, agents should also be able to harvest practical knowledge encoded in large\namounts of video demos [42, 77], multimedia tutorials [79], and forum discussions [127, 65, 54]. In a\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\narXiv:2206.08853v2  [cs.LG]  22 Nov 2022\nAt\nCraft Glass Bridge\nCombat Zombie\nFish Squid\nBuild Oak House\nMake Ice Igloo\nFarm Sugar Cane\nFind Ocean \nMonument\nTreasure Hunt\nin End City\nExplore \nDesert Temple\nOpen-ended Environments\nGeneralist Agent\nInternet-scale Knowledge Base\nWiki\nYouTube\nReddit\nFigure 1: MINEDOJO is a novel framework for developing open-ended, generally capable agents\nthat can learn and adapt continually to new goals. MINEDOJO features a benchmarking suite with\nthousands of diverse open-ended tasks speciﬁed in natural language prompts, and also provides an\ninternet-scale, multimodal knowledge base of YouTube videos, Wiki pages, and Reddit posts. The\ndatabase captures the collective experience and wisdom of millions of Minecraft gamers for an AI\nagent to learn from. Best viewed zoomed in.\ncomplex world, it would be extremely inefﬁcient for an agent to learn everything from scratch through\ntrial and error. Third, the agent’s architecture needs to be ﬂexible enough to pursue any task in open-\nended environments, and scalable enough to convert large-scale knowledge sources into actionable\ninsights [19, 96]. This motivates the design of an agent that has a uniﬁed observation\/action space,\nconditions on natural language task prompts, and adopts the Transformer pre-training paradigm [27,\n91, 15] to internalize knowledge effectively.\nIn light of these three pillars, we introduce MINEDOJO, a new framework to help the community\ndevelop open-ended, generally-capable agents. It is built on the popular Minecraft game, where a\nplayer explores a procedurally generated 3D world with diverse types of terrains to roam, materials\nto mine, tools to craft, structures to build, and wonders to discover. Unlike most other games\n[80, 85, 126], Minecraft deﬁnes no speciﬁc reward to maximize and no ﬁxed storyline to follow,\nmaking it well suited for developing open-ended environments for embodied AI research. We make\nthe following three major contributions:\n1. Simulation platform with thousands of diverse open-ended tasks.\nMINEDOJO provides\nconvenient APIs on top of Minecraft that standardize task speciﬁcation, world settings, and agent’s\nobservation\/action spaces. We introduce a benchmark suite that consists of thousands of natural\nlanguage-prompted tasks, making it two orders of magnitude larger than prior Minecraft benchmarks\nlike the MineRL Challenge [48, 62]. The suite includes long-horizon, open-ended tasks that cannot\nbe easily evaluated through automated procedures, such as “build an epic modern house with two\nﬂoors and a swimming pool”. Inspired by the Inception score [98] and FID score [55] that are\ncommonly used to assess AI-generated image quality, we introduce a novel agent evaluation protocol\nusing a large video-language model pre-trained on Minecraft YouTube videos. This complements\nhuman scoring [104] that is precise but more expensive. Our learned evaluation metric has good\nagreement with human judgment in a subset of the full task suite considered in the experiments.\n2. Internet-scale multimodal Minecraft knowledge base.\nMinecraft has more than 100 million\nactive players [131], who have collectively generated an enormous wealth of data. They record\ntutorial videos, stream live play sessions, compile recipes, and discuss tips and tricks on forums.\nMINEDOJO features a massive collection of 730K+ YouTube videos with time-aligned transcripts,\n6K+ free-form Wiki pages, and 340K+ Reddit posts with multimedia contents (Fig. 3). We hope that\nthis enormous knowledge base can help the agent acquire diverse skills, develop complex strategies,\ndiscover interesting objectives, and learn actionable representations automatically.\n2\nshear a sheep\ncombat zombie \npigman\nfind a nether \nportal\nput carpets on \nthe floor\nFigure 2: Visualization of our agent’s learned behaviors on four selected tasks. Leftmost texts are the\ntask prompts used in training. Best viewed on a color display.\n3. Novel algorithm for embodied agents with large-scale pre-training.\nWe develop a new\nlearning algorithm for embodied agents that makes use of the internet-scale domain knowledge we\nhave collected from the web. Using the massive volume of YouTube videos from MINEDOJO, we\ntrain a video-text contrastive model in the spirit of CLIP [92], which associates natural language\nsubtitles with their time-aligned video segments. We demonstrate that this learned correlation score\ncan be used effectively as an open-vocabulary, massively multi-task reward function for RL training.\nOur agent solves the majority of 12 tasks in our experiment using the learned reward model (Fig. 2).\nIt achieves competitive performance to agents trained with meticulously engineered dense-shaping\nrewards, and in some cases outperforms them, with up to 73% improvement in success rates. For\nopen-ended tasks that do not have a simple success criterion, our agents also perform well without\nany special modiﬁcations.\nIn summary, this paper proposes an open-ended task suite, internet-scale domain knowledge, and agent\nlearning with recent advances on large pre-trained models [13]. We have open-sourced MINEDOJO’s\nsimulator, knowledge bases, algorithm implementations, pretrained model checkpoints, and task\ncuration tools at https:\/\/minedojo.org\/. We hope that MINEDOJO will serve as an effective\nstarter framework for the community to develop new algorithms and advance towards generally\ncapable embodied agents.\n2\nMINEDOJO Simulator & Benchmark Suite\nMINEDOJO offers a set of simulator APIs help researchers develop generally capable, open-ended\nagents in Minecraft. It builds upon the open-source MineRL codebase [48] and makes the following\nupgrades: 1) We provide uniﬁed observation and action spaces across all tasks, facilitating the\ndevelopment of multi-task and continually learning agents that can constantly adapt to new scenarios\nand novel tasks. This deviates from the MineRL Challenge design that tailors observation and action\nspaces to individual tasks; 2) Our simulation unlocks all three types of worlds in Minecraft, including\nthe Overworld, the Nether, and the End, which substantially expands the possible task space,\nwhile MineRL only supports the Overworld natively; and 3) We provide convenient APIs to conﬁgure\ninitial conditions and world settings to standardize our tasks.\nWith this MINEDOJO simulator, we deﬁne thousands of benchmarking tasks, which are divided into\ntwo categories: 1) Programmatic tasks that can be automatically assessed based on the ground-truth\nsimulator states; and 2) Creative tasks that do not have well-deﬁned or easily-automated success\ncriteria, which motivates our novel evaluation protocol using a learned model (Sec. 4). To scale up\nthe number of Creative tasks, we mine ideas from YouTube tutorials and use OpenAI’s GPT-3 [15]\n3\nservice to generate substantially more task deﬁnitions. Compared to Creative tasks, Programmatic\ntasks are simpler to get started, but tend to have restricted scope, limited language variations, and less\nopen-endedness in general.\n2.1\nTask Suite I: Programmatic Tasks\nWe formalize each programmatic task as a 5-tuple: T = (G, G, I, fS, fR). G is an English\ndescription of the task goal, such as “ﬁnd material and craft a gold pickaxe”. G is a natural\nlanguage guidance that provides helpful hints, recipes, or advice to the agent.\nWe leverage\nOpenAI’s GPT-3-davinci API to automatically generate detailed guidance for a subset of\nthe tasks.\nFor the example goal “bring a pig into Nether”, GPT-3 returns: 1) Find a pig\nin the overworld; 2) Right-click on the pig with a lead; 3) Right-click on\nthe Nether Portal with the lead and pig selected; 4) The pig will be pulled\nthrough the portal! I is the initial conditions of the agent and the world, such as the initial\ninventory, spawn terrain, and weather. fS: st →{0, 1} is the success criterion, a deterministic\nfunction that maps the current world state st to a Boolean success label. fR: st →R is an optional\ndense reward function. We only provide fR for a small subset of the tasks in MINEDOJO due\nto the high costs of meticulously crafting dense rewards. For our current agent implementation\n(Sec. 4.1), we do not use detailed guidance. Inspired by concurrent works SayCan [3] and Socratic\nModels [143], one potential idea is to feed each step in the guidance to our learned reward model\nsequentially so that it becomes a stagewise reward function for a complex multi-stage task.\nMINEDOJO provides 4 categories of programmatic tasks with 1,581 template-generated natural\nlanguage goals to evaluate the agent’s different capabilities systematically and comprehensively:\n1. Survival: surviving for a designated number of days.\n2. Harvest: ﬁnding, obtaining, cultivating, or manufacturing hundreds of materials and objects.\n3. Tech Tree: crafting and using a hierarchy of tools.\n4. Combat: ﬁghting various monsters and creatures that require fast reﬂex and martial skills.\nEach task template has a number of variations based on the terrain, initial inventory, quantity, etc.,\nwhich form a ﬂexible spectrum of difﬁculty. In comparison, the NeurIPS MineRL Diamond challenge\n[48] is a subset of our programmatic task suite, deﬁned by the task goal “obtain 1 diamond\" in\nMINEDOJO.\n2.2\nTask Suite II: Creative Tasks\nWe deﬁne each creative task as a 3-tuple, T = (G, G, I), which differs from programmatic tasks due\nto the lack of straightforward success criteria. Inspired by model-based metrics like the Inception\nscore [98] and FID score [55] for image generation, we design a novel task evaluation metric based\non a pre-trained contrastive video-language model (Sec. 4.1). In the experiments, we ﬁnd that the\nlearned metric exhibits a high level of agreement with human evaluations (see Table 2).\nWe brainstorm and author 216 Creative tasks, such as “build a haunted house with zombie inside” and\n“race by riding a pig”. Nonetheless, such a manual approach is not scalable. Therefore, we develop\ntwo systematic approaches to extend the total number of task deﬁnitions to 1,560. This makes our\nCreative tasks 3 orders of magnitude larger than Minecraft BASALT challenge [104], which has 4\nCreative tasks.\nApproach 1. Task Mining from YouTube Tutorial Videos.\nWe identify our YouTube dataset\nas a rich source of tasks, as many human players demonstrate and narrate creative missions in\nthe tutorial playlists. To collect high-quality tasks and accompanying videos, we design a 3-stage\npipeline that makes it easy to ﬁnd and annotate interesting tasks (see Sec. C.2 for details). Through\nthis pipeline, we extract 1,042 task ideas from the common wisdom of a huge number of veteran\nMinecraft gamers, such as “make an automated mining machine” and “grow cactus up to the sky”.\nApproach 2. Task Creation by GPT-3.\nWe leverage GPT-3’s few-shot capability to generate new\ntask ideas by seeding it with the tasks we manually author or mine from YouTube. The prompt tem-\nplate is: Here are some example creative tasks in Minecraft:\n{a few examples}.\n4\nYouTube\nWiki\nReddit\nFigure 3: MINEDOJO’s internet-scale, multimodal knowledge base.\nLeft, YouTube videos:\nMinecraft gamers showcase the impressive feats they are able to achieve. Clockwise order: an\narchery range, Hogwarts castle, Taj Mahal, a Nether homebase. Middle, Wiki: Wiki pages contain\nmultimodal knowledge in structured layouts, such as comprehensive catalogs of creatures and recipes\nfor crafting. More examples in Fig. A.4 and A.5. Right, Reddit: We create a word cloud from\nReddit posts and comment threads. Gamers ask questions, share achievements, and discuss strategies\nextensively. Sample posts in Fig. A.7. Best viewed zoomed in.\nLet’s brainstorm more detailed while reasonable creative tasks in Minecraft.\nGPT-3 contributes 302 creative tasks after de-duplication, and demonstrates a surprisingly proﬁcient\nunderstanding of Minecraft terminology.\n2.3\nCollection of Starter Tasks\nWe curate a set of 64 core tasks for future researchers to get started more easily. If their agent works\nwell on these tasks, they can more conﬁdently scale to the full benchmark.\n• 32 programmatic tasks: 16 “standard” and 16 “difﬁcult”, spanning all 4 categories (survival,\nharvesting, combat, and tech tree). We rely on our Minecraft knowledge to decide the difﬁculty\nlevel. “Standard” tasks require fewer steps and lower resource dependencies to complete.\n• 32 creative tasks: 16 “standard” and 16 “difﬁcult”. Similarly, tasks labeled with “standard” are\ntypically short-horizon tasks.\nWe recommend that researchers run 100 evaluation episodes for each task and report the percentage\nsuccess rate. The programmatic tasks have ground-truth success, while the creative tasks need our\nnovel evaluation protocol (Sec. 5).\n3\nInternet-scale Knowledge Base\nTwo commonly used approaches [112, 126, 85, 36] to train embodied agents include training agents\nfrom scratch using RL with well-tuned reward functions for each task, or using a large amount of\nhuman-demonstrations to bootstrap agent learning. However, crafting well-tuned reward functions is\nchallenging or infeasible for our task suite (Sec. 2.2), and employing expert gamers to provide large\namounts of demonstration data would also be costly and infeasible [126].\nInstead, we turn to the open web as an ever-growing, virtually unlimited source of learning material\nfor embodied agents. The internet provides a vast amount of domain knowledge about Minecraft,\nwhich we harvest by extensive web scraping and ﬁltering. We collect 33 years worth of YouTube\nvideos, 6K+ Wiki pages, and millions of Reddit comment threads. Instead of hiring a handful of\nhuman demonstrators, we capture the collective wisdom of millions of Minecraft gamers around the\nworld. Furthermore, language is a key and pervasive component of our database that takes the form\nof YouTube transcripts, textual descriptions in Wiki, and Reddit discussions. Language facilitates\nopen-vocabulary understanding, provides grounding for image and video modalities, and unlocks the\npower of large language models [27, 109, 15] for embodied agents. To ensure socially responsible\nmodel development, we take special measures to ﬁlter out low-quality and toxic contents [13, 51]\nfrom our databases, detailed in the Appendix (Sec. D).\n5\n“Shear sheep to \nobtain wool”\nMineCLIP\nCorrelation = 0.95\nRGB\nVoxel\nGPS\nInventory\nObservation space\nMove\nAttack\nCam\nEquip\nAction space\nMineDojo Sim\nR\nTcuK9hpoR1KJs20sZlkSDJCGfoPblwo4tb\/cefmD4EFT0kcDjnXu69J0oZVdpxPqzC2vrG5lZxu7Szu7d\/UD48CpTIJCZtLJiQ3QgpwignbU01I91UEpREjHSiydXc79wTqajgt3qakjBI05jipE2UtBPx3QDMoVx676NfOgY3ue59erhri1hu\nc3oGs7C1TACq1B+b0\/FDhLCNeYIaV6rpPqMEdSU8zIrNTPFEkRnqAR6RnKUJUmC+2ncEzowxhLKT5XMOF+r0jR4lS0yQylQnSY\/Xbm4t\/eb1MxdhTnmacLxclCcMagFnJ8Oh1QSrNnUEIQlNbtCPEYSYW0CKpkQvi6F\/5Ogaru+7d14leblKo4\niOAGn4By4oA6a4Bq0QBtgcAcewBN4toT1aL1Yr8vSgrXqOQY\/YL19AvPQj2g=<\/latexit>φV\nStack the last 16 RGB frames\nφI\nTime\nφI\nφI\nφI\nAggregate\nVideo\nFeature\nPer-frame\nFeature\nFigure 4: Algorithm design. MINECLIP is a contrastive video-language model pre-trained on\nMINEDOJO’s massive Youtube database. It computes the correlation between an open-vocabulary\nlanguage goal string and a 16-frame video snippet. The correlation score can be used as a learned\ndense reward function to train a strong multi-task RL agent.\nYouTube Videos and Transcripts.\nMinecraft is among the most streamed games on YouTube [41].\nHuman players have demonstrated a stunning range of creative activities and sophisticated missions\nthat take hours to complete (examples in Fig. 3). We collect 730K+ narrated Minecraft videos, which\nadd up to 33 years of duration and 2.2B words in English transcripts. In comparison, HowTo100M [77]\nis a large-scale human instructional video dataset that includes 15 years of experience in total – about\nhalf of our volume. The time-aligned transcripts enable the agent to ground free-form natural lan-\nguage in video pixels and learn the semantics of diverse activities without laborious human labeling.\nWe operationalize this insight in our pre-trained video-language model (Sec. 4.1).\nMinecraft Wiki.\nThe Wiki pages cover almost every aspect of the game mechanics, and supply\na rich source of unstructured knowledge in multimodal tables, recipes, illustrations, and step-by-step\ntutorials. We use Selenium [103] to scrape 6,735 pages that interleave text, images, tables, and\ndiagrams. The pages are highly unstructured and do not share any common schema, as the Wiki\nis meant for human consumption rather than AI training. To preserve the layout information, we\nadditionally save the screenshots of entire pages and extract 2.2M bounding boxes of the visual\nelements (visualization in Fig. A.4 and A.5). We do not use Wiki data in our current experiments.\nSince the Wiki contains detailed recipes for all crafted objects, they could be provided as input\nor training data for hierarchical planning methods and policy sketches [8]. Another promising\nfuture direction is to apply document understanding models such as LayoutLM [138, 137] and\nDocFormer [9] to learn actionable knowledge from these unstructured Wiki data.\nReddit.\nWe scrape 340K+ posts along with 6.6M comments under the “r\/Minecraft” subreddit.\nThese posts ask questions on how to solve certain tasks, showcase cool architectures and achievements\nin image\/video snippets, and discuss general tips and tricks for players of all expertise levels. We\ndo not use Reddit data for training in Sec. 5, but a potential idea is to ﬁnetune large language models\n[27, 91] on our Reddit corpus to generate instructions and execution plans that are better grounded\nin the Minecraft domain. Concurrent works [3, 56, 143] have explored similar ideas and showed\nexcellent results on robot learning, which is encouraging for more future research in MINEDOJO.\n4\nAgent Learning with Large-scale Pre-training\nOne of the grand challenges of embodied AI is to build a single agent that can complete a wide range\nof open-world tasks. The MINEDOJO framework aims to facilitate new techniques towards this goal\nby providing an open-ended task suite (Sec. 2) and large-scale internet knowledge base (Sec. 3).\nHere we take an initial step towards this goal by developing a proof of concept that demonstrates\nhow a single language-prompted agent can be trained in MINEDOJO to complete several complex\nMinecraft tasks. To this end, we propose a novel agent learning algorithm that takes advantage of the\nmassive YouTube data offered by MINEDOJO. We note that this is only one of the numerous possible\n6\nTable 1: Our novel MINECLIP reward model is able to achieve competitive performance with\nmanually written dense reward function for Programmatic tasks, and signiﬁcantly outperforms the\nCLIPOpenAI method across all Creative tasks. Entries represent percentage success rates averaged\nover 3 seeds, each tested for 200 episodes. Success conditions are precise in Programmatic tasks, but\nestimated by MineCLIP for Creative tasks.\nGroup\nTasks\nOurs (Attn)\nOurs (Avg)\nManual Reward\nSparse-only\nCLIPOpenAI\nMilk Cow\n64.5 ± 37.1\n6.5 ± 3.5\n62.8 ± 40.1\n0.0 ± 0.0\n0.0 ± 0.0\nHunt Cow\n83.5 ± 7.1\n0.0 ± 0.0\n48.3 ± 35.9\n0.3 ± 0.4\n0.0 ± 0.0\nShear Sheep\n12.1 ± 9.1\n0.6 ± 0.2\n52.3 ± 33.2\n0.0 ± 0.0\n0.0 ± 0.0\nHunt Sheep\n8.1 ± 4.1\n0.0 ± 0.0\n41.9 ± 33.0\n0.3 ± 0.4\n0.0 ± 0.0\nCombat Spider\n80.5 ± 13.0\n60.1 ± 42.5\n87.5 ± 4.6\n47.8 ± 33.8\n0.0 ± 0.0\nCombat Zombie\n47.3 ± 10.6\n72.3 ± 6.4\n49.8 ± 26.9\n8.8 ± 12.4\n0.0 ± 0.0\nCombat Pigman\n1.6 ± 2.3\n0.0 ± 0.0\n13.6 ± 9.8\n0.0 ± 0.0\n0.0 ± 0.0\nCombat Enderman\n0.0 ± 0.0\n0.0 ± 0.0\n0.3 ± 0.2\n0.0 ± 0.0\n0.0 ± 0.0\nFind Nether Portal\n37.4 ± 40.8\n89.8 ± 5.7\nN\/A\nN\/A\n26.3 ± 32.6\nFind Ocean\n33.4 ± 45.6\n54.3 ± 40.7\nN\/A\nN\/A\n9.9 ± 14.1\nDig Hole\n91.6 ± 5.9\n88.1 ± 13.3\nN\/A\nN\/A\n0.0 ± 0.0\nLay Carpet\n97.6 ± 1.9\n98.8 ± 1.0\nN\/A\nN\/A\n0.0 ± 0.0\nways to use MINEDOJO’s internet database — the Wiki and Reddit corpus also hold great potential\nto drive new algorithm discoveries for the community in future works.\nIn this paper, we consider a multi-task reinforcement learning (RL) setting, where an agent is tasked\nwith completing a collection of MINEDOJO tasks speciﬁed by language instructions (Sec. 2). Solving\nthese tasks often requires the agent to interact with the Minecraft world in a prolonged fashion.\nAgents developed in popular RL benchmarks [119, 146] often rely on meticulously crafted dense and\ntask-speciﬁc reward functions to guide random explorations. However, these rewards are hard or even\ninfeasible to deﬁne for our diverse and open-ended tasks in MINEDOJO. To address this challenge, our\nkey insight is to learn a dense, language-conditioned reward function from in-the-wild YouTube\nvideos and their transcripts. Therefore, we introduce MINECLIP, a contrastive video-language\nmodel that learns to correlate video snippets and natural language descriptions (Fig. 4). MINECLIP\nis multi-task by design, as it is trained on open-vocabulary and diverse English transcripts.\nDuring RL training, MINECLIP provides a high-quality reward signal without any domain adaptation\ntechniques, despite the domain gap between noisy YouTube videos and clean simulator-rendered\nframes. MINECLIP eliminates the need to manually engineer reward functions for each and every\nMINEDOJO task. For Creative tasks that lack a simple success criterion (Sec. 2.2), MINECLIP also\nserves the dual purpose of an automatic evaluation metric that agrees well with human judgement\non a subset of tasks we investigate (Sec. 4.2, Table 2). Because the learned reward model incurs\na non-trivial computational overhead, we introduce several techniques to signiﬁcantly improve RL\ntraining efﬁciency, making MINECLIP a practical module for open-ended agent learning in Minecraft\n(Sec. 4.2).\n4.1\nPre-Training MINECLIP on Large-scale Videos\nFormally, the learned reward function can be deﬁned as ΦR : (G, V ) →R that maps a language goal\nG and a video snippet V to a scalar reward. An ideal ΦR should return a high reward if the behavior\ndepicted in the video faithfully follows the language description, and a low reward otherwise. This\ncan be achieved by optimizing the InfoNCE objective [125, 52, 20], which learns to correlate positive\nvideo and text pairs [118, 6, 78, 4, 136].\nSimilar to the image-text CLIP model [92], MINECLIP is composed of a separate text encoder φG\nthat embeds a language goal and a video encoder φV that embeds a moving window of 16 consecutive\nframes with 160 × 256 resolution (Fig. 4). Our neural architecture has a similar design as CLIP4Clip\n[75], where φG reuses OpenAI CLIP’s pretrained text encoder, and φV is factorized into a frame-wise\nimage encoder φI and a temporal aggregator φa that summarizes the sequence of 16 image features\ninto a single video embedding. Unlike CLIP4Clip, we insert two extra layers of residual CLIP\nAdapter [38] after the aggregator φa to produce a better video feature, and ﬁnetune only the last two\nlayers of the pretrained φI and φG.\n7\nTable 2: MINECLIP agrees well with the ground-truth human judgment on the Creative tasks we\nconsider. Numbers are F1 scores between MINECLIP’s binary classiﬁcation of tasks success and\nhuman labels (scaled to the percentage for better readability).\nTasks\nFind Nether Portal\nFind Ocean\nDig Hole\nLay Carpet\nOurs (Attn)\n98.7\n100.0\n99.4\n97.4\nOurs (Avg)\n100.0\n100.0\n100.0\n98.4\nCLIPOpenAI\n48.7\n98.4\n80.6\n54.1\nFrom the MINEDOJO YouTube database, we follow the procedure in VideoCLIP [136] to sample\n640K pairs of 16-second video snippets and time-aligned English transcripts, after applying a keyword\nﬁlter. We train two MINECLIP variants with different types of aggregator φa: (1) MINECLIP[avg]\ndoes simple average pooling, which is fast but agnostic to the temporal ordering; (2) MINECLIP[attn]\nencodes the sequence by two transformer layers, which is relatively slower but captures more temporal\ninformation, and thus produces a better reward signal in general. Details of data preprocessing,\narchitecture, and hyperparameters are listed in the Appendix (Sec. E).\n4.2\nRL with MINECLIP Reward\nWe train a language-conditioned policy network that takes as input raw pixels and predicts discrete\ncontrol. The policy is trained with PPO [102] on the MINECLIP rewards. In each episode, the\nagent is prompted with a language goal and takes a sequence of actions to fulﬁll this goal. When\ncalculating the MINECLIP rewards, we concatenate the agent’s latest 16 egocentric RGB frames in a\ntemporal window to form a video snippet. MINECLIP handles all task prompts zero-shot without any\nfurther ﬁnetuning. In our experiments (Sec. 5), we show that MINECLIP provides effective dense\nrewards out of the box, despite the domain shift between in-the-wild YouTube frames and simulator\nframes. Besides regular video data augmentation, we do not employ any special domain adaptation\nmethods during pre-training. Our ﬁnding is consistent with CLIP’s strong zero-shot performances on\nrobustness benchmarks in object recognition [92].\nCompared to hard-coded reward functions in popular benchmarks [146, 119, 34], the MINECLIP\nmodel has 150M parameters and is thus much more expensive to query. We make several design\nchoices to greatly accelerate RL training with MINECLIP in the loop:\n1. The language goal G is ﬁxed for a speciﬁc task, so the text features φG can be precomputed\nto avoid invoking the text encoder repeatedly.\n2. Our agent’s RGB encoder reuses the pre-trained weights of φI from MINECLIP. We do\nnot ﬁnetune φI during RL training, which saves computation and endows the agent with good\nvisual representations from the beginning.\n3. MINECLIP’s video encoder φV is factorized into an image encoder φI and a light-weight\naggregator φa. This design choice enables efﬁcient image feature caching. Consider two\noverlapping video sequences of 8 frames, V[0:8] and V[1:9]. We can cache the image\nfeatures of the 7 overlapping frames V[1] to V[7] to maximize compute savings. If φV is\na monolithic model like S3D [135] in VideoCLIP [136], then the video features from every\nsliding window must be recomputed, which would incur a much higher cost per time step.\n4. We leverage Self-Imitation Learning [84] to store the trajectories with high MINECLIP\nreward values in a buffer, and alternate between PPO and self-imitation gradient steps. It\nfurther improves sample efﬁciency as shown in the Appendix (Fig. A.8).\n5\nExperiments\nWe evaluate our agent-learning approach (Section 4) on 8 Programmatic tasks and 4 Creative tasks\nfrom the MINEDOJO benchmarking suite. We select these 12 tasks due to the diversity of skills\nrequired to solve them (e.g., harvesting, combat, building, navigation) and domain-speciﬁc entities\n(e.g., animals, resources, monsters, terrains, and structures). We split the tasks into 3 groups\nand train one multi-task agent for each group: Animal-Zoo (4 Programmatic tasks on hunting or\n8\nTable 3: MINECLIP agents have stronger zero-shot visual generalization ability to unseen terrains,\nweathers, and lighting. Numbers outside parentheses are percentage success rates averaged over 3\nseeds (each tested for 200 episodes), while those inside parentheses are relative performance changes.\nTasks\nOurs (Attn), train\nOurs (Attn), unseen test\nCLIPOpenAI, train\nCLIPOpenAI, unseen test\nMilk Cow\n64.5 ± 37.1\n64.8 ± 31.3(+ 0.8%)\n90.0 ± 0.4\n29.2 ± 3.7 (−67.6%)\nHunt Cow\n83.5 ± 7.1\n55.9 ± 7.2 (−32.9%)\n72.7 ± 3.5\n16.7 ± 1.6 (−77.0%)\nCombat Spider\n80.5 ± 13.0\n62.1 ± 29.7(−22.9%)\n79.5 ± 2.5\n54.2 ± 9.6 (−31.8%)\nCombat Zombie\n47.3 ± 10.6\n39.9 ± 25.3(−15.4%)\n50.2 ± 7.5\n30.8 ± 14.4(−38.6%)\nharvesting resource from animals), Mob-Combat (Programmatic, ﬁght 4 types of hostile monsters),\nand Creative (4 tasks).\nIn the experiments, we empirically check the quality of MINECLIP against manually written reward\nfunctions, and quantify how different variants of our learned model affect the RL performance. Table 1\npresents our main results, and Fig. 2 visualizes our learned agent behavior in 4 of the considered tasks.\nPolicy networks of all methods share the same architecture and are trained by PPO + Self-Imitation\n(Sec. 4.2, training details in the Appendix, Sec. F). We compare the following methods:\n• Ours (Attn): our agent trained with the MINECLIP[attn] reward model. For Program-\nmatic tasks, we also add the ﬁnal success condition as a binary reward. For Creative tasks,\nMINECLIP is the only source of reward.\n• Ours (Avg): the average-pooling variant of our method.\n• Manual Reward: hand-engineered dense reward using ground-truth simulator states.\n• Sparse-only: the ﬁnal binary success as a single sparse reward. Note that neither sparse-only\nnor manual reward is available for Creative tasks.\n• CLIPOpenAI: pre-trained OpenAI CLIP model that has not been ﬁnetuned on any MINEDOJO\nvideos.\nMINECLIP is competitive with manual reward.\nFor Programmatic tasks (ﬁrst 8 rows), RL\nagents guided by MINECLIP achieve competitive performance as those trained by manual reward.\nIn three of the tasks, they even outperform the hand-engineered reward functions, which rely on privi-\nleged simulator states unavailable to MINECLIP. For a more statistically sound analysis, we conduct\nthe Paired Student’s t-test to compare the mean success rate of each task (pairing column 3 “Ours\n(Attn)” and column 5 “Manual Reward” in Table 1). The test yields p-value 0.3991 ≫0.05, which\nindicates that the difference between our method and manual reward is not considered statistically\nsigniﬁcant, and therefore they are comparable with each other. Because all tasks require nontrivial ex-\nploration, our approach also dominates the Sparse-only baseline. Note that the original OpenAI CLIP\nmodel fails to achieve any success. We hypothesize that the creatures in Minecraft look dramatically\ndifferent from their real-world counterparts, which causes CLIP to produce misleading signals worse\nthan no shaping reward at all. It implies the importance of ﬁnetuning on MINEDOJO’s YouTube data.\nMINECLIP provides automated evaluation.\nFor Creative tasks (last 4 rows), there are no\nprogrammatic success criteria available. We convert MINECLIP into a binary success classiﬁer\nby thresholding the reward value it outputs for an episode. To test the quality of MINECLIP as\nan automatic evaluation metric, we ask human judges to curate a dataset of 100 successful and\n100 failed trajectories for each task. We then run both MINECLIP variants and CLIPOpenAI on the\ndataset and report the binary F1 score of their judgement against human ground-truth in Table 2.\nThe results demonstrate that both MINECLIP[attn] and MINECLIP[avg] attain a very high degree of\nagreement with human evaluation results on this subset of the Creative task suite that we investigate.\nCLIPOpenAI baseline also achieves nontrivial agreement on Find Ocean and Dig Hole tasks, likely\nbecause real-world oceans and holes have similar texture. We use the attn variant as an automated\nsuccess criterion to score the 4 Creative task results in Table 1. Our proposed method consistently\nlearns better than CLIPOpenAI-guided agents. It shows that MINECLIP is an effective approach to\nsolving open-ended tasks when no straightforward reward signal is available. We provide further\nanalysis beyond these 4 tasks in the Appendix (Sec. G.4).\n9\nTable 4: We train a single multi-task agent for all 12 tasks. All numbers represent percentage success\nrates averaged over 3 seeds, each tested for 200 episodes.\nGroup\nTasks\nSingle Agent on All Tasks\nOriginal\nPerformance Change\nMilk Cow\n91.5 ± 0.7\n64.5 ± 37.1\n↑\nHunt Cow\n46.8 ± 3.7\n83.5 ± 7.1\n↓\nShear Sheep\n73.5 ± 0.8\n12.1 ± 9.1\n↑\nHunt Sheep\n27.0 ± 1.0\n8.1 ± 4.1\n↑\nCombat Spider\n72.1 ± 1.3\n80.5 ± 13.0\n↓\nCombat Zombie\n27.1 ± 2.7\n47.3 ± 10.6\n↓\nCombat Pigman\n6.5 ± 1.2\n1.6 ± 2.3\n↑\nCombat Enderman\n0.0 ± 0.0\n0.0 ± 0.0\n=\nFind Nether Portal\n99.1 ± 0.4\n37.4 ± 40.8\n↑\nFind Ocean\n95.1 ± 1.5\n33.4 ± 45.6\n↑\nDig Hole\n85.8 ± 1.2\n91.6 ± 5.9\n↓\nLay Carpet\n96.5 ± 0.8\n97.6 ± 1.9\n=\nTable 5: We test the open-vocabulary generalization ability to two unseen tasks. All numbers represent\npercentage success rates averaged over 3 seeds, each tested for 200 episodes.\nTasks\nOurs (zero-shot)\nOurs (after RL ﬁnetune)\nBaseline (RL from scratch)\nHunt Pig\n1.3 ± 0.6\n46.0 ± 15.3\n0.0 ± 0.0\nHarvest Spider String\n1.6 ± 1.3\n36.5 ± 16.9\n12.5 ± 12.7\nMINECLIP shows good zero-shot generalization to signiﬁcant visual distribution shift.\nWe\nevaluate the learned policy without ﬁnetuning on a combination of unseen weather, lighting\nconditions, and terrains — 27 scenarios in total. For the baseline, we train agents with the original\nCLIPOpenAI image encoder (not trained on our YouTube videos) by imitation learning. The robustness\nagainst visual shift can be quantitatively measured by the relative performance degradation on\nnovel test scenarios for each task. Table 3 shows that while all methods incur performance drops,\nagents with MINECLIP encoder is more robust to visual changes than the baseline across all\ntasks. Pre-training on diverse in-the-wild YouTube videos is important to boosting zero-shot visual\ngeneralization capability, a ﬁnding consistent with literature [92, 82].\nLearning a Single Agent for All 12 Tasks\nWe have also trained a single agent for all 12 tasks.\nTo reduce the computational burden without loss of generality, the agent is trained by self-imitating\nfrom successful trajectories generated from the self-imitation learning pipeline (Section F.3). We\nsummarize the result in Table 4. Similar to our main experiments, all numbers represent percentage\nsuccess rates averaged over 3 training seeds, each tested for 200 episodes. Compared to the original\nagents, the new 12-multitask agent sees a performance boost in 6 tasks, degradation in 4 tasks,\nand roughly the same success rates in 2 tasks. This result suggests that there are both positive and\nnegative task transfers happening. To improve the multi-task performance, more advanced algorithms\n[141, 133] can be employed to mitigate the negative transfer effects.\nWe also perform Paired Student’s t-test to statistically compare the performance of the 12-multitask\nagent and those separately trained on each task group. We obtain a p-value of 0.3720 ≫0.05, which\nsuggests that the difference between the two training settings is not statistically signiﬁcant.\nGeneralize to Novel Tasks\nTo test the ability to generalize to new open-vocabulary commands,\nwe evaluate the agent on two novel tasks: “harvest spider string” and “hunt pig”. Table 5 shows\nthat the agent struggles in the zero-shot setting because it has not interacted with pigs or spider\nstrings during training, and thus does not know how to interact with them effectively. However, the\nperformance improves substantially by ﬁnetuning with the MINECLIP reward. Here the baseline\nmethods are trained from scratch using RL with the MINECLIP encoders and reward. Therefore,\nthe only difference is whether the policy has been pre-trained on the 12 tasks or not. Given the\n10\nsame environment sampling budget (only around 5% of total samples), it signiﬁcantly outperforms\nbaselines. It suggests that the multitask agent has learned transferable knowledge on hunting and\nresource collection, which enables it to quickly adapt to novel tasks.\n6\nRelated work\nOpen-ended Environments for Decision-making Agents.\nThere are many environments\ndeveloped with the goal of open-ended agent learning. Prior works include maze-style worlds\n[121, 129, 61], purely text-based game [69], grid worlds [21, 16], browser\/GUI-based environments\n[108, 124], and indoor simulators for robotics [1, 107, 114, 34, 110, 99, 89]. Minecraft offers\nan exciting alternative for open-ended agent learning. It is a 3D visual world with procedurally\ngenerated landscapes and extremely ﬂexible game mechanics that support an enormous variety\nof activities. Prior methods in open-ended agent learning [30, 57, 130, 63, 26] do not make use of\nexternal knowledge, but our approach leverages internet-scale database to learn open-vocabulary\nreward models, thanks to Minecraft’s abundance of gameplay data online.\nMinecraft for AI Research.\nThe Malmo platform [60] is the ﬁrst comprehensive release of a\nGym-style agent API [14] for Minecraft. Based on Malmo, MineRL [48] provides a codebase and\nhuman play trajectories for the annual Diamond Challenge at NeurIPS [47, 49, 62]. MINEDOJO’s\nsimulator builds upon the pioneering work of MineRL, but greatly expands the API and benchmarking\ntask suite. Other Minecraft benchmarks exist with different focuses. For example, CraftAssist [44]\nand IGLU [66] study agents with interactive dialogues. BASALT [104] applies human evaluation to 4\nopen-ended tasks. EvoCraft [45] is designed for structure building, and Crafter [50] optimizes for fast\nexperimentation. Unlike prior works, MINEDOJO’s core mission is to facilitate the development of\ngenerally capable embodied agents using internet-scale knowledge. We include a feature comparison\ntable of different Minecraft platforms for AI research in Table A.1.\nInternet-scale Multimodal Knowledge Bases.\nBig dataset such as Common Crawl [24], the Pile\n[37], LAION [100], YouTube-8M [2] and HowTo100M [77] have been fueling the success of large pre-\ntrained language models [27, 91, 15] and multimodal models [118, 6, 78, 145, 7, 4, 136]. While gener-\nally useful for learning representations, these datasets are not speciﬁcally targeted at embodied agents.\nTo provide agent-centric training data, RoboNet [25] collects video frames from 7 robot platforms,\nand Ego4D [43] recruits volunteers to record egocentric videos of household activities. In comparison,\nMINEDOJO’s knowledge base is constructed without human curation efforts, much larger in volume,\nmore diverse in data modalities, and comprehensively covers all aspects of the Minecraft environment.\nEmbodied Agents with Large-scale Pre-training.\nInspired by the success in NLP, embodied\nagent research [29, 11, 94, 23] has seen a surge in adoption of the large-scale pre-training paradigm.\nThe recent advances can be roughly divided into 4 categories.\n1) Novel agent architecture:\nDecision Transformer [19, 58, 144] applies the powerful self-attention models to sequential decision\nmaking. GATO [95] and Uniﬁed-IO [74] learn a single model to solve various decision-making\ntasks with different control interfaces. VIMA [59] uniﬁes a wide range of robot manipulation\ntasks with multimodal prompting. 2) Pre-training for better representations: R3M [82] trains a\ngeneral-purpose visual encoder for robot perception on Ego4D videos [43]. CLIPort [111] leverages\nthe pre-trained CLIP model [92] to enable free-form language instructions for robot manipulation.\n3) Pre-training for better policies: AlphaStar [126] achieves champion-level performance on\nStarCraft by imitating from numerous human demos. SayCan [3] leverages large language models\n(LMs) to ground value functions in the physical world. [72] and [96] directly reuse pre-trained\nLMs as policy backbone. VPT [10] is a concurrent work that learns an inverse dynamics model from\nhuman contractors to pseudo-label YouTube videos for behavior cloning. VPT is complementary to\nour approach, and can be ﬁnetuned to solve language-conditioned open-ended tasks with our learned\nreward model.\n4) Data-driven reward functions: Concept2Robot [105] and DVD [18] learn a\nbinary classiﬁer to score behaviors from in-the-wild videos [42]. LOReL [81] crowd-sources humans\nlabels to train language-conditioned reward function for ofﬂine RL. AVID [113] and XIRL [142]\nextract reward signals via cycle consistency. MINEDOJO’s task benchmark and internet knowledge\nbase are generally useful for developing new algorithms in all the above categories. In Sec. 4, we\nalso propose an open-vocabulary, multi-task reward model using MINEDOJO YouTube videos.\n11\n7\nConclusion\nIn this work, we introduce the MINEDOJO framework for developing generally capable embodied\nagents. MINEDOJO features a benchmarking suite of thousands of Programmatic and Creative tasks,\nand an internet-scale multimodal knowledge base of videos, wiki, and forum discussions. As an\nexample of the novel research possibilities enabled by MINEDOJO, we propose MINECLIP as an\neffective language-conditioned reward function trained with in-the-wild YouTube videos. MINECLIP\nachieves strong performance empirically and agrees well with human evaluation results, making it a\ngood automatic metric for Creative tasks. We look forward to seeing how MINEDOJO empowers the\ncommunity to make progress on the important challenge of open-ended agent learning.\n8\nAcknowledgement\nWe are extremely grateful to Anssi Kanervisto, Shikun Liu, Zhiding Yu, Chaowei Xiao, Weili Nie,\nJean Kossaiﬁ, Jonathan Raiman, Neel Kant, Saad Godil, Jaakko Haapasalo, Bryan Catanzaro, John\nSpitzer, Zhiyuan “Jerry” Lin, Yingqi Zheng, Chen Tessler, Dieter Fox, Oli Wright, Jeff Clune, Jack\nParker-Holder, and many other colleagues and friends for their helpful feedback and insightful\ndiscussions. We also thank the anonymous reviewers for offering us highly constructive advice\nand kind encouragement during the review and rebuttal period. NVIDIA provides the necessary\ncomputing resource and infrastructure for this project. Guanzhi Wang is supported by the Kortschak\nfellowship in Computing and Mathematical Sciences at Caltech.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | MineDojo：构建具有互联网规模知识的开放式具身智能体\n\n## 📌 背景痛点\/本文动机\n传统的自主智能体在特定领域（如Atari游戏和围棋）取得了巨大进步，但它们通常在孤立的环境中学习，目标有限且手动设计，因此无法在广泛的任务和能力之间进行泛化。相比之下，人类在开放世界中不断学习和适应，能够利用大量来自自身经验和他人的先验知识。本文旨在构建能够像人类一样在开放世界中学习和适应的通用智能体。\n\n## 🚀 核心方法\n💡 创新点1：开放式环境\nMineDojo基于流行的Minecraft游戏，提供了一个具有数千个多样化开放式任务的模拟套件。这些任务包括生存、采集、技术树和战斗等类别，涵盖了从简单到复杂的各种难度级别。\n\n💡 创新点2：互联网规模的多模态知识库\nMineDojo收集了大量的Minecraft相关数据，包括YouTube视频、Wiki页面和Reddit讨论。这些数据涵盖了游戏的所有方面，为智能体提供了丰富的先验知识。\n\n💡 创新点3：灵活可扩展的智能体架构\nMineDojo提出了一个基于Transformer预训练范式的智能体学习算法。该算法利用大规模预训练的视频语言模型作为学习奖励函数，能够解决各种开放式任务，而无需手动设计密集的奖励函数。\n\n## 📈 实验结果\nMineDojo的实验结果表明，其提出的MINECLIP奖励模型在程序性任务和创造性任务上都取得了良好的性能。与手动设计的密集奖励函数相比，MINECLIP在大多数任务上取得了竞争性的性能，并且在某些情况下甚至超过了它们。此外，MINECLIP还能够有效地评估创造性任务，其评估结果与人类判断高度一致。\n\n## 💬 可借鉴之处\nMineDojo为开发通用智能体提供了一个有价值的框架。其开放式环境、互联网规模的知识库和灵活可扩展的智能体架构为研究人员提供了探索开放式智能体学习的强大工具。此外，MineDojo的开源代码和数据集将促进社区对通用智能体研究的进一步发展。","llm_summary_res_status":200}
{"title":"Mastering Diverse Domains through World Models","authors":"Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap","summary":"Developing a general algorithm that learns to solve tasks across a wide range\nof applications has been a fundamental challenge in artificial intelligence.\nAlthough current reinforcement learning algorithms can be readily applied to\ntasks similar to what they have been developed for, configuring them for new\napplication domains requires significant human expertise and experimentation.\nWe present DreamerV3, a general algorithm that outperforms specialized methods\nacross over 150 diverse tasks, with a single configuration. Dreamer learns a\nmodel of the environment and improves its behavior by imagining future\nscenarios. Robustness techniques based on normalization, balancing, and\ntransformations enable stable learning across domains. Applied out of the box,\nDreamer is the first algorithm to collect diamonds in Minecraft from scratch\nwithout human data or curricula. This achievement has been posed as a\nsignificant challenge in artificial intelligence that requires exploring\nfarsighted strategies from pixels and sparse rewards in an open world. Our work\nallows solving challenging control problems without extensive experimentation,\nmaking reinforcement learning broadly applicable.","url":"http:\/\/arxiv.org\/abs\/2301.04104v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2301.04104v2","published":1673374336000,"comment":"Website: https:\/\/danijar.com\/dreamerv3","pdf_text":"Mastering Diverse Domains through World Models\nDanijar Hafner,12 Jurgis Pasukonis,1 Jimmy Ba,2 Timothy Lillicrap1\nAbstract\nDeveloping a general algorithm that learns to solve tasks across a wide range of\napplications has been a fundamental challenge in artificial intelligence. Although\ncurrent reinforcement learning algorithms can be readily applied to tasks similar to\nwhat they have been developed for, configuring them for new application domains\nrequires significant human expertise and experimentation. We present DreamerV3, a\ngeneral algorithm that outperforms specialized methods across over 150 diverse tasks,\nwith a single configuration. Dreamer learns a model of the environment and improves its\nbehavior by imagining future scenarios. Robustness techniques based on normalization,\nbalancing, and transformations enable stable learning across domains. Applied out of the\nbox, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without\nhuman data or curricula. This achievement has been posed as a significant challenge in\nartificial intelligence that requires exploring farsighted strategies from pixels and sparse\nrewards in an open world. Our work allows solving challenging control problems without\nextensive experimentation, making reinforcement learning broadly applicable.\n0\n300\n600\n900\nPPO\nRainbow\nMuZero\nDreamer\n57 tasks, 200M steps\nAtari\n10\n30\n50\n70\nPPO\nRainbow\nPPG\nDreamer\n16 tasks, 50M steps\nProcGen\n10\n30\n50\n70\nPPO\nR2D2+\n10x data\nIMPALA\n10x data\nDreamer\n30 tasks, 100M steps\nDMLab\n0\n3\n6\n9\nPPO\nRainbow\nIMPALA\nDreamer\n1 task, 100M steps\nMinecraft\n100K\n1M\n10M\n100M\nEnv steps\n0\n4\n8\n12\nReturn\nMinecraft Diamond\nMax\nMean\n10\n50\n90\n130\nPPO\nTWM\nIRIS\nDreamer\n26 tasks, 400K steps\nAtari100k\n0\n300\n600\n900\nPPO\nD4PG\nDMPO\nDreamer\n18 tasks, 500K steps\nProprio Control\n0\n300\n600\n900\nPPO\nCURL\nDrQ-v2\nDreamer\n20 tasks, 1M steps\nVisual Control\n10\n30\n50\n70\nPPO\nDQN\nBoot DQN\nDreamer\n23 tasks\nBSuite\na\nb\nTuned experts          \nUnified configuration\nFigure 1: Benchmark summary. a, Using fixed hyperparameters across all domains, Dreamer\noutperforms tuned expert algorithms across a wide range of benchmarks and data budgets. Dreamer\nalso substantially outperforms a high-quality implementation of the widely applicable PPO algorithm.\nb, Applied out of the box, Dreamer learns to obtain diamonds in the popular video game Minecraft\nfrom scratch given sparse rewards, a long-standing challenge in artificial intelligence for which\nprevious approaches required human data or domain-specific heuristics.\n1Google DeepMind. 2University of Toronto. Correspondence: mail@danijar.com\n1\narXiv:2301.04104v2  [cs.AI]  17 Apr 2024\n(a) Control Suite\n(b) Atari\n(c) ProcGen\n(d) DMLab\n(e) Minecraft\nFigure 2: Diverse visual domains used in the experiments. Dreamer succeeds across these domains,\nranging from robot locomotion and manipulation tasks over Atari games, procedurally generated\nProcGen levels, and DMLab tasks, that require spatial and temporal reasoning, to the complex and\ninfinite world of Minecraft. We also evaluate Dreamer on non-visual domains.\nIntroduction\nReinforcement learning has enabled computers to solve tasks through interaction, such as surpassing\nhumans in the games of Go and Dota1,2. It is also a key component for improving large language\nmodels beyond what is demonstrated in their pretraining data3,4. While PPO5 has become a standard\nalgorithm in the field of reinforcement learning, more specialized algorithms are often employed\nto achieve higher performance. These specialized algorithms target the unique challenges posed\nby different application domains, such as continuous control6, discrete actions7,8, sparse rewards9,\nimage inputs10, spatial environments11, and board games12. However, applying reinforcement\nlearning algorithms to sufficiently new tasks—such as moving from video games to robotics tasks—\nrequires substantial effort, expertise, and computational resources for tweaking the hyperparameters\nof the algorithm13. This brittleness poses a bottleneck in applying reinforcement learning to new\nproblems and also limits the applicability of reinforcement learning to computationally expensive\nmodels or tasks where tuning is prohibitive. Creating a general algorithm that learns to master new\ndomains without having to be reconfigured has been a central challenge in artificial intelligence and\nwould open up reinforcement learning to a wide range of practical applications.\nWe present Dreamer, a general algorithm that outperforms specialized expert algorithms across a\nwide range of domains while using fixed hyperparameters, making reinforcement learning readily\napplicable to new problems. The algorithm is based on the idea of learning a world model that equips\nthe agent with rich perception and the ability to imagine the future14,15,16. The world model predicts\nthe outcomes of potential actions, a critic neural network judges the value of each outcome, and an\nactor neural network chooses actions to reach the best outcomes. Although intuitively appealing,\nrobustly learning and leveraging world models to achieve strong task performance has been an open\nproblem17. Dreamer overcomes this challenge through a range of robustness techniques based on\nnormalization, balancing, and transformations. We observe robust learning not only across over 150\ntasks from the domains summarized in Figure 2, but also across model sizes and training budgets,\noffering a predictable way to increase performance. Notably, larger model sizes not only achieve\nhigher scores but also require less interaction to solve a task.\nTo push the boundaries of reinforcement learning, we consider the popular video game Minecraft\nthat has become a focal point of research in recent years18,19,20, with international competitions held\nfor developing algorithms that autonomously learn to collect diamonds in Minecraft*. Solving this\n*The MineRL Diamond Competitions were held in 2019, 2020, and 2021 and provided a dataset of human expert\ntrajectories: https:\/\/minerl.io\/diamond. Competitions in the following years focused on a wide range of tasks.\n2\nx1\nx2\nx3\nx̂ 1\nx̂ 2\nx̂ 3\na1\na2\nz1\nz2\nz3\nh3\nh2\nh1\nenc\nenc\nenc\ndec\ndec\ndec\n(a) World Model Learning\nh3\nh2\nh1\na1\na2\nv1\nv2\nr2\nv2\nr2\nx1\nz1\nz2\nz3\nenc\n(b) Actor Critic Learning\nFigure 3: Training process of Dreamer. The world model encodes sensory inputs into discrete\nrepresentations zt that are predicted by a sequence model with recurrent state ht given actions at.\nThe inputs are reconstructed to shape the representations. The actor and critic predict actions at and\nvalues vt and learn from trajectories of abstract representations predicted by the world model.\nproblem without human data has been widely recognized as a substantial challenge for artificial\nintelligence because of the sparse rewards, exploration difficulty, long time horizons, and the\nprocedural diversity of this open world game18. Due to these obstacles, previous approaches resorted\nto using human expert data and domain-specific curricula19,20. Applied out of the box, Dreamer is\nthe first algorithm to collect diamonds in Minecraft from scratch.\nLearning algorithm\nWe present the third generation of the Dreamer algorithm21,22. The algorithm consists of three neural\nnetworks: the world model predicts the outcomes of potential actions, the critic judges the value of\neach outcome, and the actor chooses actions to reach the most valuable outcomes. The components\nare trained concurrently from replayed experience while the agent interacts with the environment. To\nsucceed across domains, all three components need to accommodate different signal magnitudes and\nrobustly balance terms in their objectives. This is challenging as we are not only targeting similar\ntasks within the same domain but aim to learn across diverse domains with fixed hyperparameters.\nThis section introduces the world model, critic, and actor along with their robust loss functions, as\nwell as tools for robustly predicting quantities of unknown orders of magnitude.\nWorld model learning\nThe world model learns compact representations of sensory inputs through autoencoding23 and en-\nables planning by predicting future representations and rewards for potential actions. We implement\nthe world model as a Recurrent State-Space Model (RSSM)24, shown in Figure 3. First, an encoder\nmaps sensory inputs xt to stochastic representations zt. Then, a sequence model with recurrent\nstate ht predicts the sequence of these representations given past actions at−1. The concatenation of\n3\nTrue\nContext Input\nOpen Loop Prediction\nModel\nTrue\nT = 0\nModel\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nFigure 4: Multi-step video predictions of a DMLab maze (top) and a quadrupedal robot (bottom).\nGiven 5 context images and the full action sequence, the model predicts 45 frames into the future\nwithout access to intermediate images. The world model learns an understanding of the underlying\nstructure of each environment.\nht and zt forms the model state from which we predict rewards rt and episode continuation flags\nct ∈{0, 1} and reconstruct the inputs to ensure informative representations:\nRSSM\n\n\n\n\n\nSequence model:\nht = fϕ(ht−1, zt−1, at−1)\nEncoder:\nzt ∼qϕ(zt | ht, xt)\nDynamics predictor:\nˆzt ∼pϕ(ˆzt | ht)\nReward predictor:\nˆrt ∼pϕ(ˆrt | ht, zt)\nContinue predictor:\nˆct ∼pϕ(ˆct | ht, zt)\nDecoder:\nˆxt ∼pϕ(ˆxt | ht, zt)\n(1)\nFigure 4 visualizes long-term video predictions of the world world. The encoder and decoder use\nconvolutional neural networks (CNN) for image inputs and multi-layer perceptrons (MLPs) for\nvector inputs. The dynamics, reward, and continue predictors are also MLPs. The representations\nare sampled from a vector of softmax distributions and we take straight-through gradients through\nthe sampling step25,22. Given a sequence batch of inputs x1:T, actions a1:T, rewards r1:T, and\ncontinuation flags c1:T, the world model parameters ϕ are optimized end-to-end to minimize the\nprediction loss Lpred, the dynamics loss Ldyn, and the representation loss Lrep with corresponding\nloss weights βpred = 1, βdyn = 1, and βrep = 0.1:\nL(ϕ) .= Eqϕ\nh PT\nt=1(βpredLpred(ϕ) + βdynLdyn(ϕ) + βrepLrep(ϕ))\ni\n.\n(2)\nThe prediction loss trains the decoder and reward predictor via the symlog squared loss described\nlater, and the continue predictor via logistic regression. The dynamics loss trains the sequence model\nto predict the next representation by minimizing the KL divergence between the predictor pϕ(zt | ht)\nand the next stochastic representation qϕ(zt | ht, xt). The representation loss, in turn, trains the\nrepresentations to become more predictable allowing us to use a factorized dynamics predictor for\nfast sampling during imagination training. The two losses differ in the stop-gradient operator sg(·)\nand their loss scale. To avoid a degenerate solution where the dynamics are trivial to predict but fail\n4\nto contain enough information about the inputs, we employ free bits26 by clipping the dynamics and\nrepresentation losses below the value of 1 nat ≈1.44 bits. This disables them while they are already\nminimized well to focus learning on the prediction loss:\nLpred(ϕ) .= −ln pϕ(xt | zt, ht) −ln pϕ(rt | zt, ht) −ln pϕ(ct | zt, ht)\nLdyn(ϕ) .= max\n\u00001, KL\n\u0002\nsg(qϕ(zt | ht, xt))\n\r\r\npϕ(zt | ht)\n\u0003\u0001\nLrep(ϕ) .= max\n\u00001, KL\n\u0002\nqϕ(zt | ht, xt)\n\r\r sg(pϕ(zt | ht))\n\u0003\u0001\n(3)\nPrevious world models require scaling the representation loss differently based on the visual\ncomplexity of the environment21. Complex 3D environments contain details unnecessary for\ncontrol and thus prompt a stronger regularizer to simplify the representations and make them\nmore predictable. In games with static backgrounds and where individual pixels may matter for\nthe task, a weak regularizer is required to extract fine details. We find that combining free bits\nwith a small representation loss resolves this dilemma, allowing for fixed hyperparameters across\ndomains. Moreover, we transform vector observations using the symlog function described later,\nto prevent large inputs and large reconstruction gradients, further stabilizing the trade-off with the\nrepresentation loss.\nWe occasionally observed spikes the in KL losses in earlier experiments, consistent with reports for\ndeep variational autoencoders27. To prevent this, we parameterize the categorical distributions of the\nencoder and dynamics predictor as mixtures of 1% uniform and 99% neural network output, making\nit impossible for them to become deterministic and thus ensuring well-behaved KL losses. Further\nmodel details and hyperparameters are included in the supplementary material.\nCritic learning\nThe actor and critic neural networks learn behaviors purely from abstract trajectories of representa-\ntions predicted by the world model14. For environment interaction, we select actions by sampling\nfrom the actor network without lookahead planning. The actor and critic operate on model states\nst .= {ht, zt} and thus benefit from the Markovian representations learned by the recurrent world\nmodel. The actor aims to maximize the return Rt .= P∞\nτ=0 γτrt+τ with a discount factor γ = 0.997\nfor each model state. To consider rewards beyond the prediction horizon T = 16, the critic learns to\napproximate the distribution of returns28 for each state under the current actor behavior:\nActor:\nat ∼πθ(at | st)\nCritic:\nvψ(Rt | st)\n(4)\nStarting from representations of replayed inputs, the world model and actor generate a trajectory of\nimagined model states s1:T, actions a1:T, rewards r1:T, and continuation flags c1:T. Because the critic\npredicts a distribution, we read out its predicted values vt .= E[vψ( · | st)] as the expectation of the\ndistribution. To estimate returns that consider rewards beyond the prediction horizon, we compute\nbootstrapped λ-returns29 that integrate the predicted rewards and the values. The critic learns to\npredict the distribution of the return estimates Rλ\nt using the maximum likelihood loss:\nL(ψ) .= −PT\nt=1 ln pψ(Rλ\nt | st)\nRλ\nt\n.= rt + γct\n\u0010\n(1 −λ)vt + λRλ\nt+1\n\u0011\nRλ\nT\n.= vT\n(5)\nWhile a simple choice would be to parameterize the critic as a Normal distribution, the return\ndistribution can have multiple modes and vary by orders of magnitude across environments. To\nstabilize and accelerate learning under these conditions, we parameterize the critic as categorical\ndistribution with exponentially spaced bins, decoupling the scale of gradients from the prediction\n5\ntargets as described later. To improve value prediction in environments where rewards are challenging\nto predict, we apply the critic loss both to imagined trajectories with loss scale βval = 1 and to\ntrajectories sampled from the replay buffer with loss scale βrepval = 0.3. The critic replay loss\nuses the imagination returns Rλ\nt at the start states of the imagination rollouts as on-policy value\nannotations for the replay trajectory to then compute λ-returns over the replay rewards.\nBecause the critic regresses targets that depend on its own predictions, we stabilize learning by\nregularizing the critic towards predicting the outputs of an exponentially moving average of its\nown parameters. This is similar to target networks used previously in reinforcement learning7 but\nallows us to compute returns using the current critic network. We further noticed that the randomly\ninitialized reward predictor and critic networks at the start of training can result in large predicted\nrewards that can delay the onset of learning. We thus initialize the output weight matrix of the\nreward predictor and critic to zeros, which alleviates the problem and accelerates early learning.\nActor learning\nThe actor learns to choose actions that maximize return while exploring through an entropy regular-\nizer30. However, the correct scale for this regularizer depends both on the scale and frequency of\nrewards in the environment. Ideally, we would like the agent to explore more if rewards are sparse\nand exploit more if rewards are dense or nearby. At the same time, the exploration amount should\nnot be influenced by arbitrary scaling of rewards in the environment. This requires normalizing the\nreturn scale while preserving information about reward frequency.\nTo use a fixed entropy scale of η = 3 × 10−4 across domains, we normalize returns to be approx-\nimately contained in the interval [0, 1]. In practice, substracting an offset from the returns does\nnot change the actor gradient and thus dividing by the range S is sufficient. Moreover, to avoid\namplifying noise from function approximation under sparse rewards, we only scale down large return\nmagnitudes but leave small returns below the threshold of L = 1 untouched. We use the Reinforce\nestimator31 for both discrete and continuous actions, resulting in the surrogate loss function:\nL(θ) .= −PT\nt=1 sg\n\u0010\u0000Rλ\nt −vψ(st)\n\u0001\n\/ max(1, S)\n\u0011\nlog πθ(at | st) + η H\n\u0002\nπθ(at\n\f\f st)\n\u0003\n(6)\nThe return distribution can be multi-modal and include outliers, especially for randomized environ-\nments where some episodes have higher achievable returns than others. Normalizing by the smallest\nand largest observed returns would then scale returns down too much and may cause suboptimal\nconvergence. To be robust to these outliers, we compute the range from the 5th to the 95th return\npercentile over the return batch and smooth out the estimate using an exponential moving average:\nS .= EMA\n\u0000Per(Rλ\nt , 95) −Per(Rλ\nt , 5), 0.99\n\u0001\n(7)\nPrevious work typically normalizes advantages5 rather than returns, which puts a fixed amount\nof emphasis on maximizing returns over entropy regardless of whether rewards are within reach.\nScaling up advantages when rewards are sparse can amplify noise that outweighs the entropy\nregularizer and stagnates exploration. Normalizing rewards or returns by standard deviation can\nfail under sparse rewards where their standard deviation is near zero, drastically amplifying rewards\nregardless of their size. Constrained optimization targets a fixed entropy on average across states32,33\nregardless of achievable returns, which is robust but explores slowly under sparse rewards and\nconverges lower under dense rewards. We did not find stable hyperparameters across domains\nfor these approaches. Return normalization with a denominator limit overcomes these challenges,\nexploring rapidly under sparse rewards and converging to high performance across diverse domains.\n6\nRobust predictions\nReconstructing inputs and predicting rewards and returns can be challenging because the scale of\nthese quantities can vary across domains. Predicting large targets using a squared loss can lead to\ndivergence whereas absolute and Huber losses7 stagnate learning. On the other hand, normalizing\ntargets based on running statistics5 introduces non-stationarity into the optimization. We suggest\nthe symlog squared error as a simple solution to this dilemma. For this, a neural network f(x, θ)\nwith inputs x and parameters θ learns to predict a transformed version of its targets y. To read out\npredictions ˆy of the network, we apply the inverse transformation:\nL(θ) .= 1\n2\n\u0000f(x, θ) −symlog(y)\n\u00012\nˆy .= symexp\n\u0000f(x, θ)\n\u0001\n(8)\nUsing the logarithm as transformation would not allow us to predict targets that take on negative\nvalues. Therefore, we choose a function from the bi-symmetric logarithmic family34 that we name\nsymlog as the transformation with the symexp function as its inverse:\nsymlog(x) .= sign(x) ln\n\u0000|x| + 1\n\u0001\nsymexp(x) .= sign(x)\n\u0000exp(|x|) −1\n\u0001\n(9)\nThe symlog function compresses the magnitudes of both large positive and negative values. Unlike\nthe logarithm, it is symmetric around the origin while preserving the input sign. This allows the\noptimization process to quickly move the network predictions to large values when needed. The\nsymlog function approximates the identity around the origin so that it does not affect learning of\ntargets that are already small enough.\nFor potentially stochastic targets, such as rewards or returns, we introduce the symexp twohot loss.\nHere, the network outputs the logits for a softmax distribution over exponentially spaced bins bi ∈B.\nPredictions are read out as the weighted average of the bin positions weighted by their predicted\nprobabilities. Importantly, the network can output any continuous value in the interval because the\nweighted average can fall between the buckets:\nˆy .= softmax(f(x))TB\nB .= symexp(\n\u0002\n−20\n...\n+20\n\u0003\n)\n(10)\nThe network is trained on twohot encoded targets8,28, a generalization of onehot encoding to\ncontinuous values. The twohot encoding of a scalar is a vector with |B| entries that are all 0 except\nat the indices k and k + 1 of the two bins closest to the encoded scalar. The two entries sum up\nto 1, with linearly higher weight given to the bin that is closer to the encoded continuous number.\nThe network is then trained to minimize the categorical cross entropy loss for classification with\nsoft targets. Note that the loss only depends on the probabilities assigned to the bins but not on the\ncontinuous values associated with the bin locations, decoupling the size of the gradients from the\nsize of the targets:\nL(θ) .= −twohot(y)T log softmax(f(x, θ))\n(11)\nApplying these principles, Dreamer transforms vector observations using the symlog functions, both\nfor the encoder inputs and the decoder targets and employs the synexp twohot loss for the reward\npredictor and critic. We find that these techniques enable robust and fast learning across many\ndiverse domains. For critic learning, an alternative asymmetric transformation has previously been\nproposed35, which we found less effective on average across domains. Unlike alternatives, symlog\ntransformations avoid truncating large targets7, introducing non-stationary from normalization5, or\nadjusting network weights when new extreme values are detected36.\n7\n0M\n50M\n100M\nEnv steps\n0\n25\n50\n75\n100\nAgents with\n   item (%)   \nIron Ingot\n0M\n50M\n100M\nEnv steps\n0\n25\n50\n75\n100\nIron Pickaxe\n0M\n50M\n100M\nEnv steps\n0\n25\n50\n75\n100\nDiamond\nDreamer\nIMPALA\nRainbow\nPPO\nFigure 5: Fraction of trained agents that discover each of the three latest items in the Minecraft\nDiamond task. Although previous algorithms progress up to the iron pickaxe, Dreamer is the only\ncompared algorithm that manages to discover a diamond, and does so reliably.\nResults\nWe evaluate the generality of Dreamer across 8 domains—with over 150 tasks—under fixed hyper-\nparameters. We designed the experiments to compare Dreamer to the best methods in the literature,\nwhich are often specifically designed and tuned for the benchmark at hand. We further compare to a\nhigh-quality implementation of PPO5, a standard reinforcement learning algorithm that is known for\nits robustness. We run PPO with fixed hyperparameters chosen to maximize performance across\ndomains and that reproduce strong published results of PPO on ProcGen37. To push the boundaries\nof reinforcement learning, we apply Dreamer to the challenging video game Minecraft, comparing\nit to strong previous algorithms. Finally, we analyze the importance of individual components of\nDreamer and its robustness to different model sizes and computational budgets. All Dreamer agents\nare trained on a single Nvidia A100 GPU each, making it reproducible for many research labs. A\npublic implementation of Dreamer that reproduces all results is available on the project website.\nBenchmarks We perform an extensive empirical study across 8 domains that include continuous\nand discrete actions, visual and low-dimensional inputs, dense and sparse rewards, different reward\nscales, 2D and 3D worlds, and procedural generation. Figure 1 summarizes the benchmark results,\nshowing that Dreamer outperforms a wide range of previous expert algorithms across diverse\ndomains. Crucially, Dreamer substantially outperforms PPO across all domains.\n• Atari\nThis established benchmark contains 57 Atari 2600 games with a budget of 200M frames,\nposing a diverse range of challenges38. We use the sticky action simulator setting39. Dreamer\noutperforms the powerful MuZero algorithm8 while using only a fraction of the computational\nresources. Dreamer also outperforms the widely-used expert algorithms Rainbow40 and IQN41.\n• ProcGen\nThis benchmark of 16 games features randomized levels and visual distractions to test\nthe robustness and generalization of agents42. Within the budget of 50M frames, Dreamer matches\nthe tuned expert algorithm PPG37 and outperforms Rainbow42,40. Our PPO agent with fixed\nhyperparameters matches the published score of the highly tuned official PPO implementation37.\n• DMLab\nThis suite of 30 tasks features 3D environments that test spatial and temporal reason-\ning43. In 100M frames, Dreamer exceeds the performance of the scalable IMPALA and R2D2+\nagents35 at 1B environment steps, amounting to a data-efficiency gain of over 1000%. We note\nthat these baselines were not designed for data-efficiency but serve as a valuable comparison point\nfor the performance previously achievable at scale.\n8\n0\n50\n100\nEnv steps (%)\n0\n50\n100\nReturn (%)\n14 task mean\nDreamer\nNo obs symlog\nNo retnorm (advnorm)\nNo symexp twohot (Huber)\nNo KL balance & free bits\nWithout all\n0\n20\n40\nEnv steps (106)\n0\n10\n20\nReturn\nCrafter\n400M\n200M\n100M\n50M\n25M\n12M\n0\n100\n200\nEnv steps (106)\n0\n250\n500\nDMLab Goals\n0\n50\n100\nEnv steps (%)\n0\n50\n100\nReturn (%)\n14 task mean\nDreamer\nNo reward & value grads\nNo reconstruction grads\n0\n10\n20\nEnv steps (106)\n0\n9\n18\nReturn\nCrafter\n64\n32\n16\n8\n4\n2\n1\n0\n100\n200\nEnv steps (106)\n0\n150\n300\n450\nDMLab Goals\nRobustness techniques\nLearning signals\nModel size scaling\nReplay scaling\na\nb\nc\nd\nFigure 6: Ablations and robust scaling of Dreamer. a, All individual robustness techniques\ncontribute to the performance of Dreamer on average, although each individual technique may only\naffect some tasks. Training curves of individual tasks are included in the supplementary material.\nb, The performance of Dreamer predominantly rests on the unsupervised reconstruction loss of its\nworld model, unlike most prior algorithms that rely predominantly on reward and value prediction\ngradients7,5,8. c, The performance of Dreamer increases monotonically with larger model sizes,\nranging from 12M to 400M parameters. Notably, larger models not only increase task performance\nbut also require less environment interaction. d, Higher replay ratios predictably increase the\nperformance of Dreamer. Together with model size, this allows practitioners to improve task\nperformance and data-efficiency by employing more computational resources.\n• Atari100k\nThis data-efficiency benchmark comntains 26 Atari games and a budget of only\n400K frames, amounting to 2 hours of game time17. EfficientZero44 holds the state-of-the-art by\ncombining online tree search, prioritized replay, and hyperparameter scheduling, but also resets\nlevels early to increase data diversity, making a comparison difficult. Without this complexity,\nDreamer outperforms the best remaining methods, including the transformer-based IRIS and\nTWM agents, the model-free SPR, and SimPLe45.\n• Proprio Control\nThis benchmark contains 18 control tasks with continuous actions, proprio-\nceptive vector inputs, and a budget of 500K environment steps46. The tasks range from classical\ncontrol over locomotion to robot manipulation tasks, featuring dense and sparse rewards. Dreamer\nsets a new state-of-the-art on this benchmark, outperforming D4PG, DMPO, and MPO33.\n• Visual Control\nThis benchmark consists of 20 continuous control tasks where the agent receives\nonly high-dimensional images as input and has a budget of 1M environment steps46. Dreamer\nestablishes a new state-of-the-art on this benchmark, outperforming DrQ-v2 and CURL47, which\nare specialized to visual environments and leverage data augmentation.\n9\n• BSuite\nThis benchmark includes 23 environments with a total of 468 configurations that\nare specifically designed to test credit assignment, robustness to reward scale and stochasticity,\nmemory, generalization, and exploration48. Dreamer establishes a new state-of-the-art on this\nbenchmark, outperforming Boot DQN and other methods49. Dreamer improves over previous\nalgorithms especially in the scale robustness category.\nMinecraft Collecting diamonds in the popular game Minecraft has been a long-standing challenge\nin artificial intelligence18,19,20. Every episode in this game is set in a unique randomly generated\nand infinite 3D world. Episodes last until the player dies or up to 36000 steps equaling 30 minutes,\nduring which the player needs to discover a sequence of 12 items from sparse rewards by foraging\nfor resources and crafting tools. It takes about 20 minutes for experienced human players to obtain\ndiamonds20. We follow the block breaking setting of prior work19 because the provided action space\nwould make it challenging for stochastic policies to keep a key pressed for a prolonged time.\nBecause of the training time in this complex domain, extensive tuning would be difficult for\nMinecraft. Instead, we apply Dreamer out of the box with its default hyperparameters. As shown in\nFigures 1 and 5, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without\nusing human data as was required by VPT20 or adaptive curricula19. All the Dreamer agents we\ntrained on Minecraft discover diamonds in 100M environment steps. While several strong baselines\nprogress to advanced items such as the iron pickaxe, none of them discovers a diamond.\nAblations In Figure 6, we ablate the robustness techniques and learning signals on a diverse set of\n14 tasks to understand their importance. The training curves of individual tasks are included in the\nsupplementary material. We observe that all robustness techniques contribute to performance, most\nnotably the KL objective of the world model, followed by return normalization and symexp twohot\nregression for reward and value prediction. In general, we find that each individual technique is\ncritical on a subset of tasks but may not affect performance on other tasks.\nTo investigate the effect of the world model, we ablate the learning signals of Dreamer by stopping\neither the task-specific reward and value prediction gradients or the task-agnostic reconstruction\ngradients from shaping its representations. Unlike previous reinforcement learning algorithms that\noften rely only on task-specific learning signals7,8, Dreamer rests predominantly on the unsupervised\nobjective of its world model. This finding could allow for future algorithm variants that leverage\npretraining on unsupervised data.\nScaling properties To investigate whether Dreamer can scale robustly, we train 6 model sizes\nranging from 12M to 400M parameters, as well as different replay ratios on Crafter50 and a DMLab\ntask43. The replay ratio affects the number of gradient updates performed by the agent. Figure 6\nshows robust learning with fixed hyperparameters across the compared model sizes and replay ratios.\nMoreover, increasing the model size directly translates to both higher task performance and a lower\ndata requirement. Increasing the number of gradient steps further reduces the interactions needed to\nlearn successful behaviors. The results show that Dreamer learns robustly across model sizes and\nreplay ratios and that its performance and provides a predictable way for increasing performance\ngiven computational resources.\n10\nPrevious work\nDeveloping general-purpose algorithms has long been a goal of reinforcement learning research.\nPPO5 is one of the most widely used algorithms and is relatively robust but requires large amounts\nof experience and often yields lower performance than specialized alternatives. SAC32 is a popular\nchoice for continuous control and leverages experience replay for data-efficiency, but in practice\nrequires tuning, especially for its entropy scale, and struggles under high-dimensional inputs51.\nMuZero8 plans using a value prediction model and has been applied to board games and Atari, but the\nauthors did not release an implementation and the algorithm contains several complex components,\nmaking it challenging to reproduce. Gato52 fits one large model to expert demonstrations of multiple\ntasks, but is only applicable when expert data is available. In comparison, Dreamer masters a\ndiverse range of environments with fixed hyperparameters, does not require expert data, and its\nimplementation is open source.\nMinecraft has been a focus of recent research. With MALMO53, Microsoft released a free version\nof the successful game for research purposes. MineRL18 offers several competition environments,\nwhich we rely on as the basis for our experiments. The MineRL competition supports agents in\nexploring and learning meaningful skills through a diverse human dataset18. Voyager obtains items\nat a similar depth in the technology tree as Dreamer using API calls to a language model but operates\non top of the MineFlayer bot scripting layer that was specifically engineered to the game and\nexposes high-level actions54. VPT20 trained an agent to play Minecraft through behavioral cloning\nbased on expert data of keyboard and mouse actions collected by contractors and finetuning using\nreinforcement learning to obtain diamonds using 720 GPUs for 9 days. In comparison, Dreamer\nuses the MineRL competition action space to autonomously learn to collect diamonds from sparse\nrewards using 1 GPU for 9 days, without human data.\nConclusion\nWe present the third generation of the Dreamer algorithm, a general reinforcement learning algorithm\nthat masters a wide range of domains with fixed hyperparameters. Dreamer excels not only across\nover 150 tasks but also learns robustly across varying data and compute budgets, moving reinforce-\nment learning toward a wide range of practical applications. Applied out of the box, Dreamer is\nthe first algorithm to collect diamonds in Minecraft from scratch, achieving a significant milestone\nin the field of artificial intelligence. As a high-performing algorithm that is based on a learned\nworld model, Dreamer paves the way for future research directions, including teaching agents world\nknowledge from internet videos and learning a single world model across domains to allow artificial\nagents to build up increasingly general knowledge and competency.\nAcknowledgements We thank Mohammad Norouzi, Jessy Lin, Abbas Abdolmaleki, John Schul-\nman, Adam Kosiorek, and Oleh Rybkin for insightful discussions. We thank Bobak Shahriari, Denis\nYarats, Karl Cobbe, and Hubert Soyer for sharing training curves of baseline algorithms. We thank\nDaniel Furrer, Andrew Chen, and Dakshesh Garambha for help with Google Cloud infrastructure.\n11","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Mastering Diverse Domains through World Models.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nMastering Diverse Domains through World Models\n```\n#### 2. 论文摘要\n```\nDeveloping a general algorithm that learns to solve tasks across a wide range\nof applications has been a fundamental challenge in artificial intelligence.\nAlthough current reinforcement learning algorithms can be readily applied to\ntasks similar to what they have been developed for, configuring them for new\napplication domains requires significant human expertise and experimentation.\nWe present DreamerV3, a general algorithm that outperforms specialized methods\nacross over 150 diverse tasks, with a single configuration. Dreamer learns a\nmodel of the environment and improves its behavior by imagining future\nscenarios. Robustness techniques based on normalization, balancing, and\ntransformations enable stable learning across domains. Applied out of the box,\nDreamer is the first algorithm to collect diamonds in Minecraft from scratch\nwithout human data or curricula. This achievement has been posed as a\nsignificant challenge in artificial intelligence that requires exploring\nfarsighted strategies from pixels and sparse rewards in an open world. Our work\nallows solving challenging control problems without extensive experimentation,\nmaking reinforcement learning broadly applicable.\n```\n\n#### 3. 论文全文\n```\nMastering Diverse Domains through World Models\nDanijar Hafner,12 Jurgis Pasukonis,1 Jimmy Ba,2 Timothy Lillicrap1\nAbstract\nDeveloping a general algorithm that learns to solve tasks across a wide range of\napplications has been a fundamental challenge in artificial intelligence. Although\ncurrent reinforcement learning algorithms can be readily applied to tasks similar to\nwhat they have been developed for, configuring them for new application domains\nrequires significant human expertise and experimentation. We present DreamerV3, a\ngeneral algorithm that outperforms specialized methods across over 150 diverse tasks,\nwith a single configuration. Dreamer learns a model of the environment and improves its\nbehavior by imagining future scenarios. Robustness techniques based on normalization,\nbalancing, and transformations enable stable learning across domains. Applied out of the\nbox, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without\nhuman data or curricula. This achievement has been posed as a significant challenge in\nartificial intelligence that requires exploring farsighted strategies from pixels and sparse\nrewards in an open world. Our work allows solving challenging control problems without\nextensive experimentation, making reinforcement learning broadly applicable.\n0\n300\n600\n900\nPPO\nRainbow\nMuZero\nDreamer\n57 tasks, 200M steps\nAtari\n10\n30\n50\n70\nPPO\nRainbow\nPPG\nDreamer\n16 tasks, 50M steps\nProcGen\n10\n30\n50\n70\nPPO\nR2D2+\n10x data\nIMPALA\n10x data\nDreamer\n30 tasks, 100M steps\nDMLab\n0\n3\n6\n9\nPPO\nRainbow\nIMPALA\nDreamer\n1 task, 100M steps\nMinecraft\n100K\n1M\n10M\n100M\nEnv steps\n0\n4\n8\n12\nReturn\nMinecraft Diamond\nMax\nMean\n10\n50\n90\n130\nPPO\nTWM\nIRIS\nDreamer\n26 tasks, 400K steps\nAtari100k\n0\n300\n600\n900\nPPO\nD4PG\nDMPO\nDreamer\n18 tasks, 500K steps\nProprio Control\n0\n300\n600\n900\nPPO\nCURL\nDrQ-v2\nDreamer\n20 tasks, 1M steps\nVisual Control\n10\n30\n50\n70\nPPO\nDQN\nBoot DQN\nDreamer\n23 tasks\nBSuite\na\nb\nTuned experts          \nUnified configuration\nFigure 1: Benchmark summary. a, Using fixed hyperparameters across all domains, Dreamer\noutperforms tuned expert algorithms across a wide range of benchmarks and data budgets. Dreamer\nalso substantially outperforms a high-quality implementation of the widely applicable PPO algorithm.\nb, Applied out of the box, Dreamer learns to obtain diamonds in the popular video game Minecraft\nfrom scratch given sparse rewards, a long-standing challenge in artificial intelligence for which\nprevious approaches required human data or domain-specific heuristics.\n1Google DeepMind. 2University of Toronto. Correspondence: mail@danijar.com\n1\narXiv:2301.04104v2  [cs.AI]  17 Apr 2024\n(a) Control Suite\n(b) Atari\n(c) ProcGen\n(d) DMLab\n(e) Minecraft\nFigure 2: Diverse visual domains used in the experiments. Dreamer succeeds across these domains,\nranging from robot locomotion and manipulation tasks over Atari games, procedurally generated\nProcGen levels, and DMLab tasks, that require spatial and temporal reasoning, to the complex and\ninfinite world of Minecraft. We also evaluate Dreamer on non-visual domains.\nIntroduction\nReinforcement learning has enabled computers to solve tasks through interaction, such as surpassing\nhumans in the games of Go and Dota1,2. It is also a key component for improving large language\nmodels beyond what is demonstrated in their pretraining data3,4. While PPO5 has become a standard\nalgorithm in the field of reinforcement learning, more specialized algorithms are often employed\nto achieve higher performance. These specialized algorithms target the unique challenges posed\nby different application domains, such as continuous control6, discrete actions7,8, sparse rewards9,\nimage inputs10, spatial environments11, and board games12. However, applying reinforcement\nlearning algorithms to sufficiently new tasks—such as moving from video games to robotics tasks—\nrequires substantial effort, expertise, and computational resources for tweaking the hyperparameters\nof the algorithm13. This brittleness poses a bottleneck in applying reinforcement learning to new\nproblems and also limits the applicability of reinforcement learning to computationally expensive\nmodels or tasks where tuning is prohibitive. Creating a general algorithm that learns to master new\ndomains without having to be reconfigured has been a central challenge in artificial intelligence and\nwould open up reinforcement learning to a wide range of practical applications.\nWe present Dreamer, a general algorithm that outperforms specialized expert algorithms across a\nwide range of domains while using fixed hyperparameters, making reinforcement learning readily\napplicable to new problems. The algorithm is based on the idea of learning a world model that equips\nthe agent with rich perception and the ability to imagine the future14,15,16. The world model predicts\nthe outcomes of potential actions, a critic neural network judges the value of each outcome, and an\nactor neural network chooses actions to reach the best outcomes. Although intuitively appealing,\nrobustly learning and leveraging world models to achieve strong task performance has been an open\nproblem17. Dreamer overcomes this challenge through a range of robustness techniques based on\nnormalization, balancing, and transformations. We observe robust learning not only across over 150\ntasks from the domains summarized in Figure 2, but also across model sizes and training budgets,\noffering a predictable way to increase performance. Notably, larger model sizes not only achieve\nhigher scores but also require less interaction to solve a task.\nTo push the boundaries of reinforcement learning, we consider the popular video game Minecraft\nthat has become a focal point of research in recent years18,19,20, with international competitions held\nfor developing algorithms that autonomously learn to collect diamonds in Minecraft*. Solving this\n*The MineRL Diamond Competitions were held in 2019, 2020, and 2021 and provided a dataset of human expert\ntrajectories: https:\/\/minerl.io\/diamond. Competitions in the following years focused on a wide range of tasks.\n2\nx1\nx2\nx3\nx̂ 1\nx̂ 2\nx̂ 3\na1\na2\nz1\nz2\nz3\nh3\nh2\nh1\nenc\nenc\nenc\ndec\ndec\ndec\n(a) World Model Learning\nh3\nh2\nh1\na1\na2\nv1\nv2\nr2\nv2\nr2\nx1\nz1\nz2\nz3\nenc\n(b) Actor Critic Learning\nFigure 3: Training process of Dreamer. The world model encodes sensory inputs into discrete\nrepresentations zt that are predicted by a sequence model with recurrent state ht given actions at.\nThe inputs are reconstructed to shape the representations. The actor and critic predict actions at and\nvalues vt and learn from trajectories of abstract representations predicted by the world model.\nproblem without human data has been widely recognized as a substantial challenge for artificial\nintelligence because of the sparse rewards, exploration difficulty, long time horizons, and the\nprocedural diversity of this open world game18. Due to these obstacles, previous approaches resorted\nto using human expert data and domain-specific curricula19,20. Applied out of the box, Dreamer is\nthe first algorithm to collect diamonds in Minecraft from scratch.\nLearning algorithm\nWe present the third generation of the Dreamer algorithm21,22. The algorithm consists of three neural\nnetworks: the world model predicts the outcomes of potential actions, the critic judges the value of\neach outcome, and the actor chooses actions to reach the most valuable outcomes. The components\nare trained concurrently from replayed experience while the agent interacts with the environment. To\nsucceed across domains, all three components need to accommodate different signal magnitudes and\nrobustly balance terms in their objectives. This is challenging as we are not only targeting similar\ntasks within the same domain but aim to learn across diverse domains with fixed hyperparameters.\nThis section introduces the world model, critic, and actor along with their robust loss functions, as\nwell as tools for robustly predicting quantities of unknown orders of magnitude.\nWorld model learning\nThe world model learns compact representations of sensory inputs through autoencoding23 and en-\nables planning by predicting future representations and rewards for potential actions. We implement\nthe world model as a Recurrent State-Space Model (RSSM)24, shown in Figure 3. First, an encoder\nmaps sensory inputs xt to stochastic representations zt. Then, a sequence model with recurrent\nstate ht predicts the sequence of these representations given past actions at−1. The concatenation of\n3\nTrue\nContext Input\nOpen Loop Prediction\nModel\nTrue\nT = 0\nModel\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nFigure 4: Multi-step video predictions of a DMLab maze (top) and a quadrupedal robot (bottom).\nGiven 5 context images and the full action sequence, the model predicts 45 frames into the future\nwithout access to intermediate images. The world model learns an understanding of the underlying\nstructure of each environment.\nht and zt forms the model state from which we predict rewards rt and episode continuation flags\nct ∈{0, 1} and reconstruct the inputs to ensure informative representations:\nRSSM\n\n\n\n\n\nSequence model:\nht = fϕ(ht−1, zt−1, at−1)\nEncoder:\nzt ∼qϕ(zt | ht, xt)\nDynamics predictor:\nˆzt ∼pϕ(ˆzt | ht)\nReward predictor:\nˆrt ∼pϕ(ˆrt | ht, zt)\nContinue predictor:\nˆct ∼pϕ(ˆct | ht, zt)\nDecoder:\nˆxt ∼pϕ(ˆxt | ht, zt)\n(1)\nFigure 4 visualizes long-term video predictions of the world world. The encoder and decoder use\nconvolutional neural networks (CNN) for image inputs and multi-layer perceptrons (MLPs) for\nvector inputs. The dynamics, reward, and continue predictors are also MLPs. The representations\nare sampled from a vector of softmax distributions and we take straight-through gradients through\nthe sampling step25,22. Given a sequence batch of inputs x1:T, actions a1:T, rewards r1:T, and\ncontinuation flags c1:T, the world model parameters ϕ are optimized end-to-end to minimize the\nprediction loss Lpred, the dynamics loss Ldyn, and the representation loss Lrep with corresponding\nloss weights βpred = 1, βdyn = 1, and βrep = 0.1:\nL(ϕ) .= Eqϕ\nh PT\nt=1(βpredLpred(ϕ) + βdynLdyn(ϕ) + βrepLrep(ϕ))\ni\n.\n(2)\nThe prediction loss trains the decoder and reward predictor via the symlog squared loss described\nlater, and the continue predictor via logistic regression. The dynamics loss trains the sequence model\nto predict the next representation by minimizing the KL divergence between the predictor pϕ(zt | ht)\nand the next stochastic representation qϕ(zt | ht, xt). The representation loss, in turn, trains the\nrepresentations to become more predictable allowing us to use a factorized dynamics predictor for\nfast sampling during imagination training. The two losses differ in the stop-gradient operator sg(·)\nand their loss scale. To avoid a degenerate solution where the dynamics are trivial to predict but fail\n4\nto contain enough information about the inputs, we employ free bits26 by clipping the dynamics and\nrepresentation losses below the value of 1 nat ≈1.44 bits. This disables them while they are already\nminimized well to focus learning on the prediction loss:\nLpred(ϕ) .= −ln pϕ(xt | zt, ht) −ln pϕ(rt | zt, ht) −ln pϕ(ct | zt, ht)\nLdyn(ϕ) .= max\n\u00001, KL\n\u0002\nsg(qϕ(zt | ht, xt))\n\r\r\npϕ(zt | ht)\n\u0003\u0001\nLrep(ϕ) .= max\n\u00001, KL\n\u0002\nqϕ(zt | ht, xt)\n\r\r sg(pϕ(zt | ht))\n\u0003\u0001\n(3)\nPrevious world models require scaling the representation loss differently based on the visual\ncomplexity of the environment21. Complex 3D environments contain details unnecessary for\ncontrol and thus prompt a stronger regularizer to simplify the representations and make them\nmore predictable. In games with static backgrounds and where individual pixels may matter for\nthe task, a weak regularizer is required to extract fine details. We find that combining free bits\nwith a small representation loss resolves this dilemma, allowing for fixed hyperparameters across\ndomains. Moreover, we transform vector observations using the symlog function described later,\nto prevent large inputs and large reconstruction gradients, further stabilizing the trade-off with the\nrepresentation loss.\nWe occasionally observed spikes the in KL losses in earlier experiments, consistent with reports for\ndeep variational autoencoders27. To prevent this, we parameterize the categorical distributions of the\nencoder and dynamics predictor as mixtures of 1% uniform and 99% neural network output, making\nit impossible for them to become deterministic and thus ensuring well-behaved KL losses. Further\nmodel details and hyperparameters are included in the supplementary material.\nCritic learning\nThe actor and critic neural networks learn behaviors purely from abstract trajectories of representa-\ntions predicted by the world model14. For environment interaction, we select actions by sampling\nfrom the actor network without lookahead planning. The actor and critic operate on model states\nst .= {ht, zt} and thus benefit from the Markovian representations learned by the recurrent world\nmodel. The actor aims to maximize the return Rt .= P∞\nτ=0 γτrt+τ with a discount factor γ = 0.997\nfor each model state. To consider rewards beyond the prediction horizon T = 16, the critic learns to\napproximate the distribution of returns28 for each state under the current actor behavior:\nActor:\nat ∼πθ(at | st)\nCritic:\nvψ(Rt | st)\n(4)\nStarting from representations of replayed inputs, the world model and actor generate a trajectory of\nimagined model states s1:T, actions a1:T, rewards r1:T, and continuation flags c1:T. Because the critic\npredicts a distribution, we read out its predicted values vt .= E[vψ( · | st)] as the expectation of the\ndistribution. To estimate returns that consider rewards beyond the prediction horizon, we compute\nbootstrapped λ-returns29 that integrate the predicted rewards and the values. The critic learns to\npredict the distribution of the return estimates Rλ\nt using the maximum likelihood loss:\nL(ψ) .= −PT\nt=1 ln pψ(Rλ\nt | st)\nRλ\nt\n.= rt + γct\n\u0010\n(1 −λ)vt + λRλ\nt+1\n\u0011\nRλ\nT\n.= vT\n(5)\nWhile a simple choice would be to parameterize the critic as a Normal distribution, the return\ndistribution can have multiple modes and vary by orders of magnitude across environments. To\nstabilize and accelerate learning under these conditions, we parameterize the critic as categorical\ndistribution with exponentially spaced bins, decoupling the scale of gradients from the prediction\n5\ntargets as described later. To improve value prediction in environments where rewards are challenging\nto predict, we apply the critic loss both to imagined trajectories with loss scale βval = 1 and to\ntrajectories sampled from the replay buffer with loss scale βrepval = 0.3. The critic replay loss\nuses the imagination returns Rλ\nt at the start states of the imagination rollouts as on-policy value\nannotations for the replay trajectory to then compute λ-returns over the replay rewards.\nBecause the critic regresses targets that depend on its own predictions, we stabilize learning by\nregularizing the critic towards predicting the outputs of an exponentially moving average of its\nown parameters. This is similar to target networks used previously in reinforcement learning7 but\nallows us to compute returns using the current critic network. We further noticed that the randomly\ninitialized reward predictor and critic networks at the start of training can result in large predicted\nrewards that can delay the onset of learning. We thus initialize the output weight matrix of the\nreward predictor and critic to zeros, which alleviates the problem and accelerates early learning.\nActor learning\nThe actor learns to choose actions that maximize return while exploring through an entropy regular-\nizer30. However, the correct scale for this regularizer depends both on the scale and frequency of\nrewards in the environment. Ideally, we would like the agent to explore more if rewards are sparse\nand exploit more if rewards are dense or nearby. At the same time, the exploration amount should\nnot be influenced by arbitrary scaling of rewards in the environment. This requires normalizing the\nreturn scale while preserving information about reward frequency.\nTo use a fixed entropy scale of η = 3 × 10−4 across domains, we normalize returns to be approx-\nimately contained in the interval [0, 1]. In practice, substracting an offset from the returns does\nnot change the actor gradient and thus dividing by the range S is sufficient. Moreover, to avoid\namplifying noise from function approximation under sparse rewards, we only scale down large return\nmagnitudes but leave small returns below the threshold of L = 1 untouched. We use the Reinforce\nestimator31 for both discrete and continuous actions, resulting in the surrogate loss function:\nL(θ) .= −PT\nt=1 sg\n\u0010\u0000Rλ\nt −vψ(st)\n\u0001\n\/ max(1, S)\n\u0011\nlog πθ(at | st) + η H\n\u0002\nπθ(at\n\f\f st)\n\u0003\n(6)\nThe return distribution can be multi-modal and include outliers, especially for randomized environ-\nments where some episodes have higher achievable returns than others. Normalizing by the smallest\nand largest observed returns would then scale returns down too much and may cause suboptimal\nconvergence. To be robust to these outliers, we compute the range from the 5th to the 95th return\npercentile over the return batch and smooth out the estimate using an exponential moving average:\nS .= EMA\n\u0000Per(Rλ\nt , 95) −Per(Rλ\nt , 5), 0.99\n\u0001\n(7)\nPrevious work typically normalizes advantages5 rather than returns, which puts a fixed amount\nof emphasis on maximizing returns over entropy regardless of whether rewards are within reach.\nScaling up advantages when rewards are sparse can amplify noise that outweighs the entropy\nregularizer and stagnates exploration. Normalizing rewards or returns by standard deviation can\nfail under sparse rewards where their standard deviation is near zero, drastically amplifying rewards\nregardless of their size. Constrained optimization targets a fixed entropy on average across states32,33\nregardless of achievable returns, which is robust but explores slowly under sparse rewards and\nconverges lower under dense rewards. We did not find stable hyperparameters across domains\nfor these approaches. Return normalization with a denominator limit overcomes these challenges,\nexploring rapidly under sparse rewards and converging to high performance across diverse domains.\n6\nRobust predictions\nReconstructing inputs and predicting rewards and returns can be challenging because the scale of\nthese quantities can vary across domains. Predicting large targets using a squared loss can lead to\ndivergence whereas absolute and Huber losses7 stagnate learning. On the other hand, normalizing\ntargets based on running statistics5 introduces non-stationarity into the optimization. We suggest\nthe symlog squared error as a simple solution to this dilemma. For this, a neural network f(x, θ)\nwith inputs x and parameters θ learns to predict a transformed version of its targets y. To read out\npredictions ˆy of the network, we apply the inverse transformation:\nL(θ) .= 1\n2\n\u0000f(x, θ) −symlog(y)\n\u00012\nˆy .= symexp\n\u0000f(x, θ)\n\u0001\n(8)\nUsing the logarithm as transformation would not allow us to predict targets that take on negative\nvalues. Therefore, we choose a function from the bi-symmetric logarithmic family34 that we name\nsymlog as the transformation with the symexp function as its inverse:\nsymlog(x) .= sign(x) ln\n\u0000|x| + 1\n\u0001\nsymexp(x) .= sign(x)\n\u0000exp(|x|) −1\n\u0001\n(9)\nThe symlog function compresses the magnitudes of both large positive and negative values. Unlike\nthe logarithm, it is symmetric around the origin while preserving the input sign. This allows the\noptimization process to quickly move the network predictions to large values when needed. The\nsymlog function approximates the identity around the origin so that it does not affect learning of\ntargets that are already small enough.\nFor potentially stochastic targets, such as rewards or returns, we introduce the symexp twohot loss.\nHere, the network outputs the logits for a softmax distribution over exponentially spaced bins bi ∈B.\nPredictions are read out as the weighted average of the bin positions weighted by their predicted\nprobabilities. Importantly, the network can output any continuous value in the interval because the\nweighted average can fall between the buckets:\nˆy .= softmax(f(x))TB\nB .= symexp(\n\u0002\n−20\n...\n+20\n\u0003\n)\n(10)\nThe network is trained on twohot encoded targets8,28, a generalization of onehot encoding to\ncontinuous values. The twohot encoding of a scalar is a vector with |B| entries that are all 0 except\nat the indices k and k + 1 of the two bins closest to the encoded scalar. The two entries sum up\nto 1, with linearly higher weight given to the bin that is closer to the encoded continuous number.\nThe network is then trained to minimize the categorical cross entropy loss for classification with\nsoft targets. Note that the loss only depends on the probabilities assigned to the bins but not on the\ncontinuous values associated with the bin locations, decoupling the size of the gradients from the\nsize of the targets:\nL(θ) .= −twohot(y)T log softmax(f(x, θ))\n(11)\nApplying these principles, Dreamer transforms vector observations using the symlog functions, both\nfor the encoder inputs and the decoder targets and employs the synexp twohot loss for the reward\npredictor and critic. We find that these techniques enable robust and fast learning across many\ndiverse domains. For critic learning, an alternative asymmetric transformation has previously been\nproposed35, which we found less effective on average across domains. Unlike alternatives, symlog\ntransformations avoid truncating large targets7, introducing non-stationary from normalization5, or\nadjusting network weights when new extreme values are detected36.\n7\n0M\n50M\n100M\nEnv steps\n0\n25\n50\n75\n100\nAgents with\n   item (%)   \nIron Ingot\n0M\n50M\n100M\nEnv steps\n0\n25\n50\n75\n100\nIron Pickaxe\n0M\n50M\n100M\nEnv steps\n0\n25\n50\n75\n100\nDiamond\nDreamer\nIMPALA\nRainbow\nPPO\nFigure 5: Fraction of trained agents that discover each of the three latest items in the Minecraft\nDiamond task. Although previous algorithms progress up to the iron pickaxe, Dreamer is the only\ncompared algorithm that manages to discover a diamond, and does so reliably.\nResults\nWe evaluate the generality of Dreamer across 8 domains—with over 150 tasks—under fixed hyper-\nparameters. We designed the experiments to compare Dreamer to the best methods in the literature,\nwhich are often specifically designed and tuned for the benchmark at hand. We further compare to a\nhigh-quality implementation of PPO5, a standard reinforcement learning algorithm that is known for\nits robustness. We run PPO with fixed hyperparameters chosen to maximize performance across\ndomains and that reproduce strong published results of PPO on ProcGen37. To push the boundaries\nof reinforcement learning, we apply Dreamer to the challenging video game Minecraft, comparing\nit to strong previous algorithms. Finally, we analyze the importance of individual components of\nDreamer and its robustness to different model sizes and computational budgets. All Dreamer agents\nare trained on a single Nvidia A100 GPU each, making it reproducible for many research labs. A\npublic implementation of Dreamer that reproduces all results is available on the project website.\nBenchmarks We perform an extensive empirical study across 8 domains that include continuous\nand discrete actions, visual and low-dimensional inputs, dense and sparse rewards, different reward\nscales, 2D and 3D worlds, and procedural generation. Figure 1 summarizes the benchmark results,\nshowing that Dreamer outperforms a wide range of previous expert algorithms across diverse\ndomains. Crucially, Dreamer substantially outperforms PPO across all domains.\n• Atari\nThis established benchmark contains 57 Atari 2600 games with a budget of 200M frames,\nposing a diverse range of challenges38. We use the sticky action simulator setting39. Dreamer\noutperforms the powerful MuZero algorithm8 while using only a fraction of the computational\nresources. Dreamer also outperforms the widely-used expert algorithms Rainbow40 and IQN41.\n• ProcGen\nThis benchmark of 16 games features randomized levels and visual distractions to test\nthe robustness and generalization of agents42. Within the budget of 50M frames, Dreamer matches\nthe tuned expert algorithm PPG37 and outperforms Rainbow42,40. Our PPO agent with fixed\nhyperparameters matches the published score of the highly tuned official PPO implementation37.\n• DMLab\nThis suite of 30 tasks features 3D environments that test spatial and temporal reason-\ning43. In 100M frames, Dreamer exceeds the performance of the scalable IMPALA and R2D2+\nagents35 at 1B environment steps, amounting to a data-efficiency gain of over 1000%. We note\nthat these baselines were not designed for data-efficiency but serve as a valuable comparison point\nfor the performance previously achievable at scale.\n8\n0\n50\n100\nEnv steps (%)\n0\n50\n100\nReturn (%)\n14 task mean\nDreamer\nNo obs symlog\nNo retnorm (advnorm)\nNo symexp twohot (Huber)\nNo KL balance & free bits\nWithout all\n0\n20\n40\nEnv steps (106)\n0\n10\n20\nReturn\nCrafter\n400M\n200M\n100M\n50M\n25M\n12M\n0\n100\n200\nEnv steps (106)\n0\n250\n500\nDMLab Goals\n0\n50\n100\nEnv steps (%)\n0\n50\n100\nReturn (%)\n14 task mean\nDreamer\nNo reward & value grads\nNo reconstruction grads\n0\n10\n20\nEnv steps (106)\n0\n9\n18\nReturn\nCrafter\n64\n32\n16\n8\n4\n2\n1\n0\n100\n200\nEnv steps (106)\n0\n150\n300\n450\nDMLab Goals\nRobustness techniques\nLearning signals\nModel size scaling\nReplay scaling\na\nb\nc\nd\nFigure 6: Ablations and robust scaling of Dreamer. a, All individual robustness techniques\ncontribute to the performance of Dreamer on average, although each individual technique may only\naffect some tasks. Training curves of individual tasks are included in the supplementary material.\nb, The performance of Dreamer predominantly rests on the unsupervised reconstruction loss of its\nworld model, unlike most prior algorithms that rely predominantly on reward and value prediction\ngradients7,5,8. c, The performance of Dreamer increases monotonically with larger model sizes,\nranging from 12M to 400M parameters. Notably, larger models not only increase task performance\nbut also require less environment interaction. d, Higher replay ratios predictably increase the\nperformance of Dreamer. Together with model size, this allows practitioners to improve task\nperformance and data-efficiency by employing more computational resources.\n• Atari100k\nThis data-efficiency benchmark comntains 26 Atari games and a budget of only\n400K frames, amounting to 2 hours of game time17. EfficientZero44 holds the state-of-the-art by\ncombining online tree search, prioritized replay, and hyperparameter scheduling, but also resets\nlevels early to increase data diversity, making a comparison difficult. Without this complexity,\nDreamer outperforms the best remaining methods, including the transformer-based IRIS and\nTWM agents, the model-free SPR, and SimPLe45.\n• Proprio Control\nThis benchmark contains 18 control tasks with continuous actions, proprio-\nceptive vector inputs, and a budget of 500K environment steps46. The tasks range from classical\ncontrol over locomotion to robot manipulation tasks, featuring dense and sparse rewards. Dreamer\nsets a new state-of-the-art on this benchmark, outperforming D4PG, DMPO, and MPO33.\n• Visual Control\nThis benchmark consists of 20 continuous control tasks where the agent receives\nonly high-dimensional images as input and has a budget of 1M environment steps46. Dreamer\nestablishes a new state-of-the-art on this benchmark, outperforming DrQ-v2 and CURL47, which\nare specialized to visual environments and leverage data augmentation.\n9\n• BSuite\nThis benchmark includes 23 environments with a total of 468 configurations that\nare specifically designed to test credit assignment, robustness to reward scale and stochasticity,\nmemory, generalization, and exploration48. Dreamer establishes a new state-of-the-art on this\nbenchmark, outperforming Boot DQN and other methods49. Dreamer improves over previous\nalgorithms especially in the scale robustness category.\nMinecraft Collecting diamonds in the popular game Minecraft has been a long-standing challenge\nin artificial intelligence18,19,20. Every episode in this game is set in a unique randomly generated\nand infinite 3D world. Episodes last until the player dies or up to 36000 steps equaling 30 minutes,\nduring which the player needs to discover a sequence of 12 items from sparse rewards by foraging\nfor resources and crafting tools. It takes about 20 minutes for experienced human players to obtain\ndiamonds20. We follow the block breaking setting of prior work19 because the provided action space\nwould make it challenging for stochastic policies to keep a key pressed for a prolonged time.\nBecause of the training time in this complex domain, extensive tuning would be difficult for\nMinecraft. Instead, we apply Dreamer out of the box with its default hyperparameters. As shown in\nFigures 1 and 5, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without\nusing human data as was required by VPT20 or adaptive curricula19. All the Dreamer agents we\ntrained on Minecraft discover diamonds in 100M environment steps. While several strong baselines\nprogress to advanced items such as the iron pickaxe, none of them discovers a diamond.\nAblations In Figure 6, we ablate the robustness techniques and learning signals on a diverse set of\n14 tasks to understand their importance. The training curves of individual tasks are included in the\nsupplementary material. We observe that all robustness techniques contribute to performance, most\nnotably the KL objective of the world model, followed by return normalization and symexp twohot\nregression for reward and value prediction. In general, we find that each individual technique is\ncritical on a subset of tasks but may not affect performance on other tasks.\nTo investigate the effect of the world model, we ablate the learning signals of Dreamer by stopping\neither the task-specific reward and value prediction gradients or the task-agnostic reconstruction\ngradients from shaping its representations. Unlike previous reinforcement learning algorithms that\noften rely only on task-specific learning signals7,8, Dreamer rests predominantly on the unsupervised\nobjective of its world model. This finding could allow for future algorithm variants that leverage\npretraining on unsupervised data.\nScaling properties To investigate whether Dreamer can scale robustly, we train 6 model sizes\nranging from 12M to 400M parameters, as well as different replay ratios on Crafter50 and a DMLab\ntask43. The replay ratio affects the number of gradient updates performed by the agent. Figure 6\nshows robust learning with fixed hyperparameters across the compared model sizes and replay ratios.\nMoreover, increasing the model size directly translates to both higher task performance and a lower\ndata requirement. Increasing the number of gradient steps further reduces the interactions needed to\nlearn successful behaviors. The results show that Dreamer learns robustly across model sizes and\nreplay ratios and that its performance and provides a predictable way for increasing performance\ngiven computational resources.\n10\nPrevious work\nDeveloping general-purpose algorithms has long been a goal of reinforcement learning research.\nPPO5 is one of the most widely used algorithms and is relatively robust but requires large amounts\nof experience and often yields lower performance than specialized alternatives. SAC32 is a popular\nchoice for continuous control and leverages experience replay for data-efficiency, but in practice\nrequires tuning, especially for its entropy scale, and struggles under high-dimensional inputs51.\nMuZero8 plans using a value prediction model and has been applied to board games and Atari, but the\nauthors did not release an implementation and the algorithm contains several complex components,\nmaking it challenging to reproduce. Gato52 fits one large model to expert demonstrations of multiple\ntasks, but is only applicable when expert data is available. In comparison, Dreamer masters a\ndiverse range of environments with fixed hyperparameters, does not require expert data, and its\nimplementation is open source.\nMinecraft has been a focus of recent research. With MALMO53, Microsoft released a free version\nof the successful game for research purposes. MineRL18 offers several competition environments,\nwhich we rely on as the basis for our experiments. The MineRL competition supports agents in\nexploring and learning meaningful skills through a diverse human dataset18. Voyager obtains items\nat a similar depth in the technology tree as Dreamer using API calls to a language model but operates\non top of the MineFlayer bot scripting layer that was specifically engineered to the game and\nexposes high-level actions54. VPT20 trained an agent to play Minecraft through behavioral cloning\nbased on expert data of keyboard and mouse actions collected by contractors and finetuning using\nreinforcement learning to obtain diamonds using 720 GPUs for 9 days. In comparison, Dreamer\nuses the MineRL competition action space to autonomously learn to collect diamonds from sparse\nrewards using 1 GPU for 9 days, without human data.\nConclusion\nWe present the third generation of the Dreamer algorithm, a general reinforcement learning algorithm\nthat masters a wide range of domains with fixed hyperparameters. Dreamer excels not only across\nover 150 tasks but also learns robustly across varying data and compute budgets, moving reinforce-\nment learning toward a wide range of practical applications. Applied out of the box, Dreamer is\nthe first algorithm to collect diamonds in Minecraft from scratch, achieving a significant milestone\nin the field of artificial intelligence. As a high-performing algorithm that is based on a learned\nworld model, Dreamer paves the way for future research directions, including teaching agents world\nknowledge from internet videos and learning a single world model across domains to allow artificial\nagents to build up increasingly general knowledge and competency.\nAcknowledgements We thank Mohammad Norouzi, Jessy Lin, Abbas Abdolmaleki, John Schul-\nman, Adam Kosiorek, and Oleh Rybkin for insightful discussions. We thank Bobak Shahriari, Denis\nYarats, Karl Cobbe, and Hubert Soyer for sharing training curves of baseline algorithms. We thank\nDaniel Furrer, Andrew Chen, and Dakshesh Garambha for help with Google Cloud infrastructure.\n11\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | DreamerV3：通过世界模型掌握多样化领域\n\n## 📌 背景痛点\/本文动机\n强化学习在解决特定任务方面取得了显著进展，但将算法应用于新领域通常需要大量的人工调整和实验。这限制了强化学习的通用性和实用性。本文提出了 DreamerV3，一个通用的强化学习算法，能够在多种不同的任务中表现出色，而无需针对每个任务进行重新配置。\n\n## 🚀 核心方法\n💡 创新点1：世界模型学习\nDreamerV3 通过学习一个世界模型来预测潜在行动的结果，从而让智能体能够想象未来的场景并做出更好的决策。世界模型使用循环状态空间模型（RSSM）来预测未来的状态和奖励，并通过重建输入来确保表示信息丰富。\n\n💡 创新点2：鲁棒性技术\nDreamerV3 采用了一系列鲁棒性技术，包括归一化、平衡和转换，以确保算法能够在不同的领域稳定学习。这些技术帮助 DreamerV3 在各种环境中表现出色，包括连续控制、离散动作、稀疏奖励、图像输入、空间环境和棋盘游戏等。\n\n## 📈 实验结果\nDreamerV3 在超过 150 个多样化的任务中表现出色，超过了专门为特定领域设计的算法。在 Atari、ProcGen、DMLab、Atari100k、Proprio Control、Visual Control 和 BSuite 等基准测试中，DreamerV3 都取得了最佳性能。此外，DreamerV3 还成功地在 Minecraft 游戏中收集钻石，这是人工智能领域的一个重大挑战。\n\n## 💬 可借鉴之处\nDreamerV3 的成功表明，通过学习世界模型和采用鲁棒性技术，可以开发出通用的强化学习算法，从而解决各种不同的任务。这项工作为未来研究提供了新的方向，包括从互联网视频中教授智能体世界知识，以及学习跨领域的单一世界模型，以使人工智能体能够积累更广泛的知识和能力。","llm_summary_res_status":200}
{"title":"STEVE-1: A Generative Model for Text-to-Behavior in Minecraft","authors":"Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, Sheila McIlraith","summary":"Constructing AI models that respond to text instructions is challenging,\nespecially for sequential decision-making tasks. This work introduces a\nmethodology, inspired by unCLIP, for instruction-tuning generative models of\nbehavior without relying on a large dataset of instruction-labeled\ntrajectories. Using this methodology, we create an instruction-tuned Video\nPretraining (VPT) model called STEVE-1, which can follow short-horizon\nopen-ended text and visual instructions in Minecraft. STEVE-1 is trained in two\nsteps: adapting the pretrained VPT model to follow commands in MineCLIP's\nlatent space, then training a prior to predict latent codes from text. This\nallows us to finetune VPT through self-supervised behavioral cloning and\nhindsight relabeling, reducing the need for costly human text annotations, and\nall for only $60 of compute. By leveraging pretrained models like VPT and\nMineCLIP and employing best practices from text-conditioned image generation,\nSTEVE-1 sets a new bar for open-ended instruction-following in Minecraft with\nlow-level controls (mouse and keyboard) and raw pixel inputs, far outperforming\nprevious baselines and robustly completing 12 of 13 tasks in our early-game\nevaluation suite. We provide experimental evidence highlighting key factors for\ndownstream performance, including pretraining, classifier-free guidance, and\ndata scaling. All resources, including our model weights, training scripts, and\nevaluation tools are made available for further research.","url":"http:\/\/arxiv.org\/abs\/2306.00937v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2306.00937v3","published":1685641181000,"comment":null,"pdf_text":"STEVE-1: A Generative Model for\nText-to-Behavior in Minecraft\nShalev Lifshitz1,2∗\nshalev.lifshitz@mail.utoronto.ca\nKeiran Paster1,2∗\nkeirp@cs.toronto.edu\nHarris Chan1,2†\nhchan@cs.toronto.edu\nJimmy Ba1,2\njba@cs.toronto.edu\nSheila McIlraith1,2\nsheila@cs.toronto.edu\n1Department of Computer Science, University of Toronto, Toronto, Canada.\n2Vector Institute for Artificial Intelligence, Toronto, Canada.\nAbstract\nConstructing AI models that respond to text instructions is challenging, especially\nfor sequential decision-making tasks. This work introduces a methodology, inspired\nby unCLIP, for instruction-tuning generative models of behavior without relying\non a large dataset of instruction-labeled trajectories. Using this methodology, we\ncreate an instruction-tuned Video Pretraining (VPT) model called STEVE-1, which\ncan follow short-horizon open-ended text and visual instructions in Minecraft™.\nSTEVE-1 is trained in two steps: adapting the pretrained VPT model to follow com-\nmands in MineCLIP’s latent space, then training a prior to predict latent codes from\ntext. This allows us to finetune VPT through self-supervised behavioral cloning and\nhindsight relabeling, reducing the need for costly human text annotations, and all\nfor only $60 of compute. By leveraging pretrained models like VPT and MineCLIP\nand employing best practices from text-conditioned image generation, STEVE-1\nsets a new bar for open-ended instruction-following in Minecraft with low-level\ncontrols (mouse and keyboard) and raw pixel inputs, far outperforming previous\nbaselines and robustly completing 12 of 13 tasks in our early-game evaluation\nsuite. We provide experimental evidence highlighting key factors for downstream\nperformance, including pretraining, classifier-free guidance, and data scaling. All\nresources, including our model weights, training scripts, and evaluation tools are\nmade available for further research.\n1\nIntroduction\nThe ability to use text instructions to control and interact with powerful AI models has made these\nmodels accessible and customizable for the masses. Such models include ChatGPT [41], which\ncan respond to messages written in natural language and perform a wide array of tasks, and Stable\nDiffusion [50], which turns natural language into an image. While those models cost anywhere from\nhundreds of thousands to hundreds of millions of dollars to train, there has been an equally exciting\ntrend whereby powerful open-source foundation models like LLaMA [59] can be finetuned with\nsurprisingly little compute and data to become instruction-following (e.g., [58, 13]).\nIn this paper, we study whether such an approach could be applicable to sequential decision-making\ndomains. Unlike in text and image domains, diverse data for sequential decision-making is very\nexpensive and often does not come with a convenient “instruction” label like captions for images. We\npropose to instruction-tune pretrained generative models of behavior, mirroring the advancements\nseen in recent instruction-tuned LLMs like Alpaca [58], and without relying on a large dataset of\ninstruction-labeled trajectories.\n∗Equal contribution.\n†Core contribution.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.00937v3  [cs.AI]  4 Feb 2024\nText\nEncoder\n“chop a  tree”\nCVAE\nVideo\nencoder\nCLIP   Objective\nGaussian Prior\nTransformer (VPT)\nMineCLIP \n(Frozen)\nlinear\nPrior\nPolicy\nResNet\nResNet\nResNet\nMouse\/Keyboard\n\/\/\nFigure 1: Like unCLIP [48], our approach involves two models. First, we train the policy by finetuning\nVPT to achieve goals given by pretrained MineCLIP [17] visual embeddings using our gameplay\ndataset. Second, for the prior model, we train a CVAE [54] to sample MineCLIP visual embeddings\ngiven a text prompt. The combination of these two models enables our agent to follow text and visual\ninstructions.\nIn the past year, two foundation models for the popular open-ended video game Minecraft™were\nreleased: a foundation model for behavior called VPT [5] and a model aligning text and video\nclips called MineCLIP [17]. This has opened up an intriguing avenue to explore fine-tuning for\ninstruction-following in the sequential decision-making domain of Minecraft. VPT was trained on\n70k hours of Minecraft gameplay, so the agent already has vast knowledge about the Minecraft\nenvironment. However, just as the massive potential of LLMs was unlocked by aligning them\nto follow instructions, it is likely that the VPT model has the potential for general, controllable\nbehavior if it is finetuned to follow instructions. In particular, our paper demonstrates a method for\nfine-tuning VPT to follow short-horizon text instructions with only $60 of compute and around 2,000\ninstruction-labeled trajectory segments.\nOur method draws inspiration from unCLIP [48], the approach used to create the popular text-to-\nimage model DALL•E 2. We decompose the problem of creating an instruction-following Minecraft\nagent into two models: a VPT model finetuned to achieve visual goals embedded in the MineCLIP\nlatent space, and a prior model that translates text instructions into MineCLIP visual embeddings. We\nfinetune VPT using behavioral cloning with self-supervised data generated with hindsight relabeling\n[3], avoiding the use of expensive text-instruction labels in favor of visual MineCLIP embeddings.\nWe apply unCLIP with classifier-free guidance [23] to create our agent called STEVE-1, which sets\na new bar for open-ended instruction-following in Minecraft with low-level controls (mouse and\nkeyboard) and raw pixel inputs, far outperforming the baseline set by Baker et al. [5].\nOur main contributions are as follows:\n• We introduce a methodology, inspired by unCLIP [48], for instruction-tuning generative\nmodels of behavior without relying on a large dataset of expensive instruction labels.\n• We apply this methodology to create STEVE-1, a Minecraft agent that can follow short-\nhorizon open-ended text and visual instructions with a high degree of accuracy, all for only\n$60 of compute. We perform extensive evaluations of our agent, showing that it can robustly\ncomplete 12 of 13 goal-conditioned control tasks in our early-game evaluation suite in\nMinecraft. For long-horizon tasks1 like crafting and building, we show that a basic version\nof prompt chaining can dramatically improve performance.\n• We provide experimental evidence highlighting key factors for downstream performance,\nincluding pretraining, classifier-free guidance, data scaling, prompt-engineering, and other\ndesign choices. In particular, we show that unCLIP [48] and classifier-free guidance [23]\ntranslate well to sequential decision-making and are essential for strong performance.\n• We release model weights for STEVE-1 as well as training scripts and evaluation code to\nhelp foster more research into instructable, open-ended sequential decision-making agents.2\n1Short-horizon tasks require few steps: e.g., go to a tree and chop it down, dig a hole. Long-horizon tasks\ntake many steps: e.g., craft complex recipes from scratch, build a house.\n2Model weights, training code, videos, and an interactive demo script are hosted on our project webpage at\nhttps:\/\/sites.google.com\/view\/steve-1.\n2\n2\nRelated Work\nMinecraft as a Test-bed for AI\nMinecraft has gained popularity as a benchmark for AI research\ndue to its complex and dynamic environment, making it a rich test-bed for reinforcement learning\nand other AI methods (e.g., [26, 19, 17, 21, 40, 62, 38, 9]). We leverage the MineRL environment\n[19] to research the creation of agents that can follow open-ended instructions in complex visual\nenvironments using only low-level actions (mouse and keyboard). We build STEVE-1 on top of\ntwo recent foundation models. In order to align text and videos, we use MineCLIP [17], a CLIP\n[47] model trained on paired web videos of Minecraft gameplay and associated captions. To train\nSTEVE-1’s policy, we fine-tune VPT [5], a foundation model of Minecraft behavior that is pretrained\non 70k hours of web videos of Minecraft along with estimated mouse and keyboard actions. Several\nprior works [61, 62] have explored the use of LLMs in creating instructable Minecraft agents. These\nworks typically use LLMs to make high-level plans that are then executed by lower-level RL [40, 62]\nor scripted [46] policies. Since STEVE-1 is a far more flexible low-level policy, the combination\nof STEVE-1 with LLMs is a promising direction for future work. Fan et al. [17] introduced an\nagent trained using RL with MineCLIP as a shaping reward on 12 different tasks and conditioned on\nMineCLIP-embedded text-prompts. However, this agent failed to generalize beyond the original set\nof tasks without further RL finetuning using the MineCLIP reward function. Cai et al. [9] proposed\na Goal-Sensitive Backbone architecture for goal-conditioned control in Minecraft which is trained\non a fixed set of goals, while STEVE-1 learns goal-reaching behavior from a large dataset in a\nself-supervised way without training on an explicit set of tasks.\nFoundation Models for Sequential Decision-Making\nFoundation models which are pretrained on\nvast amounts of data and then finetuned for specific tasks have recently shown great promise in a\nvariety of domains including language [8, 14, 59], vision [48, 10, 47], and robotics [7, 53, 25, 39, 65].\nGATO [49] and RT-1 [7] have demonstrated the potential of training transformers to perform both\nsimulated and real-world robotic tasks. With the exception of Kumar et al. [30], which uses Q-\nlearning, the vast majority of cases [32, 7, 49] where deep learning has been scaled to large, multitask\noffline-RL datasets have used supervised RL. Supervised RL (e.g., [42, 18, 12]) works by framing the\nsequential decision-making problem as a prediction problem, where the model is trained to predict the\nnext action conditioned on some future outcome. While these approaches are simple and scale well\nwith large amounts of compute and data, more work is needed to understand the trade-offs between\nsupervised RL and Q-learning or policy gradient-based methods [43, 44, 6, 55]. Recent works explore\nthe use of hindsight relabeling [3] using vision-language models [47, 2] to produce natural language\nrelabeling instructions. DIAL [65] finetunes CLIP [47] on human-labeled trajectories, which is\nthen used to select a hindsight instruction from a candidate set. Sumers et al. [56] uses Flamingo\n[2] zero-shot for hindsight relabeling by framing it as a visual-question answering (VQA) task. In\ncontrast, STEVE-1 relabels goals using future trajectory segment embeddings given by the MineCLIP\n[17] visual embedding.\nText-Conditioned Generative Models\nThere has been a recent explosion of interest in text-to-X\nmodels, including text-to-image (e.g., [48, 51, 50]), text-to-3D (e.g., [27, 35]), and even text-to-music\n(e.g., [1]). These models are typically either autoregressive transformers modeling sequences of\ndiscrete tokens [60, 8] or diffusion models [24]. Most related to our work is unCLIP, the method used\nfor DALL•E 2 [48]. unCLIP works by training a generative diffusion model to sample images from\nCLIP [47] embeddings of those images. By combining this model with a prior that translates text\nto visual CLIP embeddings, unCLIP can produce photorealistic images for arbitrary text prompts.\nunCLIP and many other diffusion-based approaches utilize a technique called classifier-free guidance\n[23], which lets the model trade-off between mode-coverage and sample fidelity post-training. We\nutilize the basic procedure of unCLIP and classifier-free guidance for training STEVE-1.\n3\nMethod\nInspired by the rapid recent progress in instruction-tuning Large Language Models (LLMs), we\nchoose to leverage the recently released Video Pretraining (VPT) [5] model as a starting point for\nour agent. Since VPT was trained on 70k hours of Minecraft gameplay, the agent already has vast\nknowledge about the Minecraft environment. However, just as the massive potential of LLMs was\nunlocked by aligning them to follow instructions, it is likely that the VPT model has the potential\n3\nz1\nz2\nz2\nz3\nz4\nz5\nz6\nz7\nz2\nz7\nz7\nz7\nz7\nz7\nFigure 2: To create goal-conditioned data for finetuning, we randomly select timesteps from episodes\nand use hindsight relabeling to set the intermediate goals for the trajectory segments to those visual\nMineCLIP embeddings. This self-supervised data teaches the agent which actions lead to which states.\nfor general, controllable behavior if it is finetuned to follow instructions. In this work, we present\na method for finetuning VPT to follow natural, open-ended textual and visual instructions, which\nopens the door for a wide range of uses for VPT in Minecraft.\nOur approach is inspired by unCLIP, the method behind the recent text-to-image generation model,\nDALL•E 2 [48]. Our goal is to create a generative model of behavior in Minecraft conditioned on\ntext instructions y. To do so, we utilize a dataset of Minecraft trajectory segments, some of which\ncontain instruction labels y: [(τ1, y1), (τ2, y2), . . . , (τn, ∅)] where τ is a trajectory of observations\nand actions. We also employ a pretrained CLIP model called MineCLIP [17], which generates aligned\nlatent variables zτt:t+16, zy, where zτt:t+16 is an embedding of any 16 consecutive timesteps from\nthe trajectory. MineCLIP is trained using a contrastive objective on pairs of Minecraft videos and\ntranscripts from the web. For simplicity of notation, we refer to the MineCLIP embedding of the\nlast 16 timesteps of a trajectory segment as zτgoal. Like unCLIP [48], we utilize a hierarchical model\nconsisting of a prior and a policy:\n• A prior p(zτgoal|y) that produces a latent variable zτgoal conditioned on a text instruction y.\n• A policy p(τ|zτgoal) that produces a trajectory conditioned on a latent variable zτgoal.\nThese two models can then be combined to produce a generative model of behaviors conditioned on\ntext instructions:\np(τ|y) = p(τ, zτgoal|y) = p(zτgoal|y)p(τ|zτgoal)\n(3.1)\n3.1\nPolicy\nTo learn our policy, we finetune VPT, a foundation model of Minecraft behaviors pθ(τ) trained\non 70k hours of Minecraft gameplay videos. Specifically, VPT consists of a ResNet [22] that\nprocesses frames of dimension 128 × 128 × 3, and a Transformer-XL [15] which processes the frame\nrepresentations and autoregressively predicts the next action using the joint hierarchical action space\ndescribed in Baker et al. [5]. In order to modify the architecture to condition on goal information, we\nadd an affine transformation of zτgoal to the output of the ResNet before passing it to the transformer:\nProcess Frames:\nResNetθ(ot) →xt\n[+ Conditioning on MineCLIP Embedding Goal]:\nxt →xt + Wθzτgoal + bθ\nPredict Actions:\nTransformerXLθ(xt, . . . , xt+T ) →at+T\nIn order to finetune VPT to condition on goals, we finetune the model using a method inspired by\nsupervised RL approaches like Decision Transformer [12], GLAMOR [42], and GCSL [18]. We use\na modification of hindsight relabeling which we call packed hindsight relabeling (see Figure 2) to\ngenerate a new dataset of trajectories with goals pulled from future states that periodically switch. In\ncontrast with hindsight relabeling, packed hindsight relabeling packs multiple relabeled sequences\ninto a single sequence. Specifically, our method to generate this dataset consists of two steps:\n1. Given a trajectory τ with T timesteps, randomly generate indices to select goals from:\ni1, i2, . . . , in. These indices are chosen by starting at the first timestep and repeatedly\nsampling a new timestep by adding a random value to the previous timestep. This ensures\nthat the data reflects that some goals may take longer to achieve than others.\n2. For each chosen goal at timestep ij, set the goals for timesteps ij−1 + 1, . . . , ij to be the\ngoal at timestep ij, denoted zτij .\n4\nOur final dataset Drelabeled consists of observation sequences (o1, . . . , oT ), action sequences\n(a1, . . . , aT ), and packed hindsight relabeled goals (z1, . . . , zT ). We then finetune VPT on this\ndataset using a supervised loss to predict each action autoregressively using a causal attention mask:\nLpolicy(θ) = EDrelabeled[−log pθ(at|o1...t, z1...t)]\n(3.2)\n3.2\nPrior\nIn order to condition not only on embeddings of visual goals but on latent goals, we need the prior, a\nmodel that produces a latent variable zτgoal conditioned on a text instruction y. Our model is a simple\nconditional variational autoencoder (CVAE) [54, 29] with a Gaussian prior and a Gaussian posterior.\nRather than learn to condition directly on text, we choose to condition on frozen text representations\nfrom MineCLIP zy. Thus, the prior learns a function to translate from a text embedding zy to a visual\nembedding zτgoal (see Appendix C.5 for further discussion). Both the encoder and decoder of our\nCVAE are parameterized as two-layer MLPs with 512 hidden units and layer normalization [4]. We\ntrain the model on our dataset, for which we have text labels Dlabels using the following loss:\nLprior(ϕ) = E(zτgoal,zy)∼Dlabels\nh\nKL(qϕ(zτgoal|zy)∥p(zτgoal)) −Ec∼qϕ(zτgoal|zy)\n\u0002\nlog pϕ(zτgoal|c, zy)\n\u0003i\n(3.3)\n3.3\nDatasets\nTo train our policy, we gather a gameplay dataset with 54M frames (≈1 month at 20FPS) of Minecraft\ngameplay along with associated actions from two sources: contractor gameplay and VPT-generated\ngameplay. To train our prior, we use a small dataset of text-video pairs gathered by humans and\naugmented using the OpenAI API gpt-3.5-turbo model [41] and MineCLIP. See Appendix D for\nmore detailed dataset information.\nOpenAI Contractor Dataset\nWe use 39M frames sourced from the contractor dataset which VPT\n[5] used to train its inverse dynamics model and finetune its policy. The dataset was gathered by\nhiring human contractors to play Minecraft and complete tasks such as house building or obtaining a\ndiamond pickaxe. During gameplay, keypresses and mouse movements are recorded. We use the\nsame preprocessing as VPT, including filtering out null actions.\nVPT-Generated Dataset\nWe generate an additional dataset of 15M frames by generating random\ntrajectories using the various pretrained VPT agents. The diversity of this dataset is improved by\nrandomly switching between models during trajectories [44], randomly resetting the agent’s memory,\nand randomly turning the agent to face a new direction.\nText-Video Pair Dataset\nTo train our prior model, which learns a mapping between text embed-\ndings and visual embeddings, we also manually gather a small dataset of 2,000 text instructions\npaired with 16-frame video segments (less than a second) from our gameplay dataset. This dataset\ncorresponds to less than 30 minutes of gameplay and takes just a few hours to collect. We aug-\nment this dataset by using the alignment between text and video embeddings from MineCLIP. For\neach text instruction, we find the top k most similar gameplay segments in our dataset and use the\ncorresponding 16-frame segment as additional training data. For augmentation, we also add 8,000\ntext-instructions generated by the OpenAI API gpt-3.5-turbo model [41], in addition to our 2,000\nhand-labeled instructions.\n3.4\nInference\nAt inference time, we use the prior to sample a latent goal zτgoal from the text instruction y. We then\nuse the policy to autoregressively sample actions at conditioned on the observation history o1...t\nand the latent goal zτgoal. Similar to the observation in Appendix I of Baker et al. [5], even with\nconditioning, the policy often fails to follow its instruction and simply acts according to its prior\nbehavior. To mitigate this, we borrow another trick used in image generation models: classifier-free\nguidance. Specifically, during inference we simultaneously compute logits for the policy conditioned\non the goal f(ot, . . . , ot+1, zτgoal) and for the unconditional policy f(ot, . . . , ot+1). We then compute\na combination of the two logits using a λ parameter to trade-off between the two:\n5\nlogits = (1 + λ) fθ(ot, . . . , ot+1, zτgoal)\n|\n{z\n}\nconditional logits\n−λ fθ(ot, . . . , ot+1)\n|\n{z\n}\nunconditional logits\n(3.4)\nBy setting a higher value of λ, we can encourage the policy to follow actions that are more likely when\nconditioned on the goal and, as demonstrated in Section 4.5, this significantly improves performance.\nAlso, in order to train the policy to generate these unconditional logits, we occasionally dropout the\ngoal embedding zτgoal from the policy’s input (with probability 0.1). This lets us generate both the\nconditional and unconditional logits using the same model with batch processing at inference time.\n3.5\nEvaluation\nEvaluating the performance of our agent is a challenging task due to the wide variety of instructions\nthat are possible and the difficulty of evaluating whether the agent has successfully achieved its\ntask. We use a combination of programmatic evaluation metrics and automatic MineCLIP evaluation\nmetrics to get a sense of the agent’s capability level. We collectively refer to all of our evaluation tasks\nincluding the 11 evaluation tasks from Figure 3 and the two prompt chaining tasks from Section 4.3\nas our early-game evaluation suite.\nProgrammatic Evaluation\nWe compute programmatic evaluation metrics by monitoring the\nMineRL [19] environment state throughout each evaluation episode. As done in VPT [5], we compute\nmultiple programmatic metrics including travel distance and early-game item collection. The travel\ndistance is the maximum displacement of the agent along on the horizontal (X-Z) plane, measured\nfrom the initial spawn point. For early-game inventory counts, we store the maximum number of log,\nseed, and dirt items seen in the agent’s inventory during the episode.\nMineCLIP Evaluation\nWe explore the use of text-visual alignment in MineCLIP latent space\nbetween trajectories and text or visual goals to evaluate our agent over a wider variety of tasks where\nprogrammatic evaluation isn’t practical. To determine the degree to which a task has been completed\nat all during an evaluation episode, we record the minimum cosine distance between the (text or\nvisual) goal embedding and the visual MineCLIP embedding at any timestep during an episode.\n4\nResults\nIn our experiments, we aim to answer the following questions:\n1. How well does STEVE-1 perform at achieving both text and visual goals in Minecraft?\n2. How does our method scale with more data?\n3. What choices are important for the performance of our method?\n4.1\nTraining Setup\nWe base our implementation off of the official VPT codebase3. The main STEVE-1 agent is trained\nusing Pytorch [45] distributed data parallel on four A40 GPUs for 160M frames, or just under three\nepochs of our gameplay dataset. Hyperparameters are selected to match those in Baker et al. [5] with\nthe exception of learning rate, which we set to 4e-5. Our models are optimized using AdamW [37].\nSee Table 1 for a full list of hyperparameters.\n4.2\nPerformance on Textual and Visual Goals\nDue to the hierarchical nature of our model, we can evaluate the performance of our agent at achieving\neither text or visual goals simply by choosing whether to use the prior to condition on text or bypass\nthe prior and condition on a MineCLIP video embedding directly. We first tested our model on a set\nof 11 tasks that are achievable within the first 2.5 minutes of gameplay and which do not require\nmultiple steps to complete (e.g., chop a tree or dig a hole, but not build a house). A complete list\nof the tasks and prompts we used for evaluation can be found in Table 3 in the appendix. To select\nvisual goals for testing each of the evaluation tasks, we implemented a tool that searches through\n3https:\/\/github.com\/openai\/Video-Pre-Training\n6\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0\n100\n200\n300\n400\nTravel Distance (Blocks)\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nSeeds Collected\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0\n5\n10\n15\n20\nWooden Logs Collected\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0\n20\n40\n60\nDirt Collected\n(a) Programmatic Evaluation\ndig\ndirt\nsky\nleaves\nwood\nseeds\nflower\nexplore\nswim\nunderwater\ninventory\nConditioned Prompt\ndig\ndirt\nsky\nleaves\nwood\nseeds\nflower\nexplore\nswim\nunderwater\ninventory\nEvaluation Prompt\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n(b) MineCLIP Evaluation\nFigure 3: Left: In our programmatic evaluations, STEVE-1 performed far better than the unconditional\nVPT agent early-game-2x and the text-conditioned VPT agent when prompted appropriately. The\nasterisk * in the “VPT (Text)*” indicates that this result was taken from Appendix I in [5], which\nhad twice the episode length compared to our setting. On some tasks, visual outperforms text-based\nprompting, creating a gap that can likely be bridged through better prompt engineering. Right:\nAcross our 11 MineCLIP evaluation tasks, STEVE-1 achieves the shortest distance between the\nepisode and the MineCLIP goal embedding when prompted appropriately except for in two cases,\nwhere it mixes up digging and dirt and swimming and going underwater. This shows the strong\ngeneral performance of STEVE-1 across a wide variety of short-horizon tasks. The dashed box marks\nthe minimum element along the row, and the diagonal number signifies the diagonal element’s rank\n(0 means it is the minimum row element). See Figure 14 for sample frames from each of the 11 visual\ngoals and Figure 13 for a success-rate version of this matrix.\n10% of our gameplay dataset by finding the closest 16-frame videos to a given text prompt. We then\nmanually selected a 16-frame video that clearly demonstrates the task being completed and use the\ncorresponding MineCLIP video embedding as the goal embedding for that task. Screenshots of these\nvisual goals can be found in Figure 14 in the appendix.\nIn Figure 3, we compare the performance of our text and visual-conditioned agents with the uncondi-\ntional VPT agent and text-conditioned VPT agent (from Appendix I in [5]) across our programmatic\ntasks. We find that when given the relevant text instruction, STEVE-1 collects 75× more dirt, 4.9×\nmore wood, 22× more seeds, and travels 4.3× farther than the unconditional agent, and STEVE-1\ncollects 3.3× more dirt, 4.4× more wood, 8.1× more seeds, and travels 2.2× farther than the text-\nconditioned VPT agent. This represents a significant improvement over the reported performance\nof text-conditioned VPT, which collects several times fewer resources despite having twice as long\nof an episode to do so. We also run an automatic evaluation using MineCLIP embedding distances\nby measuring the minimum distance of a goal embedding to any frame in the episode. As shown\nin Figure 3b, the distance between the goal and the episode is significantly lower when the agent is\nconditioned on the corresponding visual goal than otherwise. Full results for STEVE-1 with both text\nand visual goals can be found in Appendix F.\nIn addition to our evaluations of STEVE-1, we also recorded several sample interactive sessions we had\nwith the agent (controlling it in real-time by giving it written text instructions or specific visual goals).\nThese sessions demonstrate STEVE-1’s ability to responsively follow instructions in real-time in a\nvariety of situations. We believe that such use-cases, where humans give an agent natural instructions\nthat it can follow to complete tasks, will become increasingly important and have practical uses in the\ncreation of instructable assistants and virtual-world characters. These videos, as well as videos of our\nagent performing our evaluation tasks, can be found at https:\/\/sites.google.com\/view\/steve-1.\n4.3\nPrompt Chaining\nWe also experiment with longer horizon tasks that require multiple steps, such as crafting and building.\nWe explore two different prompting methods: directly prompting with the target goal, and a simple\nform of prompt chaining [11, 64, 16] where the task is decomposed into several subtasks and the\n7\n0\n1000\n2000\n3000\nEpisode Timestep\n0\n5\n10\n15\n20\n25\n30\nDirt in Inventory\n0\n1000\n2000\n3000\nEpisode Timestep\n0\n2\n4\n6\n8\nLogs in Inventory\n20M\n40M\n60M\n80M\n100M\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBuild Tower Success\nPrompt Chaining\nDirect Prompting\n20M\n40M\n60M\n80M\n100M\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWooden Planks\nCrafting Success\n55\n60\n65\n70\n75\n80\n85\nAgent Height Position\nBuild a Tower\n0\n2\n4\n6\n8\n10\n12\n14\nPlanks in Inventory\nMake Wooden Planks\nNumber of Frames\n100\n200\n300\nTravel Distance (Blocks)\n0\n2\n4\n6\nSeeds Collected\nrelevant\nirrelevant\n20M\n40M\n60M\n80M\n100M\n0\n5\n10\n15\n20\nWooden Logs Collected\n20M\n40M\n60M\n80M\n100M\n0\n20\n40\n60\nDirt Collected\nNumber of Frames\nFigure 4: Top left: By sequentially chaining visual prompts like “get dirt” and “build a tower”,\nSTEVE-1 successfully gathers dirt and then uses this dirt to build a tower. The prompts switch at\nthe dotted vertical line. Bottom left: The success rates of the chained prompts improve steadily as\nwe train STEVE-1 on more data. Right: The performance of STEVE-1 on different tasks scales in\ndifferent ways when conditioning on a relevant visual prompt for the metric versus other irrelevant\nvisual prompts (e.g., the break wood prompt is the relevant prompt for the “Wooden Logs Collected”\nmetric, while the other prompts are “irrelevant”). For instance, in the wood-collection and dirt-\ncollection tasks, performance starts increasing after training on 60M frames of gameplay. See\nFigure 14 for sample frames from each visual prompt.\nprompts are given sequentially for a fixed number of steps. We explore prompt chaining with visual\ngoals for two tasks: 1) building a tower and 2) making wooden planks. When using prompt chaining,\nwe first prompt STEVE-1 to gather dirt before building a tower, and to gather wooden logs before\ncrafting wooden planks. Figure 4 shows that directly prompting STEVE-1 with the final tasks results\nin near-zero success rates. However, prompt chaining allows STEVE-1 to build a tower 50% of the\ntime and craft wooden planks 70% of the time. For the tower building task, STEVE-1 immediately\nstarts collecting dirt until the prompt switches, at which point its average height starts increasing\nrapidly and its dirt decreases as it builds a tower. Similarly, for the crafting wooden planks task,\nSTEVE-1 immediately starts collecting a large amount of wooden logs until the prompt switches and\nit rapidly converts these wooden logs into wooden planks (causing the amount of wooden logs in its\ninventory to immediately decrease and the number of wooden planks to increase as it crafts more).\nFigure 4 visualizes the average item counts and agent height for the prompt chaining episodes. See\nFigure 18 and Figure 19 in the appendix for visualizations of specific prompt chaining episodes.\n4.4\nScaling\nRecent works in language modeling have found that scaling up pretraining FLOPs, by training on\nmore data or by training a model with more parameters, can improve performance on downstream\ntasks [28, 57, 63]. In certain cases when measuring performance with metrics such as exact-match\n[52], performance improvement may appear to be “emergent” [63], appearing suddenly as the model\nis trained with more compute. Here, we aim to gain a basic understanding of how the performance\nof STEVE-1 on various tasks scales by training with more data (learning rate schedule is chosen\nappropriately).\nTo assess performance gain, we isolate the performance of the policy from the prior, measuring\nperformance of the agent on programmatic tasks (travel distance, seeds, logs, dirt) with visual goals.\nDue to compute constraints, we chose to use the 2x VPT model, which has 248M parameters. We\nfound that both seed collection and travel distance did not improve significantly past 20M frames.\nFrom inspecting gameplay, we suspect that travel distance is a relatively easy task since it is close to\nVPT’s default behavior of running around and exploring. For seed collection, performance remains\nsuboptimal, suggesting that further scaling may be beneficial. This hypothesis is supported by the\nobservation that performance on log and dirt collection remained roughly level until 60M frames when\nit began to rapidly improve. Figure 4 shows the scaling curves for STEVE-1 on each programmatic\ntask when conditioning on relevant vs. irrelevant visual prompts for that task. Since we do not\nobserve regression on any tasks as we train the model with more compute, we expect the model to\ncontinue to perform better as we train larger models on larger datasets.\n8\nWe also evaluated the scaling properties of STEVE-1 for our multi-step tasks with and without prompt\nchaining. Without prompt chaining, the tasks remain challenging for STEVE-1 throughout training.\nHowever, we note that after 60M frames, STEVE-1 sometimes gathers wooden logs and then builds\na small tower when directly prompted to build a tower. This is likely because our visual prompt\nfor tower building shows a video of a tower being built out of wooden logs. With prompt chaining,\nthe performance of STEVE-1 steadily increases with more data. We conjecture that this is because\nthe success of a chained prompt requires the success of each element in the chain. Since different\nabilities emerge at different scales, one would expect chained prompts to steadily get more reliable\nas these subgoals become more reliably completed. In the case of crafting wooden planks, we note\nthat crafting is one such task that gets significantly more reliable as the agent is trained on more data.\nFigure 4 shows the scaling curves for STEVE-1 on the prompt chaining tasks.\nIn summary, we see evidence of tasks that do not require much data for STEVE-1 to learn, tasks\nthat steadily get more reliable as the agent is trained longer, and tasks where capability suddenly\nspikes after the agent reaches some threshold. Put together, this suggests that further scaling would\nlikely significantly improve the agent, although we leave the task of predicting exactly how much\nperformance there is to gain to future studies.\n4.5\nWhat Matters for Downstream Performance?\nPretraining\nBaker et al. [5] finds that by pretraining a behavioral prior with imitation learning on\ninternet-scale datasets for Minecraft, the learned policy can be effectively finetuned to accomplish\ntasks that are impossible without pretraining. In this section, we demonstrate that pretraining is\nalso massively beneficial for instruction-tuning in Minecraft. We hypothesize that due to the strong\nperformance of STEVE-1 and the relatively small amount of compute (≈1% additional compute)\nused for instruction finetuning, most of the capabilities of our agent come from the pretraining rather\nthan the finetuning. To test this hypothesis, we finetune several varients of STEVE-1 from various\npretrained weights: foundation-2x, bc-early-game-2x, rl-from-foundation-2x, and with\nrandomly initialized weights. In this experiment, each model was finetuned on 100M frames.\nFigure 5 shows the performance of these models on our programmatic tasks with visual goals. Note\nthat while an agent trained on our dataset from scratch can accomplish basic tasks like dirt collection\nfairly well, it is unable to find and chop down trees, in contrast to the pretrained agents. This demon-\nstrates that the abilities present in the agent due to pretraining are successfully transferred to the fine-\ntuned agent. Out of all the pretrained weights we tried, we noticed that rl-from-foundation-2x\nperformed the best, having qualitatively better performance at tasks like crafting and chopping down\ntrees. Indeed, Figure 5 shows that this model has strong performance, likely due to the massive\namount of compute it was trained with during its RL training [5].\nClassifier-Free Guidance\nBaker et al. [5] observed that when conditioning the agent on text, it\ntended to ignore its instruction and instead perform the prior behavior learned during pretraining.\nAs discussed in Section 3.4, classifier-free guidance [23] gives a knob for trading off between goal-\nconditioned and prior behaviors. Figure 5 shows the effect of this parameter λ on the log and dirt\ncollection tasks. The performance of the agent reaches its maximum around λ = 5.0 to λ = 7.0,\nafter which it starts to drop off. These results demonstrate the importance of classifier-free guidance,\nwhich improves the performance of STEVE-1 by orders of magnitude.\nPrompt Engineering\nPrompt engineering as a discipline has rapidly emerged over the last year\ndue to the observation that the quality of the output of text-to-X models can dramatically change\ndepending on the prompt [67]. For example, Table 5 in the appendix shows how a prompt for Stable\nDiffusion [50] might be written. By listing out the various attributes of the image such as visual\nmedium, style, and the phrase “trending on ArtStation”, the user is able to get a higher quality image\n[20, 36]. In this section, we explore how this same style of prompt engineering can improve the\nperformance of STEVE-1. Figure 6 shows how a simple prompt of “get dirt” might be changed in\norder to more accurately specify the type of behavior that is desired. Just like in image generation\nmodels, the performance of STEVE-1 significantly improves by modifying the prompt in this fashion.\nBy changing to more complicated prompts, STEVE-1 is able to collect 1.6× more wood, 2× more\ndirt, and 3.3× more seeds.\n9\nscratch\nfd\nbc\nrl\n0\n5\n10\nWooden Logs Collected\nscratch\nfd\nbc\nrl\n0\n25\n50\n75\nDirt Collected\n0\n2\n4\n6\n8\n10\n0\n10\n20\nWooden Logs Collected\n0\n2\n4\n6\n8\n10\n20\n40\n60\nDirt Collected\nConditional Scale λ\nPretrained Weights\nFigure 5: Left: We trained STEVE-1 on 100M frames starting from four different pretrained\nweights: random initialization (scratch), foundation-2x (fd), bc-early-game-2x (bc), and\nrl-from-foundation-2x (rl). The rl-from-foundation-2x agent is generally the most per-\nformant after fine-tuning. Using pretrained weights performs better than training from scratch,\nespecially for more complicated tasks like collecting wood. Right: By using classifier-free guidance\n[23], STEVE-1 collects 7.5× more dirt and 15× more wood than when λ = 0 (no guidance). See\nFigure 17 in the Appendix for similar results with other programmatic tasks.\nPrompt\nDirt Collected\n“break a flower”\n0.7 (-0.2, 1.6)\n“collect seeds”\n2.7 (0.9, 4.5)\n“dig as far as possible”\n3.9 (2.8, 5.0)\n“get dirt”\n9.2 (5.7, 12.7)\n“get dirt, dig hole, dig dirt, gather a ton of dirt, collect dirt”\n26.7 (19.9, 33.5)\nFigure 6: Similar to text-to-image generation, switching to a longer, more specific prompt dramatically\nimproves the performance of STEVE-1. Values in parentheses are 95% confidence intervals.\n5\nLimitations and Conclusion\nIn this paper, we present a methodology for creating instruction-following foundation models of\nbehavior. Specifically, by leveraging two existing pretrained foundation models: a behavioral prior\n(VPT [5]) and a domain-specific CLIP model (MineCLIP [17]), we create a powerful Minecraft agent\nthat can follow short-horizon open-ended text and visual instructions, all for only $60 of compute.\nThe resulting foundation model, STEVE-1, sets a new bar for open-ended instruction-following in\nMinecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming\nprevious baselines and robustly completing 12 of 13 tasks in our early-game evaluation suite. We note\nthat generalist agents such as STEVE-1 can have potential negative effects on society. We include a\nthorough discussion of these issues in Appendix A.\nSTEVE-1 is a significant advancement in creating generative models of text-to-behavior, but it has\nseveral limitations, as described in Appendix B. First, STEVE-1 is mostly proficient at achieving\nshort-horizon tasks while struggling with longer-horizon tasks. While prompt chaining is a promising\napproach for improving performance on complex tasks, more can be done in future work to improve\nperformance. Another limitation we observe is that prompt engineering, as with other generative\nmodels, can be unintuitive and time-consuming. Future work should investigate improving the\nsteerability of STEVE-1 through a better understanding of natural language prompts. Additionally,\nwe note that evaluating and describing the capabilities of open-ended generalist agents is an open\nresearch problem itself since capability depends strongly on preconditions, prompt engineering, and\nour own ability to come up with varied and challenging tasks. Finally, since our approach is not\nspecific to the Minecraft domain, we hope that the method used to create STEVE-1 can inspire future\nwork in creating powerful generalist agents in other domains and environments.\n10\nAcknowledgements\nAll of the authors gratefully acknowledge funding for this research from the Natural Sciences and\nEngineering Research Council of Canada (NSERC) and the Canada CIFAR AI Chairs Program\n(Vector Institute for Artificial Intelligence). SL is supported by a Vector Institute internship and by\nan NSERC Discovery Grant. KP is supported by an NSERC PGS-D award. HC is supported by an\nNSERC CGS-D award. JB acknowledges funding from the Canada CIFAR AI Chairs program, Fujitsu\nJapan, and an Amazon Research Award. In addition to NSERC and CIFAR (Vector Institute), SM\nacknowledges funding from Microsoft Research. We thank Silviu Pitis, Romi Lifshitz, Forest Yang,\nand Yongchao Zhou for their helpful comments; Alisa Wu and Ziming Chen for their contributions\nto the text-video pair dataset; and Finn Paster for the logo and graphic for the website. Resources\nused in preparing this research were provided, in part, by the Province of Ontario, the Government\nof Canada through CIFAR, and companies sponsoring the Vector Institute for Artificial Intelligence\n(www.vectorinstitute.ai\/partners).","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/STEVE-1: A Generative Model for Text-to-Behavior in Minecraft.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nSTEVE-1: A Generative Model for Text-to-Behavior in Minecraft\n```\n#### 2. 论文摘要\n```\nConstructing AI models that respond to text instructions is challenging,\nespecially for sequential decision-making tasks. This work introduces a\nmethodology, inspired by unCLIP, for instruction-tuning generative models of\nbehavior without relying on a large dataset of instruction-labeled\ntrajectories. Using this methodology, we create an instruction-tuned Video\nPretraining (VPT) model called STEVE-1, which can follow short-horizon\nopen-ended text and visual instructions in Minecraft. STEVE-1 is trained in two\nsteps: adapting the pretrained VPT model to follow commands in MineCLIP's\nlatent space, then training a prior to predict latent codes from text. This\nallows us to finetune VPT through self-supervised behavioral cloning and\nhindsight relabeling, reducing the need for costly human text annotations, and\nall for only $60 of compute. By leveraging pretrained models like VPT and\nMineCLIP and employing best practices from text-conditioned image generation,\nSTEVE-1 sets a new bar for open-ended instruction-following in Minecraft with\nlow-level controls (mouse and keyboard) and raw pixel inputs, far outperforming\nprevious baselines and robustly completing 12 of 13 tasks in our early-game\nevaluation suite. We provide experimental evidence highlighting key factors for\ndownstream performance, including pretraining, classifier-free guidance, and\ndata scaling. All resources, including our model weights, training scripts, and\nevaluation tools are made available for further research.\n```\n\n#### 3. 论文全文\n```\nSTEVE-1: A Generative Model for\nText-to-Behavior in Minecraft\nShalev Lifshitz1,2∗\nshalev.lifshitz@mail.utoronto.ca\nKeiran Paster1,2∗\nkeirp@cs.toronto.edu\nHarris Chan1,2†\nhchan@cs.toronto.edu\nJimmy Ba1,2\njba@cs.toronto.edu\nSheila McIlraith1,2\nsheila@cs.toronto.edu\n1Department of Computer Science, University of Toronto, Toronto, Canada.\n2Vector Institute for Artificial Intelligence, Toronto, Canada.\nAbstract\nConstructing AI models that respond to text instructions is challenging, especially\nfor sequential decision-making tasks. This work introduces a methodology, inspired\nby unCLIP, for instruction-tuning generative models of behavior without relying\non a large dataset of instruction-labeled trajectories. Using this methodology, we\ncreate an instruction-tuned Video Pretraining (VPT) model called STEVE-1, which\ncan follow short-horizon open-ended text and visual instructions in Minecraft™.\nSTEVE-1 is trained in two steps: adapting the pretrained VPT model to follow com-\nmands in MineCLIP’s latent space, then training a prior to predict latent codes from\ntext. This allows us to finetune VPT through self-supervised behavioral cloning and\nhindsight relabeling, reducing the need for costly human text annotations, and all\nfor only $60 of compute. By leveraging pretrained models like VPT and MineCLIP\nand employing best practices from text-conditioned image generation, STEVE-1\nsets a new bar for open-ended instruction-following in Minecraft with low-level\ncontrols (mouse and keyboard) and raw pixel inputs, far outperforming previous\nbaselines and robustly completing 12 of 13 tasks in our early-game evaluation\nsuite. We provide experimental evidence highlighting key factors for downstream\nperformance, including pretraining, classifier-free guidance, and data scaling. All\nresources, including our model weights, training scripts, and evaluation tools are\nmade available for further research.\n1\nIntroduction\nThe ability to use text instructions to control and interact with powerful AI models has made these\nmodels accessible and customizable for the masses. Such models include ChatGPT [41], which\ncan respond to messages written in natural language and perform a wide array of tasks, and Stable\nDiffusion [50], which turns natural language into an image. While those models cost anywhere from\nhundreds of thousands to hundreds of millions of dollars to train, there has been an equally exciting\ntrend whereby powerful open-source foundation models like LLaMA [59] can be finetuned with\nsurprisingly little compute and data to become instruction-following (e.g., [58, 13]).\nIn this paper, we study whether such an approach could be applicable to sequential decision-making\ndomains. Unlike in text and image domains, diverse data for sequential decision-making is very\nexpensive and often does not come with a convenient “instruction” label like captions for images. We\npropose to instruction-tune pretrained generative models of behavior, mirroring the advancements\nseen in recent instruction-tuned LLMs like Alpaca [58], and without relying on a large dataset of\ninstruction-labeled trajectories.\n∗Equal contribution.\n†Core contribution.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.00937v3  [cs.AI]  4 Feb 2024\nText\nEncoder\n“chop a  tree”\nCVAE\nVideo\nencoder\nCLIP   Objective\nGaussian Prior\nTransformer (VPT)\nMineCLIP \n(Frozen)\nlinear\nPrior\nPolicy\nResNet\nResNet\nResNet\nMouse\/Keyboard\n\/\/\nFigure 1: Like unCLIP [48], our approach involves two models. First, we train the policy by finetuning\nVPT to achieve goals given by pretrained MineCLIP [17] visual embeddings using our gameplay\ndataset. Second, for the prior model, we train a CVAE [54] to sample MineCLIP visual embeddings\ngiven a text prompt. The combination of these two models enables our agent to follow text and visual\ninstructions.\nIn the past year, two foundation models for the popular open-ended video game Minecraft™were\nreleased: a foundation model for behavior called VPT [5] and a model aligning text and video\nclips called MineCLIP [17]. This has opened up an intriguing avenue to explore fine-tuning for\ninstruction-following in the sequential decision-making domain of Minecraft. VPT was trained on\n70k hours of Minecraft gameplay, so the agent already has vast knowledge about the Minecraft\nenvironment. However, just as the massive potential of LLMs was unlocked by aligning them\nto follow instructions, it is likely that the VPT model has the potential for general, controllable\nbehavior if it is finetuned to follow instructions. In particular, our paper demonstrates a method for\nfine-tuning VPT to follow short-horizon text instructions with only $60 of compute and around 2,000\ninstruction-labeled trajectory segments.\nOur method draws inspiration from unCLIP [48], the approach used to create the popular text-to-\nimage model DALL•E 2. We decompose the problem of creating an instruction-following Minecraft\nagent into two models: a VPT model finetuned to achieve visual goals embedded in the MineCLIP\nlatent space, and a prior model that translates text instructions into MineCLIP visual embeddings. We\nfinetune VPT using behavioral cloning with self-supervised data generated with hindsight relabeling\n[3], avoiding the use of expensive text-instruction labels in favor of visual MineCLIP embeddings.\nWe apply unCLIP with classifier-free guidance [23] to create our agent called STEVE-1, which sets\na new bar for open-ended instruction-following in Minecraft with low-level controls (mouse and\nkeyboard) and raw pixel inputs, far outperforming the baseline set by Baker et al. [5].\nOur main contributions are as follows:\n• We introduce a methodology, inspired by unCLIP [48], for instruction-tuning generative\nmodels of behavior without relying on a large dataset of expensive instruction labels.\n• We apply this methodology to create STEVE-1, a Minecraft agent that can follow short-\nhorizon open-ended text and visual instructions with a high degree of accuracy, all for only\n$60 of compute. We perform extensive evaluations of our agent, showing that it can robustly\ncomplete 12 of 13 goal-conditioned control tasks in our early-game evaluation suite in\nMinecraft. For long-horizon tasks1 like crafting and building, we show that a basic version\nof prompt chaining can dramatically improve performance.\n• We provide experimental evidence highlighting key factors for downstream performance,\nincluding pretraining, classifier-free guidance, data scaling, prompt-engineering, and other\ndesign choices. In particular, we show that unCLIP [48] and classifier-free guidance [23]\ntranslate well to sequential decision-making and are essential for strong performance.\n• We release model weights for STEVE-1 as well as training scripts and evaluation code to\nhelp foster more research into instructable, open-ended sequential decision-making agents.2\n1Short-horizon tasks require few steps: e.g., go to a tree and chop it down, dig a hole. Long-horizon tasks\ntake many steps: e.g., craft complex recipes from scratch, build a house.\n2Model weights, training code, videos, and an interactive demo script are hosted on our project webpage at\nhttps:\/\/sites.google.com\/view\/steve-1.\n2\n2\nRelated Work\nMinecraft as a Test-bed for AI\nMinecraft has gained popularity as a benchmark for AI research\ndue to its complex and dynamic environment, making it a rich test-bed for reinforcement learning\nand other AI methods (e.g., [26, 19, 17, 21, 40, 62, 38, 9]). We leverage the MineRL environment\n[19] to research the creation of agents that can follow open-ended instructions in complex visual\nenvironments using only low-level actions (mouse and keyboard). We build STEVE-1 on top of\ntwo recent foundation models. In order to align text and videos, we use MineCLIP [17], a CLIP\n[47] model trained on paired web videos of Minecraft gameplay and associated captions. To train\nSTEVE-1’s policy, we fine-tune VPT [5], a foundation model of Minecraft behavior that is pretrained\non 70k hours of web videos of Minecraft along with estimated mouse and keyboard actions. Several\nprior works [61, 62] have explored the use of LLMs in creating instructable Minecraft agents. These\nworks typically use LLMs to make high-level plans that are then executed by lower-level RL [40, 62]\nor scripted [46] policies. Since STEVE-1 is a far more flexible low-level policy, the combination\nof STEVE-1 with LLMs is a promising direction for future work. Fan et al. [17] introduced an\nagent trained using RL with MineCLIP as a shaping reward on 12 different tasks and conditioned on\nMineCLIP-embedded text-prompts. However, this agent failed to generalize beyond the original set\nof tasks without further RL finetuning using the MineCLIP reward function. Cai et al. [9] proposed\na Goal-Sensitive Backbone architecture for goal-conditioned control in Minecraft which is trained\non a fixed set of goals, while STEVE-1 learns goal-reaching behavior from a large dataset in a\nself-supervised way without training on an explicit set of tasks.\nFoundation Models for Sequential Decision-Making\nFoundation models which are pretrained on\nvast amounts of data and then finetuned for specific tasks have recently shown great promise in a\nvariety of domains including language [8, 14, 59], vision [48, 10, 47], and robotics [7, 53, 25, 39, 65].\nGATO [49] and RT-1 [7] have demonstrated the potential of training transformers to perform both\nsimulated and real-world robotic tasks. With the exception of Kumar et al. [30], which uses Q-\nlearning, the vast majority of cases [32, 7, 49] where deep learning has been scaled to large, multitask\noffline-RL datasets have used supervised RL. Supervised RL (e.g., [42, 18, 12]) works by framing the\nsequential decision-making problem as a prediction problem, where the model is trained to predict the\nnext action conditioned on some future outcome. While these approaches are simple and scale well\nwith large amounts of compute and data, more work is needed to understand the trade-offs between\nsupervised RL and Q-learning or policy gradient-based methods [43, 44, 6, 55]. Recent works explore\nthe use of hindsight relabeling [3] using vision-language models [47, 2] to produce natural language\nrelabeling instructions. DIAL [65] finetunes CLIP [47] on human-labeled trajectories, which is\nthen used to select a hindsight instruction from a candidate set. Sumers et al. [56] uses Flamingo\n[2] zero-shot for hindsight relabeling by framing it as a visual-question answering (VQA) task. In\ncontrast, STEVE-1 relabels goals using future trajectory segment embeddings given by the MineCLIP\n[17] visual embedding.\nText-Conditioned Generative Models\nThere has been a recent explosion of interest in text-to-X\nmodels, including text-to-image (e.g., [48, 51, 50]), text-to-3D (e.g., [27, 35]), and even text-to-music\n(e.g., [1]). These models are typically either autoregressive transformers modeling sequences of\ndiscrete tokens [60, 8] or diffusion models [24]. Most related to our work is unCLIP, the method used\nfor DALL•E 2 [48]. unCLIP works by training a generative diffusion model to sample images from\nCLIP [47] embeddings of those images. By combining this model with a prior that translates text\nto visual CLIP embeddings, unCLIP can produce photorealistic images for arbitrary text prompts.\nunCLIP and many other diffusion-based approaches utilize a technique called classifier-free guidance\n[23], which lets the model trade-off between mode-coverage and sample fidelity post-training. We\nutilize the basic procedure of unCLIP and classifier-free guidance for training STEVE-1.\n3\nMethod\nInspired by the rapid recent progress in instruction-tuning Large Language Models (LLMs), we\nchoose to leverage the recently released Video Pretraining (VPT) [5] model as a starting point for\nour agent. Since VPT was trained on 70k hours of Minecraft gameplay, the agent already has vast\nknowledge about the Minecraft environment. However, just as the massive potential of LLMs was\nunlocked by aligning them to follow instructions, it is likely that the VPT model has the potential\n3\nz1\nz2\nz2\nz3\nz4\nz5\nz6\nz7\nz2\nz7\nz7\nz7\nz7\nz7\nFigure 2: To create goal-conditioned data for finetuning, we randomly select timesteps from episodes\nand use hindsight relabeling to set the intermediate goals for the trajectory segments to those visual\nMineCLIP embeddings. This self-supervised data teaches the agent which actions lead to which states.\nfor general, controllable behavior if it is finetuned to follow instructions. In this work, we present\na method for finetuning VPT to follow natural, open-ended textual and visual instructions, which\nopens the door for a wide range of uses for VPT in Minecraft.\nOur approach is inspired by unCLIP, the method behind the recent text-to-image generation model,\nDALL•E 2 [48]. Our goal is to create a generative model of behavior in Minecraft conditioned on\ntext instructions y. To do so, we utilize a dataset of Minecraft trajectory segments, some of which\ncontain instruction labels y: [(τ1, y1), (τ2, y2), . . . , (τn, ∅)] where τ is a trajectory of observations\nand actions. We also employ a pretrained CLIP model called MineCLIP [17], which generates aligned\nlatent variables zτt:t+16, zy, where zτt:t+16 is an embedding of any 16 consecutive timesteps from\nthe trajectory. MineCLIP is trained using a contrastive objective on pairs of Minecraft videos and\ntranscripts from the web. For simplicity of notation, we refer to the MineCLIP embedding of the\nlast 16 timesteps of a trajectory segment as zτgoal. Like unCLIP [48], we utilize a hierarchical model\nconsisting of a prior and a policy:\n• A prior p(zτgoal|y) that produces a latent variable zτgoal conditioned on a text instruction y.\n• A policy p(τ|zτgoal) that produces a trajectory conditioned on a latent variable zτgoal.\nThese two models can then be combined to produce a generative model of behaviors conditioned on\ntext instructions:\np(τ|y) = p(τ, zτgoal|y) = p(zτgoal|y)p(τ|zτgoal)\n(3.1)\n3.1\nPolicy\nTo learn our policy, we finetune VPT, a foundation model of Minecraft behaviors pθ(τ) trained\non 70k hours of Minecraft gameplay videos. Specifically, VPT consists of a ResNet [22] that\nprocesses frames of dimension 128 × 128 × 3, and a Transformer-XL [15] which processes the frame\nrepresentations and autoregressively predicts the next action using the joint hierarchical action space\ndescribed in Baker et al. [5]. In order to modify the architecture to condition on goal information, we\nadd an affine transformation of zτgoal to the output of the ResNet before passing it to the transformer:\nProcess Frames:\nResNetθ(ot) →xt\n[+ Conditioning on MineCLIP Embedding Goal]:\nxt →xt + Wθzτgoal + bθ\nPredict Actions:\nTransformerXLθ(xt, . . . , xt+T ) →at+T\nIn order to finetune VPT to condition on goals, we finetune the model using a method inspired by\nsupervised RL approaches like Decision Transformer [12], GLAMOR [42], and GCSL [18]. We use\na modification of hindsight relabeling which we call packed hindsight relabeling (see Figure 2) to\ngenerate a new dataset of trajectories with goals pulled from future states that periodically switch. In\ncontrast with hindsight relabeling, packed hindsight relabeling packs multiple relabeled sequences\ninto a single sequence. Specifically, our method to generate this dataset consists of two steps:\n1. Given a trajectory τ with T timesteps, randomly generate indices to select goals from:\ni1, i2, . . . , in. These indices are chosen by starting at the first timestep and repeatedly\nsampling a new timestep by adding a random value to the previous timestep. This ensures\nthat the data reflects that some goals may take longer to achieve than others.\n2. For each chosen goal at timestep ij, set the goals for timesteps ij−1 + 1, . . . , ij to be the\ngoal at timestep ij, denoted zτij .\n4\nOur final dataset Drelabeled consists of observation sequences (o1, . . . , oT ), action sequences\n(a1, . . . , aT ), and packed hindsight relabeled goals (z1, . . . , zT ). We then finetune VPT on this\ndataset using a supervised loss to predict each action autoregressively using a causal attention mask:\nLpolicy(θ) = EDrelabeled[−log pθ(at|o1...t, z1...t)]\n(3.2)\n3.2\nPrior\nIn order to condition not only on embeddings of visual goals but on latent goals, we need the prior, a\nmodel that produces a latent variable zτgoal conditioned on a text instruction y. Our model is a simple\nconditional variational autoencoder (CVAE) [54, 29] with a Gaussian prior and a Gaussian posterior.\nRather than learn to condition directly on text, we choose to condition on frozen text representations\nfrom MineCLIP zy. Thus, the prior learns a function to translate from a text embedding zy to a visual\nembedding zτgoal (see Appendix C.5 for further discussion). Both the encoder and decoder of our\nCVAE are parameterized as two-layer MLPs with 512 hidden units and layer normalization [4]. We\ntrain the model on our dataset, for which we have text labels Dlabels using the following loss:\nLprior(ϕ) = E(zτgoal,zy)∼Dlabels\nh\nKL(qϕ(zτgoal|zy)∥p(zτgoal)) −Ec∼qϕ(zτgoal|zy)\n\u0002\nlog pϕ(zτgoal|c, zy)\n\u0003i\n(3.3)\n3.3\nDatasets\nTo train our policy, we gather a gameplay dataset with 54M frames (≈1 month at 20FPS) of Minecraft\ngameplay along with associated actions from two sources: contractor gameplay and VPT-generated\ngameplay. To train our prior, we use a small dataset of text-video pairs gathered by humans and\naugmented using the OpenAI API gpt-3.5-turbo model [41] and MineCLIP. See Appendix D for\nmore detailed dataset information.\nOpenAI Contractor Dataset\nWe use 39M frames sourced from the contractor dataset which VPT\n[5] used to train its inverse dynamics model and finetune its policy. The dataset was gathered by\nhiring human contractors to play Minecraft and complete tasks such as house building or obtaining a\ndiamond pickaxe. During gameplay, keypresses and mouse movements are recorded. We use the\nsame preprocessing as VPT, including filtering out null actions.\nVPT-Generated Dataset\nWe generate an additional dataset of 15M frames by generating random\ntrajectories using the various pretrained VPT agents. The diversity of this dataset is improved by\nrandomly switching between models during trajectories [44], randomly resetting the agent’s memory,\nand randomly turning the agent to face a new direction.\nText-Video Pair Dataset\nTo train our prior model, which learns a mapping between text embed-\ndings and visual embeddings, we also manually gather a small dataset of 2,000 text instructions\npaired with 16-frame video segments (less than a second) from our gameplay dataset. This dataset\ncorresponds to less than 30 minutes of gameplay and takes just a few hours to collect. We aug-\nment this dataset by using the alignment between text and video embeddings from MineCLIP. For\neach text instruction, we find the top k most similar gameplay segments in our dataset and use the\ncorresponding 16-frame segment as additional training data. For augmentation, we also add 8,000\ntext-instructions generated by the OpenAI API gpt-3.5-turbo model [41], in addition to our 2,000\nhand-labeled instructions.\n3.4\nInference\nAt inference time, we use the prior to sample a latent goal zτgoal from the text instruction y. We then\nuse the policy to autoregressively sample actions at conditioned on the observation history o1...t\nand the latent goal zτgoal. Similar to the observation in Appendix I of Baker et al. [5], even with\nconditioning, the policy often fails to follow its instruction and simply acts according to its prior\nbehavior. To mitigate this, we borrow another trick used in image generation models: classifier-free\nguidance. Specifically, during inference we simultaneously compute logits for the policy conditioned\non the goal f(ot, . . . , ot+1, zτgoal) and for the unconditional policy f(ot, . . . , ot+1). We then compute\na combination of the two logits using a λ parameter to trade-off between the two:\n5\nlogits = (1 + λ) fθ(ot, . . . , ot+1, zτgoal)\n|\n{z\n}\nconditional logits\n−λ fθ(ot, . . . , ot+1)\n|\n{z\n}\nunconditional logits\n(3.4)\nBy setting a higher value of λ, we can encourage the policy to follow actions that are more likely when\nconditioned on the goal and, as demonstrated in Section 4.5, this significantly improves performance.\nAlso, in order to train the policy to generate these unconditional logits, we occasionally dropout the\ngoal embedding zτgoal from the policy’s input (with probability 0.1). This lets us generate both the\nconditional and unconditional logits using the same model with batch processing at inference time.\n3.5\nEvaluation\nEvaluating the performance of our agent is a challenging task due to the wide variety of instructions\nthat are possible and the difficulty of evaluating whether the agent has successfully achieved its\ntask. We use a combination of programmatic evaluation metrics and automatic MineCLIP evaluation\nmetrics to get a sense of the agent’s capability level. We collectively refer to all of our evaluation tasks\nincluding the 11 evaluation tasks from Figure 3 and the two prompt chaining tasks from Section 4.3\nas our early-game evaluation suite.\nProgrammatic Evaluation\nWe compute programmatic evaluation metrics by monitoring the\nMineRL [19] environment state throughout each evaluation episode. As done in VPT [5], we compute\nmultiple programmatic metrics including travel distance and early-game item collection. The travel\ndistance is the maximum displacement of the agent along on the horizontal (X-Z) plane, measured\nfrom the initial spawn point. For early-game inventory counts, we store the maximum number of log,\nseed, and dirt items seen in the agent’s inventory during the episode.\nMineCLIP Evaluation\nWe explore the use of text-visual alignment in MineCLIP latent space\nbetween trajectories and text or visual goals to evaluate our agent over a wider variety of tasks where\nprogrammatic evaluation isn’t practical. To determine the degree to which a task has been completed\nat all during an evaluation episode, we record the minimum cosine distance between the (text or\nvisual) goal embedding and the visual MineCLIP embedding at any timestep during an episode.\n4\nResults\nIn our experiments, we aim to answer the following questions:\n1. How well does STEVE-1 perform at achieving both text and visual goals in Minecraft?\n2. How does our method scale with more data?\n3. What choices are important for the performance of our method?\n4.1\nTraining Setup\nWe base our implementation off of the official VPT codebase3. The main STEVE-1 agent is trained\nusing Pytorch [45] distributed data parallel on four A40 GPUs for 160M frames, or just under three\nepochs of our gameplay dataset. Hyperparameters are selected to match those in Baker et al. [5] with\nthe exception of learning rate, which we set to 4e-5. Our models are optimized using AdamW [37].\nSee Table 1 for a full list of hyperparameters.\n4.2\nPerformance on Textual and Visual Goals\nDue to the hierarchical nature of our model, we can evaluate the performance of our agent at achieving\neither text or visual goals simply by choosing whether to use the prior to condition on text or bypass\nthe prior and condition on a MineCLIP video embedding directly. We first tested our model on a set\nof 11 tasks that are achievable within the first 2.5 minutes of gameplay and which do not require\nmultiple steps to complete (e.g., chop a tree or dig a hole, but not build a house). A complete list\nof the tasks and prompts we used for evaluation can be found in Table 3 in the appendix. To select\nvisual goals for testing each of the evaluation tasks, we implemented a tool that searches through\n3https:\/\/github.com\/openai\/Video-Pre-Training\n6\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0\n100\n200\n300\n400\nTravel Distance (Blocks)\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nSeeds Collected\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0\n5\n10\n15\n20\nWooden Logs Collected\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0\n20\n40\n60\nDirt Collected\n(a) Programmatic Evaluation\ndig\ndirt\nsky\nleaves\nwood\nseeds\nflower\nexplore\nswim\nunderwater\ninventory\nConditioned Prompt\ndig\ndirt\nsky\nleaves\nwood\nseeds\nflower\nexplore\nswim\nunderwater\ninventory\nEvaluation Prompt\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n(b) MineCLIP Evaluation\nFigure 3: Left: In our programmatic evaluations, STEVE-1 performed far better than the unconditional\nVPT agent early-game-2x and the text-conditioned VPT agent when prompted appropriately. The\nasterisk * in the “VPT (Text)*” indicates that this result was taken from Appendix I in [5], which\nhad twice the episode length compared to our setting. On some tasks, visual outperforms text-based\nprompting, creating a gap that can likely be bridged through better prompt engineering. Right:\nAcross our 11 MineCLIP evaluation tasks, STEVE-1 achieves the shortest distance between the\nepisode and the MineCLIP goal embedding when prompted appropriately except for in two cases,\nwhere it mixes up digging and dirt and swimming and going underwater. This shows the strong\ngeneral performance of STEVE-1 across a wide variety of short-horizon tasks. The dashed box marks\nthe minimum element along the row, and the diagonal number signifies the diagonal element’s rank\n(0 means it is the minimum row element). See Figure 14 for sample frames from each of the 11 visual\ngoals and Figure 13 for a success-rate version of this matrix.\n10% of our gameplay dataset by finding the closest 16-frame videos to a given text prompt. We then\nmanually selected a 16-frame video that clearly demonstrates the task being completed and use the\ncorresponding MineCLIP video embedding as the goal embedding for that task. Screenshots of these\nvisual goals can be found in Figure 14 in the appendix.\nIn Figure 3, we compare the performance of our text and visual-conditioned agents with the uncondi-\ntional VPT agent and text-conditioned VPT agent (from Appendix I in [5]) across our programmatic\ntasks. We find that when given the relevant text instruction, STEVE-1 collects 75× more dirt, 4.9×\nmore wood, 22× more seeds, and travels 4.3× farther than the unconditional agent, and STEVE-1\ncollects 3.3× more dirt, 4.4× more wood, 8.1× more seeds, and travels 2.2× farther than the text-\nconditioned VPT agent. This represents a significant improvement over the reported performance\nof text-conditioned VPT, which collects several times fewer resources despite having twice as long\nof an episode to do so. We also run an automatic evaluation using MineCLIP embedding distances\nby measuring the minimum distance of a goal embedding to any frame in the episode. As shown\nin Figure 3b, the distance between the goal and the episode is significantly lower when the agent is\nconditioned on the corresponding visual goal than otherwise. Full results for STEVE-1 with both text\nand visual goals can be found in Appendix F.\nIn addition to our evaluations of STEVE-1, we also recorded several sample interactive sessions we had\nwith the agent (controlling it in real-time by giving it written text instructions or specific visual goals).\nThese sessions demonstrate STEVE-1’s ability to responsively follow instructions in real-time in a\nvariety of situations. We believe that such use-cases, where humans give an agent natural instructions\nthat it can follow to complete tasks, will become increasingly important and have practical uses in the\ncreation of instructable assistants and virtual-world characters. These videos, as well as videos of our\nagent performing our evaluation tasks, can be found at https:\/\/sites.google.com\/view\/steve-1.\n4.3\nPrompt Chaining\nWe also experiment with longer horizon tasks that require multiple steps, such as crafting and building.\nWe explore two different prompting methods: directly prompting with the target goal, and a simple\nform of prompt chaining [11, 64, 16] where the task is decomposed into several subtasks and the\n7\n0\n1000\n2000\n3000\nEpisode Timestep\n0\n5\n10\n15\n20\n25\n30\nDirt in Inventory\n0\n1000\n2000\n3000\nEpisode Timestep\n0\n2\n4\n6\n8\nLogs in Inventory\n20M\n40M\n60M\n80M\n100M\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBuild Tower Success\nPrompt Chaining\nDirect Prompting\n20M\n40M\n60M\n80M\n100M\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWooden Planks\nCrafting Success\n55\n60\n65\n70\n75\n80\n85\nAgent Height Position\nBuild a Tower\n0\n2\n4\n6\n8\n10\n12\n14\nPlanks in Inventory\nMake Wooden Planks\nNumber of Frames\n100\n200\n300\nTravel Distance (Blocks)\n0\n2\n4\n6\nSeeds Collected\nrelevant\nirrelevant\n20M\n40M\n60M\n80M\n100M\n0\n5\n10\n15\n20\nWooden Logs Collected\n20M\n40M\n60M\n80M\n100M\n0\n20\n40\n60\nDirt Collected\nNumber of Frames\nFigure 4: Top left: By sequentially chaining visual prompts like “get dirt” and “build a tower”,\nSTEVE-1 successfully gathers dirt and then uses this dirt to build a tower. The prompts switch at\nthe dotted vertical line. Bottom left: The success rates of the chained prompts improve steadily as\nwe train STEVE-1 on more data. Right: The performance of STEVE-1 on different tasks scales in\ndifferent ways when conditioning on a relevant visual prompt for the metric versus other irrelevant\nvisual prompts (e.g., the break wood prompt is the relevant prompt for the “Wooden Logs Collected”\nmetric, while the other prompts are “irrelevant”). For instance, in the wood-collection and dirt-\ncollection tasks, performance starts increasing after training on 60M frames of gameplay. See\nFigure 14 for sample frames from each visual prompt.\nprompts are given sequentially for a fixed number of steps. We explore prompt chaining with visual\ngoals for two tasks: 1) building a tower and 2) making wooden planks. When using prompt chaining,\nwe first prompt STEVE-1 to gather dirt before building a tower, and to gather wooden logs before\ncrafting wooden planks. Figure 4 shows that directly prompting STEVE-1 with the final tasks results\nin near-zero success rates. However, prompt chaining allows STEVE-1 to build a tower 50% of the\ntime and craft wooden planks 70% of the time. For the tower building task, STEVE-1 immediately\nstarts collecting dirt until the prompt switches, at which point its average height starts increasing\nrapidly and its dirt decreases as it builds a tower. Similarly, for the crafting wooden planks task,\nSTEVE-1 immediately starts collecting a large amount of wooden logs until the prompt switches and\nit rapidly converts these wooden logs into wooden planks (causing the amount of wooden logs in its\ninventory to immediately decrease and the number of wooden planks to increase as it crafts more).\nFigure 4 visualizes the average item counts and agent height for the prompt chaining episodes. See\nFigure 18 and Figure 19 in the appendix for visualizations of specific prompt chaining episodes.\n4.4\nScaling\nRecent works in language modeling have found that scaling up pretraining FLOPs, by training on\nmore data or by training a model with more parameters, can improve performance on downstream\ntasks [28, 57, 63]. In certain cases when measuring performance with metrics such as exact-match\n[52], performance improvement may appear to be “emergent” [63], appearing suddenly as the model\nis trained with more compute. Here, we aim to gain a basic understanding of how the performance\nof STEVE-1 on various tasks scales by training with more data (learning rate schedule is chosen\nappropriately).\nTo assess performance gain, we isolate the performance of the policy from the prior, measuring\nperformance of the agent on programmatic tasks (travel distance, seeds, logs, dirt) with visual goals.\nDue to compute constraints, we chose to use the 2x VPT model, which has 248M parameters. We\nfound that both seed collection and travel distance did not improve significantly past 20M frames.\nFrom inspecting gameplay, we suspect that travel distance is a relatively easy task since it is close to\nVPT’s default behavior of running around and exploring. For seed collection, performance remains\nsuboptimal, suggesting that further scaling may be beneficial. This hypothesis is supported by the\nobservation that performance on log and dirt collection remained roughly level until 60M frames when\nit began to rapidly improve. Figure 4 shows the scaling curves for STEVE-1 on each programmatic\ntask when conditioning on relevant vs. irrelevant visual prompts for that task. Since we do not\nobserve regression on any tasks as we train the model with more compute, we expect the model to\ncontinue to perform better as we train larger models on larger datasets.\n8\nWe also evaluated the scaling properties of STEVE-1 for our multi-step tasks with and without prompt\nchaining. Without prompt chaining, the tasks remain challenging for STEVE-1 throughout training.\nHowever, we note that after 60M frames, STEVE-1 sometimes gathers wooden logs and then builds\na small tower when directly prompted to build a tower. This is likely because our visual prompt\nfor tower building shows a video of a tower being built out of wooden logs. With prompt chaining,\nthe performance of STEVE-1 steadily increases with more data. We conjecture that this is because\nthe success of a chained prompt requires the success of each element in the chain. Since different\nabilities emerge at different scales, one would expect chained prompts to steadily get more reliable\nas these subgoals become more reliably completed. In the case of crafting wooden planks, we note\nthat crafting is one such task that gets significantly more reliable as the agent is trained on more data.\nFigure 4 shows the scaling curves for STEVE-1 on the prompt chaining tasks.\nIn summary, we see evidence of tasks that do not require much data for STEVE-1 to learn, tasks\nthat steadily get more reliable as the agent is trained longer, and tasks where capability suddenly\nspikes after the agent reaches some threshold. Put together, this suggests that further scaling would\nlikely significantly improve the agent, although we leave the task of predicting exactly how much\nperformance there is to gain to future studies.\n4.5\nWhat Matters for Downstream Performance?\nPretraining\nBaker et al. [5] finds that by pretraining a behavioral prior with imitation learning on\ninternet-scale datasets for Minecraft, the learned policy can be effectively finetuned to accomplish\ntasks that are impossible without pretraining. In this section, we demonstrate that pretraining is\nalso massively beneficial for instruction-tuning in Minecraft. We hypothesize that due to the strong\nperformance of STEVE-1 and the relatively small amount of compute (≈1% additional compute)\nused for instruction finetuning, most of the capabilities of our agent come from the pretraining rather\nthan the finetuning. To test this hypothesis, we finetune several varients of STEVE-1 from various\npretrained weights: foundation-2x, bc-early-game-2x, rl-from-foundation-2x, and with\nrandomly initialized weights. In this experiment, each model was finetuned on 100M frames.\nFigure 5 shows the performance of these models on our programmatic tasks with visual goals. Note\nthat while an agent trained on our dataset from scratch can accomplish basic tasks like dirt collection\nfairly well, it is unable to find and chop down trees, in contrast to the pretrained agents. This demon-\nstrates that the abilities present in the agent due to pretraining are successfully transferred to the fine-\ntuned agent. Out of all the pretrained weights we tried, we noticed that rl-from-foundation-2x\nperformed the best, having qualitatively better performance at tasks like crafting and chopping down\ntrees. Indeed, Figure 5 shows that this model has strong performance, likely due to the massive\namount of compute it was trained with during its RL training [5].\nClassifier-Free Guidance\nBaker et al. [5] observed that when conditioning the agent on text, it\ntended to ignore its instruction and instead perform the prior behavior learned during pretraining.\nAs discussed in Section 3.4, classifier-free guidance [23] gives a knob for trading off between goal-\nconditioned and prior behaviors. Figure 5 shows the effect of this parameter λ on the log and dirt\ncollection tasks. The performance of the agent reaches its maximum around λ = 5.0 to λ = 7.0,\nafter which it starts to drop off. These results demonstrate the importance of classifier-free guidance,\nwhich improves the performance of STEVE-1 by orders of magnitude.\nPrompt Engineering\nPrompt engineering as a discipline has rapidly emerged over the last year\ndue to the observation that the quality of the output of text-to-X models can dramatically change\ndepending on the prompt [67]. For example, Table 5 in the appendix shows how a prompt for Stable\nDiffusion [50] might be written. By listing out the various attributes of the image such as visual\nmedium, style, and the phrase “trending on ArtStation”, the user is able to get a higher quality image\n[20, 36]. In this section, we explore how this same style of prompt engineering can improve the\nperformance of STEVE-1. Figure 6 shows how a simple prompt of “get dirt” might be changed in\norder to more accurately specify the type of behavior that is desired. Just like in image generation\nmodels, the performance of STEVE-1 significantly improves by modifying the prompt in this fashion.\nBy changing to more complicated prompts, STEVE-1 is able to collect 1.6× more wood, 2× more\ndirt, and 3.3× more seeds.\n9\nscratch\nfd\nbc\nrl\n0\n5\n10\nWooden Logs Collected\nscratch\nfd\nbc\nrl\n0\n25\n50\n75\nDirt Collected\n0\n2\n4\n6\n8\n10\n0\n10\n20\nWooden Logs Collected\n0\n2\n4\n6\n8\n10\n20\n40\n60\nDirt Collected\nConditional Scale λ\nPretrained Weights\nFigure 5: Left: We trained STEVE-1 on 100M frames starting from four different pretrained\nweights: random initialization (scratch), foundation-2x (fd), bc-early-game-2x (bc), and\nrl-from-foundation-2x (rl). The rl-from-foundation-2x agent is generally the most per-\nformant after fine-tuning. Using pretrained weights performs better than training from scratch,\nespecially for more complicated tasks like collecting wood. Right: By using classifier-free guidance\n[23], STEVE-1 collects 7.5× more dirt and 15× more wood than when λ = 0 (no guidance). See\nFigure 17 in the Appendix for similar results with other programmatic tasks.\nPrompt\nDirt Collected\n“break a flower”\n0.7 (-0.2, 1.6)\n“collect seeds”\n2.7 (0.9, 4.5)\n“dig as far as possible”\n3.9 (2.8, 5.0)\n“get dirt”\n9.2 (5.7, 12.7)\n“get dirt, dig hole, dig dirt, gather a ton of dirt, collect dirt”\n26.7 (19.9, 33.5)\nFigure 6: Similar to text-to-image generation, switching to a longer, more specific prompt dramatically\nimproves the performance of STEVE-1. Values in parentheses are 95% confidence intervals.\n5\nLimitations and Conclusion\nIn this paper, we present a methodology for creating instruction-following foundation models of\nbehavior. Specifically, by leveraging two existing pretrained foundation models: a behavioral prior\n(VPT [5]) and a domain-specific CLIP model (MineCLIP [17]), we create a powerful Minecraft agent\nthat can follow short-horizon open-ended text and visual instructions, all for only $60 of compute.\nThe resulting foundation model, STEVE-1, sets a new bar for open-ended instruction-following in\nMinecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming\nprevious baselines and robustly completing 12 of 13 tasks in our early-game evaluation suite. We note\nthat generalist agents such as STEVE-1 can have potential negative effects on society. We include a\nthorough discussion of these issues in Appendix A.\nSTEVE-1 is a significant advancement in creating generative models of text-to-behavior, but it has\nseveral limitations, as described in Appendix B. First, STEVE-1 is mostly proficient at achieving\nshort-horizon tasks while struggling with longer-horizon tasks. While prompt chaining is a promising\napproach for improving performance on complex tasks, more can be done in future work to improve\nperformance. Another limitation we observe is that prompt engineering, as with other generative\nmodels, can be unintuitive and time-consuming. Future work should investigate improving the\nsteerability of STEVE-1 through a better understanding of natural language prompts. Additionally,\nwe note that evaluating and describing the capabilities of open-ended generalist agents is an open\nresearch problem itself since capability depends strongly on preconditions, prompt engineering, and\nour own ability to come up with varied and challenging tasks. Finally, since our approach is not\nspecific to the Minecraft domain, we hope that the method used to create STEVE-1 can inspire future\nwork in creating powerful generalist agents in other domains and environments.\n10\nAcknowledgements\nAll of the authors gratefully acknowledge funding for this research from the Natural Sciences and\nEngineering Research Council of Canada (NSERC) and the Canada CIFAR AI Chairs Program\n(Vector Institute for Artificial Intelligence). SL is supported by a Vector Institute internship and by\nan NSERC Discovery Grant. KP is supported by an NSERC PGS-D award. HC is supported by an\nNSERC CGS-D award. JB acknowledges funding from the Canada CIFAR AI Chairs program, Fujitsu\nJapan, and an Amazon Research Award. In addition to NSERC and CIFAR (Vector Institute), SM\nacknowledges funding from Microsoft Research. We thank Silviu Pitis, Romi Lifshitz, Forest Yang,\nand Yongchao Zhou for their helpful comments; Alisa Wu and Ziming Chen for their contributions\nto the text-video pair dataset; and Finn Paster for the logo and graphic for the website. Resources\nused in preparing this research were provided, in part, by the Province of Ontario, the Government\nof Canada through CIFAR, and companies sponsoring the Vector Institute for Artificial Intelligence\n(www.vectorinstitute.ai\/partners).\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Minecraft 中的文本到行为生成模型：STEVE-1\n\n## 📌 背景痛点\/本文动机\n构建能够响应文本指令的 AI 模型是一项挑战，尤其是在需要顺序决策的任务中。现有的模型往往需要大量带有指令标签的轨迹数据集，这既昂贵又难以获取。本文旨在解决这一问题，提出了一种无需依赖大量指令标签轨迹数据集的方法，用于构建能够响应文本指令的行为生成模型。\n\n## 🚀 核心方法\n💡 创新点1：受 unCLIP 启发的指令微调方法\n本文提出了一种受 unCLIP 启发的指令微调方法，用于构建行为生成模型。该方法将问题分解为两个模型：一个用于生成行为轨迹的策略模型，另一个用于将文本指令转换为视觉嵌入的先验模型。通过这种方式，可以避免使用昂贵的文本指令标签，而是利用视觉嵌入进行训练。\n\n💡 创新点2：基于 VPT 和 MineCLIP 的模型构建\n本文利用了两个预训练模型：VPT 和 MineCLIP。VPT 是一个基于 Minecraft 游戏视频预训练的行为模型，而 MineCLIP 是一个将文本和视频片段对齐的模型。通过将这两个模型结合起来，可以构建一个能够响应文本指令的行为生成模型。\n\n💡 创新点3：基于自监督学习和回溯重标记的微调\n本文使用自监督学习和回溯重标记技术对 VPT 模型进行微调。通过这种方式，可以减少对昂贵的人类文本注释的需求，并利用现有的数据集进行训练。\n\n## 📈 实验结果\n实验结果表明，STEVE-1 在 Minecraft 中能够有效地响应文本指令，并完成各种任务。与之前的基线模型相比，STEVE-1 在低级控制（鼠标和键盘）和原始像素输入方面取得了显著的性能提升。此外，实验还表明，预训练、分类器无关引导和数据缩放等因素对下游性能至关重要。\n\n## 💬 可借鉴之处\n本文提出的指令微调方法可以应用于其他领域和任务，例如机器人控制、虚拟现实等。此外，本文还强调了预训练、分类器无关引导和数据缩放等因素对下游性能的重要性，为构建更强大的 AI 模型提供了参考。","llm_summary_res_status":200}
{"title":"Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction","authors":"Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, Yitao Liang","summary":"We study the problem of learning goal-conditioned policies in Minecraft, a\npopular, widely accessible yet challenging open-ended environment for\ndeveloping human-level multi-task agents. We first identify two main challenges\nof learning such policies: 1) the indistinguishability of tasks from the state\ndistribution, due to the vast scene diversity, and 2) the non-stationary nature\nof environment dynamics caused by partial observability. To tackle the first\nchallenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage\nthe emergence of goal-relevant visual state representations. To tackle the\nsecond challenge, the policy is further fueled by an adaptive horizon\nprediction module that helps alleviate the learning uncertainty brought by the\nnon-stationary dynamics. Experiments on 20 Minecraft tasks show that our method\nsignificantly outperforms the best baseline so far; in many of them, we double\nthe performance. Our ablation and exploratory studies then explain how our\napproach beat the counterparts and also unveil the surprising bonus of\nzero-shot generalization to new scenes (biomes). We hope our agent could help\nshed some light on learning goal-conditioned, multi-task agents in challenging,\nopen-ended environments like Minecraft.","url":"http:\/\/arxiv.org\/abs\/2301.10034v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2301.10034v3","published":1674288938000,"comment":"This paper is accepted by CVPR2023","pdf_text":"Open-World Multi-Task Control Through\nGoal-Aware Representation Learning and Adaptive Horizon Prediction\nShaofei Cai1,2, Zihao Wang1,2, Xiaojian Ma3, Anji Liu3, Yitao Liang1,4\nTeam CraftJarvis\n1Institute for Artificial Intelligence, Peking University\n2School of Intelligence Science and Technology, Peking University\n3Computer Science Department, University of California, Los Angeles\n4Beijing Institute for General Artificial Intelligence (BIGAI)\n{caishaofei,zhwang}@stu.pku.edu.cn,xiaojian.ma@ucla.edu\nliuanji@cs.ucla.edu,yitaol@pku.edu.cn\nAbstract\nWe study the problem of learning goal-conditioned poli-\ncies in Minecraft, a popular, widely accessible yet challeng-\ning open-ended environment for developing human-level\nmulti-task agents. We first identify two main challenges of\nlearning such policies: 1) the indistinguishability of tasks\nfrom the state distribution, due to the vast scene diversity,\nand 2) the non-stationary nature of environment dynamics\ncaused by partial observability. To tackle the first challenge,\nwe propose Goal-Sensitive Backbone (GSB) for the policy\nto encourage the emergence of goal-relevant visual state\nrepresentations. To tackle the second challenge, the pol-\nicy is further fueled by an adaptive horizon prediction mod-\nule that helps alleviate the learning uncertainty brought by\nthe non-stationary dynamics. Experiments on 20 Minecraft\ntasks show that our method significantly outperforms the\nbest baseline so far; in many of them, we double the perfor-\nmance. Our ablation and exploratory studies then explain\nhow our approach beat the counterparts and also unveil the\nsurprising bonus of zero-shot generalization to new scenes\n(biomes). We hope our agent could help shed some light on\nlearning goal-conditioned, multi-task agents in challeng-\ning, open-ended environments like Minecraft. The code is\nreleased at https:\/\/github.com\/CraftJarvis\/\nMC-Controller.\n1. Introduction\nBuilding agents that can accomplish a vast and diverse\nsuite of tasks in an open-ended world is considered a key\nchallenge towards devising generally capable artificial in-\ntelligence [2, 3, 6, 35]. In recent years, environments like\nMinecraft have drawn much attention from the related re-\ncombat pig\nharvest poppy\nharvest wood\ncombat sheep\npick-place\nwindow-open\nbox-close\nstick-push\nMinecraft\nMeta-world\nFigure 1. Comparison of states between Meta-world [49] (left) and\nMinecraft [24] (right) based on t-SNE visualization. The points\nwith the same color represent states from the trajectories that com-\nplete the same task. It can be seen that the states are much more\ndistinguishable in terms of tasks in Meta-world than in Minecraft,\nimplying the higher diversity of states and tasks in open worlds\nlike Minecraft over traditional multi-task agent learning environ-\nments like Meta-world.\nsearch communities [16,18–20,26], since they are not only\npopular, and widely accessible, but also offer an open-\nended universe with myriad of tasks, making them great\nplatforms for developing human-level multi-task agents.\nAlthough groundbreaking successes have been observed\nin many challenging sequential decision-making problems\nsuch as Atari[32], Go[39], and MOBA games[13, 44, 45],\nsuch successes have not been transferred to those open\nworlds. To understand the gap and design corresponding so-\nlutions, we need to first understand the distinct challenges\n1\narXiv:2301.10034v3  [cs.AI]  12 Oct 2023\nbrought by these environments. Let’s take Minecraft [24]\nas an example: there are over twenty types of landscapes\nranging from flat lands like Savannah and desert to rough\nmountains with forests and caves.\nThese diverse land-\nscapes also enable countless tasks that could be achieved by\nthe agents: mining, harvesting, farming, combating, con-\nstructing, etc. Compared to canonical agent learning en-\nvironments like Go [39], Atari [32], and robotic control\nsuite [41, 43, 48], Minecraft provides a substantially more\ndiverse distribution of states thanks to the rich scenes and\ntasks built with the game, making it exceptionally diffi-\ncult to extract the pivotal task-relevant visual state repre-\nsentations for goal-conditioned policies. To help our read-\ners understand the significance of this challenge, we visual-\nize the states from trajectories that complete some tasks in\nMinecraft and Meta-world [48] (a popular multi-task learn-\ning environment but with fewer states and tasks) in Fig. 1.\nStates of different tasks are annotated with different colors.\nClearly, the states in Minecraft are much less distinguish-\nable in terms of tasks than in Meta-world. Therefore goal-\nconditioned policies are more likely to struggle in mapping\nthose states and tasks (served as goals) to actions.\nAnother grand challenge in an open-ended environment\nlike Minecraft hails from the setting of such games, where\nan agent can only have very limited observations of the\nworld.\nFor example, in MineDoJo [16] (a recent agent\nbenchmark built on Minecraft), the observation space com-\nprises a first-person view image and a list of possessed\nitems. However, many more aspects of the surroundings re-\nmain hidden from the agents. That is, the agent now has to\nwork with a partially observable environment. A plague\nembedded with such an environment is non-stationary dy-\nnamics, which makes it almost impossible to predict what\nwill happen next. Therefore, the distances from states to the\ncurrent goal become much less clear due to the world un-\ncertainty, leading to less distinguishable states in terms of\ngoal completeness and more faulty decisions emitted by the\ngoal-conditioned policies.\nThis paper aims at mitigating both aforementioned chal-\nlenges that emerge from most open-world environments.\nFirst, we observe that the architecture of the policy network\nis crucial to learning goal-relevant visual state representa-\ntions that allow goal-conditioned actions in domains with\nlow inter-goal state diversity (cf. Fig. 1). To this end, we\npropose Goal-Sensitive Backbone (GSB), which enables ef-\nfective learning goal-conditioned policies over 20 tasks in\nthe Minecraft domain. Next, to mitigate the challenge posed\nby the partially observed and non-stationary environment,\nwe introduce horizon as an extra condition for the policy\nand a corresponding horizon prediction module. Specifi-\ncally, the policy is also explicitly conditioned on the remain-\ning time steps till achieving certain goals (i.e., distance-to-\ngoal). We find it significantly boosts the performance of\nour agents in open-world multi-task domains. However, the\nground-truth distance-to-goal is unavailable during evalu-\nation. To fix this problem, we train a horizon prediction\nmodule and feed the estimated distance-to-goal to the hori-\nzon commanding policy in evaluation. This leads to a 27%\ngain in average success rate under the multi-task settings.\nWe evaluate the proposed approaches based on the sim-\nple yet effective behavior cloning algorithm [10]. The ex-\nperiments are conducted in three common biomes. In multi-\ntask settings, our proposed method outperforms the base-\nline in terms of success rate and precision by a large mar-\ngin. It also achieves consistent improvement in single-task\nsettings. Our ablation and exploratory studies then explain\nhow our approach beat the counterparts and also unveil the\nsurprising bonus of zero-shot generalization to new scenes\n(biomes).\nTo summarize, targeting two identified challenges dis-\ntinct to open worlds, our contributions are threefold:\n• We propose Goal-Sensitive Backbone (GSB), a neural\nnetwork that enables effective learning goal-relevant vi-\nsual state representations at multiple levels for goal-\nconditioned policies, aiming at addressing the challenge\nof diverse state distribution in open-ended environments.\n• We further introduce adaptive horizon prediction to ex-\nplicitly condition the policy on the distance from the cur-\nrent state to the goal, yielding much better performances\nin a partially observable open-ended environment with\nnon-stationary dynamics.\n• We conduct extensive studies on the popular yet challeng-\ning Minecraft domain with baselines and our proposed\nmethod. The results demonstrate superior advantages of\nour approach over the counterparts in terms of both suc-\ncess rate and precision of task completion.\n2. Preliminaries\nGoal-conditioned policy, as its name suggests, is a type of\nagent’s policy π for decision-making that is conditioned on\ngoals besides states. Specifically, we denote π(a|s, g) as\na goal-conditioned policy that maps the current state s and\ngoal g to an action a. Compared to the canonical formula-\ntion of policy where the goal is absent, the goal-conditioned\npolicy offers flexibility of learning multi-task agent as it al-\nlows different behaviors for different tasks by simply alter-\ning the goal. There are multiple ways to specify the goal,\ne.g., natural language instructions [2] and goal images [36].\nGoal-conditioned imitation learning is a simple yet ef-\nfective way to learn goal-conditioned policies. Specifically,\nπ(a|s, g) is optimized by imitating the demonstrations D,\nwhere D = {τ 1, τ 2, τ 3, . . . } is a collection of trajectories\nτ i. A trajectory is a sequence of states, actions, and goals,\ndefined as τ i = {(si\nt, ai\nt, gi)}T\nt=0, where T is the trajectory\nlength. The imitation learning objective is to maximize the\n2\nCross-biome\nEnvironment\nSingle-biome\nEnvironment\nkill sheep in Snowy Plains\nchop tree in Plains\nkill sheep in Plains\nFigure 2. Demonstrations of the cross-biome environment and\nthe more challenging single-biome environment. The challenge\ncomes from the fact that the agent needs to learn diverse behaviors\nin similar states conditioned on different goals.\nlikelihood of the action in demonstrations when attempting\nto reach the desired goal\nJIL(π) = Eτ∼D\n\u0002 XT\nt=0 log π(at|st, g)\n\u0003\n.\n(1)\nNotation. At each timestep, our architecture takes in a tu-\nple (st, at, ht, g, at−1) as the input, where st = {oI\nt , oE\nt },\noI\nt is the raw image observation, oE\nt is the extra observation\nprovides by the environments. ht comes from the demon-\nstration. ˜ht and ˜at are the predicted horizon and action,\nrespectively. For simplicity, we also use the same symbols\n(oE\nt , g, at−1) to represent their embeddings.\n3. Method\nIn this section, we describe the proposed algorithm for\nlearning goal-conditioned policies that are capable of com-\npleting various preliminary tasks in open-world domains.\nFirst, we revisit and provide a detailed illustration of the\nidentified challenges in open-world domains (§3.1). Aim-\ning at solving these challenges, we proceed to introduce\nthe proposed goal-sensitive backbone (§3.2) and adaptive\nhorizon prediction module (§3.3). Finally, we provide an\noverview of the proposed method in Section 3.4.\n3.1. Challenges\nAs demonstrated in Section 1, the first major challenge\nof open-world environments is the indistinguishability of\nstates in terms of different goals (cf. Fig. 1). That is, it is\noften hard to identify the task\/goal by looking at individual\nstates. Compared to environments with clear goal indicators\nin their states, agents in open-world domains need to learn\ngoal-conditioned diverse behaviors under similar states.\nThis challenge can be reflected by the illustrative exper-\niment in Fig. 2. Two multi-task environments are created\nbased on the Minecraft domain. Both environments consist\nof two preliminary tasks: collect logs and hunt sheep, where\nthe former can be done by chopping trees and the latter re-\nquires the agent to slaughter sheep. Both tasks require the\nagent to first locate and approach the corresponding target.\nAs shown in Fig. 2 (center), in the single-biome environ-\nment (blue blob in Fig. 2), the agent is tasked to collect\nlogs and hunt sheep both inside a randomly generated plain\narea with grass, trees, and various mobs. In contrast, in\nthe cross-biome environment (red blob in Fig. 2), whenever\nthe agent is tasked to hunt sheep, it is spawned randomly\nin a snowy plain. Although different in visual appearance,\nsnowy plains and plains have very similar terrains, so the\ndifficulty of each task in the cross-biome environment is\nsimilar to its counterpart in the single-biome environment.\nThe main consequence of this change is that the agent can\ndetermine its goal by solely looking at the current state,\nwhich mimics the setting of Meta-World in Fig. 1(left).\nWe collect demonstrations by filtering successful trajec-\ntories played by VPT [4] (see §4.1 for more details) and use\nbehavior cloning to train multi-task policies on both envi-\nronments. Perhaps surprisingly, as shown in Fig. 2, despite\nthe minor difference, performance in the single-biome envi-\nronment is significantly weaker than in the cross-biome one.\nThis clearly demonstrates that the common practice of di-\nrectly concatenating observation features and goal features\nsuffer from learning diverse actions (e.g., locate trees, find\nsheep) given similar observations. In contrast, in the cross-\nbiome environment, the difficulty of the two tasks funda-\nmentally remains the same, yet the agent only needs to learn\na consistent behavior in each biome (i.e., plains and snow\nfields). This alleviates the need to learn goal-conditioned\ndiverse behaviors in similar states and leads to a better suc-\ncess rate.\nThe second key challenge comes from the partial ob-\nservability of the game and non-stationary environment dy-\nnamics.\nSpecifically, in Minecraft, the biome and mobs\nsurrounding the agent are generated procedurally and ran-\ndomly after each reset. Further, only a small fraction of the\nwhole terrain is visible to the agent in one observation, lead-\ning to more uncertainty of the world. From the perspective\nof learning goal-conditioned policies, the distances from\nstates to the current goal will become much less clear com-\npared to canonical learning environments like Atari [12].\nWe refer to Appendix B for more discussion on this. Since\nthe goal-conditioned policies also rely on distinguishable\nstates in terms of goal completeness, they’re more likely to\nmake wrong decisions as a result of world uncertainty.\n3\nGoal Space\nHunt a cow\nShear a sheep\n…\nChop Trees\nAction Space\nExtra Observation\nMove\nCam\nAttack\nUse\nImage Observation\nCompass GPS\nBiome\nVoxels\n𝒐!\n\"\n𝒈\n𝒐!\n#\n𝒂!$%\nEmbed\nEmbed\nEmbed\nℎ%!\nℎ!\nEmbed\n𝒂&!\n𝒂!\nℒ!\nℒ\"\nGSB\n𝑰!\n&\n𝒇!\nTraining\n𝜇\n𝜋!\nConcantenate\nHorizon Loss\n𝒐!\n\"\n𝒈\n𝒐!\n#\n𝒂!$%\nEmbed\nEmbed\nEmbed\nℎ%!\nℎ)!\nEmbed\n𝒂!\nGSB\n𝑰!\n&\n𝒇!\nEvaluation\n𝜇\n𝜋!\nConcantenate\nAdjust\nAdaptive Horizon \nPrediction\nGoal-Sensitive Backbone (GSB)\n𝒙(()\n𝒈\nFC\nReLU\nFC\nConv\nReLU\nConv\n𝒙((*%)\nReLU\nSigmoid\n×\n𝒐!\n\"\n𝒈\nConv\nMax\nG-Conv\nBlock\nG-Conv\nBlock\n×𝟑\n+\nFigure 3. Our Goal-conditioned Policy Architecture. Our contributions are in red and purple. Right: The goal-sensitive backbone\n(GSB) is a key component to incentivize goal-condition behaviors. It consists of a stack of g-conv blocks. It takes the image observation\noI\nt and the goal embedding g as input, and outputs the goal-attended visual representation Ig\nt . The multimodal joint representation f t is\nthe concatenation of visual representation Ig\nt , goal embedding g, extra observation embedding oE\nt and previous action embedding at−1.\nThe horizon prediction module µ uses it to predict the horizon ˜ht while the horizon commanding policy πθ uses it to predict the action ˜at.\nTop: During the training, the predicted horizon ˜ht is only used to compute the horizon loss Lh. The policy is conditioned on ht that comes\nfrom the demonstration. Bottom: During the evaluation, the policy is conditioned on the predicted horizon ˜ht which needs to be adjusted.\n3.2. Incentivize Goal-Conditioned Behavior with\nStacked Goal-Sensitive Backbone\nAs elaborated in Section 3.1, learning goal-conditioned\npolicies becomes extremely hard when states collected from\ntrajectories that accomplish different tasks are indistin-\nguishable. While certain algorithmic design choices could\nimprove multi-task performance in such open-world envi-\nronments, we find that the structure of the policy network\nis a key factor towards higher episode reward.\nSpecifi-\ncally, we observe that existing CNN-based backbones can\nexcel at completing many single tasks (e.g., hunt cow, col-\nlect stone), but struggle to learn goal-conditioned behavior\nwhen training on the tasks in a goal-conditioned manner.\nThis motivates the need to properly fuse goal information\ninto the network. Despite the existence of various feature\nfusion approaches such as concatenation and Bilinear lay-\ners [27], they all perform poorly even with a moderate num-\nber of tasks. This motivates the need to carry goal informa-\ntion into multiple layers of the network. Specifically, we\npropose goal-sensitive backbone (GSB), which effectively\nblends goal information to the state features at multiple lev-\nels. As shown in Fig. 3 (right), GSB is composed with mul-\ntiple goal convolution blocks (g-conv block), which are ob-\ntained by augmenting the vanilla convolution block with a\ngoal branch. Functionally, it can provide deep feature fu-\nsion between multi-level visual features and the goal infor-\nmation. As we will proceed to show in Section 4.3, adding\nGSB can lead to significant performance boost in multi-task\nenvironments. The g-conv block processes its input visual\nfeatures x(l) ∈RC×H×W with two convolution layers\nˆx(l) = ReLU(Conv(ReLU(Conv(x(l))))).\n(2)\nMeanwhile, it maps the goal embedding g to the same fea-\nture space as the intermediate features ˆx(l) with two fully-\nconnected layers, decribed as\nˆg(l) = FC(ReLU(FC(g))).\n(3)\nThe goal feature ˆg(l) is then used to modulate the interme-\ndiate features ˆx(l) channel-wise. By adding a residual con-\nnection [21], the output feature x(l+1) is expressed by\nx(l+1) = σ(ˆg(l)) ⊙ˆx(l) + x(l),\n(4)\nwhere σ(·) is the sigmoid function and ⊙is the element-\nwise product.\nThis channel-wise modulation encourages\nthe module to focus on goal-specific regions and discard the\nbackground information by adaptively weighing the chan-\nnel importance.\nWe highlight that the g-conv block can\nbe plugged into any convolution backbone to improve its\ncapability of extracting goal-aware visual features.\nThe\nproposed goal-sensitive backbone is constructed by replac-\ning 6 convolution blocks of the widely-adopted Impala\nCNN [14] to g-conv blocks. In our experiments, a GSB\nis used to compute goal-conditioned state features Ig\nt =\nGSB(oI\nt , g). Such an idea of fusing condition information\ninto the backbone layer by layer was also used by some\nprior works [5, 22, 33, 34]. Here, we demonstrate that it\nworks in a critical role for open-world multi-task control.\n4\n3.3. Combat World Uncertainty with Adaptive\nHorizon Prediction\nTo address the challenge brought by the uncertainty of\nthe world, we need to ensure the goal-conditioned policies\nto be more aware of goal-completeness given the current\nstate.\nWe observe that conditioning the policy addition-\nally on the number of remaining steps toward achieving a\ngoal, i.e., distance-to-goal, or horizon, can significantly im-\nprove the accuracy of predicted actions on held-out offline\ndatasets [17,37]. Here, we define the horizon ht := T −t,\nwhere T is the trajectory length, as the remaining time steps\nto complete the given goal. This motivates the design of a\nhorizon commanding policy πθ : S × G × H →A that\ntakes a state s, a goal g, and a horizon h as inputs and out-\nputs an action a. A key problem of the horizon commanding\npolicy is that it cannot be directly used for evaluation: dur-\ning gameplay, horizon is unknown as it requires completing\nthe whole trajectory. To fix this problem, we introduce an\nadditional horizon prediction module, which estimates the\nhorizon given a state s and a goal g. Combining the two\nmodules together, we can apply the fruitful horizon com-\nmanding policy during gameplay.\nBoth modules can be trained efficiently with dense su-\npervision. Specifically, the horizon commanding policy πθ\ncan be learned by any policy loss specified by RL algo-\nrithms. For example, when behavior cloning is used, πθ\ncan be optimized by minimizing the loss\nLa = −log πθ(at|ht, f t),\n(5)\nwhere f t is the joint representation of the state and goal\nembedded by a neural network (see §3.4). The horizon pre-\ndiction module is trained by a supervised learning loss\nLh = −log µ(ht|f t),\n(6)\nwhere µ is a network that predicts the horizon.\nDuring the evaluation, after computing the embedding\nf t for st and g, the horizon prediction module µ is first in-\nvoked to compute an estimated horizon ˜ht = µ(f t). This\npredicted horizon can then be fed to the horizon command-\ning policy to compute the action distribution πθ(at|˜ht, f t).\nIn practice, we observe that feeding an adaptive version of\n˜ht, defined as ˆht := max(˜ht −c, 0) (c is a hyperparame-\nter), to πθ leads to better performance. We hypothesize that\nthis advantageous behavior comes from the fact that by sup-\nplying the adaptive horizon ˆht, the agent is encouraged to\nchoose actions that lead to speedy completion of the goal.\nThe effectiveness of the adaptive horizon will be demon-\nstrated in Section 4.3.\n3.4. Model Summary\nAs shown in Fig. 3, our model sequentially connects the\nproposed goal-sensitive backbone, horizon prediction mod-\nule, and horizon commanding policy. At each time step\n(a) Flat\n(b) Plains\n(c) Forest\nFigure 4. Snapshots of the RGB camera view in three biomes.\nt, the image observation and goal information are first fed\nforward into the goal-sensitive backbone to compute goal-\naware visual feature Ig\nt . The visual feature is then fused\nwith additional input information including the extra obser-\nvation embedding oE\nt , the goal embedding g, and the pre-\nvious action embedding at−1 by concatenation and a feed-\nforward network:\nf t = FFN(\n\u0002\nIg\nt ∥oE\nt ∥g ∥at−1\n\u0003\n).\n(7)\nThen, f t is input to the horizon prediction module to predict\nhorizon ˜ht = µ(f t). And the horizon commanding policy\ntakes in the horizon and features f t to compute the action.\nWhen trained with behavior cloning, the overall objective\nfunction is L = La + Lh. During the evaluation, the adap-\ntive horizon ˆht is fed to the horizon commanding policy in\nreplacement of ˜ht.\n4. Experiments\nThis section analyzes and evaluates the proposed goal-\nsensitive backbone and the adaptive horizon prediction\nmodule in the open-world domain Minecraft. To minimize\nperformance variation caused by the design choices in RL\nalgorithms, we build the proposed method on top of the sim-\nple yet effective behavior cloning algorithm. In Section 4.1,\nwe first introduce three suites of tasks; the agent is asked\nto collect and combat various target objects\/mobs with in-\ndistinguishable states conditioned on different goals (chal-\nlenge #1) and non-stationary environment dynamics (chal-\nlenge #2). Single-task and multi-task performance on the\nbenchmarks is evaluated and analyzed in Section 4.2, and\nablation studies are conducted in Section 4.3. Finally, we\nunveil the surprising bonus of zero-shot generalization to\nnew scenes and tasks in Section 4.4.\n4.1. Experimental Setup\nEnvironment and task. To best expose the challenges de-\nscribed in Sections 1 and 3.1, a key design principle of our\nbenchmark environments is to task the agent to complete\nmultiple preliminary tasks in similar yet highly random-\nized scenes. By specifying the biome that surrounds the\nagent, Minecraft provides a perfect way to create such en-\nvironments. Specifically, as shown in Fig. 4, every biome\nhas unique and consistent observations; randomness comes\nfrom the fact that the terrain is generated randomly in each\nepisode. To evaluate the scalability of the proposed method\nin terms of the number of tasks, we choose Plains and\n5\nForest, the two most common biomes that contain a large\nnumber of resources and mobs.\nIn addition to the two challenges, Plains and Forest\nalso add unique difficulties to learning goal-conditioned\npolicies.\nSpecifically, although we have better views in\nPlains, the resources\/targets are located further away\nfrom the agent and require more exploration. In contrast,\nthere exist more occlusions and obstacles in Forest.\nThe Plains benchmark consists of four tasks: har-\nvest oak wood (\n), and Combat sheep (\n), cow (\n),\npig (\n).\nIn the Forest benchmark, the agent is\ntasked to complete thirteen tasks: combat sheep (\n),\ncow (\n), pig (\n), harvest dirt (\n), sand (\n),\noak wood (\n), birch wood (\n), oak leaves (\n),\nbirch leaves (\n), wool (\n), grass (\n),\npoppy ( ), orange tulip ( ).\nIn addition to the above two benchmarks, we also\ntest the agent on a “hunt animals” benchmark based\non the Flat biome, which contains a flattened world.\nSpecifically, the agent needs to combat sheep (\n),\ncow (\n), pig (\n), spider (\n), polar bear (\n),\nchicken (\n), donkey (\n), horse (\n), wolf (\n),\nllama (\n), mushroom cow (\n) in the Flat environ-\nment.\nCompared to other benchmarks, the challenge of\nFlat comes from the fact that the mobs are constantly\nwondering around, which makes it hard to locate and ap-\nproach the correct target.\nWe adopt the original observation space provided by\nMineDoJo [16], which includes a RGB camera-view,\nyaw\/pitch angle, GPS location, and the type of 3 × 3 blocks\nsurrounding the agent.\nWe discretize the original multi-\ndiscrete action space provided by MineDojo into 42 discrete\nactions. Details are included in Appendix A.1.\nData collection pipeline.\nOne significant downside of\nbehavior cloning algorithms is the need for high-quality\nand densely-labeled trajectories, which often requires enor-\nmous human effort to collect. To mitigate this problem,\nwe collect goal-conditioned demonstrations by filtering suc-\ncessful trajectories from gameplays by pretrained non-goal-\nconditioned policies.\nSpecifically, we adopt Video Pre-\nTraining (VPT) [4], which is trained on tremendous amount\nof non-goal-conditioned gameplays. We rollout the VPT\npolicy in the three benchmarks and record all episodes that\naccomplishes any of the defined goals.\nThese trajecto-\nries are then converted to a goal-conditioned demonstration\ndataset. Please refer to Appendix A.2 for detailed settings\nand efficiency analysis of our data collection pipeline.\nEvaluation. During the evaluation, the maximum episode\nlength is set to 600, 600, and 300 on the Flat, Plains\nand Forest benchmarks, respectively.\nPlains and\nForest are given more time steps since, in these environ-\nments, the agent needs more time to locate and approach the\ntarget. We use Success Rate and Precision as our evaluation\nmetrics. A gameplay is successful if the agent completes the\ngoal within the episode. Precision is defined as the number\nof times the specified goal is achieved divided by the total\nnumber of goals completed in an episode. It measures how\nwell the agent can be aware of the specified goal, instead of\nsimply accomplishing any goal during gameplay.\n4.2. Experimental Results\nWe first focus on the simpler single-task learning set-\nting in order to isolate the challenge introduced by non-\nstationary dynamics and partial observability (§4.2.1). We\nthen examine whether the proposed method can better ad-\ndress both challenges by examining its multi-task perfor-\nmance (§4.2.2).\n4.2.1\nSingle task experiments\nWe select three typical tasks, i.e., harvest log, hunt cow,\nand hunt sheep, from the Plains benchmark for single-\ntask training. We compare the proposed method against\nthe following baselines. First, MineAgent [16] is an online\nRL algorithm that leverages pretrained state representations\nand dense reward functions to boost training. BC (VPT) [4],\nBC (CLIP) [16], and BC (I-CNN) [14] are variants of the be-\nhavior cloning algorithm that use different backbone mod-\nels (indicated in the corresponding brackets) for state fea-\nture extraction. The backbones are finetuned with the BC\nloss (see Appendix A.3 for more details).\nResults are reported in Table 1. First, we observe that\neven the individual tasks are extremely challenging for on-\nline RL algorithms such as MineAgent, even its networks\nare pretrained on Minecraft data.\nWe attribute this fail-\nure to its inconsistent dense reward when facing a hard-\nexploration task (e.g., the additional provided reward is not\nconsistently higher when the agent is moving closer to a\ntarget object).\nNext, compared to BC (I-CNN) that uses\na randomly initialized impala CNN model, the Minecraft-\npretrained backbones in BC (VPT) and BC (CLIP) do not\nbring any benefit. This could be caused by the lack of plas-\nticity, i.e., the ability to learn in these well-trained models,\nechoing similar findings in computer vision and RL [11].\nFinally, our approach outperforms all baseline methods, es-\npecially in terms of precision. This demonstrates that our\nmethod is more robust against non-stationary dynamics and\npartially observable observations.\n4.2.2\nMulti-task experiments\nWe move on to evaluate the proposed method on the three\nmulti-task benchmarks introduced in Section 4.1. The base-\nline includes three behavior cloning methods (we use “MT-\nBC” as an abbreviation of multi-task behavior cloning). We\nalso include two variations of our method: one without the\ngoal-sensitive backbone, and the other without the adaptive\n6\nTable 1. Results of single-goal tasks (§4.2.1) on Plains.\nMethod\nSuccess Rate (%)\nPrecision (%)\nMineAgent [16] 00±00 01±00 01±00\n–\n–\n–\nBC (CLIP) [16]\n18±06 26±05 25±06 51±08 43±08 44±05\nBC (VPT) [4]\n22±08 27±06 22±06 58±09 46±05 42±05\nBC (I-CNN) [14] 45±05 46±04 48±07 86±05 55±12 45±07\nOurs\n50±07 58±10 60±08 83±10 75±10 75±06\nTable 2. Results of multi-goal tasks (§4.2.2) on three biomes.\nMethod\nAvg. Success Rate (%) Avg. Precision (%)\nPlains\nFlat\nForest\nPlains\nFlat\nForest\nMT-BC (VPT) [4]\n25±06 17±05 15±04 22±05 17±03 14±04\nMT-BC (CLIP) [16] 22±05 14±03 14±03 23±04 15±03 13±03\nMT-BC (I-CNN) [14] 25±02 18±02 15±03 23±04 14±02 13±03\nMT-BC (w\/ GSB)\n32±05 36±03 19±05 43±06 36±02 17±03\nOurs (I-CNN)\n31±06 31±04 18±02 22±03 28±04 15±04\nOurs (w\/ GSB)\n55±09 57±09 30±06 70±09 50±06 29±06\nhorizon prediction module. Results on the Plains, Flat,\nand Forest environments are reported in Table 2, respec-\ntively. First, we observe that our method significantly out-\nperforms all baselines in terms of both success rate and pre-\ncision in all three benchmarks. Moreover, scaling up the\nnumber of tasks does not necessarily deteriorate the per-\nformance of our method. Specifically, we compare the av-\nerage success rate on the Plains and Flat benchmark,\nwhich contain 4 and 9 tasks, respectively. While the base-\nlines struggle to maintain their success rate on the Flat\nenvironment, our approach is capable of maintaining high\nperformance despite the increased number of tasks. Putting\ntogether, results on multi-task benchmarks clearly demon-\nstrate the superiority of our method when facing open-world\nenvironments with the two elaborated challenges (cf. §3.1).\n4.3. Ablation Study\nAblation study on goal-sensitive backbone. To examine\nthe effectiveness of our proposed goal-sensitive backbone,\nwe compare the following two groups of architectures: 1)\nOurs (I-CNN) v.s.\nOurs (w\/ GSB), 2) MT-BC (I-CNN) v.s.\nMT-BC (w\/ GSB). The key distinction between the groups is\nwhether the backbone employs a standard Impala CNN or\na goal-sensitive backbone. As depicted in Table 2, our find-\nings indicate that the goal-sensitive backbone consistently\nenhances performance in terms of both success rate and pre-\ncision across all environments. Remarkably, in the Flat\nbiome, our approach with the goal-sensitive backbone at-\ntains a 26% and 22% performance improvement in success\nrate and precision, respectively. This demonstrates that the\ngoal-sensitive backbone effectively fuses the goal informa-\ntion into visual features and leads to goal-aware behavior.\nTable 3. Additional ablation experiments on Plains biome.\n# Method\nAvg. SR (%) Avg. P (%)\n1 Ours (GSB + horizon pred)\n55±09\n70±09\n2 Ours + RNN\n65±07\n67±08\n3 Ours −horizon pred + RNN\n39±08\n51±08\n4 Ours −horizon pred\n35±08\n45±15\n5 w\/o horizon loss\n47±06\n54±08\n6 w\/o extra obs\n50±07\n69±07\n7 w\/o language condition\n25±03\n26±05\nTable 4. The success rate (SR) under condition-free policy.\nGoal\nAvg.\nSuccess Rate (%) 44±19 24±06 23±11 11±07 25±03\nParameter sensitivity on horizon prediction. To investi-\ngate the sensitivity of the horizon-based control policy to\nthe constant c (outlined in §3.3), we perform experiments\nwith c values ranging from 0 to 14. We train and evaluate\nthe model using the multi-task setting on the Flat bench-\nmark, shown in Figure 5. Our findings indicate that within\nthe 0 to 10 range, decreasing c enhances performance, while\nfurther reduction leads to decline. This implies that sub-\ntracting a small constant from the predicted horizon-to-goal\nyields a more effective policy. However, subtracting a larger\nvalue results in performance deterioration, as attaining the\ngoal within such a limited horizon may be unfeasible.\nComparision with recurrent architecture. We built two\nrecurrent variants ( “Ours + RNN”, “Ours −horizon pred +\nRNN”) by using a GRU module to fuse the joint representa-\ntion ft and optionally also removing the horizon prediction\nmodule. During training, the batch size, frame number, and\nskipping frame are set to 8, 16, and 5, respectively. Ta-\nble 3 (exp1 vs. exp3) shows that “Ours −horizon pred +\nRNN” becomes significantly worse, likely due to the par-\ntial observability issue (−26% SR). However, when com-\nbining RNN and horizon module (exp2), the performance\ngains significantly more than our original method (+10%\nSR). To sum up, while RNNs can aid in addressing partial\nobservability, our findings indicate that in our open-world\nscenario, they are considerably more effective when com-\nbined with our horizon prediction module.\nAblation on horizon loss, extra observation, and lan-\nguage condition.\nTable 3 demonstrates that excluding\nhorizon loss (exp5) and extra observation (exp6) can result\nin a decrease of success rate by 8% and 5%, respectively.\nFurthermore, as depicted in Table 4, when the language con-\ndition is removed from the input (exp7), the policy primar-\nily accomplishes the “chopping tree” task (44% SR) while\nscarcely completing the “hunting pig” task (11% SR). The\ntasks “hunting sheep” and “hunting cow” are executed fairly\nevenly (around 24% SR). This is likely due to trees appear-\ning more frequently than animals in the environment.\n7\n0\n2\n4\n6\n8\n10\n12\n14\nSubtract Constant\n0.40\n0.45\n0.50\n0.55\n0.60\nSuccess Rate\n0\n2\n4\n6\n8\n10\n12\n14\nSubtract Constant\n0.55\n0.60\n0.65\n0.70\n0.75\nPrecision\nFigure 5. Multi-task performance as a function of subtracting the\nhorizon constant c. Results show that setting c to a small constant\nlead to better overall performance as it incentivizes the agent to\nexhibit behaviors that lead to faster task completion.\n4.4. Generalization Performance\nIn the open-ended Minecraft environment, which fea-\ntures a variety of biomes with distinct appearances, a de-\ncent agent should be capable of generalizing across these\ndiverse biomes. To evaluate the agent’s zero-shot gener-\nalization ability in a new biome, we initially train the agent\nusing data exclusively from the Plains biome. Subsequently,\nwe test it in the Flat biome, where it faces the challenge\nof combatting sheep, cows, and pigs. Complicating\nthe task, numerous distracting mobs, such as wolves and\nmushroom cows, appear in the testing biome but not in\nthe training biome. The results are presented in Table 5.\nOur zero-shot agent demonstrates success rates comparable\nto those of an agent trained directly on the Flat biome. The\nhigh precision of our zero-shot agent also indicates its ro-\nbust performance, even amidst numerous novel distracting\nmobs in the new testing biome. Therefore, we believe that\nour agent displays a degree of zero-shot generalization to\nnew environments, achieved through goal-aware represen-\ntation learning and adaptive horizon prediction.\n5. Related Works\nOpen-ended Environments.\nA variety of environments\nhave been developed for open-ended agent training, such\nas grid worlds [8, 9], maze worlds [25, 42, 46], and indoor\nworlds [1,15,38,40]. Although these benchmarks have ad-\nvanced agent development, they generally lack complexity\nin perception and task domains. This paper concentrates on\nMinecraft, a voxel-based 3D, first-person, open-world game\ncentered around survival and creation. Microsoft introduced\nthe first Gym-style API platform called Malmo [24] for\nMinecraft, which has spawned numerous secondary devel-\nopment variants.\nBuilding on Malmo, MineRL [20] of-\nfers a human-interface simulator and a dataset of human\nplay demonstrations for the annual Diamond Challenge at\nNeurIPS [18,19,26]. MineDoJo [16], an extension of Min-\neRL, broadens the APIs for customizing tasks and provides\nthousands of pre-defined compositional tasks aimed at de-\nveloping a generally capable embodied agent, which we use\nto evaluate our method.\nTable 5. Quantitive results on generalization to a novel biome.\nTrain →Eval\nSuccess Rate (%)\nPrecision (%)\nAvg.\nAvg.\nFlat→Flat\n72\n60\n57\n63\n44\n48\n54\n49\nPlains→Flat\n67\n47\n60\n58\n89\n89\n70\n83\nEmbodied Agents in Minecraft.\nSome prior studies\nhave utilized a hierarchical reinforcement learning frame-\nwork to develop sophisticated embodied agents.\nFor in-\nstance, SEIHAI [31] divides a long-horizon task into sev-\neral subtasks, training an appropriate agent for each sub-\ntask and designing a scheduler to manage the execution of\nthese agents. Similarly, JueWu-MC [28] adopts this con-\ncept but enhances the agent with action-aware representa-\ntion learning capabilities. In recent times, the internet-scale\npretraining paradigm has made a significant impact on em-\nbodied research in open-ended environments. VPT [4], for\nexample, undergoes pretraining on an extensive collection\nof online gameplay videos using imitation learning. How-\never, it lacks the ability to process any command input.\nMineAgent [16] takes a different approach by pretraining a\nlanguage-conditioned reward function using online video-\ntranscript pairs, which is then utilized to support multi-task\nreinforcement learning.\nProgress Monitor.\nThe horizon-to-goal prediction tech-\nnology has already been employed as a progress moni-\ntor in the Vision-Language Navigation (VLN) communi-\nties [29, 30, 47]. This technology aids in understanding the\ntask structure and expediting the training procedure. Gen-\nerally, current progress monitors primarily function as sup-\nplementary objectives. Their estimated progress is utilized\nto reassess actions or execute beam search. In contrast, our\nestimated horizon is explicitly incorporated into the policy\nnetwork to guide agent behaviors. During inference, the\nhorizon input can be adjusted for enhanced performance.\n6. Conclusion\nIn this paper, we explore the issue of learning goal-\noriented policies in open-world environments. We pinpoint\ntwo major challenges unique to such settings: 1) the diffi-\nculty in distinguishing tasks from the state distribution due\nto immense scene variety, and 2) the non-stationary nature\nof environmental dynamics resulting from partial observ-\nability. We propose a goal-sensitive backbone and an adap-\ntive horizon prediction module to overcome both. Our ex-\nperiments on challenging Minecraft confirm the advantages\nof our proposed methods over baselines in terms of both\nsuccess rate and precision of task completeness.\nAcknowledgement. This work was supported by the Na-\ntional Key R&D Program of China 2022ZD0160301, and\nin part by the NSF grants #IIS-1943641, #IIS-1956441,\n#CCF-1837129, Samsung, CISCO, and a Sloan Fellowship.\nWe thank Hongming Xu for his engineering support.\n8","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nOpen-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction\n```\n#### 2. 论文摘要\n```\nWe study the problem of learning goal-conditioned policies in Minecraft, a\npopular, widely accessible yet challenging open-ended environment for\ndeveloping human-level multi-task agents. We first identify two main challenges\nof learning such policies: 1) the indistinguishability of tasks from the state\ndistribution, due to the vast scene diversity, and 2) the non-stationary nature\nof environment dynamics caused by partial observability. To tackle the first\nchallenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage\nthe emergence of goal-relevant visual state representations. To tackle the\nsecond challenge, the policy is further fueled by an adaptive horizon\nprediction module that helps alleviate the learning uncertainty brought by the\nnon-stationary dynamics. Experiments on 20 Minecraft tasks show that our method\nsignificantly outperforms the best baseline so far; in many of them, we double\nthe performance. Our ablation and exploratory studies then explain how our\napproach beat the counterparts and also unveil the surprising bonus of\nzero-shot generalization to new scenes (biomes). We hope our agent could help\nshed some light on learning goal-conditioned, multi-task agents in challenging,\nopen-ended environments like Minecraft.\n```\n\n#### 3. 论文全文\n```\nOpen-World Multi-Task Control Through\nGoal-Aware Representation Learning and Adaptive Horizon Prediction\nShaofei Cai1,2, Zihao Wang1,2, Xiaojian Ma3, Anji Liu3, Yitao Liang1,4\nTeam CraftJarvis\n1Institute for Artificial Intelligence, Peking University\n2School of Intelligence Science and Technology, Peking University\n3Computer Science Department, University of California, Los Angeles\n4Beijing Institute for General Artificial Intelligence (BIGAI)\n{caishaofei,zhwang}@stu.pku.edu.cn,xiaojian.ma@ucla.edu\nliuanji@cs.ucla.edu,yitaol@pku.edu.cn\nAbstract\nWe study the problem of learning goal-conditioned poli-\ncies in Minecraft, a popular, widely accessible yet challeng-\ning open-ended environment for developing human-level\nmulti-task agents. We first identify two main challenges of\nlearning such policies: 1) the indistinguishability of tasks\nfrom the state distribution, due to the vast scene diversity,\nand 2) the non-stationary nature of environment dynamics\ncaused by partial observability. To tackle the first challenge,\nwe propose Goal-Sensitive Backbone (GSB) for the policy\nto encourage the emergence of goal-relevant visual state\nrepresentations. To tackle the second challenge, the pol-\nicy is further fueled by an adaptive horizon prediction mod-\nule that helps alleviate the learning uncertainty brought by\nthe non-stationary dynamics. Experiments on 20 Minecraft\ntasks show that our method significantly outperforms the\nbest baseline so far; in many of them, we double the perfor-\nmance. Our ablation and exploratory studies then explain\nhow our approach beat the counterparts and also unveil the\nsurprising bonus of zero-shot generalization to new scenes\n(biomes). We hope our agent could help shed some light on\nlearning goal-conditioned, multi-task agents in challeng-\ning, open-ended environments like Minecraft. The code is\nreleased at https:\/\/github.com\/CraftJarvis\/\nMC-Controller.\n1. Introduction\nBuilding agents that can accomplish a vast and diverse\nsuite of tasks in an open-ended world is considered a key\nchallenge towards devising generally capable artificial in-\ntelligence [2, 3, 6, 35]. In recent years, environments like\nMinecraft have drawn much attention from the related re-\ncombat pig\nharvest poppy\nharvest wood\ncombat sheep\npick-place\nwindow-open\nbox-close\nstick-push\nMinecraft\nMeta-world\nFigure 1. Comparison of states between Meta-world [49] (left) and\nMinecraft [24] (right) based on t-SNE visualization. The points\nwith the same color represent states from the trajectories that com-\nplete the same task. It can be seen that the states are much more\ndistinguishable in terms of tasks in Meta-world than in Minecraft,\nimplying the higher diversity of states and tasks in open worlds\nlike Minecraft over traditional multi-task agent learning environ-\nments like Meta-world.\nsearch communities [16,18–20,26], since they are not only\npopular, and widely accessible, but also offer an open-\nended universe with myriad of tasks, making them great\nplatforms for developing human-level multi-task agents.\nAlthough groundbreaking successes have been observed\nin many challenging sequential decision-making problems\nsuch as Atari[32], Go[39], and MOBA games[13, 44, 45],\nsuch successes have not been transferred to those open\nworlds. To understand the gap and design corresponding so-\nlutions, we need to first understand the distinct challenges\n1\narXiv:2301.10034v3  [cs.AI]  12 Oct 2023\nbrought by these environments. Let’s take Minecraft [24]\nas an example: there are over twenty types of landscapes\nranging from flat lands like Savannah and desert to rough\nmountains with forests and caves.\nThese diverse land-\nscapes also enable countless tasks that could be achieved by\nthe agents: mining, harvesting, farming, combating, con-\nstructing, etc. Compared to canonical agent learning en-\nvironments like Go [39], Atari [32], and robotic control\nsuite [41, 43, 48], Minecraft provides a substantially more\ndiverse distribution of states thanks to the rich scenes and\ntasks built with the game, making it exceptionally diffi-\ncult to extract the pivotal task-relevant visual state repre-\nsentations for goal-conditioned policies. To help our read-\ners understand the significance of this challenge, we visual-\nize the states from trajectories that complete some tasks in\nMinecraft and Meta-world [48] (a popular multi-task learn-\ning environment but with fewer states and tasks) in Fig. 1.\nStates of different tasks are annotated with different colors.\nClearly, the states in Minecraft are much less distinguish-\nable in terms of tasks than in Meta-world. Therefore goal-\nconditioned policies are more likely to struggle in mapping\nthose states and tasks (served as goals) to actions.\nAnother grand challenge in an open-ended environment\nlike Minecraft hails from the setting of such games, where\nan agent can only have very limited observations of the\nworld.\nFor example, in MineDoJo [16] (a recent agent\nbenchmark built on Minecraft), the observation space com-\nprises a first-person view image and a list of possessed\nitems. However, many more aspects of the surroundings re-\nmain hidden from the agents. That is, the agent now has to\nwork with a partially observable environment. A plague\nembedded with such an environment is non-stationary dy-\nnamics, which makes it almost impossible to predict what\nwill happen next. Therefore, the distances from states to the\ncurrent goal become much less clear due to the world un-\ncertainty, leading to less distinguishable states in terms of\ngoal completeness and more faulty decisions emitted by the\ngoal-conditioned policies.\nThis paper aims at mitigating both aforementioned chal-\nlenges that emerge from most open-world environments.\nFirst, we observe that the architecture of the policy network\nis crucial to learning goal-relevant visual state representa-\ntions that allow goal-conditioned actions in domains with\nlow inter-goal state diversity (cf. Fig. 1). To this end, we\npropose Goal-Sensitive Backbone (GSB), which enables ef-\nfective learning goal-conditioned policies over 20 tasks in\nthe Minecraft domain. Next, to mitigate the challenge posed\nby the partially observed and non-stationary environment,\nwe introduce horizon as an extra condition for the policy\nand a corresponding horizon prediction module. Specifi-\ncally, the policy is also explicitly conditioned on the remain-\ning time steps till achieving certain goals (i.e., distance-to-\ngoal). We find it significantly boosts the performance of\nour agents in open-world multi-task domains. However, the\nground-truth distance-to-goal is unavailable during evalu-\nation. To fix this problem, we train a horizon prediction\nmodule and feed the estimated distance-to-goal to the hori-\nzon commanding policy in evaluation. This leads to a 27%\ngain in average success rate under the multi-task settings.\nWe evaluate the proposed approaches based on the sim-\nple yet effective behavior cloning algorithm [10]. The ex-\nperiments are conducted in three common biomes. In multi-\ntask settings, our proposed method outperforms the base-\nline in terms of success rate and precision by a large mar-\ngin. It also achieves consistent improvement in single-task\nsettings. Our ablation and exploratory studies then explain\nhow our approach beat the counterparts and also unveil the\nsurprising bonus of zero-shot generalization to new scenes\n(biomes).\nTo summarize, targeting two identified challenges dis-\ntinct to open worlds, our contributions are threefold:\n• We propose Goal-Sensitive Backbone (GSB), a neural\nnetwork that enables effective learning goal-relevant vi-\nsual state representations at multiple levels for goal-\nconditioned policies, aiming at addressing the challenge\nof diverse state distribution in open-ended environments.\n• We further introduce adaptive horizon prediction to ex-\nplicitly condition the policy on the distance from the cur-\nrent state to the goal, yielding much better performances\nin a partially observable open-ended environment with\nnon-stationary dynamics.\n• We conduct extensive studies on the popular yet challeng-\ning Minecraft domain with baselines and our proposed\nmethod. The results demonstrate superior advantages of\nour approach over the counterparts in terms of both suc-\ncess rate and precision of task completion.\n2. Preliminaries\nGoal-conditioned policy, as its name suggests, is a type of\nagent’s policy π for decision-making that is conditioned on\ngoals besides states. Specifically, we denote π(a|s, g) as\na goal-conditioned policy that maps the current state s and\ngoal g to an action a. Compared to the canonical formula-\ntion of policy where the goal is absent, the goal-conditioned\npolicy offers flexibility of learning multi-task agent as it al-\nlows different behaviors for different tasks by simply alter-\ning the goal. There are multiple ways to specify the goal,\ne.g., natural language instructions [2] and goal images [36].\nGoal-conditioned imitation learning is a simple yet ef-\nfective way to learn goal-conditioned policies. Specifically,\nπ(a|s, g) is optimized by imitating the demonstrations D,\nwhere D = {τ 1, τ 2, τ 3, . . . } is a collection of trajectories\nτ i. A trajectory is a sequence of states, actions, and goals,\ndefined as τ i = {(si\nt, ai\nt, gi)}T\nt=0, where T is the trajectory\nlength. The imitation learning objective is to maximize the\n2\nCross-biome\nEnvironment\nSingle-biome\nEnvironment\nkill sheep in Snowy Plains\nchop tree in Plains\nkill sheep in Plains\nFigure 2. Demonstrations of the cross-biome environment and\nthe more challenging single-biome environment. The challenge\ncomes from the fact that the agent needs to learn diverse behaviors\nin similar states conditioned on different goals.\nlikelihood of the action in demonstrations when attempting\nto reach the desired goal\nJIL(π) = Eτ∼D\n\u0002 XT\nt=0 log π(at|st, g)\n\u0003\n.\n(1)\nNotation. At each timestep, our architecture takes in a tu-\nple (st, at, ht, g, at−1) as the input, where st = {oI\nt , oE\nt },\noI\nt is the raw image observation, oE\nt is the extra observation\nprovides by the environments. ht comes from the demon-\nstration. ˜ht and ˜at are the predicted horizon and action,\nrespectively. For simplicity, we also use the same symbols\n(oE\nt , g, at−1) to represent their embeddings.\n3. Method\nIn this section, we describe the proposed algorithm for\nlearning goal-conditioned policies that are capable of com-\npleting various preliminary tasks in open-world domains.\nFirst, we revisit and provide a detailed illustration of the\nidentified challenges in open-world domains (§3.1). Aim-\ning at solving these challenges, we proceed to introduce\nthe proposed goal-sensitive backbone (§3.2) and adaptive\nhorizon prediction module (§3.3). Finally, we provide an\noverview of the proposed method in Section 3.4.\n3.1. Challenges\nAs demonstrated in Section 1, the first major challenge\nof open-world environments is the indistinguishability of\nstates in terms of different goals (cf. Fig. 1). That is, it is\noften hard to identify the task\/goal by looking at individual\nstates. Compared to environments with clear goal indicators\nin their states, agents in open-world domains need to learn\ngoal-conditioned diverse behaviors under similar states.\nThis challenge can be reflected by the illustrative exper-\niment in Fig. 2. Two multi-task environments are created\nbased on the Minecraft domain. Both environments consist\nof two preliminary tasks: collect logs and hunt sheep, where\nthe former can be done by chopping trees and the latter re-\nquires the agent to slaughter sheep. Both tasks require the\nagent to first locate and approach the corresponding target.\nAs shown in Fig. 2 (center), in the single-biome environ-\nment (blue blob in Fig. 2), the agent is tasked to collect\nlogs and hunt sheep both inside a randomly generated plain\narea with grass, trees, and various mobs. In contrast, in\nthe cross-biome environment (red blob in Fig. 2), whenever\nthe agent is tasked to hunt sheep, it is spawned randomly\nin a snowy plain. Although different in visual appearance,\nsnowy plains and plains have very similar terrains, so the\ndifficulty of each task in the cross-biome environment is\nsimilar to its counterpart in the single-biome environment.\nThe main consequence of this change is that the agent can\ndetermine its goal by solely looking at the current state,\nwhich mimics the setting of Meta-World in Fig. 1(left).\nWe collect demonstrations by filtering successful trajec-\ntories played by VPT [4] (see §4.1 for more details) and use\nbehavior cloning to train multi-task policies on both envi-\nronments. Perhaps surprisingly, as shown in Fig. 2, despite\nthe minor difference, performance in the single-biome envi-\nronment is significantly weaker than in the cross-biome one.\nThis clearly demonstrates that the common practice of di-\nrectly concatenating observation features and goal features\nsuffer from learning diverse actions (e.g., locate trees, find\nsheep) given similar observations. In contrast, in the cross-\nbiome environment, the difficulty of the two tasks funda-\nmentally remains the same, yet the agent only needs to learn\na consistent behavior in each biome (i.e., plains and snow\nfields). This alleviates the need to learn goal-conditioned\ndiverse behaviors in similar states and leads to a better suc-\ncess rate.\nThe second key challenge comes from the partial ob-\nservability of the game and non-stationary environment dy-\nnamics.\nSpecifically, in Minecraft, the biome and mobs\nsurrounding the agent are generated procedurally and ran-\ndomly after each reset. Further, only a small fraction of the\nwhole terrain is visible to the agent in one observation, lead-\ning to more uncertainty of the world. From the perspective\nof learning goal-conditioned policies, the distances from\nstates to the current goal will become much less clear com-\npared to canonical learning environments like Atari [12].\nWe refer to Appendix B for more discussion on this. Since\nthe goal-conditioned policies also rely on distinguishable\nstates in terms of goal completeness, they’re more likely to\nmake wrong decisions as a result of world uncertainty.\n3\nGoal Space\nHunt a cow\nShear a sheep\n…\nChop Trees\nAction Space\nExtra Observation\nMove\nCam\nAttack\nUse\nImage Observation\nCompass GPS\nBiome\nVoxels\n𝒐!\n\"\n𝒈\n𝒐!\n#\n𝒂!$%\nEmbed\nEmbed\nEmbed\nℎ%!\nℎ!\nEmbed\n𝒂&!\n𝒂!\nℒ!\nℒ\"\nGSB\n𝑰!\n&\n𝒇!\nTraining\n𝜇\n𝜋!\nConcantenate\nHorizon Loss\n𝒐!\n\"\n𝒈\n𝒐!\n#\n𝒂!$%\nEmbed\nEmbed\nEmbed\nℎ%!\nℎ)!\nEmbed\n𝒂!\nGSB\n𝑰!\n&\n𝒇!\nEvaluation\n𝜇\n𝜋!\nConcantenate\nAdjust\nAdaptive Horizon \nPrediction\nGoal-Sensitive Backbone (GSB)\n𝒙(()\n𝒈\nFC\nReLU\nFC\nConv\nReLU\nConv\n𝒙((*%)\nReLU\nSigmoid\n×\n𝒐!\n\"\n𝒈\nConv\nMax\nG-Conv\nBlock\nG-Conv\nBlock\n×𝟑\n+\nFigure 3. Our Goal-conditioned Policy Architecture. Our contributions are in red and purple. Right: The goal-sensitive backbone\n(GSB) is a key component to incentivize goal-condition behaviors. It consists of a stack of g-conv blocks. It takes the image observation\noI\nt and the goal embedding g as input, and outputs the goal-attended visual representation Ig\nt . The multimodal joint representation f t is\nthe concatenation of visual representation Ig\nt , goal embedding g, extra observation embedding oE\nt and previous action embedding at−1.\nThe horizon prediction module µ uses it to predict the horizon ˜ht while the horizon commanding policy πθ uses it to predict the action ˜at.\nTop: During the training, the predicted horizon ˜ht is only used to compute the horizon loss Lh. The policy is conditioned on ht that comes\nfrom the demonstration. Bottom: During the evaluation, the policy is conditioned on the predicted horizon ˜ht which needs to be adjusted.\n3.2. Incentivize Goal-Conditioned Behavior with\nStacked Goal-Sensitive Backbone\nAs elaborated in Section 3.1, learning goal-conditioned\npolicies becomes extremely hard when states collected from\ntrajectories that accomplish different tasks are indistin-\nguishable. While certain algorithmic design choices could\nimprove multi-task performance in such open-world envi-\nronments, we find that the structure of the policy network\nis a key factor towards higher episode reward.\nSpecifi-\ncally, we observe that existing CNN-based backbones can\nexcel at completing many single tasks (e.g., hunt cow, col-\nlect stone), but struggle to learn goal-conditioned behavior\nwhen training on the tasks in a goal-conditioned manner.\nThis motivates the need to properly fuse goal information\ninto the network. Despite the existence of various feature\nfusion approaches such as concatenation and Bilinear lay-\ners [27], they all perform poorly even with a moderate num-\nber of tasks. This motivates the need to carry goal informa-\ntion into multiple layers of the network. Specifically, we\npropose goal-sensitive backbone (GSB), which effectively\nblends goal information to the state features at multiple lev-\nels. As shown in Fig. 3 (right), GSB is composed with mul-\ntiple goal convolution blocks (g-conv block), which are ob-\ntained by augmenting the vanilla convolution block with a\ngoal branch. Functionally, it can provide deep feature fu-\nsion between multi-level visual features and the goal infor-\nmation. As we will proceed to show in Section 4.3, adding\nGSB can lead to significant performance boost in multi-task\nenvironments. The g-conv block processes its input visual\nfeatures x(l) ∈RC×H×W with two convolution layers\nˆx(l) = ReLU(Conv(ReLU(Conv(x(l))))).\n(2)\nMeanwhile, it maps the goal embedding g to the same fea-\nture space as the intermediate features ˆx(l) with two fully-\nconnected layers, decribed as\nˆg(l) = FC(ReLU(FC(g))).\n(3)\nThe goal feature ˆg(l) is then used to modulate the interme-\ndiate features ˆx(l) channel-wise. By adding a residual con-\nnection [21], the output feature x(l+1) is expressed by\nx(l+1) = σ(ˆg(l)) ⊙ˆx(l) + x(l),\n(4)\nwhere σ(·) is the sigmoid function and ⊙is the element-\nwise product.\nThis channel-wise modulation encourages\nthe module to focus on goal-specific regions and discard the\nbackground information by adaptively weighing the chan-\nnel importance.\nWe highlight that the g-conv block can\nbe plugged into any convolution backbone to improve its\ncapability of extracting goal-aware visual features.\nThe\nproposed goal-sensitive backbone is constructed by replac-\ning 6 convolution blocks of the widely-adopted Impala\nCNN [14] to g-conv blocks. In our experiments, a GSB\nis used to compute goal-conditioned state features Ig\nt =\nGSB(oI\nt , g). Such an idea of fusing condition information\ninto the backbone layer by layer was also used by some\nprior works [5, 22, 33, 34]. Here, we demonstrate that it\nworks in a critical role for open-world multi-task control.\n4\n3.3. Combat World Uncertainty with Adaptive\nHorizon Prediction\nTo address the challenge brought by the uncertainty of\nthe world, we need to ensure the goal-conditioned policies\nto be more aware of goal-completeness given the current\nstate.\nWe observe that conditioning the policy addition-\nally on the number of remaining steps toward achieving a\ngoal, i.e., distance-to-goal, or horizon, can significantly im-\nprove the accuracy of predicted actions on held-out offline\ndatasets [17,37]. Here, we define the horizon ht := T −t,\nwhere T is the trajectory length, as the remaining time steps\nto complete the given goal. This motivates the design of a\nhorizon commanding policy πθ : S × G × H →A that\ntakes a state s, a goal g, and a horizon h as inputs and out-\nputs an action a. A key problem of the horizon commanding\npolicy is that it cannot be directly used for evaluation: dur-\ning gameplay, horizon is unknown as it requires completing\nthe whole trajectory. To fix this problem, we introduce an\nadditional horizon prediction module, which estimates the\nhorizon given a state s and a goal g. Combining the two\nmodules together, we can apply the fruitful horizon com-\nmanding policy during gameplay.\nBoth modules can be trained efficiently with dense su-\npervision. Specifically, the horizon commanding policy πθ\ncan be learned by any policy loss specified by RL algo-\nrithms. For example, when behavior cloning is used, πθ\ncan be optimized by minimizing the loss\nLa = −log πθ(at|ht, f t),\n(5)\nwhere f t is the joint representation of the state and goal\nembedded by a neural network (see §3.4). The horizon pre-\ndiction module is trained by a supervised learning loss\nLh = −log µ(ht|f t),\n(6)\nwhere µ is a network that predicts the horizon.\nDuring the evaluation, after computing the embedding\nf t for st and g, the horizon prediction module µ is first in-\nvoked to compute an estimated horizon ˜ht = µ(f t). This\npredicted horizon can then be fed to the horizon command-\ning policy to compute the action distribution πθ(at|˜ht, f t).\nIn practice, we observe that feeding an adaptive version of\n˜ht, defined as ˆht := max(˜ht −c, 0) (c is a hyperparame-\nter), to πθ leads to better performance. We hypothesize that\nthis advantageous behavior comes from the fact that by sup-\nplying the adaptive horizon ˆht, the agent is encouraged to\nchoose actions that lead to speedy completion of the goal.\nThe effectiveness of the adaptive horizon will be demon-\nstrated in Section 4.3.\n3.4. Model Summary\nAs shown in Fig. 3, our model sequentially connects the\nproposed goal-sensitive backbone, horizon prediction mod-\nule, and horizon commanding policy. At each time step\n(a) Flat\n(b) Plains\n(c) Forest\nFigure 4. Snapshots of the RGB camera view in three biomes.\nt, the image observation and goal information are first fed\nforward into the goal-sensitive backbone to compute goal-\naware visual feature Ig\nt . The visual feature is then fused\nwith additional input information including the extra obser-\nvation embedding oE\nt , the goal embedding g, and the pre-\nvious action embedding at−1 by concatenation and a feed-\nforward network:\nf t = FFN(\n\u0002\nIg\nt ∥oE\nt ∥g ∥at−1\n\u0003\n).\n(7)\nThen, f t is input to the horizon prediction module to predict\nhorizon ˜ht = µ(f t). And the horizon commanding policy\ntakes in the horizon and features f t to compute the action.\nWhen trained with behavior cloning, the overall objective\nfunction is L = La + Lh. During the evaluation, the adap-\ntive horizon ˆht is fed to the horizon commanding policy in\nreplacement of ˜ht.\n4. Experiments\nThis section analyzes and evaluates the proposed goal-\nsensitive backbone and the adaptive horizon prediction\nmodule in the open-world domain Minecraft. To minimize\nperformance variation caused by the design choices in RL\nalgorithms, we build the proposed method on top of the sim-\nple yet effective behavior cloning algorithm. In Section 4.1,\nwe first introduce three suites of tasks; the agent is asked\nto collect and combat various target objects\/mobs with in-\ndistinguishable states conditioned on different goals (chal-\nlenge #1) and non-stationary environment dynamics (chal-\nlenge #2). Single-task and multi-task performance on the\nbenchmarks is evaluated and analyzed in Section 4.2, and\nablation studies are conducted in Section 4.3. Finally, we\nunveil the surprising bonus of zero-shot generalization to\nnew scenes and tasks in Section 4.4.\n4.1. Experimental Setup\nEnvironment and task. To best expose the challenges de-\nscribed in Sections 1 and 3.1, a key design principle of our\nbenchmark environments is to task the agent to complete\nmultiple preliminary tasks in similar yet highly random-\nized scenes. By specifying the biome that surrounds the\nagent, Minecraft provides a perfect way to create such en-\nvironments. Specifically, as shown in Fig. 4, every biome\nhas unique and consistent observations; randomness comes\nfrom the fact that the terrain is generated randomly in each\nepisode. To evaluate the scalability of the proposed method\nin terms of the number of tasks, we choose Plains and\n5\nForest, the two most common biomes that contain a large\nnumber of resources and mobs.\nIn addition to the two challenges, Plains and Forest\nalso add unique difficulties to learning goal-conditioned\npolicies.\nSpecifically, although we have better views in\nPlains, the resources\/targets are located further away\nfrom the agent and require more exploration. In contrast,\nthere exist more occlusions and obstacles in Forest.\nThe Plains benchmark consists of four tasks: har-\nvest oak wood (\n), and Combat sheep (\n), cow (\n),\npig (\n).\nIn the Forest benchmark, the agent is\ntasked to complete thirteen tasks: combat sheep (\n),\ncow (\n), pig (\n), harvest dirt (\n), sand (\n),\noak wood (\n), birch wood (\n), oak leaves (\n),\nbirch leaves (\n), wool (\n), grass (\n),\npoppy ( ), orange tulip ( ).\nIn addition to the above two benchmarks, we also\ntest the agent on a “hunt animals” benchmark based\non the Flat biome, which contains a flattened world.\nSpecifically, the agent needs to combat sheep (\n),\ncow (\n), pig (\n), spider (\n), polar bear (\n),\nchicken (\n), donkey (\n), horse (\n), wolf (\n),\nllama (\n), mushroom cow (\n) in the Flat environ-\nment.\nCompared to other benchmarks, the challenge of\nFlat comes from the fact that the mobs are constantly\nwondering around, which makes it hard to locate and ap-\nproach the correct target.\nWe adopt the original observation space provided by\nMineDoJo [16], which includes a RGB camera-view,\nyaw\/pitch angle, GPS location, and the type of 3 × 3 blocks\nsurrounding the agent.\nWe discretize the original multi-\ndiscrete action space provided by MineDojo into 42 discrete\nactions. Details are included in Appendix A.1.\nData collection pipeline.\nOne significant downside of\nbehavior cloning algorithms is the need for high-quality\nand densely-labeled trajectories, which often requires enor-\nmous human effort to collect. To mitigate this problem,\nwe collect goal-conditioned demonstrations by filtering suc-\ncessful trajectories from gameplays by pretrained non-goal-\nconditioned policies.\nSpecifically, we adopt Video Pre-\nTraining (VPT) [4], which is trained on tremendous amount\nof non-goal-conditioned gameplays. We rollout the VPT\npolicy in the three benchmarks and record all episodes that\naccomplishes any of the defined goals.\nThese trajecto-\nries are then converted to a goal-conditioned demonstration\ndataset. Please refer to Appendix A.2 for detailed settings\nand efficiency analysis of our data collection pipeline.\nEvaluation. During the evaluation, the maximum episode\nlength is set to 600, 600, and 300 on the Flat, Plains\nand Forest benchmarks, respectively.\nPlains and\nForest are given more time steps since, in these environ-\nments, the agent needs more time to locate and approach the\ntarget. We use Success Rate and Precision as our evaluation\nmetrics. A gameplay is successful if the agent completes the\ngoal within the episode. Precision is defined as the number\nof times the specified goal is achieved divided by the total\nnumber of goals completed in an episode. It measures how\nwell the agent can be aware of the specified goal, instead of\nsimply accomplishing any goal during gameplay.\n4.2. Experimental Results\nWe first focus on the simpler single-task learning set-\nting in order to isolate the challenge introduced by non-\nstationary dynamics and partial observability (§4.2.1). We\nthen examine whether the proposed method can better ad-\ndress both challenges by examining its multi-task perfor-\nmance (§4.2.2).\n4.2.1\nSingle task experiments\nWe select three typical tasks, i.e., harvest log, hunt cow,\nand hunt sheep, from the Plains benchmark for single-\ntask training. We compare the proposed method against\nthe following baselines. First, MineAgent [16] is an online\nRL algorithm that leverages pretrained state representations\nand dense reward functions to boost training. BC (VPT) [4],\nBC (CLIP) [16], and BC (I-CNN) [14] are variants of the be-\nhavior cloning algorithm that use different backbone mod-\nels (indicated in the corresponding brackets) for state fea-\nture extraction. The backbones are finetuned with the BC\nloss (see Appendix A.3 for more details).\nResults are reported in Table 1. First, we observe that\neven the individual tasks are extremely challenging for on-\nline RL algorithms such as MineAgent, even its networks\nare pretrained on Minecraft data.\nWe attribute this fail-\nure to its inconsistent dense reward when facing a hard-\nexploration task (e.g., the additional provided reward is not\nconsistently higher when the agent is moving closer to a\ntarget object).\nNext, compared to BC (I-CNN) that uses\na randomly initialized impala CNN model, the Minecraft-\npretrained backbones in BC (VPT) and BC (CLIP) do not\nbring any benefit. This could be caused by the lack of plas-\nticity, i.e., the ability to learn in these well-trained models,\nechoing similar findings in computer vision and RL [11].\nFinally, our approach outperforms all baseline methods, es-\npecially in terms of precision. This demonstrates that our\nmethod is more robust against non-stationary dynamics and\npartially observable observations.\n4.2.2\nMulti-task experiments\nWe move on to evaluate the proposed method on the three\nmulti-task benchmarks introduced in Section 4.1. The base-\nline includes three behavior cloning methods (we use “MT-\nBC” as an abbreviation of multi-task behavior cloning). We\nalso include two variations of our method: one without the\ngoal-sensitive backbone, and the other without the adaptive\n6\nTable 1. Results of single-goal tasks (§4.2.1) on Plains.\nMethod\nSuccess Rate (%)\nPrecision (%)\nMineAgent [16] 00±00 01±00 01±00\n–\n–\n–\nBC (CLIP) [16]\n18±06 26±05 25±06 51±08 43±08 44±05\nBC (VPT) [4]\n22±08 27±06 22±06 58±09 46±05 42±05\nBC (I-CNN) [14] 45±05 46±04 48±07 86±05 55±12 45±07\nOurs\n50±07 58±10 60±08 83±10 75±10 75±06\nTable 2. Results of multi-goal tasks (§4.2.2) on three biomes.\nMethod\nAvg. Success Rate (%) Avg. Precision (%)\nPlains\nFlat\nForest\nPlains\nFlat\nForest\nMT-BC (VPT) [4]\n25±06 17±05 15±04 22±05 17±03 14±04\nMT-BC (CLIP) [16] 22±05 14±03 14±03 23±04 15±03 13±03\nMT-BC (I-CNN) [14] 25±02 18±02 15±03 23±04 14±02 13±03\nMT-BC (w\/ GSB)\n32±05 36±03 19±05 43±06 36±02 17±03\nOurs (I-CNN)\n31±06 31±04 18±02 22±03 28±04 15±04\nOurs (w\/ GSB)\n55±09 57±09 30±06 70±09 50±06 29±06\nhorizon prediction module. Results on the Plains, Flat,\nand Forest environments are reported in Table 2, respec-\ntively. First, we observe that our method significantly out-\nperforms all baselines in terms of both success rate and pre-\ncision in all three benchmarks. Moreover, scaling up the\nnumber of tasks does not necessarily deteriorate the per-\nformance of our method. Specifically, we compare the av-\nerage success rate on the Plains and Flat benchmark,\nwhich contain 4 and 9 tasks, respectively. While the base-\nlines struggle to maintain their success rate on the Flat\nenvironment, our approach is capable of maintaining high\nperformance despite the increased number of tasks. Putting\ntogether, results on multi-task benchmarks clearly demon-\nstrate the superiority of our method when facing open-world\nenvironments with the two elaborated challenges (cf. §3.1).\n4.3. Ablation Study\nAblation study on goal-sensitive backbone. To examine\nthe effectiveness of our proposed goal-sensitive backbone,\nwe compare the following two groups of architectures: 1)\nOurs (I-CNN) v.s.\nOurs (w\/ GSB), 2) MT-BC (I-CNN) v.s.\nMT-BC (w\/ GSB). The key distinction between the groups is\nwhether the backbone employs a standard Impala CNN or\na goal-sensitive backbone. As depicted in Table 2, our find-\nings indicate that the goal-sensitive backbone consistently\nenhances performance in terms of both success rate and pre-\ncision across all environments. Remarkably, in the Flat\nbiome, our approach with the goal-sensitive backbone at-\ntains a 26% and 22% performance improvement in success\nrate and precision, respectively. This demonstrates that the\ngoal-sensitive backbone effectively fuses the goal informa-\ntion into visual features and leads to goal-aware behavior.\nTable 3. Additional ablation experiments on Plains biome.\n# Method\nAvg. SR (%) Avg. P (%)\n1 Ours (GSB + horizon pred)\n55±09\n70±09\n2 Ours + RNN\n65±07\n67±08\n3 Ours −horizon pred + RNN\n39±08\n51±08\n4 Ours −horizon pred\n35±08\n45±15\n5 w\/o horizon loss\n47±06\n54±08\n6 w\/o extra obs\n50±07\n69±07\n7 w\/o language condition\n25±03\n26±05\nTable 4. The success rate (SR) under condition-free policy.\nGoal\nAvg.\nSuccess Rate (%) 44±19 24±06 23±11 11±07 25±03\nParameter sensitivity on horizon prediction. To investi-\ngate the sensitivity of the horizon-based control policy to\nthe constant c (outlined in §3.3), we perform experiments\nwith c values ranging from 0 to 14. We train and evaluate\nthe model using the multi-task setting on the Flat bench-\nmark, shown in Figure 5. Our findings indicate that within\nthe 0 to 10 range, decreasing c enhances performance, while\nfurther reduction leads to decline. This implies that sub-\ntracting a small constant from the predicted horizon-to-goal\nyields a more effective policy. However, subtracting a larger\nvalue results in performance deterioration, as attaining the\ngoal within such a limited horizon may be unfeasible.\nComparision with recurrent architecture. We built two\nrecurrent variants ( “Ours + RNN”, “Ours −horizon pred +\nRNN”) by using a GRU module to fuse the joint representa-\ntion ft and optionally also removing the horizon prediction\nmodule. During training, the batch size, frame number, and\nskipping frame are set to 8, 16, and 5, respectively. Ta-\nble 3 (exp1 vs. exp3) shows that “Ours −horizon pred +\nRNN” becomes significantly worse, likely due to the par-\ntial observability issue (−26% SR). However, when com-\nbining RNN and horizon module (exp2), the performance\ngains significantly more than our original method (+10%\nSR). To sum up, while RNNs can aid in addressing partial\nobservability, our findings indicate that in our open-world\nscenario, they are considerably more effective when com-\nbined with our horizon prediction module.\nAblation on horizon loss, extra observation, and lan-\nguage condition.\nTable 3 demonstrates that excluding\nhorizon loss (exp5) and extra observation (exp6) can result\nin a decrease of success rate by 8% and 5%, respectively.\nFurthermore, as depicted in Table 4, when the language con-\ndition is removed from the input (exp7), the policy primar-\nily accomplishes the “chopping tree” task (44% SR) while\nscarcely completing the “hunting pig” task (11% SR). The\ntasks “hunting sheep” and “hunting cow” are executed fairly\nevenly (around 24% SR). This is likely due to trees appear-\ning more frequently than animals in the environment.\n7\n0\n2\n4\n6\n8\n10\n12\n14\nSubtract Constant\n0.40\n0.45\n0.50\n0.55\n0.60\nSuccess Rate\n0\n2\n4\n6\n8\n10\n12\n14\nSubtract Constant\n0.55\n0.60\n0.65\n0.70\n0.75\nPrecision\nFigure 5. Multi-task performance as a function of subtracting the\nhorizon constant c. Results show that setting c to a small constant\nlead to better overall performance as it incentivizes the agent to\nexhibit behaviors that lead to faster task completion.\n4.4. Generalization Performance\nIn the open-ended Minecraft environment, which fea-\ntures a variety of biomes with distinct appearances, a de-\ncent agent should be capable of generalizing across these\ndiverse biomes. To evaluate the agent’s zero-shot gener-\nalization ability in a new biome, we initially train the agent\nusing data exclusively from the Plains biome. Subsequently,\nwe test it in the Flat biome, where it faces the challenge\nof combatting sheep, cows, and pigs. Complicating\nthe task, numerous distracting mobs, such as wolves and\nmushroom cows, appear in the testing biome but not in\nthe training biome. The results are presented in Table 5.\nOur zero-shot agent demonstrates success rates comparable\nto those of an agent trained directly on the Flat biome. The\nhigh precision of our zero-shot agent also indicates its ro-\nbust performance, even amidst numerous novel distracting\nmobs in the new testing biome. Therefore, we believe that\nour agent displays a degree of zero-shot generalization to\nnew environments, achieved through goal-aware represen-\ntation learning and adaptive horizon prediction.\n5. Related Works\nOpen-ended Environments.\nA variety of environments\nhave been developed for open-ended agent training, such\nas grid worlds [8, 9], maze worlds [25, 42, 46], and indoor\nworlds [1,15,38,40]. Although these benchmarks have ad-\nvanced agent development, they generally lack complexity\nin perception and task domains. This paper concentrates on\nMinecraft, a voxel-based 3D, first-person, open-world game\ncentered around survival and creation. Microsoft introduced\nthe first Gym-style API platform called Malmo [24] for\nMinecraft, which has spawned numerous secondary devel-\nopment variants.\nBuilding on Malmo, MineRL [20] of-\nfers a human-interface simulator and a dataset of human\nplay demonstrations for the annual Diamond Challenge at\nNeurIPS [18,19,26]. MineDoJo [16], an extension of Min-\neRL, broadens the APIs for customizing tasks and provides\nthousands of pre-defined compositional tasks aimed at de-\nveloping a generally capable embodied agent, which we use\nto evaluate our method.\nTable 5. Quantitive results on generalization to a novel biome.\nTrain →Eval\nSuccess Rate (%)\nPrecision (%)\nAvg.\nAvg.\nFlat→Flat\n72\n60\n57\n63\n44\n48\n54\n49\nPlains→Flat\n67\n47\n60\n58\n89\n89\n70\n83\nEmbodied Agents in Minecraft.\nSome prior studies\nhave utilized a hierarchical reinforcement learning frame-\nwork to develop sophisticated embodied agents.\nFor in-\nstance, SEIHAI [31] divides a long-horizon task into sev-\neral subtasks, training an appropriate agent for each sub-\ntask and designing a scheduler to manage the execution of\nthese agents. Similarly, JueWu-MC [28] adopts this con-\ncept but enhances the agent with action-aware representa-\ntion learning capabilities. In recent times, the internet-scale\npretraining paradigm has made a significant impact on em-\nbodied research in open-ended environments. VPT [4], for\nexample, undergoes pretraining on an extensive collection\nof online gameplay videos using imitation learning. How-\never, it lacks the ability to process any command input.\nMineAgent [16] takes a different approach by pretraining a\nlanguage-conditioned reward function using online video-\ntranscript pairs, which is then utilized to support multi-task\nreinforcement learning.\nProgress Monitor.\nThe horizon-to-goal prediction tech-\nnology has already been employed as a progress moni-\ntor in the Vision-Language Navigation (VLN) communi-\nties [29, 30, 47]. This technology aids in understanding the\ntask structure and expediting the training procedure. Gen-\nerally, current progress monitors primarily function as sup-\nplementary objectives. Their estimated progress is utilized\nto reassess actions or execute beam search. In contrast, our\nestimated horizon is explicitly incorporated into the policy\nnetwork to guide agent behaviors. During inference, the\nhorizon input can be adjusted for enhanced performance.\n6. Conclusion\nIn this paper, we explore the issue of learning goal-\noriented policies in open-world environments. We pinpoint\ntwo major challenges unique to such settings: 1) the diffi-\nculty in distinguishing tasks from the state distribution due\nto immense scene variety, and 2) the non-stationary nature\nof environmental dynamics resulting from partial observ-\nability. We propose a goal-sensitive backbone and an adap-\ntive horizon prediction module to overcome both. Our ex-\nperiments on challenging Minecraft confirm the advantages\nof our proposed methods over baselines in terms of both\nsuccess rate and precision of task completeness.\nAcknowledgement. This work was supported by the Na-\ntional Key R&D Program of China 2022ZD0160301, and\nin part by the NSF grants #IIS-1943641, #IIS-1956441,\n#CCF-1837129, Samsung, CISCO, and a Sloan Fellowship.\nWe thank Hongming Xu for his engineering support.\n8\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 在开放世界中实现多任务控制：目标感知表示学习和自适应预测\n\n## 📌 背景痛点\/本文动机\n开放世界环境，如Minecraft，为开发能够执行各种任务的智能体提供了丰富的平台。然而，这些环境也带来了独特的挑战，包括：\n1. **状态分布的多样性**：由于场景的多样性，不同任务的状态难以区分，这使得学习目标条件策略变得困难。\n2. **环境动态的非平稳性**：由于部分可观察性，环境动态具有非平稳性，导致学习的不确定性增加。\n\n## 🚀 核心方法\n为了解决这些挑战，本文提出了以下创新方法：\n💡 创新点1：**目标感知骨干网络（GSB**）\n   - GSB通过在多个层次上融合目标信息，鼓励出现与目标相关的视觉状态表示，从而解决状态分布多样性的问题。\n   - GSB由多个目标卷积块（g-conv block）组成，这些块通过通道调制将目标信息与视觉特征融合。\n\n💡 创新点2：**自适应预测模块**\n   - 为了应对环境动态的非平稳性，本文引入了自适应预测模块，该模块预测从当前状态到目标的剩余时间步数（即距离到目标的距离）。\n   - 自适应预测模块通过预测剩余时间步数，帮助智能体更好地理解目标的完成程度，从而提高决策的准确性。\n\n## 📈 实验结果\n在Minecraft的20个任务上进行的实验表明，本文提出的方法显著优于现有基线，在许多任务中性能翻倍。消融研究和探索性研究解释了本文方法如何优于现有方法，并揭示了令人惊讶的零样本泛化到新场景（生物群落）的额外优势。\n\n## 💬 可借鉴之处\n本文提出的GSB和自适应预测模块为在开放世界环境中学习目标条件策略提供了新的思路，并为开发能够在复杂环境中执行多任务的智能体提供了有价值的参考。","llm_summary_res_status":200}
{"title":"MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control","authors":"Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, Jing Shao","summary":"It is a long-lasting goal to design a generalist-embodied agent that can\nfollow diverse instructions in human-like ways. However, existing approaches\noften fail to steadily follow instructions due to difficulties in understanding\nabstract and sequential natural language instructions. To this end, we\nintroduce MineDreamer, an open-ended embodied agent built upon the challenging\nMinecraft simulator with an innovative paradigm that enhances\ninstruction-following ability in low-level control signal generation.\nSpecifically, MineDreamer is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs) and diffusion models, and we employ a\nChain-of-Imagination (CoI) mechanism to envision the step-by-step process of\nexecuting instructions and translating imaginations into more precise visual\nprompts tailored to the current state; subsequently, the agent generates\nkeyboard-and-mouse actions to efficiently achieve these imaginations, steadily\nfollowing the instructions at each step. Extensive experiments demonstrate that\nMineDreamer follows single and multi-step instructions steadily, significantly\noutperforming the best generalist agent baseline and nearly doubling its\nperformance. Moreover, qualitative analysis of the agent's imaginative ability\nreveals its generalization and comprehension of the open world.","url":"http:\/\/arxiv.org\/abs\/2403.12037v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2403.12037v2","published":1710784782000,"comment":"Project page: https:\/\/sites.google.com\/view\/minedreamer\/main","pdf_text":"MineDreamer: Learning to Follow Instructions\nvia Chain-of-Imagination for\nSimulated-World Control\nEnshen Zhou1,2∗, Yiran Qin1,3∗,\nZhenfei Yin1,4, Yuzhou Huang3, Ruimao Zhang3†, Lu Sheng2†,\nYu Qiao1, Jing Shao1‡\n1 Shanghai Artificial Intelligence Laboratory\n2 Beihang University\n3 The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen)\n4 The University of Sydney\nzhouenshen@buaa.edu.cn\nyiranqin@link.cuhk.edu.cn\nhttps:\/\/sites.google.com\/view\/minedreamer\/main\nAbstract. It is a long-lasting goal to design a generalist-embodied agent\nthat can follow diverse instructions in human-like ways. However, exist-\ning approaches often fail to steadily follow instructions due to difficul-\nties in understanding abstract and sequential natural language instruc-\ntions. To this end, we introduce MineDreamer, an open-ended embodied\nagent built upon the challenging Minecraft simulator with an innovative\nparadigm that enhances instruction-following ability in low-level con-\ntrol signal generation. Specifically, MineDreamer is developed on top of\nrecent advances in Multimodal Large Language Models (MLLMs) and\ndiffusion models, and we employ a Chain-of-Imagination (CoI) mecha-\nnism to envision the step-by-step process of executing instructions and\ntranslating imaginations into more precise visual prompts tailored to the\ncurrent state; subsequently, the agent generates keyboard-and-mouse ac-\ntions to efficiently achieve these imaginations, steadily following the in-\nstructions at each step. Extensive experiments demonstrate that Mine-\nDreamer follows single and multi-step instructions steadily, significantly\noutperforming the best generalist agent baseline and nearly doubling its\nperformance. Moreover, qualitative analysis of the agent’s imaginative\nability reveals its generalization and comprehension of the open world.\nKeywords: Chain-of-Imagination · multimodal large language model ·\ninstruction following · low-level control\n1\nIntroduction\nOne of the core objectives of current embodied intelligence is to develop a gener-\nalist low-level control agent that can follow diverse instructions to solve endless\nopen-world embodied tasks [4, 5, 8, 42, 57]. Recent studies [4, 5, 8, 42] success-\nfully unlock the instruction-following ability of foundation models [3, 12, 15] in\n∗Equal contribution\n† Corresponding author\n‡ Project leader\narXiv:2403.12037v2  [cs.CV]  19 Mar 2024\n2\nE. Zhou et al.\nMineDreamer\nPrevious Studies\n“Chop a tree.” \n…\n…\n“Chop a tree.” \n“Chop a tree.” \n“Chop a tree.” \nFig. 1: Comparison between MineDreamer and previous studies. In “Chop\na tree”\ntask, MineDreamer employs a Chain-of-Imagination mechanism, where it\nimagines step by step what to do next tailored to the current state. Imaginations\ncontain environmental understanding and physical rules (e.g., perspective-based size\nchanges). These can serve as more precise visual prompts to steadily guide the agent in\ngenerating actions to achieve these imaginations as effectively as possible at each step.\nPrevious approaches have seen a tree, but missed the opportunity to chop it down.\nthe sequential decision-making domain [8, 11, 33, 57, 63, 69, 73]. However, these\nmethods [5,42] struggle to enable agents to follow textual instructions steadily,\ndue to the: (1) Many textual instructions are abstract for low-level control and\nmodels struggle to effectively understand. They should be transformed into more\neffective prompts that consider how to execute instructions based on the current\nstate. Hence, simple textual instructions cannot provide a precise demonstration\nof the desired behavior. (2) Many textual instructions are sequential, and exe-\ncuting them may require considering the current state and breaking down the\ntask into multiple stages for step-by-step completion. Therefore, steady action\ngeneration driven by single-text instructions often fails.\nTo address the above issues, this work aims to explore how to unlock the\nsituation-aware reasoning ability for a pre-trained decision-making foundation\nmodel. We introduce a simple yet effective mechanism called Chain-of-Imagination\n(CoI), which enables the agent to imagine and act upon the next stage step by\nstep according to the instructions. Our method is motivated by two ideas: (1)\nWhen solving complex problems, humans often envision the goal of the next\nstage based on the current state. If we can break down the sequential instructions\ninto multiple stages according to the current state, step by step, we can enable\nagents to follow instructions steadily. (2) Inspired by prompt tuning [34,78,79],\nif we can provide visual prompts containing physical rules and environmental\nunderstanding for each imagined step, tailored to optimally describe the desired\nbehavior in the current state, which are more intuitive and efficient than task\ninstructions, we can better guide the foundation model in predicting actions.\nTo this end, we propose MineDreamer within Minecraft, which generates a\nseries of “imagined” sub-steps based on the textual instructions and current state.\nThese visual sub-steps are then fed into a pre-trained decision-making founda-\ntion model to generate low-level control actions aimed at achieving the sub-steps.\nSpecifically, MineDreamer comprises three modules: (1) An Imaginator, a dif-\nfusion model enhanced by a Multimodal Large Language Model (MLLM), can\nbetter generate imaginations that contain the physical rules and environmental\nMineDreamer\n3\nunderstanding. (2) A Prompt Generator, the bridge between Imaginator and\nPolicyNet, can convert future imaginations into latent visual prompts that offer\nmore logical and precise demonstrations of the desired behavior. (3) A PolicyNet,\na foundation model, can use latent prompts as guidance to predict actions for\nagents in an open-world environment.\nNotably, as shown in Fig. 1, MineDreamer leverages a Chain-of-Imagination\nmechanism through multi-turn interaction between the Imaginator and the Pol-\nicyNet and cyclically generates latent visual prompts that better align with the\ncurrent state to guide the PolicyNet in following instructions steadily in action\ngeneration. This mechanism represents an attempt to implement “self multi-turn\ninteraction” in the sequential decision-making domain. Training an Imaginator\nin an open-world environment to envision the image of the next step requires\nextensive data. We employ the Goal Drift Collection method to gather a large\namount of egocentric embodied data, which helps the Imaginator to understand\nhow to achieve the instruction sequentially and how to achieve it repeatedly.\nOur main contributions are as follows:\n– We introduce the Chain-of-Imagination(CoI) method, which introduces “self\nmulti-turn interaction” to the sequential decision-making domain and en-\nables the agent to follow human instructions steadily in action generation.\n– We propose the Goal Drift Collection method and an MLLM-enhanced dif-\nfusion model that can generate imaginations adhering to physical rules and\nenvironmental understanding, providing more precise visual prompts rele-\nvant to the current state and instructions.\n– Leveraging these methods, we create an embodied agent in Minecraft named\nMineDreamer that has achieved nearly double the performance of the best\ngeneralist agent baseline in executing single and multi-step instructions steadily.\n2\nRelated Work\n2.1\nBuild Instruction-Following Agents in Minecraft\nResearch on generalist agents in Minecraft’s complex and dynamic environment\nis increasingly popular in AI. Despite the exploration of Large Language Mod-\nels [7,14,48,54,65,66] as high-level task planners that guide agents in executing\nlong-horizon tasks [25,50,70–72,81] like Voyager [70] and MP5 [50], we still re-\nquire lower-level controllers [3,8,20,27,42] to execute the generated plans. In the\nsequential decision-making domain, DreamerV3 [27] trains agents using a world\nmodel, while VPT [3] builds a large foundational model to generate actions by\nlearning from extensive video data. However, neither can follow instructions.\nGROOT [8] is developed to follow video instructions but fails to follow text\ninstructions. STEVE-1 [42], an evolution of VPT [3], is built for text instruc-\ntions but struggles to understand natural language prompts, despite extensive\nprompt engineering. Therefore, we create MineDreamer, which, leveraging the\nChain-of-Imagination mechanism, generates more precise visual prompts step-\nby-step, enabling it to follow instructions steadily in action generation.\n4\nE. Zhou et al.\n2.2\nConditioned Diffusion Models in Embodied Scenario\nWith the development of the text-to-image diffusion model [18,30,46,55,58,60],\nthe instruction-based diffusion methods [6, 9, 21, 23, 29, 32, 35, 67, 76] have re-\ncently marked considerable progress in generative tasks, especially in embod-\nied scenarios. UniPi [19] and HiP [1] integrate video diffusion with inverse dy-\nnamics to generate robot control signals for specific tasks. SkillDiffuser [41]\napplies interpretable hierarchical planning via skill abstractions in diffusion-\nbased task execution. While existing methods can only handle embodied tasks\nlimited to fixed environments, the emergence of Multimodal Large Language\nModels (MLLMs) [13, 22, 43, 49, 62, 74, 75, 80] has showcased superior reasoning\nand perceptual abilities in open-world environment. Inspired by this, we create\nan MLLM-enhanced diffusion model, focusing on the model’s understanding of\nphysics rules and environmental understanding, and its ability to create high-\nquality egocentric images for guiding low-level action generation.\n3\nMethod\nIn this section, we first provide an overview (Sec. 3.1) of our MineDreamer, in-\ncluding its mechanisms and features. Next, we introduce the purpose and work-\nflow of the Chain-of-Imagination (CoI) mechanism (Sec. 3.2) regarding Fig. 2.\nTo implement CoI and collect extensive embodied data to train Imaginator, we\nelaborate on the dataset construction (Sec. 3.3), including Goal Drift Collec-\ntion method. Finally, we provide the necessary details of each part, including\nImaginator (Sec. 3.4), Prompt Generator, and PolicyNet (Sec. 3.5).\n3.1\nOverview\nOur MineDreamer comprises three modules, i.e., Imaginator, Prompt Genera-\ntor, and PolicyNet. Our objective is to empower agents, especially foundation\nmodels in the sequential decision-making domain, to follow human instructions\nsteadily and act accordingly. The Imaginator is a parameter-efficiently fine-tuned\ndiffusion model specific to Minecraft utilizing the visual reasoning ability of\na Multimodal Large Language Model (MLLM). The Prompt Generator recon-\nstructs latent visual prompts from the current observations, future imaginations,\nand instructions. PolicyNet is the existing Video Pretraining (VPT) [3] model,\ntrained on 70k hours of Minecraft gameplay.\nWhy future goal imagination? Given a pre-trained model that can predict\nactions, the intuitive approach is to input the current state and instructions to\nguide it directly. So why the future goal imagination? In practice, we find that\nfuture goal imagination proves more interpretable for humans, easing debugging,\nand improving interaction and safety assessment [40, 51, 56, 77]. Furthermore,\nimages yield flexible, explicit representations, facilitating natural language goal\ndecomposition into clearer stages by learned physical rules and environmental\nunderstanding, helping the low-level control model “plan” what to do now.\nMineDreamer\n5\nImaginator\n…\nVPT\nVPT\nVPT\n𝑓𝑡\n𝑝𝑡\n𝑓𝑡+1\n𝑝𝑡+1\n𝑓𝑡+𝑇−1\n𝑝𝑡+𝑇−1\n𝑬𝑽\n𝑬𝑽\n𝑬𝑽\n𝑮\n𝑮\n𝑮\n𝑬𝑽\n𝑮\nVisual \nEncoder\nPrompt\nGenerator\nVisual &\nVisual Only\nImaginator\nImaginator\nText Instruction(𝑦) \nChop a tree\nCurrent Observation\nGoal Imagination\nInstruction\nFig. 2: The Overview of Chain-of-Imagination. The Imaginator imagines a goal\nimagination based on the instruction and current observation. The Prompt Generator\ntransforms this into a precise visual prompt, considering both the instruction and\nobserved image. The Visual Encoder encodes the current observation, integrates it\nwith this prompt, and inputs this into VPT. VPT then determines the agent’s next\naction, leading to a new observation, and the cycle continues. Note that VPT’s input is\nhistorical observations, so the figure cannot fully represent the autoregressive process.\nMore details about VPT as PolicyNet can be found in Sec. 3.5.\nWhy can MineDreamer follow instructions more steadily? Firstly, Mine-\nDreamer employs a Chain-of-Imagination (CoI) mechanism for incremental goal\nachievement via self-multi-turn interactions, enabling the agent to appropriately\nrespond to the current state. In addition, with the help of this mechanism, the\nPrompt Generator crafts logical latent visual prompts that provide clear demon-\nstrations of desired behaviors, ensuring that the agent steadily follows instruc-\ntions. Furthermore, the enhanced Imaginator not only comprehends open-ended\nvisual concepts, enabling it to imagine images of novel instructions it has never\nseen before but also ensures these images adhere to physical rules and envi-\nronmental understanding, thereby sharpening the precision of prompts. Thus,\nMineDreamer can follow instructions steadily in an open-world environment.\n3.2\nChain-of-Imagination\nChain-of-Imagination (CoI) enables the agent to envision the steps needed to\nachieve a goal iteratively. As shown in Fig. 2, it is an example to demonstrate\nhow CoI works. First, the Imaginator takes in the user’s instructions y and\ncurrent observations Ot and imagines a future image It+1 depicting a moment\nwithin the process of completing the given instruction y, which is closely related\nto the current observation Ot. Next, the Prompt Generator progressively creates\na more precise latent visual prompt pt in awareness of the current observation Ot,\ninstruction y and future imagination It+1, aligning with the visual input space of\nthe Video Pretraining (VPT) [3] model. The Visual Encoder then processes Ot\ninto a representation ft, which is combined with pt and fed into VPT [3]. Finally,\nVPT [3] progressively predicts an action (i.e., keyboard and mouse) from the\nobservation history, interacts with the environment, gathers a new observation\nOt+1, and repeats the cycle later.\n3.3\nDatasets\nWe train the Imaginator with the Goal Drift Dataset, which includes 500k\ntriplets (current observation, future goal imagination, instruction) from the Ope-\nnAI Contractor Gameplay Dataset [3], using the Goal Drift Collection method.\n6\nE. Zhou et al.\n𝑡∗\n𝑡𝑏1\nBackward\nForward\nBackward\nDrift\nForward\nDrift\n…\n…\n…\n…\n𝑡𝑏2\n𝑡𝑏𝑛\n𝑡𝑏𝑛+1\n𝑡𝑓𝑛\n𝑡𝑓𝑚\nInstruction(𝑦): “ Chop a tree\/Collect wood\/…”\nFig. 3: Goal Drift Collection. For each timestamp t∗, we form many triplets com-\nprising (current observation, goal imagination, instruction) associated with the game\nevent-related instructions completed by contractors. Each pair of linked images forms\na training triplet with its instruction for the Imaginator in this figure.\nOpenAI Contractor Gameplay Dataset. OpenAI Contractor Gameplay\nDataset [3] is created by hiring human contractors to play Minecraft and com-\nplete tasks like house building. Game events, like “mine_block”, noting the type\nof block broken, are logged with timestamps. These timestamps (t∗) provide\nprecise progress tracking and align with completed event-related instructions.\nGoal Drift Collection. The Gameplay Dataset allows us to construct numer-\nous embodied data by using specific event-related instructions achieved at each\ntimestamp t∗. Yet, directly pairing images from these timestamps t∗as future\ngoal imaginations Ot∗with images from a fixed timestep T earlier as current\nobservations Ot∗−T , along with instruction y, could lead to certain problems:\n(1) Goal Illusion: The Imaginator edits the observation to depict the\ncompleted instruction. Training the Imaginator on such data may reduce it\nto an image editor, as it generates imaginations without regard to the environ-\nment because all goal imaginations in the dataset represent the moment when\ninstruction is completed. For instance, given the instruction “Break dirt”\nwhile facing the sky, the Imaginator may unrealistically insert a broken dirt\nblock\ninto the sky. (2) Imagination Stagnation: The Imaginator fails\nto conceive repeated task completion. The Imaginator is trained to envision\nthe instructions’ fulfillment once, not recognizing the need for repetition, as all\ncurrent observations precede the achievement of instructions. For instance, given\n“Chop a tree”\n, after cutting the uppermost wood\nby looking up, the agent\nwill not look down for more trees\n, impeding continuous task performance.\nTo address the aforementioned issues, we propose the Goal Drift Collection\nmethod to gather Goal Drift Dataset. From the Gameplay Dataset, we form\nmany triplets (current observation, goal imagination, instruction) at each times-\ntamp t∗, all associated with the same event-related instructions y completed\nby the contractors. Fig. 3 shows that a pair of linked images with instructions\ny constitutes a training triplet. Our approach has both Backward Drift, which\nhelps the model understand the step-by-step completion of tasks to mitigate\nGoal Illusion, and Forward Drift, which enables the model to learn how to ac-\ncomplish instructions repeatedly to reduce Imagination Stagnation. The details\nof collecting three kinds of data samples corresponding to each t∗are as follows:\n1. Backward Drift 1: We set tb1 as t∗backward by fixed Tb time steps and\nthen select m −2 random timestamps between tb1 and t∗to form the se-\nquence tb1, . . . , tbm , where t∗is tbm. At each time step, the current and next\nobservations are paired as the current observations and goal imagination,\nrespectively, which can form m −1 samples.\nMineDreamer\n7\nLarge Language Model (LLaMA) \nLoRA\nImage Encoder\nText Encoder\nLearnable Goal Tokens\nDiffusion\nModel\nGoal \nQ-former\n[GOAL0]\n[GOAL1]\n[GOAL2]\n[GOALN]\n…\nText Instruction (𝑦): Chop a tree\nCurrent Observation         \n𝑓∗\nLearnable \nDream Query\nGoal Imagination\nFinetune\/Train\nFrozen\nFig. 4: The Overall Framework of Imaginator. For the goal understanding, we\nadd k [GOAL] tokens to the end of instruction y and input them with current obser-\nvation Ot into LLaVA [43]. Then LLaVA [43] generates hidden states for the [GOAL]\ntokens, which the Q-Former processes to produce the feature f ∗. Subsequently, the\nimage encoder Ev combines its output with f ∗in the diffusion models for instruction-\nbased future goal imagination generation.\n2. Backward Drift 2: In tb1, . . . , tbm, the observations at each timestamp except\nfor tbm are used as the current observations, and the observation at t∗serve\nas the goal imagination, which can form m −1 samples.\n3. Forward Drift: We set tfm as t∗forward by fixed Tf time steps and randomly\nselect m −2 timestamps between t∗and tfm , where t∗is tf1. The observa-\ntion at t∗serves as the current observation, and the observations at future\ntimestamps serve as the goal imaginations, which can form m −1 samples.\nFor more details about the dataset and collection method, please check Supp. B.\n3.4\nImaginator\nInspired by prompt tuning [34, 78, 79], we introduce Imaginator, an MLLM-\nenhanced diffusion model that imagines step by step what to do next based on\nthe current state and instruction, enabling the creation of more precise visual\nprompts for improved low-level control demonstrations of the desired behavior.\nImaginator’s training data utilizes the Goal Drift Dataset from Sec 3.3, consisting\nof (current observation, goal imagination, instruction) triplets.\nGoal Understanding via Task Instruction Following. Given a current ob-\nservation Ot and a textual instruction y, the Imaginator generates a future goal\nimagination It+1 for the PromptGenerator’s visual prompt. In Fig. 4, current\nobservation Ot is encoded by a frozen image encoder Ev into Ev(Ot), textual in-\nstruction y is tokenized into (x1, ..., xT ), they are sent to the LLM together. Imag-\ninator now can acquire a goal imagination of the instruction intention but are\nlimited to the language modality. Inspired by GILL [37], we bridge the language-\nvision modalities gap by extending the LLM’s vocabulary with k Learnable Goal\nTokens [GOAL1], . . . , [GOALk], appending them to instruction y. Specifically, a\ntrainable matrix Eg, representing these [GOAL] embeddings, is added to the\nLLM’s embedding matrix. We aim to minimize the negative log-likelihood of\npredicting the next [GOAL] token given previously generated [GOAL] tokens:\n  \\m a t\nh\nc\nal \n{L} _{\\mathrm {LLM}}=-\\ s um _{i= 1}^ k \\l og  p_{\\left  \\ { \\ theta _{L} \\cup \\theta _{l} \\cup \\mathbf {E}_{g}\\right \\}} ([\\operatorname {GOAL}_{i}] \\mid \\mathbf {E}_{v}(O_{t}),\\nonumber x_{1}, ..., x_{T}, [\\operatorname {GOAL}_{1}], \\ldots ,[\\operatorname {GOAL}_{i-1}]) \\tag {1} \n(1)\n8\nE. Zhou et al.\nWe add LoRA [31] parameters θl into the LLM’s self-attention projection layers\nfor efficient fine-tuning while keeping all LLM parameters θL frozen. During\ntraining, only the LoRA [31] parameters θl and the Learnable Goal Tokens Eg\nare updated. The hidden states h[GOAL] corresponding to Eg tokens are used to\ngenerate imaginations in the following module.\nGoal Imagination Generation via Latent Imagination. To address the\ndisparity between the LLM’s hidden states and the CLIP [53] text encoder’s\nfeature spaces, we must transform the LLM’s sequential goal tokens into seman-\ntically relevant representations for guiding goal imagination generation. Inspired\nby BLIP2 [39] and InstructBLIP [16], we employ a Goal Q-Former Q with several\nLearnable Dream Query, to derive the goal imagination representation f ∗:\n  f ^\n*\n=\\mathc\nal {Q}\\left (h_{[\\operatorname {GOAL}]}\\right ) \\tag {2} \n(2)\nTo enhance goal imagination with representation f ∗to guide imagination gen-\neration, we utilize a latent diffusion model combining a variational autoen-\ncoder (VAE) [36] for latent space denoising diffusion. Drawing from Instruct-\nPix2Pix’s [6] latent diffusion approach, a cornerstone in instruction-based image\nediting, our model introduces noise to the latent encoding z = E(It+1) of the goal\nimagination It+1 through encoder E, yielding a noisy latent zs across timesteps\ns ∈S. A U-Net [59] ϵδ is trained to estimate this noise, conditional on the cur-\nrent observation co = E(Ot) and text instruction cT , by merging co with zs. The\nspecific process can be formulated as follows:\n  \\mat h cal {L}_{\\mathrm {dre am}}=\\math b b {E} _{\\mathcal  {E}(\\ m a thca\nl {I}_{t+1}), \\mathcal {E}(O_{t}), c_{T}, \\epsilon \\sim \\mathcal {N}(0,1), s}[\\| \\epsilon \\nonumber -\\epsilon _\\delta (s, \\mathrm {concat}[z_s, \\mathcal {E}(O_{t})]+f^*) \\|_2^2] \\tag {3} \\label {eq:diffusion} \n(3)\nwhere ϵ is unscaled noise, s is the sampling step, zs is latent noise at step\ns, E(Otn) is the current observation condition, and cT is the text instruction\ncondition. The concat corresponds to the concatenation operation.\n3.5\nPrompt Generator and PolicyNet\nTo transform goal imaginations into precise latent visual prompts that the Poli-\ncyNet can understand, we require a Prompt Generator to serve as the bridge be-\ntween the Imaginator and the PolicyNet. Inspired by STEVE-1 [42], our prompt\ngenerator is a conditional variational autoencoder (CVAE) [36,64] model trained\non the Goal Drift subset dataset. It encodes the current observations, goal imagi-\nnations, and instructions by MineCLIP [20] to produce three embeddings. These\nembeddings are then reconstructed into a latent visual embedding within the\nMineCLIP [20] visual space and a linear layer then projects it into the visual\ninput space of our PolicyNet.\nIn our PolicyNet, we utilize the architecture of the existing model named\nVPT [3] and the training parameters of STEVE-1 [42]. Specifically, as shown\nin Fig. 2, we first process the current observation with a Visual Encoder (i.e.,\nResNet [28]) of VPT [3] and get representation ft. After adding it with the\nlatent visual prompts pt generated by the Prompt Generator, the sum result ot is\nthen fed into the PolicyNet. PolicyNet, whose backbone is Transformer-XL [17],\nprocesses the current input representations ot and autoregressively predicts the\nnext action at. We can describe the process where the Prompt Generator creates\nMineDreamer\n9\nlatent visual prompts pt and PolicyNet predicts the next action at based on them\nand historical observations using the following simple notation:\n p _ {t} \\ lefta rro\nw \\ mathca\nl { G} ( \\ma\nth c a l {O} _ { t } ,  \\mathcal {I}_{t+1}, y) , ~~~ f_{t} \\leftarrow \\mathcal {V}(\\mathcal {O}_{t}),~~~ o_{t} \\leftarrow f_{t} + p_{t}, ~~~ {a}_{t} \\leftarrow \\mathcal {T}(o_{t-T}, \\ldots , o_{t})\\tag {4}\n(4)\nwhere G is PromptGenerator, V is VisualEncoder, and T is TransformerXL [17].\n4\nExperiments\n4.1\nExperimental Setup\nTraining Process. The training process of Imaginator is divided into three\nmain stages. In the first stage, the MLLM is aligned with the CLIP [54] text\nencoder [53] using the QFormer [39]. In the second stage, we apply Instruct-\nPix2Pix [6] to warm up the weights for the diffusion model in Minecraft. In the\nthird stage, we optimize Imaginator in an end-to-end manner. To be specific, the\nweights of LLaVA [43] are frozen and LoRA [31] is added for efficient fine-tuning.\nFor the diffusion model, we directly use the weights pre-trained in the second\nstage as the initial weights in Imaginator. The CVAE [36,64] within the Prompt\nGenerator features a Gaussian prior and a Gaussian posterior, with its encoder\nand decoder, parameterized as three-layer MLPs, each with 512 hidden units\nand layer normalization [2], similar to the architecture of STEVE-1’s [42] prior.\nMore training details can be found in Supp. C.\nTraining Datasets. In the first stage of Imaginator, we use the extensive corpus\nCC12M [10], and our Goal Drift Dataset is used in the second and third stages.\nWe follow STEVE-1’s [42] approach for CVAE [36,64] training, curating a subset\nof approximately 10k quadruplets from the Goal Drift Dataset for our test tasks.\nThis subset includes current observations, goal imaginations, and instructions\nthat match the Goal Drift Dataset. We use the MineCLIP [20] video encoder to\ntransform the goal imagination and the previous 16 frames into a visual prompt\nembedding, which acts as the ground truth. More details can be found in Supp. B.\nEnvironment Setting. We employ MineRL [26] as the Minecraft simulation.\nThe observation space is limited to RGB images, and the action space is confined\nto keyboard and mouse controls, which are consistent with human interaction.\nFor more details about the simulator, please check Supp. A.\nBaseline. We compare MineDreamer with three baseline:\n1. VPT [3], a foundation model pretrained on 70k hours gameplay. Here, we se-\nlect the VPT(rl), which is finetuned by reinforcement learning on the original\nVPT [3] foundation model but cannot follow instructions.\n2. STEVE-1 [42], an instruction-following agent finetuned from VPT(rl). Here,\nwe select STEVE-1(text), which uses a simple prior to aligning the text with\nthe visual space, without considering the current observation.\n3. Multi-Modal Memory, a substitute for the Imaginator and Prompt Genera-\ntor in MineDreamer, efficiently searches through extensive instruction-video\npairs to find the most relevant video as a visual prompt based on the given\ninstruction and the current observation, which effectively leverages the\ncurrent observation and incorporates a CoI mechanism.\n10\nE. Zhou et al.\nFig. 5: Performance on Programmatic Evaluation. MineDreamer surpasses the\nunconditional VPT [3], the text-conditioned STEVE-1 [42] that ignores current state,\nand the Multi-Modal Memory that utilizes current state with a CoI mechanism.\nFor more details about the baseline, please check Supp. D.1.\nEvaluation. We utilize STEVE-1’s [42] early-game evaluation suite, which com-\nprises two evaluations: (1) Programmatic Evaluation, a quantitative evaluation\nused to evaluate an agent’s ability to execute single-step instruction steadily.\nWe track the states provided by the simulator to calculate metrics (e.g., wooden\nlog collection, travel distance). (2) Command-Switching Evaluation, a quanti-\ntative evaluation designed to assess whether the agent can successfully execute\nmulti-step instructions in sequence to complete long-horizon tasks (e.g., ob-\ntaining diamond\n). We use the success rate as the metric for evaluation. More\nevaluation details can be found in Supp. D.2 and Supp. D.3.\n4.2\nPerformance on Textul Instructions Control\nProgrammatic Evaluation. We quantitatively evaluate all agents on 5 tasks\nand plot the programmatic metric performances(mean and 95% confidence in-\ntervals). Each task runs 10 trials with distinct environment seeds, limiting 3,000\nframes (i.e., 2.5 minutes of gameplay) which are consistent with STEVE-1 [42].\nUnlike STEVE-1 [42], we condition all agents with the most suitable biome.\nFig. 5 compares the performance of our MineDreamer with the uncondi-\ntional VPT [3], the text-conditioned STEVE-1 [42] and MineDreamer using\nMulti-Modal Memory. With appropriate text instructions, MineDreamer signif-\nicantly outperforms the unconditional VPT [3], collecting 64× more seeds\n,\n7× more wood\n, 41× more dirt\n, traveling 2.7× further\n, and digging 22×\ndeeper\n. It also surpasses the STEVE-1 [42], collecting 1.7× more seeds\n,\n1.4× more wood\n, 2.1× more dirt\n, traveling 1.2× further\n, and digging\n1.9× deeper\n. Compared to Multi-Modal Memory, MineDreamer collects 1.8×\nmore seeds\n, 1.5× more wood\n, 1.8× more dirt\n, travels 1.3× further\n, and\ndigs 1.1× deeper\n. This demonstrates that our CoI mechanism, which breaks\ndown instructions into multiple stages and executes them step by step, leads to\nsteadier instruction following compared to STEVE-1 [42] which uses direct text\ninstruction guidance. Unlike Multi-Modal Memory, which also features the CoI\nmechanism, our method generates future imaginations that closely resemble the\ncurrent state at each stage, resulting in providing more precise visual prompts\nof the desired behavior, thus enhancing the stability of action generation.\nWe also observe an interesting phenomenon: while Multi-Modal Memory,\nusing the CoI mechanism and current observations, outperforms unconditional\nMineDreamer\n11\n4\n8\n16\n32\n64\nHorizontal   Altitude    (Blocks)\nSwitch instructions when reaching 13th level\n(Dig down -> Mine horizontally)\nSuccess Rate: 10%\nFailed to maintain \nhorizontal altitude Success Rate: 0%\nFig. 6: Performance on Command-Switching Evaluation. (Left) MineDreamer\nswiftly adapts to instructions and follows them steadily, achieving a higher success rate\nthan the unconditional VPT [3], the text-conditioned STEVE-1 [42], and the Multi-\nModal Memory with CoI mechanism. (Right) MineDreamer can dig down\nto a\ndepth of 13 and steadily mine horizontally\nto obtain diamonds\nwith an average\nsuccess rate of 10%, while STEVE-1 [42] struggles to maintain a consistent altitude.\nVPT [3], it sometimes underperforms compared to STEVE-1 [42]. Upon review-\ning the recorded videos and the results of memory retrieval, we find that due\nto the vast diversity of open-world environments, the videos retrieved by Multi-\nModal Memory still exhibit slight differences from the current state. This dis-\ncrepancy misguides the PolicyNet in predicting agent actions, indicating that the\nCoI’s effectiveness hinges on the relevancy and precision of future imaginations\nor visual prompts to the current state.\nCommand-Switching Evaluation for Long-Horizon Tasks. In this part,\nwe explore agents’ ability to solve long-horizon tasks that require executing\nmulti-step instructions in sequence, including (1) collect wood\nand then craft\nplanks\n, (2) gather dirt\nand then build a tower\nand (3) dig down\nand\nthen mine horizontally\nfor diamonds\n, each with 50 trials. Tasks 1 and 2\nlimits 3,000 frames (i.e., 2.5 minutes of gameplay), with instructions changing\nat 1,500 and 2,000 frames. Task 3 limits 12,000 frames (i.e., 10 minutes of game-\nplay), switching instructions upon reaching the 13th floor, as diamonds\nare\ncommonly found between the 7th and 14th floors.\nIn Fig. 6 (Left), MineDreamer consistently surpasses VPT [3] and STEVE-\n1 [42] in Command-Switching tasks. VPT ’s [3] inability to follow instructions\nleads to a complete failure in executing sequential instructions, as evidenced by\na 0% success rate in the evaluation. Although STEVE-1 [42] occasionally com-\npletes Command-Switching tasks, it underperforms compared to MineDreamer.\nFor instance, in the Obtain diamond\ntask, STEVE-1’s [42] success rate is\n0%, while Multi-Modal Memory’s success rate is 2%, notably lower than Mine-\nDreamer’s 10%. As shown in Fig. 6 (Right), we reconstruct an instance where\ntwo agents act in the same environment based on the simulator records. Initially,\nboth MineDreamer and STEVE-1 [42] rapidly dig down\nto the target depth\nand then mine horizontally\nto obtain diamonds\n. Compared to STEVE-\n1 [42], MineDreamer can consistently maintain the specified horizontal level over\nan extended period and successfully obtains diamonds\naround the 10k steps\nin this instance. While STEVE-1 [42] manages to maintain its specified horizon-\ntal level for a long time, it ultimately fails to do so and becomes stuck in the\nbedrock layer (i.e., the agent cannot break any block), resulting in a 0% success\nrate. This demonstrates that, even when instructions are switched rapidly, the\n12\nE. Zhou et al.\nCoI mechanism can still drive the agent to generate future goal imaginations\nthat align with the current state. Visual prompts generated from these imagina-\ntions enable the agent to quickly adapt its actions to correspond with the new\ninstructions while steadily following the instructions in action generation.\nCurrent observation\nInstructPix2Pix\nMineDreamer\nGround Truth\n“Go explore.” \n“Chop a tree, collect the log.” \n“Place a torch on the wall.” \nFig. 7: Qualitative Comparison of Goal Imagination Generation. When compared to\nInstructPix2Pix [6] that have undergone further fine-tuning on our Goal Drift Dataset,\nour approach demonstrates superior goal imagination capabilities in embodied scenar-\nios. See Sec. 4.3 for a more detailed analysis.\n4.3\nQualitative Results of Imaginator\nWe compare Imaginator with the existing state-of-the-art instruction-based im-\nage editing model, namely InstructPix2Pix [6]. Given this model has been trained\non specific datasets, its performance would inevitably be suboptimal if directly\napplied to the Minecraft domain. To facilitate a fair comparison, we fine-tune\nInstructPix2Pix [6] using the same training set employed by the Imaginator and\nassess the performance of the fine-tuned models in addressing tasks in Minecraft.\nFig 7 shows qualitative results in the evaluation set, our methodology exhibits\nenhanced abilities in Goal Imagination Generation within intricate scenarios.\nThe first comparison shows that the Imaginator adeptly captures the agent’s\nperspective shift as it advances, whereas InstructPix2Pix [6] struggles to generate\nimages in alignment with the provided instructions. In the second instance, the\nImaginator specifically visualizes the region with felled trees\n, contrasting with\nInstructPix2Pix [6], which yields an image markedly divergent from the exist-\ning observation background. The third comparison highlights the Imaginator’s\nability to depict enhanced visibility following torch placement, in contrast to\nInstructPix2Pix [6], which merely adds torches without the associated increase\nin illumination. These observations suggest that in scenarios requiring instruc-\ntion reasoning and goal understanding, a simple CLIP [54] text encoder may\nstruggle to guide the diffusion model to generate reasonable goal imagination.\nMineDreamer\n13\nCurrent Observation\nCurrent Observation\nGoal Imagination\nGoal Imagination\nNext Observation\nNext Observation\nFig. 8: The Generalizability of MineDreamer. (Left) Despite excluding data in-\nvolving ‘Dirt’\nor ‘Dig’\nfrom Goal Drift Dataset and retraining, Imaginator can\nstill generate relatively high-quality imaginations aligned with the instruction’s con-\ncept. (Right) The retrained Imaginator remains operational with the CoI mechanism\nand can handle unseen instructions while largely preserving its previous performance.\nHowever, the MLLM can fully utilize its powerful reasoning ability, vast environ-\nmental knowledge, and intrinsic physical rules to correctly understand the goal\nand generate goal imagination. More visual results can be found in Supp. F and\nSupp. G.\n4.4\nDiscussion on Generalization\nIn this part, we will explore the generalizability of MineDreamer, as the agent’s\nability to generalize is key to its behavior in the open world where environments\nare complex and instructions vary widely. Since STEVE-1 [42] has shown its prior\nability to map text to visual prompts effectively, and our Prompt Generator is\nbuilt upon it, we will now concentrate on the generalizability of our Imaginator\nand the entire agent. At first, we exclude data related to the words ‘Dirt’\nor\n‘Dig’\nfrom the Goal Drift Dataset and retrain the model. Then, we observe\nthe images generated in response to the instruction “Collect dirt”\nbased on\nthe current state and the quantity of dirt\ncollected by the agent.\nAs shown in Fig. 8, we find that even after completely removing the concepts\nof ‘Dirt’\nor ‘Dig’\n, Imaginator is still able to generate goal imaginations\nof relatively good quality aligned with the instruction’s concept (i.e., agent\npoints towards the dirt\nand attempt to break it), which can still guide\nthe PolicyNet to follow instructions. The resulting collection of dirt\nis about\n70% of the original amount, which shows that the Imaginator can respond to\nunseen novel instructions while largely maintaining its previous performance.\nWe attribute this to three key factors: (1) The MLLM within Imaginator has\nthe relevant environmental knowledge to map the text ‘Dirt’\nto its corre-\nsponding element in Minecraft images, recognizing its visual counterpart; (2)\ntraining data for related tasks, such as “Collect seeds”\n, enables the MLLM\nTable 1: We study the impact of dataset collection methods on agent performance.\nValues in parentheses represent 95% confidence intervals.\nInstruction\nFixed Timestep\nOnly Backward\nOnly Forward\nNormal\nBackward\nDrift\nDrift\n“Chop a tree”\n7.60(3.84, 11.36)\n10.10(2.82, 5.58)\n4.20(2.82, 5.58)\n24.30(21.71, 26.89)\n“Collect dirt”\n38.60(21.97, 55.23) 30.30(20.71, 39.89) 18.10(6.74, 29.46) 65.20(55.81, 74.59)\n14\nE. Zhou et al.\nto comprehend the meaning of action ‘Collect’ in Minecraft task; (3) The pre-\ntrained Diffusion model can generalize to the Minecraft domain and generate\ngoal imaginations by leveraging the MLLM’s latent representations for under-\nstanding textual semantics mentioned above from the instructions.\nTable 2: We study the impact of the Chain-of-Imagination and diffusion model ability\non agent performance. Values in parentheses represent 95% confidence intervals.\nInstruction\nwo CoI\nRandom\nInstruct-\nNormal\nNoise\nPix2Pix\n“Chop a tree”\n18.70(15.26, 22.14)\n2.70(0.85, 4.55)\n22.90(20.17, 25.63) 24.30(21.71, 26.89)\n“Collect dirt”\n53.50(36.93, 70.07) 10.90(3.95, 17.85) 59.50(54.00, 65.00) 65.20(55.81, 74.59)\n4.5\nWhat Contributes to Performance\nDataset Collection Method. In Tab. 1, we study the impact on agent per-\nformance by training with datasets of equal size collected using fixed Back-\nward timesteps, only Backward Drift, only Forward Drift, and normal Goal Drift\nDataset Collection. Although data collected using the first three methods can\nenable the agent to follow instructions, the Imaginator is affected by Goal Illu-\nsion and Imagination Stagnation, which are discussed in Sec. 3.3. This results\nin the Imaginator’s inability to envision the step-by-step process of completing\nthe instruction and how to steadily complete the instruction multiple times.\nChain-of-Imagination. In Tab. 2, we explore the effect of the CoI mech-\nanism on agent performance, where “wo-CoI” denotes the scenario where the\nagent generates the goal imagination and visual prompt only at the beginning\nand remains unchanged thereafter. Compared to normal performance, “wo-CoI”\nachieves about 77%. This is because the visual prompts generated at the be-\nginning become less capable of providing precise demonstrations of the desired\nbehavior in later stages, resulting in hindering the ability to guide the agent step\nby step more steadily.\nDiffusion Model Ability. In Tab. 2, we explore the impact of diffusion model\nability on performance. Using “random noise” as a goal imagination results in\nvague visual prompts, which drastically reduce performance to merely 10% of its\noriginal level. The performance of InstructPix2Pix [6] and our MLLM-enhanced\ndiffusion model are comparable; however, by leveraging MLLM, our generated\nimages adhere more closely to physical rules and environmental knowledge, as\nshown in Fig. 7. Additionally, as discussed in Sec. 4.2, we discover that the CoI\nmechanism demands a certain quality of goal imagination, suggesting that the\nstronger the Imaginator, the better it can guide agents to follow instructions.\nMore ablation studies can be found in Supp. E.\n5\nConclusion and Limitation\nIn this paper, we introduce an innovative paradigm for enhancing the instruction-\nfollowing ability of agents in simulated-world control. We prove that by employ-\ning a Chain-of-Imagination mechanism to envision the step-by-step process of\nexecuting instructions, and translating imaginations into precise visual prompts\nMineDreamer\n15\ntailored to the current state and instruction, can significantly help the foundation\nmodel follow instructions steadily in action generation. Our Agent, MineDreamer\nin Minecraft, showcases its strong instruction-following ability. Furthermore, we\nshow its potential as a high-level planner’s downstream controller in the chal-\nlenging “Obtain diamond”\ntask. We believe this novel paradigm will inspire\nfuture research and generalize to other domains and open-world environments.\nLimitation.\nFirstly, generating high-quality imagination can take seconds,\nslowing down frequent-use scenarios. Speed enhancements via distillation [61]\nand quantization [24] may mitigate this. Secondly, the Imaginator may pro-\nduce unrealistic hallucinations. Integrating world knowledge via methods such\nas RAG [38] or reducing MLLM hallucinations [45] could mitigate this.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nMineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control\n```\n#### 2. 论文摘要\n```\nIt is a long-lasting goal to design a generalist-embodied agent that can\nfollow diverse instructions in human-like ways. However, existing approaches\noften fail to steadily follow instructions due to difficulties in understanding\nabstract and sequential natural language instructions. To this end, we\nintroduce MineDreamer, an open-ended embodied agent built upon the challenging\nMinecraft simulator with an innovative paradigm that enhances\ninstruction-following ability in low-level control signal generation.\nSpecifically, MineDreamer is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs) and diffusion models, and we employ a\nChain-of-Imagination (CoI) mechanism to envision the step-by-step process of\nexecuting instructions and translating imaginations into more precise visual\nprompts tailored to the current state; subsequently, the agent generates\nkeyboard-and-mouse actions to efficiently achieve these imaginations, steadily\nfollowing the instructions at each step. Extensive experiments demonstrate that\nMineDreamer follows single and multi-step instructions steadily, significantly\noutperforming the best generalist agent baseline and nearly doubling its\nperformance. Moreover, qualitative analysis of the agent's imaginative ability\nreveals its generalization and comprehension of the open world.\n```\n\n#### 3. 论文全文\n```\nMineDreamer: Learning to Follow Instructions\nvia Chain-of-Imagination for\nSimulated-World Control\nEnshen Zhou1,2∗, Yiran Qin1,3∗,\nZhenfei Yin1,4, Yuzhou Huang3, Ruimao Zhang3†, Lu Sheng2†,\nYu Qiao1, Jing Shao1‡\n1 Shanghai Artificial Intelligence Laboratory\n2 Beihang University\n3 The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen)\n4 The University of Sydney\nzhouenshen@buaa.edu.cn\nyiranqin@link.cuhk.edu.cn\nhttps:\/\/sites.google.com\/view\/minedreamer\/main\nAbstract. It is a long-lasting goal to design a generalist-embodied agent\nthat can follow diverse instructions in human-like ways. However, exist-\ning approaches often fail to steadily follow instructions due to difficul-\nties in understanding abstract and sequential natural language instruc-\ntions. To this end, we introduce MineDreamer, an open-ended embodied\nagent built upon the challenging Minecraft simulator with an innovative\nparadigm that enhances instruction-following ability in low-level con-\ntrol signal generation. Specifically, MineDreamer is developed on top of\nrecent advances in Multimodal Large Language Models (MLLMs) and\ndiffusion models, and we employ a Chain-of-Imagination (CoI) mecha-\nnism to envision the step-by-step process of executing instructions and\ntranslating imaginations into more precise visual prompts tailored to the\ncurrent state; subsequently, the agent generates keyboard-and-mouse ac-\ntions to efficiently achieve these imaginations, steadily following the in-\nstructions at each step. Extensive experiments demonstrate that Mine-\nDreamer follows single and multi-step instructions steadily, significantly\noutperforming the best generalist agent baseline and nearly doubling its\nperformance. Moreover, qualitative analysis of the agent’s imaginative\nability reveals its generalization and comprehension of the open world.\nKeywords: Chain-of-Imagination · multimodal large language model ·\ninstruction following · low-level control\n1\nIntroduction\nOne of the core objectives of current embodied intelligence is to develop a gener-\nalist low-level control agent that can follow diverse instructions to solve endless\nopen-world embodied tasks [4, 5, 8, 42, 57]. Recent studies [4, 5, 8, 42] success-\nfully unlock the instruction-following ability of foundation models [3, 12, 15] in\n∗Equal contribution\n† Corresponding author\n‡ Project leader\narXiv:2403.12037v2  [cs.CV]  19 Mar 2024\n2\nE. Zhou et al.\nMineDreamer\nPrevious Studies\n“Chop a tree.” \n…\n…\n“Chop a tree.” \n“Chop a tree.” \n“Chop a tree.” \nFig. 1: Comparison between MineDreamer and previous studies. In “Chop\na tree”\ntask, MineDreamer employs a Chain-of-Imagination mechanism, where it\nimagines step by step what to do next tailored to the current state. Imaginations\ncontain environmental understanding and physical rules (e.g., perspective-based size\nchanges). These can serve as more precise visual prompts to steadily guide the agent in\ngenerating actions to achieve these imaginations as effectively as possible at each step.\nPrevious approaches have seen a tree, but missed the opportunity to chop it down.\nthe sequential decision-making domain [8, 11, 33, 57, 63, 69, 73]. However, these\nmethods [5,42] struggle to enable agents to follow textual instructions steadily,\ndue to the: (1) Many textual instructions are abstract for low-level control and\nmodels struggle to effectively understand. They should be transformed into more\neffective prompts that consider how to execute instructions based on the current\nstate. Hence, simple textual instructions cannot provide a precise demonstration\nof the desired behavior. (2) Many textual instructions are sequential, and exe-\ncuting them may require considering the current state and breaking down the\ntask into multiple stages for step-by-step completion. Therefore, steady action\ngeneration driven by single-text instructions often fails.\nTo address the above issues, this work aims to explore how to unlock the\nsituation-aware reasoning ability for a pre-trained decision-making foundation\nmodel. We introduce a simple yet effective mechanism called Chain-of-Imagination\n(CoI), which enables the agent to imagine and act upon the next stage step by\nstep according to the instructions. Our method is motivated by two ideas: (1)\nWhen solving complex problems, humans often envision the goal of the next\nstage based on the current state. If we can break down the sequential instructions\ninto multiple stages according to the current state, step by step, we can enable\nagents to follow instructions steadily. (2) Inspired by prompt tuning [34,78,79],\nif we can provide visual prompts containing physical rules and environmental\nunderstanding for each imagined step, tailored to optimally describe the desired\nbehavior in the current state, which are more intuitive and efficient than task\ninstructions, we can better guide the foundation model in predicting actions.\nTo this end, we propose MineDreamer within Minecraft, which generates a\nseries of “imagined” sub-steps based on the textual instructions and current state.\nThese visual sub-steps are then fed into a pre-trained decision-making founda-\ntion model to generate low-level control actions aimed at achieving the sub-steps.\nSpecifically, MineDreamer comprises three modules: (1) An Imaginator, a dif-\nfusion model enhanced by a Multimodal Large Language Model (MLLM), can\nbetter generate imaginations that contain the physical rules and environmental\nMineDreamer\n3\nunderstanding. (2) A Prompt Generator, the bridge between Imaginator and\nPolicyNet, can convert future imaginations into latent visual prompts that offer\nmore logical and precise demonstrations of the desired behavior. (3) A PolicyNet,\na foundation model, can use latent prompts as guidance to predict actions for\nagents in an open-world environment.\nNotably, as shown in Fig. 1, MineDreamer leverages a Chain-of-Imagination\nmechanism through multi-turn interaction between the Imaginator and the Pol-\nicyNet and cyclically generates latent visual prompts that better align with the\ncurrent state to guide the PolicyNet in following instructions steadily in action\ngeneration. This mechanism represents an attempt to implement “self multi-turn\ninteraction” in the sequential decision-making domain. Training an Imaginator\nin an open-world environment to envision the image of the next step requires\nextensive data. We employ the Goal Drift Collection method to gather a large\namount of egocentric embodied data, which helps the Imaginator to understand\nhow to achieve the instruction sequentially and how to achieve it repeatedly.\nOur main contributions are as follows:\n– We introduce the Chain-of-Imagination(CoI) method, which introduces “self\nmulti-turn interaction” to the sequential decision-making domain and en-\nables the agent to follow human instructions steadily in action generation.\n– We propose the Goal Drift Collection method and an MLLM-enhanced dif-\nfusion model that can generate imaginations adhering to physical rules and\nenvironmental understanding, providing more precise visual prompts rele-\nvant to the current state and instructions.\n– Leveraging these methods, we create an embodied agent in Minecraft named\nMineDreamer that has achieved nearly double the performance of the best\ngeneralist agent baseline in executing single and multi-step instructions steadily.\n2\nRelated Work\n2.1\nBuild Instruction-Following Agents in Minecraft\nResearch on generalist agents in Minecraft’s complex and dynamic environment\nis increasingly popular in AI. Despite the exploration of Large Language Mod-\nels [7,14,48,54,65,66] as high-level task planners that guide agents in executing\nlong-horizon tasks [25,50,70–72,81] like Voyager [70] and MP5 [50], we still re-\nquire lower-level controllers [3,8,20,27,42] to execute the generated plans. In the\nsequential decision-making domain, DreamerV3 [27] trains agents using a world\nmodel, while VPT [3] builds a large foundational model to generate actions by\nlearning from extensive video data. However, neither can follow instructions.\nGROOT [8] is developed to follow video instructions but fails to follow text\ninstructions. STEVE-1 [42], an evolution of VPT [3], is built for text instruc-\ntions but struggles to understand natural language prompts, despite extensive\nprompt engineering. Therefore, we create MineDreamer, which, leveraging the\nChain-of-Imagination mechanism, generates more precise visual prompts step-\nby-step, enabling it to follow instructions steadily in action generation.\n4\nE. Zhou et al.\n2.2\nConditioned Diffusion Models in Embodied Scenario\nWith the development of the text-to-image diffusion model [18,30,46,55,58,60],\nthe instruction-based diffusion methods [6, 9, 21, 23, 29, 32, 35, 67, 76] have re-\ncently marked considerable progress in generative tasks, especially in embod-\nied scenarios. UniPi [19] and HiP [1] integrate video diffusion with inverse dy-\nnamics to generate robot control signals for specific tasks. SkillDiffuser [41]\napplies interpretable hierarchical planning via skill abstractions in diffusion-\nbased task execution. While existing methods can only handle embodied tasks\nlimited to fixed environments, the emergence of Multimodal Large Language\nModels (MLLMs) [13, 22, 43, 49, 62, 74, 75, 80] has showcased superior reasoning\nand perceptual abilities in open-world environment. Inspired by this, we create\nan MLLM-enhanced diffusion model, focusing on the model’s understanding of\nphysics rules and environmental understanding, and its ability to create high-\nquality egocentric images for guiding low-level action generation.\n3\nMethod\nIn this section, we first provide an overview (Sec. 3.1) of our MineDreamer, in-\ncluding its mechanisms and features. Next, we introduce the purpose and work-\nflow of the Chain-of-Imagination (CoI) mechanism (Sec. 3.2) regarding Fig. 2.\nTo implement CoI and collect extensive embodied data to train Imaginator, we\nelaborate on the dataset construction (Sec. 3.3), including Goal Drift Collec-\ntion method. Finally, we provide the necessary details of each part, including\nImaginator (Sec. 3.4), Prompt Generator, and PolicyNet (Sec. 3.5).\n3.1\nOverview\nOur MineDreamer comprises three modules, i.e., Imaginator, Prompt Genera-\ntor, and PolicyNet. Our objective is to empower agents, especially foundation\nmodels in the sequential decision-making domain, to follow human instructions\nsteadily and act accordingly. The Imaginator is a parameter-efficiently fine-tuned\ndiffusion model specific to Minecraft utilizing the visual reasoning ability of\na Multimodal Large Language Model (MLLM). The Prompt Generator recon-\nstructs latent visual prompts from the current observations, future imaginations,\nand instructions. PolicyNet is the existing Video Pretraining (VPT) [3] model,\ntrained on 70k hours of Minecraft gameplay.\nWhy future goal imagination? Given a pre-trained model that can predict\nactions, the intuitive approach is to input the current state and instructions to\nguide it directly. So why the future goal imagination? In practice, we find that\nfuture goal imagination proves more interpretable for humans, easing debugging,\nand improving interaction and safety assessment [40, 51, 56, 77]. Furthermore,\nimages yield flexible, explicit representations, facilitating natural language goal\ndecomposition into clearer stages by learned physical rules and environmental\nunderstanding, helping the low-level control model “plan” what to do now.\nMineDreamer\n5\nImaginator\n…\nVPT\nVPT\nVPT\n𝑓𝑡\n𝑝𝑡\n𝑓𝑡+1\n𝑝𝑡+1\n𝑓𝑡+𝑇−1\n𝑝𝑡+𝑇−1\n𝑬𝑽\n𝑬𝑽\n𝑬𝑽\n𝑮\n𝑮\n𝑮\n𝑬𝑽\n𝑮\nVisual \nEncoder\nPrompt\nGenerator\nVisual &\nVisual Only\nImaginator\nImaginator\nText Instruction(𝑦) \nChop a tree\nCurrent Observation\nGoal Imagination\nInstruction\nFig. 2: The Overview of Chain-of-Imagination. The Imaginator imagines a goal\nimagination based on the instruction and current observation. The Prompt Generator\ntransforms this into a precise visual prompt, considering both the instruction and\nobserved image. The Visual Encoder encodes the current observation, integrates it\nwith this prompt, and inputs this into VPT. VPT then determines the agent’s next\naction, leading to a new observation, and the cycle continues. Note that VPT’s input is\nhistorical observations, so the figure cannot fully represent the autoregressive process.\nMore details about VPT as PolicyNet can be found in Sec. 3.5.\nWhy can MineDreamer follow instructions more steadily? Firstly, Mine-\nDreamer employs a Chain-of-Imagination (CoI) mechanism for incremental goal\nachievement via self-multi-turn interactions, enabling the agent to appropriately\nrespond to the current state. In addition, with the help of this mechanism, the\nPrompt Generator crafts logical latent visual prompts that provide clear demon-\nstrations of desired behaviors, ensuring that the agent steadily follows instruc-\ntions. Furthermore, the enhanced Imaginator not only comprehends open-ended\nvisual concepts, enabling it to imagine images of novel instructions it has never\nseen before but also ensures these images adhere to physical rules and envi-\nronmental understanding, thereby sharpening the precision of prompts. Thus,\nMineDreamer can follow instructions steadily in an open-world environment.\n3.2\nChain-of-Imagination\nChain-of-Imagination (CoI) enables the agent to envision the steps needed to\nachieve a goal iteratively. As shown in Fig. 2, it is an example to demonstrate\nhow CoI works. First, the Imaginator takes in the user’s instructions y and\ncurrent observations Ot and imagines a future image It+1 depicting a moment\nwithin the process of completing the given instruction y, which is closely related\nto the current observation Ot. Next, the Prompt Generator progressively creates\na more precise latent visual prompt pt in awareness of the current observation Ot,\ninstruction y and future imagination It+1, aligning with the visual input space of\nthe Video Pretraining (VPT) [3] model. The Visual Encoder then processes Ot\ninto a representation ft, which is combined with pt and fed into VPT [3]. Finally,\nVPT [3] progressively predicts an action (i.e., keyboard and mouse) from the\nobservation history, interacts with the environment, gathers a new observation\nOt+1, and repeats the cycle later.\n3.3\nDatasets\nWe train the Imaginator with the Goal Drift Dataset, which includes 500k\ntriplets (current observation, future goal imagination, instruction) from the Ope-\nnAI Contractor Gameplay Dataset [3], using the Goal Drift Collection method.\n6\nE. Zhou et al.\n𝑡∗\n𝑡𝑏1\nBackward\nForward\nBackward\nDrift\nForward\nDrift\n…\n…\n…\n…\n𝑡𝑏2\n𝑡𝑏𝑛\n𝑡𝑏𝑛+1\n𝑡𝑓𝑛\n𝑡𝑓𝑚\nInstruction(𝑦): “ Chop a tree\/Collect wood\/…”\nFig. 3: Goal Drift Collection. For each timestamp t∗, we form many triplets com-\nprising (current observation, goal imagination, instruction) associated with the game\nevent-related instructions completed by contractors. Each pair of linked images forms\na training triplet with its instruction for the Imaginator in this figure.\nOpenAI Contractor Gameplay Dataset. OpenAI Contractor Gameplay\nDataset [3] is created by hiring human contractors to play Minecraft and com-\nplete tasks like house building. Game events, like “mine_block”, noting the type\nof block broken, are logged with timestamps. These timestamps (t∗) provide\nprecise progress tracking and align with completed event-related instructions.\nGoal Drift Collection. The Gameplay Dataset allows us to construct numer-\nous embodied data by using specific event-related instructions achieved at each\ntimestamp t∗. Yet, directly pairing images from these timestamps t∗as future\ngoal imaginations Ot∗with images from a fixed timestep T earlier as current\nobservations Ot∗−T , along with instruction y, could lead to certain problems:\n(1) Goal Illusion: The Imaginator edits the observation to depict the\ncompleted instruction. Training the Imaginator on such data may reduce it\nto an image editor, as it generates imaginations without regard to the environ-\nment because all goal imaginations in the dataset represent the moment when\ninstruction is completed. For instance, given the instruction “Break dirt”\nwhile facing the sky, the Imaginator may unrealistically insert a broken dirt\nblock\ninto the sky. (2) Imagination Stagnation: The Imaginator fails\nto conceive repeated task completion. The Imaginator is trained to envision\nthe instructions’ fulfillment once, not recognizing the need for repetition, as all\ncurrent observations precede the achievement of instructions. For instance, given\n“Chop a tree”\n, after cutting the uppermost wood\nby looking up, the agent\nwill not look down for more trees\n, impeding continuous task performance.\nTo address the aforementioned issues, we propose the Goal Drift Collection\nmethod to gather Goal Drift Dataset. From the Gameplay Dataset, we form\nmany triplets (current observation, goal imagination, instruction) at each times-\ntamp t∗, all associated with the same event-related instructions y completed\nby the contractors. Fig. 3 shows that a pair of linked images with instructions\ny constitutes a training triplet. Our approach has both Backward Drift, which\nhelps the model understand the step-by-step completion of tasks to mitigate\nGoal Illusion, and Forward Drift, which enables the model to learn how to ac-\ncomplish instructions repeatedly to reduce Imagination Stagnation. The details\nof collecting three kinds of data samples corresponding to each t∗are as follows:\n1. Backward Drift 1: We set tb1 as t∗backward by fixed Tb time steps and\nthen select m −2 random timestamps between tb1 and t∗to form the se-\nquence tb1, . . . , tbm , where t∗is tbm. At each time step, the current and next\nobservations are paired as the current observations and goal imagination,\nrespectively, which can form m −1 samples.\nMineDreamer\n7\nLarge Language Model (LLaMA) \nLoRA\nImage Encoder\nText Encoder\nLearnable Goal Tokens\nDiffusion\nModel\nGoal \nQ-former\n[GOAL0]\n[GOAL1]\n[GOAL2]\n[GOALN]\n…\nText Instruction (𝑦): Chop a tree\nCurrent Observation         \n𝑓∗\nLearnable \nDream Query\nGoal Imagination\nFinetune\/Train\nFrozen\nFig. 4: The Overall Framework of Imaginator. For the goal understanding, we\nadd k [GOAL] tokens to the end of instruction y and input them with current obser-\nvation Ot into LLaVA [43]. Then LLaVA [43] generates hidden states for the [GOAL]\ntokens, which the Q-Former processes to produce the feature f ∗. Subsequently, the\nimage encoder Ev combines its output with f ∗in the diffusion models for instruction-\nbased future goal imagination generation.\n2. Backward Drift 2: In tb1, . . . , tbm, the observations at each timestamp except\nfor tbm are used as the current observations, and the observation at t∗serve\nas the goal imagination, which can form m −1 samples.\n3. Forward Drift: We set tfm as t∗forward by fixed Tf time steps and randomly\nselect m −2 timestamps between t∗and tfm , where t∗is tf1. The observa-\ntion at t∗serves as the current observation, and the observations at future\ntimestamps serve as the goal imaginations, which can form m −1 samples.\nFor more details about the dataset and collection method, please check Supp. B.\n3.4\nImaginator\nInspired by prompt tuning [34, 78, 79], we introduce Imaginator, an MLLM-\nenhanced diffusion model that imagines step by step what to do next based on\nthe current state and instruction, enabling the creation of more precise visual\nprompts for improved low-level control demonstrations of the desired behavior.\nImaginator’s training data utilizes the Goal Drift Dataset from Sec 3.3, consisting\nof (current observation, goal imagination, instruction) triplets.\nGoal Understanding via Task Instruction Following. Given a current ob-\nservation Ot and a textual instruction y, the Imaginator generates a future goal\nimagination It+1 for the PromptGenerator’s visual prompt. In Fig. 4, current\nobservation Ot is encoded by a frozen image encoder Ev into Ev(Ot), textual in-\nstruction y is tokenized into (x1, ..., xT ), they are sent to the LLM together. Imag-\ninator now can acquire a goal imagination of the instruction intention but are\nlimited to the language modality. Inspired by GILL [37], we bridge the language-\nvision modalities gap by extending the LLM’s vocabulary with k Learnable Goal\nTokens [GOAL1], . . . , [GOALk], appending them to instruction y. Specifically, a\ntrainable matrix Eg, representing these [GOAL] embeddings, is added to the\nLLM’s embedding matrix. We aim to minimize the negative log-likelihood of\npredicting the next [GOAL] token given previously generated [GOAL] tokens:\n  \\m a t\nh\nc\nal \n{L} _{\\mathrm {LLM}}=-\\ s um _{i= 1}^ k \\l og  p_{\\left  \\ { \\ theta _{L} \\cup \\theta _{l} \\cup \\mathbf {E}_{g}\\right \\}} ([\\operatorname {GOAL}_{i}] \\mid \\mathbf {E}_{v}(O_{t}),\\nonumber x_{1}, ..., x_{T}, [\\operatorname {GOAL}_{1}], \\ldots ,[\\operatorname {GOAL}_{i-1}]) \\tag {1} \n(1)\n8\nE. Zhou et al.\nWe add LoRA [31] parameters θl into the LLM’s self-attention projection layers\nfor efficient fine-tuning while keeping all LLM parameters θL frozen. During\ntraining, only the LoRA [31] parameters θl and the Learnable Goal Tokens Eg\nare updated. The hidden states h[GOAL] corresponding to Eg tokens are used to\ngenerate imaginations in the following module.\nGoal Imagination Generation via Latent Imagination. To address the\ndisparity between the LLM’s hidden states and the CLIP [53] text encoder’s\nfeature spaces, we must transform the LLM’s sequential goal tokens into seman-\ntically relevant representations for guiding goal imagination generation. Inspired\nby BLIP2 [39] and InstructBLIP [16], we employ a Goal Q-Former Q with several\nLearnable Dream Query, to derive the goal imagination representation f ∗:\n  f ^\n*\n=\\mathc\nal {Q}\\left (h_{[\\operatorname {GOAL}]}\\right ) \\tag {2} \n(2)\nTo enhance goal imagination with representation f ∗to guide imagination gen-\neration, we utilize a latent diffusion model combining a variational autoen-\ncoder (VAE) [36] for latent space denoising diffusion. Drawing from Instruct-\nPix2Pix’s [6] latent diffusion approach, a cornerstone in instruction-based image\nediting, our model introduces noise to the latent encoding z = E(It+1) of the goal\nimagination It+1 through encoder E, yielding a noisy latent zs across timesteps\ns ∈S. A U-Net [59] ϵδ is trained to estimate this noise, conditional on the cur-\nrent observation co = E(Ot) and text instruction cT , by merging co with zs. The\nspecific process can be formulated as follows:\n  \\mat h cal {L}_{\\mathrm {dre am}}=\\math b b {E} _{\\mathcal  {E}(\\ m a thca\nl {I}_{t+1}), \\mathcal {E}(O_{t}), c_{T}, \\epsilon \\sim \\mathcal {N}(0,1), s}[\\| \\epsilon \\nonumber -\\epsilon _\\delta (s, \\mathrm {concat}[z_s, \\mathcal {E}(O_{t})]+f^*) \\|_2^2] \\tag {3} \\label {eq:diffusion} \n(3)\nwhere ϵ is unscaled noise, s is the sampling step, zs is latent noise at step\ns, E(Otn) is the current observation condition, and cT is the text instruction\ncondition. The concat corresponds to the concatenation operation.\n3.5\nPrompt Generator and PolicyNet\nTo transform goal imaginations into precise latent visual prompts that the Poli-\ncyNet can understand, we require a Prompt Generator to serve as the bridge be-\ntween the Imaginator and the PolicyNet. Inspired by STEVE-1 [42], our prompt\ngenerator is a conditional variational autoencoder (CVAE) [36,64] model trained\non the Goal Drift subset dataset. It encodes the current observations, goal imagi-\nnations, and instructions by MineCLIP [20] to produce three embeddings. These\nembeddings are then reconstructed into a latent visual embedding within the\nMineCLIP [20] visual space and a linear layer then projects it into the visual\ninput space of our PolicyNet.\nIn our PolicyNet, we utilize the architecture of the existing model named\nVPT [3] and the training parameters of STEVE-1 [42]. Specifically, as shown\nin Fig. 2, we first process the current observation with a Visual Encoder (i.e.,\nResNet [28]) of VPT [3] and get representation ft. After adding it with the\nlatent visual prompts pt generated by the Prompt Generator, the sum result ot is\nthen fed into the PolicyNet. PolicyNet, whose backbone is Transformer-XL [17],\nprocesses the current input representations ot and autoregressively predicts the\nnext action at. We can describe the process where the Prompt Generator creates\nMineDreamer\n9\nlatent visual prompts pt and PolicyNet predicts the next action at based on them\nand historical observations using the following simple notation:\n p _ {t} \\ lefta rro\nw \\ mathca\nl { G} ( \\ma\nth c a l {O} _ { t } ,  \\mathcal {I}_{t+1}, y) , ~~~ f_{t} \\leftarrow \\mathcal {V}(\\mathcal {O}_{t}),~~~ o_{t} \\leftarrow f_{t} + p_{t}, ~~~ {a}_{t} \\leftarrow \\mathcal {T}(o_{t-T}, \\ldots , o_{t})\\tag {4}\n(4)\nwhere G is PromptGenerator, V is VisualEncoder, and T is TransformerXL [17].\n4\nExperiments\n4.1\nExperimental Setup\nTraining Process. The training process of Imaginator is divided into three\nmain stages. In the first stage, the MLLM is aligned with the CLIP [54] text\nencoder [53] using the QFormer [39]. In the second stage, we apply Instruct-\nPix2Pix [6] to warm up the weights for the diffusion model in Minecraft. In the\nthird stage, we optimize Imaginator in an end-to-end manner. To be specific, the\nweights of LLaVA [43] are frozen and LoRA [31] is added for efficient fine-tuning.\nFor the diffusion model, we directly use the weights pre-trained in the second\nstage as the initial weights in Imaginator. The CVAE [36,64] within the Prompt\nGenerator features a Gaussian prior and a Gaussian posterior, with its encoder\nand decoder, parameterized as three-layer MLPs, each with 512 hidden units\nand layer normalization [2], similar to the architecture of STEVE-1’s [42] prior.\nMore training details can be found in Supp. C.\nTraining Datasets. In the first stage of Imaginator, we use the extensive corpus\nCC12M [10], and our Goal Drift Dataset is used in the second and third stages.\nWe follow STEVE-1’s [42] approach for CVAE [36,64] training, curating a subset\nof approximately 10k quadruplets from the Goal Drift Dataset for our test tasks.\nThis subset includes current observations, goal imaginations, and instructions\nthat match the Goal Drift Dataset. We use the MineCLIP [20] video encoder to\ntransform the goal imagination and the previous 16 frames into a visual prompt\nembedding, which acts as the ground truth. More details can be found in Supp. B.\nEnvironment Setting. We employ MineRL [26] as the Minecraft simulation.\nThe observation space is limited to RGB images, and the action space is confined\nto keyboard and mouse controls, which are consistent with human interaction.\nFor more details about the simulator, please check Supp. A.\nBaseline. We compare MineDreamer with three baseline:\n1. VPT [3], a foundation model pretrained on 70k hours gameplay. Here, we se-\nlect the VPT(rl), which is finetuned by reinforcement learning on the original\nVPT [3] foundation model but cannot follow instructions.\n2. STEVE-1 [42], an instruction-following agent finetuned from VPT(rl). Here,\nwe select STEVE-1(text), which uses a simple prior to aligning the text with\nthe visual space, without considering the current observation.\n3. Multi-Modal Memory, a substitute for the Imaginator and Prompt Genera-\ntor in MineDreamer, efficiently searches through extensive instruction-video\npairs to find the most relevant video as a visual prompt based on the given\ninstruction and the current observation, which effectively leverages the\ncurrent observation and incorporates a CoI mechanism.\n10\nE. Zhou et al.\nFig. 5: Performance on Programmatic Evaluation. MineDreamer surpasses the\nunconditional VPT [3], the text-conditioned STEVE-1 [42] that ignores current state,\nand the Multi-Modal Memory that utilizes current state with a CoI mechanism.\nFor more details about the baseline, please check Supp. D.1.\nEvaluation. We utilize STEVE-1’s [42] early-game evaluation suite, which com-\nprises two evaluations: (1) Programmatic Evaluation, a quantitative evaluation\nused to evaluate an agent’s ability to execute single-step instruction steadily.\nWe track the states provided by the simulator to calculate metrics (e.g., wooden\nlog collection, travel distance). (2) Command-Switching Evaluation, a quanti-\ntative evaluation designed to assess whether the agent can successfully execute\nmulti-step instructions in sequence to complete long-horizon tasks (e.g., ob-\ntaining diamond\n). We use the success rate as the metric for evaluation. More\nevaluation details can be found in Supp. D.2 and Supp. D.3.\n4.2\nPerformance on Textul Instructions Control\nProgrammatic Evaluation. We quantitatively evaluate all agents on 5 tasks\nand plot the programmatic metric performances(mean and 95% confidence in-\ntervals). Each task runs 10 trials with distinct environment seeds, limiting 3,000\nframes (i.e., 2.5 minutes of gameplay) which are consistent with STEVE-1 [42].\nUnlike STEVE-1 [42], we condition all agents with the most suitable biome.\nFig. 5 compares the performance of our MineDreamer with the uncondi-\ntional VPT [3], the text-conditioned STEVE-1 [42] and MineDreamer using\nMulti-Modal Memory. With appropriate text instructions, MineDreamer signif-\nicantly outperforms the unconditional VPT [3], collecting 64× more seeds\n,\n7× more wood\n, 41× more dirt\n, traveling 2.7× further\n, and digging 22×\ndeeper\n. It also surpasses the STEVE-1 [42], collecting 1.7× more seeds\n,\n1.4× more wood\n, 2.1× more dirt\n, traveling 1.2× further\n, and digging\n1.9× deeper\n. Compared to Multi-Modal Memory, MineDreamer collects 1.8×\nmore seeds\n, 1.5× more wood\n, 1.8× more dirt\n, travels 1.3× further\n, and\ndigs 1.1× deeper\n. This demonstrates that our CoI mechanism, which breaks\ndown instructions into multiple stages and executes them step by step, leads to\nsteadier instruction following compared to STEVE-1 [42] which uses direct text\ninstruction guidance. Unlike Multi-Modal Memory, which also features the CoI\nmechanism, our method generates future imaginations that closely resemble the\ncurrent state at each stage, resulting in providing more precise visual prompts\nof the desired behavior, thus enhancing the stability of action generation.\nWe also observe an interesting phenomenon: while Multi-Modal Memory,\nusing the CoI mechanism and current observations, outperforms unconditional\nMineDreamer\n11\n4\n8\n16\n32\n64\nHorizontal   Altitude    (Blocks)\nSwitch instructions when reaching 13th level\n(Dig down -> Mine horizontally)\nSuccess Rate: 10%\nFailed to maintain \nhorizontal altitude Success Rate: 0%\nFig. 6: Performance on Command-Switching Evaluation. (Left) MineDreamer\nswiftly adapts to instructions and follows them steadily, achieving a higher success rate\nthan the unconditional VPT [3], the text-conditioned STEVE-1 [42], and the Multi-\nModal Memory with CoI mechanism. (Right) MineDreamer can dig down\nto a\ndepth of 13 and steadily mine horizontally\nto obtain diamonds\nwith an average\nsuccess rate of 10%, while STEVE-1 [42] struggles to maintain a consistent altitude.\nVPT [3], it sometimes underperforms compared to STEVE-1 [42]. Upon review-\ning the recorded videos and the results of memory retrieval, we find that due\nto the vast diversity of open-world environments, the videos retrieved by Multi-\nModal Memory still exhibit slight differences from the current state. This dis-\ncrepancy misguides the PolicyNet in predicting agent actions, indicating that the\nCoI’s effectiveness hinges on the relevancy and precision of future imaginations\nor visual prompts to the current state.\nCommand-Switching Evaluation for Long-Horizon Tasks. In this part,\nwe explore agents’ ability to solve long-horizon tasks that require executing\nmulti-step instructions in sequence, including (1) collect wood\nand then craft\nplanks\n, (2) gather dirt\nand then build a tower\nand (3) dig down\nand\nthen mine horizontally\nfor diamonds\n, each with 50 trials. Tasks 1 and 2\nlimits 3,000 frames (i.e., 2.5 minutes of gameplay), with instructions changing\nat 1,500 and 2,000 frames. Task 3 limits 12,000 frames (i.e., 10 minutes of game-\nplay), switching instructions upon reaching the 13th floor, as diamonds\nare\ncommonly found between the 7th and 14th floors.\nIn Fig. 6 (Left), MineDreamer consistently surpasses VPT [3] and STEVE-\n1 [42] in Command-Switching tasks. VPT ’s [3] inability to follow instructions\nleads to a complete failure in executing sequential instructions, as evidenced by\na 0% success rate in the evaluation. Although STEVE-1 [42] occasionally com-\npletes Command-Switching tasks, it underperforms compared to MineDreamer.\nFor instance, in the Obtain diamond\ntask, STEVE-1’s [42] success rate is\n0%, while Multi-Modal Memory’s success rate is 2%, notably lower than Mine-\nDreamer’s 10%. As shown in Fig. 6 (Right), we reconstruct an instance where\ntwo agents act in the same environment based on the simulator records. Initially,\nboth MineDreamer and STEVE-1 [42] rapidly dig down\nto the target depth\nand then mine horizontally\nto obtain diamonds\n. Compared to STEVE-\n1 [42], MineDreamer can consistently maintain the specified horizontal level over\nan extended period and successfully obtains diamonds\naround the 10k steps\nin this instance. While STEVE-1 [42] manages to maintain its specified horizon-\ntal level for a long time, it ultimately fails to do so and becomes stuck in the\nbedrock layer (i.e., the agent cannot break any block), resulting in a 0% success\nrate. This demonstrates that, even when instructions are switched rapidly, the\n12\nE. Zhou et al.\nCoI mechanism can still drive the agent to generate future goal imaginations\nthat align with the current state. Visual prompts generated from these imagina-\ntions enable the agent to quickly adapt its actions to correspond with the new\ninstructions while steadily following the instructions in action generation.\nCurrent observation\nInstructPix2Pix\nMineDreamer\nGround Truth\n“Go explore.” \n“Chop a tree, collect the log.” \n“Place a torch on the wall.” \nFig. 7: Qualitative Comparison of Goal Imagination Generation. When compared to\nInstructPix2Pix [6] that have undergone further fine-tuning on our Goal Drift Dataset,\nour approach demonstrates superior goal imagination capabilities in embodied scenar-\nios. See Sec. 4.3 for a more detailed analysis.\n4.3\nQualitative Results of Imaginator\nWe compare Imaginator with the existing state-of-the-art instruction-based im-\nage editing model, namely InstructPix2Pix [6]. Given this model has been trained\non specific datasets, its performance would inevitably be suboptimal if directly\napplied to the Minecraft domain. To facilitate a fair comparison, we fine-tune\nInstructPix2Pix [6] using the same training set employed by the Imaginator and\nassess the performance of the fine-tuned models in addressing tasks in Minecraft.\nFig 7 shows qualitative results in the evaluation set, our methodology exhibits\nenhanced abilities in Goal Imagination Generation within intricate scenarios.\nThe first comparison shows that the Imaginator adeptly captures the agent’s\nperspective shift as it advances, whereas InstructPix2Pix [6] struggles to generate\nimages in alignment with the provided instructions. In the second instance, the\nImaginator specifically visualizes the region with felled trees\n, contrasting with\nInstructPix2Pix [6], which yields an image markedly divergent from the exist-\ning observation background. The third comparison highlights the Imaginator’s\nability to depict enhanced visibility following torch placement, in contrast to\nInstructPix2Pix [6], which merely adds torches without the associated increase\nin illumination. These observations suggest that in scenarios requiring instruc-\ntion reasoning and goal understanding, a simple CLIP [54] text encoder may\nstruggle to guide the diffusion model to generate reasonable goal imagination.\nMineDreamer\n13\nCurrent Observation\nCurrent Observation\nGoal Imagination\nGoal Imagination\nNext Observation\nNext Observation\nFig. 8: The Generalizability of MineDreamer. (Left) Despite excluding data in-\nvolving ‘Dirt’\nor ‘Dig’\nfrom Goal Drift Dataset and retraining, Imaginator can\nstill generate relatively high-quality imaginations aligned with the instruction’s con-\ncept. (Right) The retrained Imaginator remains operational with the CoI mechanism\nand can handle unseen instructions while largely preserving its previous performance.\nHowever, the MLLM can fully utilize its powerful reasoning ability, vast environ-\nmental knowledge, and intrinsic physical rules to correctly understand the goal\nand generate goal imagination. More visual results can be found in Supp. F and\nSupp. G.\n4.4\nDiscussion on Generalization\nIn this part, we will explore the generalizability of MineDreamer, as the agent’s\nability to generalize is key to its behavior in the open world where environments\nare complex and instructions vary widely. Since STEVE-1 [42] has shown its prior\nability to map text to visual prompts effectively, and our Prompt Generator is\nbuilt upon it, we will now concentrate on the generalizability of our Imaginator\nand the entire agent. At first, we exclude data related to the words ‘Dirt’\nor\n‘Dig’\nfrom the Goal Drift Dataset and retrain the model. Then, we observe\nthe images generated in response to the instruction “Collect dirt”\nbased on\nthe current state and the quantity of dirt\ncollected by the agent.\nAs shown in Fig. 8, we find that even after completely removing the concepts\nof ‘Dirt’\nor ‘Dig’\n, Imaginator is still able to generate goal imaginations\nof relatively good quality aligned with the instruction’s concept (i.e., agent\npoints towards the dirt\nand attempt to break it), which can still guide\nthe PolicyNet to follow instructions. The resulting collection of dirt\nis about\n70% of the original amount, which shows that the Imaginator can respond to\nunseen novel instructions while largely maintaining its previous performance.\nWe attribute this to three key factors: (1) The MLLM within Imaginator has\nthe relevant environmental knowledge to map the text ‘Dirt’\nto its corre-\nsponding element in Minecraft images, recognizing its visual counterpart; (2)\ntraining data for related tasks, such as “Collect seeds”\n, enables the MLLM\nTable 1: We study the impact of dataset collection methods on agent performance.\nValues in parentheses represent 95% confidence intervals.\nInstruction\nFixed Timestep\nOnly Backward\nOnly Forward\nNormal\nBackward\nDrift\nDrift\n“Chop a tree”\n7.60(3.84, 11.36)\n10.10(2.82, 5.58)\n4.20(2.82, 5.58)\n24.30(21.71, 26.89)\n“Collect dirt”\n38.60(21.97, 55.23) 30.30(20.71, 39.89) 18.10(6.74, 29.46) 65.20(55.81, 74.59)\n14\nE. Zhou et al.\nto comprehend the meaning of action ‘Collect’ in Minecraft task; (3) The pre-\ntrained Diffusion model can generalize to the Minecraft domain and generate\ngoal imaginations by leveraging the MLLM’s latent representations for under-\nstanding textual semantics mentioned above from the instructions.\nTable 2: We study the impact of the Chain-of-Imagination and diffusion model ability\non agent performance. Values in parentheses represent 95% confidence intervals.\nInstruction\nwo CoI\nRandom\nInstruct-\nNormal\nNoise\nPix2Pix\n“Chop a tree”\n18.70(15.26, 22.14)\n2.70(0.85, 4.55)\n22.90(20.17, 25.63) 24.30(21.71, 26.89)\n“Collect dirt”\n53.50(36.93, 70.07) 10.90(3.95, 17.85) 59.50(54.00, 65.00) 65.20(55.81, 74.59)\n4.5\nWhat Contributes to Performance\nDataset Collection Method. In Tab. 1, we study the impact on agent per-\nformance by training with datasets of equal size collected using fixed Back-\nward timesteps, only Backward Drift, only Forward Drift, and normal Goal Drift\nDataset Collection. Although data collected using the first three methods can\nenable the agent to follow instructions, the Imaginator is affected by Goal Illu-\nsion and Imagination Stagnation, which are discussed in Sec. 3.3. This results\nin the Imaginator’s inability to envision the step-by-step process of completing\nthe instruction and how to steadily complete the instruction multiple times.\nChain-of-Imagination. In Tab. 2, we explore the effect of the CoI mech-\nanism on agent performance, where “wo-CoI” denotes the scenario where the\nagent generates the goal imagination and visual prompt only at the beginning\nand remains unchanged thereafter. Compared to normal performance, “wo-CoI”\nachieves about 77%. This is because the visual prompts generated at the be-\nginning become less capable of providing precise demonstrations of the desired\nbehavior in later stages, resulting in hindering the ability to guide the agent step\nby step more steadily.\nDiffusion Model Ability. In Tab. 2, we explore the impact of diffusion model\nability on performance. Using “random noise” as a goal imagination results in\nvague visual prompts, which drastically reduce performance to merely 10% of its\noriginal level. The performance of InstructPix2Pix [6] and our MLLM-enhanced\ndiffusion model are comparable; however, by leveraging MLLM, our generated\nimages adhere more closely to physical rules and environmental knowledge, as\nshown in Fig. 7. Additionally, as discussed in Sec. 4.2, we discover that the CoI\nmechanism demands a certain quality of goal imagination, suggesting that the\nstronger the Imaginator, the better it can guide agents to follow instructions.\nMore ablation studies can be found in Supp. E.\n5\nConclusion and Limitation\nIn this paper, we introduce an innovative paradigm for enhancing the instruction-\nfollowing ability of agents in simulated-world control. We prove that by employ-\ning a Chain-of-Imagination mechanism to envision the step-by-step process of\nexecuting instructions, and translating imaginations into precise visual prompts\nMineDreamer\n15\ntailored to the current state and instruction, can significantly help the foundation\nmodel follow instructions steadily in action generation. Our Agent, MineDreamer\nin Minecraft, showcases its strong instruction-following ability. Furthermore, we\nshow its potential as a high-level planner’s downstream controller in the chal-\nlenging “Obtain diamond”\ntask. We believe this novel paradigm will inspire\nfuture research and generalize to other domains and open-world environments.\nLimitation.\nFirstly, generating high-quality imagination can take seconds,\nslowing down frequent-use scenarios. Speed enhancements via distillation [61]\nand quantization [24] may mitigate this. Secondly, the Imaginator may pro-\nduce unrealistic hallucinations. Integrating world knowledge via methods such\nas RAG [38] or reducing MLLM hallucinations [45] could mitigate this.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | MineDreamer：基于想象链的模拟世界控制指令跟随\n\n## 📌 背景痛点\/本文动机\n在人工智能领域，设计一个能够以人类方式理解和执行多样化指令的通用型具身智能体一直是长期目标。然而，现有的方法往往难以稳定地遵循指令，尤其是在理解和执行抽象和顺序的自然语言指令方面存在困难。\n\n## 🚀 核心方法\n💡 创新点1：引入“想象链”（Chain-of-Imagination, CoI）机制\nMineDreamer 通过 CoI 机制，使智能体能够根据指令和当前状态逐步想象并执行指令。这种方法模拟了人类在解决问题时，根据当前状态逐步想象下一步目标的过程。\n\n💡 创新点2：多模态大型语言模型（MLLM）增强的扩散模型\nMineDreamer 使用 MLLM 增强的扩散模型来生成包含物理规则和环境理解的想象图像，这些图像作为更精确的视觉提示，引导智能体生成低级控制信号。\n\n💡 创新点3：目标漂移收集方法\n为了训练 Imaginator，MineDreamer 使用了目标漂移收集方法来收集大量具身数据，帮助 Imaginator 理解如何逐步完成指令以及如何重复完成指令。\n\n## 📈 实验结果\nMineDreamer 在执行单步和多步指令方面表现出色，显著优于最佳通用型智能体基线，性能几乎翻倍。此外，对智能体想象能力的定性分析表明，它能够理解和适应开放世界的环境。\n\n## 💬 可借鉴之处\nMineDreamer 的 CoI 机制为解决指令跟随问题提供了一种新颖的方法，其 MLLM 增强的扩散模型和目标漂移收集方法也为具身智能体的发展提供了新的思路。此外，MineDreamer 的成功也表明，具身智能体在开放世界环境中具有巨大的潜力。","llm_summary_res_status":200}
{"title":"Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations","authors":"Ziyu Jiang, Yinpeng Chen, Mengchen Liu, Dongdong Chen, Xiyang Dai, Lu Yuan, Zicheng Liu, Zhangyang Wang","summary":"Recently, both Contrastive Learning (CL) and Mask Image Modeling (MIM)\ndemonstrate that self-supervision is powerful to learn good representations.\nHowever, naively combining them is far from success. In this paper, we start by\nmaking the empirical observation that a naive joint optimization of CL and MIM\nlosses leads to conflicting gradient directions - more severe as the layers go\ndeeper. This motivates us to shift the paradigm from combining loss at the end,\nto choosing the proper learning method per network layer. Inspired by\nexperimental observations, we find that MIM and CL are suitable to lower and\nhigher layers, respectively. We hence propose to combine them in a surprisingly\nsimple, \"sequential cascade\" fashion: early layers are first trained under one\nMIM loss, on top of which latter layers continue to be trained under another CL\nloss. The proposed Layer Grafted Pre-training learns good visual\nrepresentations that demonstrate superior label efficiency in downstream\napplications, in particular yielding strong few-shot performance besides linear\nevaluation. For instance, on ImageNet-1k, Layer Grafted Pre-training yields\n65.5% Top-1 accuracy in terms of 1% few-shot learning with ViT-B\/16, which\nimproves MIM and CL baselines by 14.4% and 2.1% with no bells and whistles. The\ncode is available at\nhttps:\/\/github.com\/VITA-Group\/layerGraftedPretraining_ICLR23.git.","url":"http:\/\/arxiv.org\/abs\/2302.14138v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2302.14138v1","published":1677531130000,"comment":"Accepted by ICLR 2023","pdf_text":"Published as a conference paper at ICLR 2023\nLAYER GRAFTED PRE-TRAINING:\nBRIDGING CON-\nTRASTIVE LEARNING AND MASKED IMAGE MODEL-\nING FOR LABEL-EFFICIENT REPRESENTATIONS\nZiyu Jiang†,‡∗, Yinpeng Chen‡, Mengchen Liu‡, Dongdong Chen‡, Xiyang Dai‡,\nLu Yuan‡, Zicheng Liu‡, Zhangyang Wang§\n†Texas A&M University\n‡Microsoft\n§University of Texas at Austin\njiangziyu@tamu.edu, atlaswang@utexas.edu,\n{yiche,mengcliu,dochen,xidai,luyuan,zliu}@microsoft.com\nABSTRACT\nRecently, both Contrastive Learning (CL) and Mask Image Modeling (MIM)\ndemonstrate that self-supervision is powerful to learn good representations. How-\never, naively combining them is far from success. In this paper, we start by making\nthe empirical observation that a naive joint optimization of CL and MIM losses\nleads to conﬂicting gradient directions - more severe as the layers go deeper. This\nmotivates us to shift the paradigm from combining loss at the end, to choosing\nthe proper learning method per network layer. Inspired by experimental observa-\ntions, we ﬁnd that MIM and CL are suitable to lower and higher layers, respec-\ntively. We hence propose to combine them in a surprisingly simple, “sequential\ncascade” fashion: early layers are ﬁrst trained under one MIM loss, on top of\nwhich latter layers continue to be trained under another CL loss. The proposed\nLayer Grafted Pre-training learns good visual representations that demonstrate\nsuperior label efﬁciency in downstream applications, in particular yielding strong\nfew-shot performance besides linear evaluation. For instance, on ImageNet-1k,\nLayer Grafted Pre-training yields 65.5% Top-1 accuracy in terms of 1% few-shot\nlearning with ViT-B\/16, which improves MIM and CL baselines by 14.4% and\n2.1% with no bells and whistles. The code is available at https:\/\/github.\ncom\/VITA-Group\/layerGraftedPretraining_ICLR23.git.\n1\nINTRODUCTION\nSelf-supervision has demonstrated undoubted power in learning strong visual representation, with\ntwo mainstream representative methods: Contrastive learning (CL) (Chen et al., 2020b; He et al.,\n2020; Chen et al., 2020d; 2021; Grill et al., 2020; Caron et al., 2021), and Mask Image Modeling\n(MIM) (Bao et al., 2021; He et al., 2021; Xie et al., 2022; Dong et al., 2021; 2022). The two methods\nfollow different mechanisms, and often manifest different strengths. Generally, CL performs the\ninstance-level task that pulls augmented views from the same image to be similar while pushing\ndifferent images to distribute diversely, making it versatile at learning semantic-aware clustering\nstructures across images. In contrast, MIM draws inspiration from BERT (Devlin et al., 2018) and\nperforms masked token or pixel reconstruction that facilitates the learning of rich local structures\nwithin the same image. In particular, although the latter one, MIM, has recently surpassed CL on the\nﬁne-tuning performance of many datasets, CL often remains to be a top competitor in data-scarce,\nfew-shot downstream applications (Chen et al., 2020c;d; Tian et al., 2020).\nA natural question then follows: are CL and MIM indeed complementary to each other, and is\nthere a way to best combine their strengths?. One immediate, conceptually simple idea is to refer\nto multiple task learning (MTL) and jointly optimize the two losses on top of the same backbone.\nUnfortunately, our preliminary experiment (See Section 2.2) shows that such a vanilla combination\nfails to improve over either baseline, in fact often compromising the single loss’s performance. A\ndeeper dive reveals that the two losses, when being optimized together, will incur increasingly severe\n∗Part of this work was conducted during a summer internship at Microsoft.\n1\narXiv:2302.14138v1  [cs.CV]  27 Feb 2023\nPublished as a conference paper at ICLR 2023\nconﬂicts in the gradient directions, as the layers go deeper (see Figure 1). That causes considerable\nhurdles for the (pre-)training to effectively proceed.\nWe are then inspired to ask: if the two losses conﬂict when both are placed at the end, how about\nplacing them differently, such as appending them to different layers? Based on experimental ob-\nservations, it appears that lower layers tend to learn better from the MIM loss in order to capture\nlocal spatial details; while higher layers tend to beneﬁt more from the CL loss in order to learn\nsemantically-aware grouping and invariance. Inspired by so, we propose a simple MIM→CL Graft-\ning idea to combine the bests of both worlds: (step i) ﬁrst training the lower layers with MIM loss\nand ﬁxing their weights, on top of which (step ii) higher layer weights continue to be trained un-\nder another CL loss. This simple cascaded training idea neatly separates MIM and CL losses to\navoid their conﬂicts against each other if placed together; each loss is also strategically placed to\npre-training its most suitable portion. Practically, we “‘smooth out” the grafting by allowing lower\nlayers to be slowly tuned in step ii. Our ablation experiments also ﬁnd that the order of graft-\ning matters, i.e., reversing MIM\/CL loss locations and performing CL→MIM will considerably\ndamage the performance. The contributions of this paper are summarized as follows:\n• We propose Layer Grafted Pre-training, a principled framework to merge MIM and CL,\nimproving representation learning beyond both, with no bells and whistles.\n• We investigate the different preferences of lower and higher layers towards CL and MIM\nlosses, and show the order of grafting to matter.\n• Despite its embarrassing simplicity, the proposed Layer Grafted Pre-training demonstrates\nmore desirable representation quality, and consequently superior label efﬁciency in down-\nstream applications, yielding strong few-shot performance besides linear evaluation. For\nexample, we achieve [65.5%, 77.8%, 77.7%] in terms of [1% few-shot, 10% few-shot,\nlinear evaluation] performance, improving over MIM and CL baselines by [14.4%, 4.5%,\n9.7%] and [2.1%, 2.4%, 1.0%], respectively.\n2\nMETHOD\n2.1\nPRELIMINARY AND OVERVIEW\nIn Contrastive Learning (CL), the learning target is to pull the positive pairs together in the feature\nspace while pushing negative pairs apart. Formally, the loss can be deﬁned as:\nM(vi, v+\ni , V −, τ) = 1\nN\nN\nX\ni=1\n−log\nexp\n\u0000vi · v+\ni \/τ\n\u0001\nexp\n\u0000vi · v+\ni \/τ\n\u0001\n+ P\nv−\ni ∈V −exp\n\u0000vi · v−\ni \/τ\n\u0001\n(1)\nwhere (vi, v+\ni ) represents features of the positive pairs while (vi, v−\ni ) means features of negative\npairs. Also, V −is the pool of negative features. τ denotes the temperature. N is the number of\nsamples. In practice, the positive pairs are often the different augmented views from the same image\nwhile the negative pool is composed by all the views from different images (Chen et al., 2021).\nOn the other hand, Mask Image Modeling (MIM) learns to reconstruct a corrupted image where\nsome parts of the image or feature map are masked out. The learning target can be formulated as:\nL(xi, M) = 1\nN\nN\nX\ni=1\nD(d(f(Mxi)), xi)\n(2)\nwhere xi and M are input images and randomly generated masks, respectively. f and d represent\nthe encoding and decoding functions, respectively. d(f(Mxi)) is the generated image conditioned\nby masked image Mxi. D measures the difference between d(f(Mxi)) and the original image xi.\nOverview. In the following parts of this section, we ﬁrst introduce our preliminary exploration on the\nMTL of MIM and CL tasks in Section 2.2, which reveals the existence of the conﬂicting gradient\ndirection. Afterward, in Section 2.3, we provide a simple separating idea towards mitigating the\nconﬂicts, which further leads to the proposed Layer Grafted Pre-training in Section 2.4.\n2\nPublished as a conference paper at ICLR 2023\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nBlock 9\nBlock 10\nBlock 11\nBlock 12\nBlock Index\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\nCMIM, CL(x)\nFigure 1: The box plot of CMIM,CL(x) across different blocks for MTL combination of MIM and CL.\nThis is measured on training datasets when the network is trained to 100 epochs (total 300 epochs).\nThe red dash line indicates the linear regression of median numbers.\n2.2\nCONFLICTS PREVENT MULTI-TASK LEARNING FROM WORKING\nOur ﬁrst step towards answering the question of whether CL and MIM can complement each other\nis to examine the most straightforward and conceptually simple idea - Multi-Task Learning (MTL)\ncombination. Speciﬁcally, each iteration of MTL is composed of two steps. Firstly, the images\nare augmented twice for computing the CL loss following Moco V3 Chen et al. (2021). Then, the\nimage with minimal augmentation would be utilized for computing MIM loss following MAE He\net al. (2021). These two losses share the same encoder and would be added together as the ﬁnal loss.\nAs summarized in Table 1, MTL only yields a marginal performance improvement of 0.4% on linear\nevaluation compared to the MIM baseline. However, it is still much lower that the CL baseline (-\n8.3%). Moreover, on both 1% few-shot and ﬁne-tuning performance, MTL is even inferior to both\nMIM and CL baselines. Similar observations were also made by Wang et al. (2022a).\nWe further conjecture that the conﬂicts between these two targets in the MTL combination is the\ncause of the bad performance. To verify it, we design a gradient surgery experiment by computing\nthe cosine similarity between gradients of two tasks following Yu et al. (2020). Formally, the cosine\nsimilarity is calculated as follows:\nCMIM,CL(x) = ∇θLMIM (x)T\n∥∇θLMIM (x)∥\n∇θLCL (x)\n∥∇θLCL (x)∥\n(3)\nwhere LMIM and LCL denote the losses for MIM and CL, respectively. x is a batch of input samples.\nWe measure the distribution of CMIM,CL(x) across different layers of a pre-trained MTL model. As\nshown in Figure 1, there always exist negative values for CMIM,CL(x), where the MIM and CL are\noptimized in opposite directions. Moreover, the gradient direction varies across layers - more severe\nas layers go deeper.\nAlso, the conﬂicts can be reﬂected in two losses’ contradictory targets to enforce. The MIM loss, for\ninstance, requires that the reconstruction have the same brightness, color distribution, and positions\nas the input image, therefore the model needs to be sensitive to all these augmentations. Conversely,\nCL loss is designed to ensure that the model remains invariant regardless of different augmentations.\n2.3\nADDRESSING THE CONFLICTS VIA SEPARATING\nGiven the conﬂicts of the MTL combination, we ask the following question: if the two losses conﬂict\nwhen both are placed at the end, how about placing them differently, such as appending them to dif-\nferent layers? Fortunately, recent empirical evidence suggests that CL and MIM may favor different\npre-training methods. For MIM, Wang et al. (2022c) points out that, when only the pre-trained lower\n3\nPublished as a conference paper at ICLR 2023\nLayer Grafted Pre-training\nLearning Rate Decay\nCL loss\nMIM     CL Grafting\nCL     MIM Grafting\nStep 1\nStep 2\nMIM loss\nMIM loss\nCL loss\nCL loss\nMIM loss\nFigure 2: The pipelines of the MIM→CL, CL→MIM Grafting, and Layer Grafted Pre-training. The\nformer two are employed for preliminary experiments. The latter one is the ﬁnal adopt pipeline,\nwhich is the ‘smooth out’ version of MIM→CL Grafting.\nTable 1: Illustration of preliminary study experiments’ performance on ViT-B\/16. Linear, 1% and\nFine-tuning denote linear evaluation, 1% few-shot and ﬁne-tuning performance, respectively. The\nperformance of MIM and CL are from MAE (He et al., 2021) and MocoV3 (Chen et al., 2021), re-\nspectively. MTL combination denotes the Muti-Task Learning (MTL) Combination of MIM and CL.\nMTL combination is pretrained for 300 epochs. For step 1 of MIM→CL and CL→MIM Grafting,\nwe directly adopt the pre-trained model of MAE and MoCo V3, respectively. Step 2 of MIM→CL\nand CL→MIM Grafting is trained for 100 epochs.\nMethod\nLinear\n1%\nFine-tuning\nMIM (MAE)\n68.0\n51.1\n83.6\nCL (Moco V3)\n76.7\n63.4\n83.2\nMTL combination\n68.4\n47.6\n81.0\nCL→MIM Grafting\n65.5\n32.5\n82.5\nMIM→CL Grafting\n74.5\n56.5\n83.6\nlayers are retained while the higher layers are reset to random initialization, most of the gain is still\npreserved for downstream ﬁne-tuning tasks. Based on this observation, the lower layers appear to\nbe a key element in MIM. On the other hand, Chen et al. (2021) ﬁnds CL to be ineffective and even\nunstable for training the projection layer, the earliest layer of ViT (Dosovitskiy et al., 2020). Fixing\nits weight to be random initialization can even yield signiﬁcantly higher performance. Additionally,\nCL excels at semantic concepts, which happen often at higher layers of the neural network.\nDriven by the above analysis, we propose a simple MIM→CL Grafting framework. As shown in\nFigure 2, MIM→CL Grafting can be separated into two steps: (step i) the lower layers are ﬁrst\ntrained with MIM and then ﬁxed, on the top of which (step ii) higher layers continue to learn with\nCL. Despite the simplicity of this framework, it yields promising preliminary results as shown in\nTable 1, exceeding the MTL combination by [6.1%, 8.9%, 2.6%] for [linear evaluation, 1% few-\nshot, Fine-tuning] performance, respectively. In contrast, when the order of two tasks is reversed,\nthe resulting CL→MIM Grafting, would suffer a dramatic drop in performance, which is even lower\nthan the MTL combination by [2.9%, 15.1%] in terms of [linear evaluation, 1% few-shot] perfor-\nmance, respectively. The huge gap between CL→MIM and MIM→CL Grafting further conﬁrms\nthe preference of MIM and CL towards lower and higher layers, respectively.\nThe example discussed at the end of Section 2.2 can also explain why this preference difference\nhappens: The two different prior knowledge types requested by MIM and CL, while seemingly at\nodds, may work together at different layers of the model. For example, the sensitivity to augmenta-\ntions can be helpful for recognizing the local feature with strong color patterns (Xiao et al., 2020)\nin the lower layers (i.e. the fur of leopards). Meanwhile, for a consistent semantic understanding,\n4\nPublished as a conference paper at ICLR 2023\nthe inﬂuence of the lightness difference should be eliminated when it comes to understanding the\ncontext feature at higher layers.\n2.4\nLAYER GRAFTED PRE-TRAINING\nTo fully unleash the power of Grafting, we ‘smooth out’ the boundary of MIM→CL grafting to\navoid a sudden change in the feature space. Speciﬁcally, rather than ﬁxing the lower layers, we\nassign them a small learning rate. The resultant method, termed as Layer Grafted Pre-training, is\nshown in Figure 2. In Section 3.3, we also explore other LR choices and our results indicate that\nemploying small and large LR for lower and higher layers, respectively, yields the best performance.\nBy effectively capturing the augmentation-sensitive features (i.e. the colors) in the lower layers with\nMIM while learning semantic alignment in the higher layers with CL. The proposed Layer Grafted\nPre-training enables the learning of strong visual representations. It not only provides strong inter-\nclass variance that helps to cluster but also beneﬁts the intra-class variance by keeping the diversity\nof samples in the early features.\n3\nEXPERIMENT\n3.1\nSETTING\nGeneral. We conduct all the experiments on ImageNet-1k (Deng et al., 2009) with Nvidia V100\nGPUs. The code is implemented with Pytorch (Paszke et al., 2019).\nBackbone. We adopt the standard ViT-B and ViT-L architecture (Dosovitskiy et al., 2020) with the\ntoken size of 16×16. The ViT-B is by default employed unless speciﬁed. When training with CL\nloss, we employ the projection and prediction head following Moco V3 (Chen et al., 2021). The\nsettings for pre-training and evaluation protocols can be found at Appendix A.5.\n3.2\nCOMPARISON WITH STATE-OF-THE-ART METHODS\nWe start by verifying the effectiveness of the proposed Layer Grafted Pre-training by comparing it\nwith state-of-the-art methods. As shown in Table 2, in ViT-B\/16, compared to the employed MIM\nand CL baselines, the proposed Layer Grafted Pre-training leads to a consistent improvement. For\ninstance, it improves MAE and Moco V3 by [9.7%, 14.4%, 4.5%] and [1.0%, 2.1%, 2.4%] for\n[linear evaluation, 1% few-shot, 10% few-shot], respectively.\nCompared to close competitors which also attempt to combine MIM and CL, the proposed Layer\nGrafted Pre-training surpasses iBoT by 1.5% for linear evaluation performance. Compared to SIM,\nthe proposed Layer Grafted Pre-training yields an improvement of 1.1% and 0.2% for linear evalu-\nation and 1% few-shot learning performance, respectively.\nTable 2: Comparison with State-of-The-Arts on ViT-B\/16 and ViT-L\/16. Linear, 1% and 10% denote\nthe top-1 accuracy (%) of linear evaluation, 1% and 10% few-shot learning, respectively. †: We\nemploy the result of iBoT without augmentations from Zhou et al. (2021) for fair comparison.\nBackBone\nMethod\nLinear\n1%\n10%\nViT-B\/16\nMAE (He et al., 2021)\n68.0\n51.1\n73.3\nMoco V3 (Chen et al., 2021)\n76.7\n63.4\n75.4\niBoT† (Zhou et al., 2021)\n76.0\n-\n-\nSIM (Tao et al., 2022)\n76.4\n65.3\n-\nC-MAE (Huang et al., 2022)\n73.9\n65.3\n77.3\nMimCo (Zhou et al., 2022)\n70.2\n62.7\n-\nLayer Grafted Pre-training (Ours)\n77.7\n65.5\n77.8\nViT-L\/16\nMAE (He et al., 2021)\n75.8\n55.2\n78.7\nMoco V3 (Chen et al., 2021)\n77.6\n-\n-\nLayer Grafted Pre-training (Ours)\n81.0\n69.3\n80.1\n5\nPublished as a conference paper at ICLR 2023\n(a) Moco V3\n(b) Layer Grafted Pre-training\nFigure 3: t-SNE (Van der Maaten & Hinton, 2008) visualization for feature distribution of Moco V3\nand Layer Grafted Pre-training. Different colors represent different classes. Best viewed in color.\nOur method also demonstrates good scalability toward larger models size. For instance, when scal-\ning from ViT-B\/16 to ViT-L\/16, Layer Grafted Pre-training further improves accuracy by [3.3%,\n3.8%, 2.3%] in terms of [linear evaluation, 1% few-shot, 10% few-shot], respectively. Remarkably,\nthe gap above Moco V3 in linear evaluation performance also increases from 1.0% to 3.4%.\nWe further qualitatively evaluate the representations learned by the proposed Layer Grafted Pre-\ntraining using t-SNE (Van der Maaten & Hinton, 2008). As shown in Figure 3b, the proposed Layer\nGrafted Pre-training shows better inter-class variance. For example, the categories represented by\npink (•) and light blue (•) points are hard to separate given they are very close with each other in\nFigure 3a. In contrast, for representation of the proposed Layer Grafted Pre-training, they form two\nclusters with a clear boundary in Figure 3b. Besides, the proposed Layer Grafted Pre-training also\n(a) Linear, 3rd Stage LR: 1.5e-4\n(b) Linear, 3rd Stage LR: 1.5e-5\n(c) Linear, 3rd Stage LR: 1.5e-6\n(d) Tuning, 3rd Stage LR: 1.5e-4\n(e) Tuning, 3rd Stage LR: 1.5e-5\n(f) Tuning, 3rd Stage LR: 1.5e-6\nFigure 4: Illustration of the LR grid search results for different stages in terms of linear evaluations\nand ﬁne-tuning performance. The grid is [1.5e-6,1.5e-5,1.5e-4] for each stage. [(a), (b), (c)] and\n[(d), (e), (f)] denotes the linear evaluation and ﬁne-tuning performance with third stage LR of [1.5e-\n4, 1.5e-5, 1.5e-6], respectively. [(a), (b)] and [(e), (f)] share the same color bar with (c) and (g),\nrespectively. That of (d), (e) and (f) are also the same. The tested points are highlighted with blue\ndots in each plot. Best view in color.\n6\nPublished as a conference paper at ICLR 2023\nshows better intra-variance: the red (•), green (•) and yellow (•) points of Moco V3 collapse to a\nsmaller region than the proposed Layer Grafted Pre-training.\n3.3\nLR SEARCH LAYER GRAFTED PRE-TRAINING\nWe further verify if small and large LR work the best for the proposed Layer Grafted Pre-training.\nSpeciﬁcally, we study the performance with different LR settings on the three stages on ViT-B\/16\n(Refer to ﬁne-tuning part of Appendix A.5 for the deﬁnition of stages), where each stage is searched\nwith a grid of [1.5e-6,1.5e-5,1.5e-4]. As demonstrated in Figure 4, when the LR increase from\n1.5e-6 to 1.5e-4 for the third stage, both linear evaluation and ﬁne-tuning performance achieve an\nimprovement. Taking linear evaluation as an example, as shown in Figure 4a, 4b and 4c, when\nthe LR of the third stage increase from 1.5e-6 to 1.5e-5 and 1.5e-4, the performance range could\nimprove from 2.5%-63.0% to 64.8%-70.9% and 73.7%-75.1%, respectively. In contrast, a big LR\nof the ﬁrst stage would lead to a drop in performance. For instance, in terms of the linear evalu-\nation performance with third stage LR of 1.5e-4, as shown in Figure 4a, the performance ranges\nare 74.9%-75.1%, 74.8%-75.0% and 73.7%-73.8% for ﬁrst stage LR of 1.5e-6, 1.5e-5 and 1.5e-\n4, respectively. The LR of the second stage is not as sensitive as that of the ﬁrst or third stage.\n0\n2\n4\n6\n8\n10\n12\nThe number of fixing blocks\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\nTop 1 accuracy (%)\n0.3\n9.5\nMAE\nLayer Grafted Pre-training\nFigure 5: Comparison between the pro-\nposed Layer Grafted Pre-training and\nMAE under different numbers of ﬁx-\ning blocks on ViT-B\/16 in terms of\nﬁne-tuning performance. The training\ndataset is the full ImageNet-1k.\nBest\nview in color.\nThe trend of ﬁne-tuning performance is also similar to\nthat of the linear evaluation performance. The preference\nfor larger LR for higher layers indicates that they can ben-\neﬁt by performing CL. Meanwhile, lower layers prefer a\nsmaller LR means that keeping MIM features for these\nlayers can be helpful.\n3.4\nMORE ABLATIONS\nFine-tuning Performance Comparison. We also com-\npare State-of-The-Art methods for ﬁne-tuning perfor-\nmance. As shown in Table 3, the proposed Layer Grafted\nPre-training yields a competitive ﬁne-tuning performance\nof 83.9%, which is 0.3% and 0.7% higher than the em-\nployed MIM (MAE) and CL (Moco V3) baselines, re-\nspectively.\nMoreover, Layer Grafted Pre-training also\nsurpasses SIM by 0.1%.\nPartial Fine-tuning. Follow MAE (He et al., 2021), we\nevaluate the performance of the proposed Layer Grafted\nPre-training with different number of ﬁxing blocks. As\nillustrated in Figure 5, Layer Grafted Pre-training consistently yields higher performance than MAE.\nAnd this gap continue to grow larger when more layers are ﬁxed, indicating the superiority of the\nrepresentations learned by the proposed method.\nVariance-Invariance-Covariance Analysis. A study of the Variance-Invariance-Covariance pat-\ntern for the output of each block is conducted in order to better understand the Layer Grafted Pre-\ntraining. As illustrated in Figure 6, we ﬁnd that the VIC pattern of Layer Grafted Pre-training tends\nTable 3: Top 1 Fine-tuning performance comparison.\nBackBone\nMethod\nFine-tuning\nViT-B\/16\nMAE (He et al., 2021)\n83.6\nMoco V3 (Chen et al., 2021)\n83.2\nSIM (Tao et al., 2022)\n83.8\nConMIM (Yi et al., 2022)\n83.7\nLayer Grafted Pre-training (Ours)\n83.9\nViT-L\/16\nMAE (He et al., 2021)\n85.9\nMoco V3 (Chen et al., 2021)\n84.1\nLayer Grafted Pre-training (Ours)\n85.9\n7\nPublished as a conference paper at ICLR 2023\n0\n2\n4\n6\n8\n10\nBlock Index\n0.026\n0.028\n0.030\n0.032\n0.034\n0.036\n0.038\n0.040\nMAE\nMAE Fine-tune\nLayer Grafted Pre-training\n(a) Variance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.0000\n0.0001\n0.0002\n0.0003\n0.0004\n0.0005\n0.0006\n0.0007\nMAE\nMAE Fine-tune\nLayer Grafted Pre-training\n(b) Invariance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nMAE\nMAE Fine-tune\nLayer Grafted Pre-training\n(c) Covariance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.026\n0.028\n0.030\n0.032\n0.034\n0.036\n0.038\n0.040\nMoco V3\nMoco V3 Fine-tune\nLayer Grafted Pre-training\n(d) Variance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.0000\n0.0001\n0.0002\n0.0003\n0.0004\n0.0005\n0.0006\n0.0007\nMoco V3\nMoco V3 Fine-tune\nLayer Grafted Pre-training\n(e) Invariance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nMoco V3\nMoco V3 Fine-tune\nLayer Grafted Pre-training\n(f) Covariance\nFigure 6: The Variance-Invariance-Covariance (VIC) analysis for different methods. VIC are com-\nputed on ViT-B\/16 following Bardes et al. (2021). The input features are ﬁrst averaged over all\ntokens and then normalized to remove the effect from magnitude. [(a), (d)], [(b), (e)] and [(c), (f)]\nstudy variance, covariance and invariance, respectively. Best view in color.\nTable 4: Layer Grafted Pre-training on ViT-B\/16 with VICReg (Bardes et al., 2021). We train ViT-\nB\/16 with VICReg for 100 epochs as the baseline. For Layer Grafted Pre-training - VICReg, the CL\nloss of stage ii is replaced with VICReg loss.\nMethod\nLinear\nFine-tuning\nVICReg (Bardes et al., 2021)\n70.1%\n81.2%\nLayer Grafted Pre-training - VICReg (Ours)\n74.9%\n83.6%\nto be similar to that of ﬁne-tuning. In the MAE case, the VIC curve of Layer Grafted Pre-training\nclosely matches that of MAE Fine-tune, much closer than MAE itself. The similarity between\nthe proposed Layer Grafted Pre-training and ﬁne-tuning in the VIC pattern also explains the high\nfew-shot performance: weights of the pre-training do not need to change substantially to match the\ndownstream task.\nLayer Grafted Pre-training with VICReg. We further examine the generalizability of the pro-\nposed idea on a different pre-training method - VICRegn (Bardes et al., 2021). As shown in Table 4,\nwhen replacing the CL loss with the VICReg loss, the proposed Layer Grafted Pre-training still\nyields strong performance, surpassing the VICReg baseline by [4.8%, 2.4%] for [linear evaluation,\nﬁne-tuning] performance, respectively.\n4\nRELATED WORKS\nContrastive Learning.\nCL performs instance classiﬁcation tasks by contrasting positive pairs\nagainst negative pairs (Chen et al., 2020b; He et al., 2020; Zhuang et al., 2019; Dosovitskiy et al.,\n2014). Other close works also explore learning without negative samples (Grill et al., 2020; Bardes\net al., 2021; Caron et al., 2021; Zbontar et al., 2021; Chen & He, 2021) and the clustering based\napproachs (Caron et al., 2020).\n8\nPublished as a conference paper at ICLR 2023\nOne common merit of these methods is their strong performance on learning good representa-\ntions, which shows strong clustering pattern (Dwibedi et al., 2021) and leads to state-of-the-art\nfew-shot\/semi-supervised performance (Chen et al., 2020c; Tian et al., 2020; Li et al., 2021; Jiang\net al., 2022; You et al., 2022). However, they contain an implicit assumption that the features should\nbe invariant to heavy augmentations, which, however, could further lead to worse performance when\nthe downstream performance violates it (Xiao et al., 2020). The proposed Layer Grafted Pre-training\naddress this via leveraging MIM for processing the features of lower layers.\nMask Image Modeling. Mask Image Modeling (MIM) is inspired by the success of BERT (Devlin\net al., 2018) in Natural Language Processing (NLP). iGPT (Chen et al., 2020a) begins the exploration\nof this idea in Computer Vision (CV). The emergence of ViT (Dosovitskiy et al., 2020) further\nshrinks the gap of backbones between CV and NLP, motivating more researchers to delve into this\ndirection. Beit (Bao et al., 2021; Peng et al., 2022), MaskFeat (Wei et al., 2022) and Peco (Dong\net al., 2021) focus on predicting tokens. MAE (He et al., 2021) and simMIM (Xie et al., 2022)\nfurther show the possibility of directly reconstructing the original pixels. Following works (Dong\net al., 2022; Chen et al., 2022; Wang et al., 2022b) continue to improve the performance or extend to\nother modalities. However, MIM achieves the most success in ﬁne-tuning with enough data points.\nFor downstream tasks with limited data, MIM fails to surpass CL given the lack of linear separability\nfor its representations (Tao et al., 2022). We address this drawback by employing CL for learning\nthe semantic alignment for higher layers.\nBridging Contrastive Learning and Mask Image Modeling. Only recently, researchers begin\nto explore the potential of combining MIM and CL. iBoT (Zhou et al., 2021), one of the pioneers\nin this direction, proposes to switch the modeling of images to the modeling of features. Some\nconcurrent works also follow this self-distillation paradigm (Tao et al., 2022; Assran et al., 2022).\nHowever, just like CL, this paradigm still relays on the involvement of strong augmentations to avoid\ncollapsing, which could lead to over-suppressing of some features (i.e. color) (Xiao et al., 2020). In\ncontrast, we employ MAE (He et al., 2021), an image modeling framework that is free from strong\naugmentations. Besides, previous combination works treat the network as a whole while we provide\na new layer-wise perspective.\nComparison to other multiple-step pre-training tasks. One may relate the proposed method\nwith previous multiple-step pre-training tasks like intermediate ﬁne-tuning ( e.g., ﬁnetuning a MIM\nmodel using ImageNet22k and transfer to ImageNet1k (Bao et al., 2021)) or self-supervised ﬁne-\ntuning like Reed et al. (2022). The main differences lie in two aspects: (1) The key innovation of the\nproposed method is to reveal and utilize the layerwise difference between MIM and CL. In contrast,\nintermediate ﬁnetuning and self-supervised ﬁne-tuning are treating the model as a whole; and (2)\nWhile intermediate ﬁnetuning and self-supervised are designed for the same pretraining methods\nacross different domains, the proposed method is devised for different pretraining methods in\nthe same domain.\n5\nCONCLUSION\nIn this work, we propose Layer Grafted Pre-training, a simple yet principled method for under-\nstanding and bridging two popular types of self-supervised learning methods - Mask Image Mod-\neling (MIM) and Contrastive Learning (CL). Our work provides a simple remedy to the conﬂicts\nbetween MIM and CL and further reveals the different preferences of these two methods toward\ndifferent parts of the neural network. It advances the quality of the self-supervised representations\nand achieves strong performance on linear evaluation and few-shot performance. Potential future\nwork includes assessing or extending the proposed method to real-world unlabeled data with more\nchallenges such as long-tail distribution or imbalance (Jiang et al., 2021).\nREFERENCES\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent,\nArmand Joulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-efﬁcient\nlearning. arXiv preprint arXiv:2204.07141, 2022.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254, 2021.\n9\nPublished as a conference paper at ICLR 2023\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization\nfor self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. Advances in Neural\nInformation Processing Systems, 33:9912–9924, 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of\nthe IEEE\/CVF International Conference on Computer Vision, pp. 9650–9660, 2021.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International conference on machine learning, pp. 1691–\n1703. PMLR, 2020a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning,\npp. 1597–1607. PMLR, 2020b.\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big\nself-supervised models are strong semi-supervised learners. Advances in neural information pro-\ncessing systems, 33:22243–22255, 2020c.\nXiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han,\nPing Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representa-\ntion learning. arXiv preprint arXiv:2202.03026, 2022.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020d.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\ntransformers. In Proceedings of the IEEE\/CVF International Conference on Computer Vision, pp.\n9640–9649, 2021.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training\ntext encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nXiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen,\nFang Wen, and Nenghai Yu. Peco: Perceptual codebook for bert pre-training of vision transform-\ners. arXiv preprint arXiv:2111.12710, 2021.\nXiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen,\nFang Wen, and Nenghai Yu. Bootstrapped masked autoencoders for vision bert pretraining. In\nECCV, 2022.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discrimi-\nnative unsupervised feature learning with convolutional neural networks. Advances in neural\ninformation processing systems, 27, 2014.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n10\nPublished as a conference paper at ICLR 2023\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With\na little help from my friends: Nearest-neighbor contrastive learning of visual representations. In\nProceedings of the IEEE\/CVF International Conference on Computer Vision, pp. 9588–9597,\n2021.\nPriya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-\ndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\nJean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\net al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural\ninformation processing systems, 33:21271–21284, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for\nunsupervised visual representation learning.\nIn Proceedings of the IEEE\/CVF conference on\ncomputer vision and pattern recognition, pp. 9729–9738, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick.\nMasked\nautoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\nZhicheng Huang, Xiaojie Jin, Chengze Lu, Qibin Hou, Ming-Ming Cheng, Dongmei Fu, Xiaohui\nShen, and Jiashi Feng. Contrastive masked autoencoders are stronger vision learners. arXiv\npreprint arXiv:2207.13532, 2022.\nZiyu Jiang, Tianlong Chen, Bobak J Mortazavi, and Zhangyang Wang. Self-damaging contrastive\nlearning. In International Conference on Machine Learning, pp. 4927–4939. PMLR, 2021.\nZiyu Jiang, Tianlong Chen, Xuxi Chen, Yu Cheng, Luowei Zhou, Lu Yuan, Ahmed Awadallah, and\nZhangyang Wang. Dna: Improving few-shot transfer learning with low-rank decomposition and\nalignment. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n23–27, 2022, Proceedings, Part XX, pp. 239–256. Springer, 2022.\nSuichan Li, Dongdong Chen, Yinpeng Chen, Lu Yuan, Lei Zhang, Qi Chu, Bin Liu, and Nenghai\nYu. Improve unsupervised pretraining for few-label transfer. In Proceedings of the IEEE\/CVF\nInternational Conference on Computer Vision, pp. 10201–10210, 2021.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the\nIEEE\/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance\ndeep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran\nAssociates, Inc., 2019.\nZhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling\nwith vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022.\nColorado J Reed, Xiangyu Yue, Ani Nrusimha, Sayna Ebrahimi, Vivek Vijaykumar, Richard Mao,\nBo Li, Shanghang Zhang, Devin Guillory, Sean Metzger, et al. Self-supervised pretraining im-\nproves self-supervised pretraining. In Proceedings of the IEEE\/CVF Winter Conference on Ap-\nplications of Computer Vision, pp. 2584–2594, 2022.\n11\nPublished as a conference paper at ICLR 2023\nChenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, and Jifeng Dai. Siamese image\nmodeling for self-supervised vision representation learning. arXiv preprint arXiv:2206.01204,\n2022.\nYonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking\nfew-shot image classiﬁcation: a good embedding is all you need? In European Conference on\nComputer Vision, pp. 266–282. Springer, 2020.\nHugo Touvron, Matthieu Cord, and Herv´e J´egou. Deit iii: Revenge of the vit. arXiv preprint\narXiv:2204.07118, 2022.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\nLuya Wang, Feng Liang, Yangguang Li, Wanli Ouyang, Honggang Zhang, and Jing Shao. Repre:\nImproving self-supervised vision transformer with reconstructive pre-training.\narXiv preprint\narXiv:2201.06857, 2022a.\nRui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang\nJiang, Luowei Zhou, and Lu Yuan. Bevt: Bert pretraining of video transformers. In IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR 2022), 2022b.\nShaoru Wang, Jin Gao, Zeming Li, Jian Sun, and Weiming Hu. A closer look at self-supervised\nlightweight vision transformers. arXiv preprint arXiv:2205.14443, 2022c.\nChen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichten-\nhofer. Masked feature prediction for self-supervised visual pre-training. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 14668–14678, 2022.\nTete Xiao, Xiaolong Wang, Alexei A Efros, and Trevor Darrell. What should not be contrastive in\ncontrastive learning. arXiv preprint arXiv:2008.05659, 2020.\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu.\nSimmim: A simple framework for masked image modeling. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition, pp. 9653–9663, 2022.\nKun Yi, Yixiao Ge, Xiaotong Li, Shusheng Yang, Dian Li, Jianping Wu, Ying Shan, and Xiaohu\nQie. Masked image modeling with denoising contrast. arXiv preprint arXiv:2205.09616, 2022.\nChenyu You, Weicheng Dai, Fenglin Liu, Haoran Su, Xiaoran Zhang, Lawrence Staib, and James S\nDuncan. Mine your own anatomy: Revisiting medical image segmentation with extremely limited\nlabels. arXiv preprint arXiv:2209.13476, 2022.\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.\nGradient surgery for multi-task learning. Advances in Neural Information Processing Systems,\n33:5824–5836, 2020.\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St´ephane Deny. Barlow twins: Self-supervised\nlearning via redundancy reduction. In International Conference on Machine Learning, pp. 12310–\n12320. PMLR, 2021.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot:\nImage bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.\nQiang Zhou, Chaohui Yu, Hao Luo, Zhibin Wang, and Hao Li. Mimco: Masked image modeling\npre-training with contrastive teacher. In Proceedings of the 30th ACM International Conference\non Multimedia, pp. 4487–4495, 2022.\nChengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning\nof visual embeddings. In Proceedings of the IEEE\/CVF International Conference on Computer\nVision, pp. 6002–6012, 2019.\n12\nPublished as a conference paper at ICLR 2023\nTable 5: Comparison between MIM (MAE) and CL (Moco V3) for frozen features from blocks\n[6,9,12], on top of which we employ various numbers of blocks (#Blocks) for ﬁne-tuning. 0 block\nindicates that only a linear classiﬁcation head is employed (identical to linear evaluation). Top 1\naccuracy on ImageNet-1K is reported and the best performance under each setting is highlighted\nwith bold text.\n#Blocks\nThe block index of the feature\n6\n9\n12\nMIM\nCL\nMIM\nCL\nMIM\nCL\n0\n38.9%\n43.6%\n59.3%\n65.5%\n68.0%\n76.7%\n1\n70.1%\n72.7%\n77.8%\n78.1%\n78.9%\n79.1%\n2\n76.3%\n76.8%\n80.2%\n79.2%\n80.6%\n79.7%\n4\n78.5%\n78.1%\n81.2%\n79.4%\n81.4%\n79.2%\nA\nAPPENDIX\nThis appendix contains the following details that we could not include in the main paper due to\nspace restrictions.\nA.1\nMORE ANALYSIS FOR THE LAYER-WISE DIFFERENCE BETWEEN MIM AND CL\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nThe blocks index\n40\n50\n60\n70\n80\n90\n100\n110\nMIM\nCL\nLayer Grafted Pre-training\nFigure 7: Demonstration of average attention dis-\ntance (Dosovitskiy et al., 2020) for MIM (MAE),\nCL (Moco V3) and Graft in terms of the across all\nattention heads.\nWe provide more analysis to further understand\nthe layerwise difference between MIM and CL.\nWe start by analyzing the average attention dis-\ntance across different layers. As shown in Fig-\nure 7, on the one hand, for the deep layers (i.e.\nfrom 8th to 12th blocks), the average attention\ndistance of CL would keep increasing, where\nthe aggregation of local features is likely to\nhappen. In contrast, the average attention dis-\ntance of MIM keeps the same level for the deep\nlayers. On the other hand, the average attn dis-\ntance of shallow layers (i.e. 1st and 2nd blocks)\nof CL is much larger than that of MIM, which\nmay distract the model from extracting local\nfeatures. The proposed method combines the\nlower and higher layers’ patterns of MIM and\nCL, respectively, forming a gradually increas-\ning attention distance pattern. Remarkably, this\npattern echos the philosophy of gradually in-\ncreasing receptive ﬁeld for designing network architecture He et al. (2016); Liu et al. (2021).\nSecondly, we study the different properties of features across different layers. Speciﬁcally, in Im-\nageNet 1K, we turn several random initialized blocks on top of features from different layers. As\ndemonstrated in Table 5, when only ﬁne-tuning the classiﬁcation head (#Block = 0), the perfor-\nmance of MIM is much lower than CL, indicating that the feature of CL is more close to semantic\nrepresentation. By contrast, when increasing the turnable blocks, the performance of MIM would\nsigniﬁcantly increase and even surpass CL, demonstrating the features of MIM better encode the\nlocal features. The potential of these local features can be stimulated by adding enough modules to\naggregate them. The proposed Layer Grafted Pre-training employs MIM for producing high-quality\nearly features while utilizing the CL in higher layers for aggregation.\nA.2\nDISCUSSION AND COMPARISON WITH CONCURRENT WORKS\nIn this section, we discuss the difference between the proposed method with four concurrent\nworks (Tao et al., 2022; Huang et al., 2022; Zhou et al., 2022; Yi et al., 2022) and provide more\n13\nPublished as a conference paper at ICLR 2023\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nBlock 9\nBlock 10\nBlock 11\nBlock 12\nBlock Index\n0.10\n0.05\n0.00\n0.05\n0.10\nCMIM, CL(x)\n(a) 200 epochs\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nBlock 9\nBlock 10\nBlock 11\nBlock 12\nBlock Index\n0.075\n0.050\n0.025\n0.000\n0.025\n0.050\n0.075\nCMIM, CL(x)\n(b) 300 epochs\nFigure 8: The box plot of CMIM,CL(x) across different blocks for MTL combination of MIM and\nCL. This is measured on training datasets when the network is trained to (a) 200 epochs and (b) 300\nepochs (total 300 epochs). The red dash line indicates the linear regression of median numbers.\ncomparisons. To combine the strength of MIM and CL, SIM (Tao et al., 2022) proposes to pre-\ndict the dense representations of an augmented view for enforcing semantic alignment and spatial\nsensitivity simultaneously. CMAE (Huang et al., 2022) proposes two new components: pixel shift\nand feature decoder. MimCo (Zhou et al., 2022) utilizes the CL pre-trained model as a teacher\nand performs patch-level and image-level reconstruction tasks. ConMIM (Yi et al., 2022) utilizes\ncontrastive constraints to produce a dynamic masked prediction target. The difference between the\nproposed Layer Grafted Pre-training and these works are as follows:\n• While all the concurrent works are treating the network as a whole, we reveal the different\npreferences of MIM and CL towards their internal different layers, which motivates us a\ndesign a novel layerwise method.\n• Our method employs the original design of MIM and CL, which not only is simple but also\nenables an apple-to-apple comparison. In contrast, It’s non-straightforward to tell whether\nthe improvements of the concurrent works are from the newly introduced module or the\noriginal MIM\/CL design.\n• The proposed Layer Grafted Pre-training provides an in-depth analysis of the reason why\nMIM and CL cannot be directly combined together through gradient analysis.\nAlso, here we further analyze why the proposed method fails to surpass the concurrent work C-\nMAE (Huang et al., 2022) in terms of ﬁne-tuning performance. One possible reason lies in whether\nthe masked view is employed for contrastive learning. C-MAE is contrasting a masked and a full\nimage while the proposed method is contrasting two full images following Moco V3. The difference\nin design further leads to different strengths: On the one hand, empirical results highlight that the\n14\nPublished as a conference paper at ICLR 2023\nTable 6: ADE20K semantic segmentation comparison using UperNet with different pre-training\nmethods.\nMethod\nmIoU\nMAE (He et al., 2021)\n48.1\nMoco V3 (Chen et al., 2021)\n47.3\nLayer Grafted Pre-training (Ours)\n48.7\nmasked view can beneﬁt the downstream ﬁne-tuning task (Touvron et al., 2022; He et al., 2021),\nwhich may be because it helps to learn the correlation between sparse patches that cannot be built\nunder full view. On the other hand, contrasting full images leads to a smaller gap with downstream\ntasks and thus beneﬁting the downstream few-shot and linear evaluation tasks.\nA.3\nMORE GRADIENT ANALYSIS RESULTS\nWe measure the distribution of CMIM,CL(x) across different training epochs and conﬁrmed that the\nstatistical results persist the same across the entire training process, rather than just a few speciﬁc\nepochs. As two examples, Figures 8a and 8b show that a considerable part of values is negative in\nthe 200 and 300 epochs.\nA.4\nTRANSFER LEARNING RESULTS\nWe further evaluate the proposed method for the standard transfer semantic segmentation task. As\nshown in Table 6, on ADE20K with UperNet, the proposed method achieves higher performance\nthan both MAE and Moco V3.\nA.5\nMORE SETTINGS\nPre-training. We by default adopt MAE (He et al., 2021) and Moco V3 (Chen et al., 2021) as our\nMIM and CL frameworks, respectively. For the ﬁrst step of Layer Grafted Pre-training, since it\nidentically follows the original MIM pipeline, we directly adopt the pre-trained model of MAE (He\net al., 2021) and conduct the second step, where we initialize the network with the MIM pre-trained\nmodel and train with Moco V3 (Chen et al., 2021) for 300 epochs. For LR, We ﬁrst split the network\ninto three stages. Each of them contains the same number of blocks. (i.e. 4 and 8 blocks for ViT-B\nand ViT-L, respectively.) Then, the base LR of the ﬁrst and second stages (corresponding to lower\nlayers) is assigned as 1.5e-5 while the third stage is set as 1.5e-4 by default. In the second stage of\nViT-L, we further ensure the early layers of the resultant model are close to the MIM pre-training\nby minimizing the l2 distance between the ﬁrst 12 layers between them (Refer Section A.6 for more\nablations). Other settings are identically followed from Moco V3 (Chen et al., 2021).\nFine-tuning. For ﬁne-tuning, we train with AdamW (Loshchilov & Hutter, 2017) for 100 epochs\nfollowing MAE (He et al., 2021). We employ a base LR of 5e-4 with linear scaling rule (Goyal\net al., 2017). The layer-wise LR decay ratio is set as 0.6 (Clark et al., 2020). For other settings such\nas data augmentation, LR scheduling or weight decay, we identically follow He et al. (2021).\nFew-shot Learning. We conduct few-shot learning with 1% or 10% available labels. The sub-\nsampling splits are adopted from Chen et al. (2020c). For 1% few-shot evaluation, following Caron\net al. (2021), we ﬁrst generate frozen features on training images without data augmentation, on top\nof which a logistic regression classiﬁer is trained for prediction. For 10% semi-supervised learning,\nwe train from the ﬁrst layer of the projection head following Chen et al. (2020c). We train for 400\nepochs with an initial base LR of 3e-5. Other settings are identical to the ﬁne-tuning.\nLinear Evaluation. For linear evaluation, we train a linear classiﬁer on top of frozen pre-train\nfeatures to measure the quality of the visual representations following common practices (Chen\net al., 2020b). Following Moco V3 (Chen et al., 2021), the classiﬁer is trained for 90 epochs with\nSGD optimizer and weight decay of 0. The LR is swept for each case.\n15\nPublished as a conference paper at ICLR 2023\nTable 7: Ablation study for l2 regularization on ViT-L. Top 1 accuracy on ImageNet-1K is reported\nand the best performance under each setting is highlighted with bold text.\nl2 regularization\nLinear\n1%\n10%\nFinetune\n\u0017\n80.5\n68.9\n79.8\n85.7\n✓\n81.0\n69.3\n80.1\n85.9\nA.6\nABLATION STUDY FOR l2 REGULARIZATION\nIn this section, we ablation study the effectiveness of l2 regularization mentioned at section A.5. As\nshown in Table 7, employing the l2 regularization can help to preserve the Mask Image Modeling\nfeatures of the lower layers and yield consistent improvement across multiple benchmarks.\n16\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nLayer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations\n```\n#### 2. 论文摘要\n```\nRecently, both Contrastive Learning (CL) and Mask Image Modeling (MIM)\ndemonstrate that self-supervision is powerful to learn good representations.\nHowever, naively combining them is far from success. In this paper, we start by\nmaking the empirical observation that a naive joint optimization of CL and MIM\nlosses leads to conflicting gradient directions - more severe as the layers go\ndeeper. This motivates us to shift the paradigm from combining loss at the end,\nto choosing the proper learning method per network layer. Inspired by\nexperimental observations, we find that MIM and CL are suitable to lower and\nhigher layers, respectively. We hence propose to combine them in a surprisingly\nsimple, \"sequential cascade\" fashion: early layers are first trained under one\nMIM loss, on top of which latter layers continue to be trained under another CL\nloss. The proposed Layer Grafted Pre-training learns good visual\nrepresentations that demonstrate superior label efficiency in downstream\napplications, in particular yielding strong few-shot performance besides linear\nevaluation. For instance, on ImageNet-1k, Layer Grafted Pre-training yields\n65.5% Top-1 accuracy in terms of 1% few-shot learning with ViT-B\/16, which\nimproves MIM and CL baselines by 14.4% and 2.1% with no bells and whistles. The\ncode is available at\nhttps:\/\/github.com\/VITA-Group\/layerGraftedPretraining_ICLR23.git.\n```\n\n#### 3. 论文全文\n```\nPublished as a conference paper at ICLR 2023\nLAYER GRAFTED PRE-TRAINING:\nBRIDGING CON-\nTRASTIVE LEARNING AND MASKED IMAGE MODEL-\nING FOR LABEL-EFFICIENT REPRESENTATIONS\nZiyu Jiang†,‡∗, Yinpeng Chen‡, Mengchen Liu‡, Dongdong Chen‡, Xiyang Dai‡,\nLu Yuan‡, Zicheng Liu‡, Zhangyang Wang§\n†Texas A&M University\n‡Microsoft\n§University of Texas at Austin\njiangziyu@tamu.edu, atlaswang@utexas.edu,\n{yiche,mengcliu,dochen,xidai,luyuan,zliu}@microsoft.com\nABSTRACT\nRecently, both Contrastive Learning (CL) and Mask Image Modeling (MIM)\ndemonstrate that self-supervision is powerful to learn good representations. How-\never, naively combining them is far from success. In this paper, we start by making\nthe empirical observation that a naive joint optimization of CL and MIM losses\nleads to conﬂicting gradient directions - more severe as the layers go deeper. This\nmotivates us to shift the paradigm from combining loss at the end, to choosing\nthe proper learning method per network layer. Inspired by experimental observa-\ntions, we ﬁnd that MIM and CL are suitable to lower and higher layers, respec-\ntively. We hence propose to combine them in a surprisingly simple, “sequential\ncascade” fashion: early layers are ﬁrst trained under one MIM loss, on top of\nwhich latter layers continue to be trained under another CL loss. The proposed\nLayer Grafted Pre-training learns good visual representations that demonstrate\nsuperior label efﬁciency in downstream applications, in particular yielding strong\nfew-shot performance besides linear evaluation. For instance, on ImageNet-1k,\nLayer Grafted Pre-training yields 65.5% Top-1 accuracy in terms of 1% few-shot\nlearning with ViT-B\/16, which improves MIM and CL baselines by 14.4% and\n2.1% with no bells and whistles. The code is available at https:\/\/github.\ncom\/VITA-Group\/layerGraftedPretraining_ICLR23.git.\n1\nINTRODUCTION\nSelf-supervision has demonstrated undoubted power in learning strong visual representation, with\ntwo mainstream representative methods: Contrastive learning (CL) (Chen et al., 2020b; He et al.,\n2020; Chen et al., 2020d; 2021; Grill et al., 2020; Caron et al., 2021), and Mask Image Modeling\n(MIM) (Bao et al., 2021; He et al., 2021; Xie et al., 2022; Dong et al., 2021; 2022). The two methods\nfollow different mechanisms, and often manifest different strengths. Generally, CL performs the\ninstance-level task that pulls augmented views from the same image to be similar while pushing\ndifferent images to distribute diversely, making it versatile at learning semantic-aware clustering\nstructures across images. In contrast, MIM draws inspiration from BERT (Devlin et al., 2018) and\nperforms masked token or pixel reconstruction that facilitates the learning of rich local structures\nwithin the same image. In particular, although the latter one, MIM, has recently surpassed CL on the\nﬁne-tuning performance of many datasets, CL often remains to be a top competitor in data-scarce,\nfew-shot downstream applications (Chen et al., 2020c;d; Tian et al., 2020).\nA natural question then follows: are CL and MIM indeed complementary to each other, and is\nthere a way to best combine their strengths?. One immediate, conceptually simple idea is to refer\nto multiple task learning (MTL) and jointly optimize the two losses on top of the same backbone.\nUnfortunately, our preliminary experiment (See Section 2.2) shows that such a vanilla combination\nfails to improve over either baseline, in fact often compromising the single loss’s performance. A\ndeeper dive reveals that the two losses, when being optimized together, will incur increasingly severe\n∗Part of this work was conducted during a summer internship at Microsoft.\n1\narXiv:2302.14138v1  [cs.CV]  27 Feb 2023\nPublished as a conference paper at ICLR 2023\nconﬂicts in the gradient directions, as the layers go deeper (see Figure 1). That causes considerable\nhurdles for the (pre-)training to effectively proceed.\nWe are then inspired to ask: if the two losses conﬂict when both are placed at the end, how about\nplacing them differently, such as appending them to different layers? Based on experimental ob-\nservations, it appears that lower layers tend to learn better from the MIM loss in order to capture\nlocal spatial details; while higher layers tend to beneﬁt more from the CL loss in order to learn\nsemantically-aware grouping and invariance. Inspired by so, we propose a simple MIM→CL Graft-\ning idea to combine the bests of both worlds: (step i) ﬁrst training the lower layers with MIM loss\nand ﬁxing their weights, on top of which (step ii) higher layer weights continue to be trained un-\nder another CL loss. This simple cascaded training idea neatly separates MIM and CL losses to\navoid their conﬂicts against each other if placed together; each loss is also strategically placed to\npre-training its most suitable portion. Practically, we “‘smooth out” the grafting by allowing lower\nlayers to be slowly tuned in step ii. Our ablation experiments also ﬁnd that the order of graft-\ning matters, i.e., reversing MIM\/CL loss locations and performing CL→MIM will considerably\ndamage the performance. The contributions of this paper are summarized as follows:\n• We propose Layer Grafted Pre-training, a principled framework to merge MIM and CL,\nimproving representation learning beyond both, with no bells and whistles.\n• We investigate the different preferences of lower and higher layers towards CL and MIM\nlosses, and show the order of grafting to matter.\n• Despite its embarrassing simplicity, the proposed Layer Grafted Pre-training demonstrates\nmore desirable representation quality, and consequently superior label efﬁciency in down-\nstream applications, yielding strong few-shot performance besides linear evaluation. For\nexample, we achieve [65.5%, 77.8%, 77.7%] in terms of [1% few-shot, 10% few-shot,\nlinear evaluation] performance, improving over MIM and CL baselines by [14.4%, 4.5%,\n9.7%] and [2.1%, 2.4%, 1.0%], respectively.\n2\nMETHOD\n2.1\nPRELIMINARY AND OVERVIEW\nIn Contrastive Learning (CL), the learning target is to pull the positive pairs together in the feature\nspace while pushing negative pairs apart. Formally, the loss can be deﬁned as:\nM(vi, v+\ni , V −, τ) = 1\nN\nN\nX\ni=1\n−log\nexp\n\u0000vi · v+\ni \/τ\n\u0001\nexp\n\u0000vi · v+\ni \/τ\n\u0001\n+ P\nv−\ni ∈V −exp\n\u0000vi · v−\ni \/τ\n\u0001\n(1)\nwhere (vi, v+\ni ) represents features of the positive pairs while (vi, v−\ni ) means features of negative\npairs. Also, V −is the pool of negative features. τ denotes the temperature. N is the number of\nsamples. In practice, the positive pairs are often the different augmented views from the same image\nwhile the negative pool is composed by all the views from different images (Chen et al., 2021).\nOn the other hand, Mask Image Modeling (MIM) learns to reconstruct a corrupted image where\nsome parts of the image or feature map are masked out. The learning target can be formulated as:\nL(xi, M) = 1\nN\nN\nX\ni=1\nD(d(f(Mxi)), xi)\n(2)\nwhere xi and M are input images and randomly generated masks, respectively. f and d represent\nthe encoding and decoding functions, respectively. d(f(Mxi)) is the generated image conditioned\nby masked image Mxi. D measures the difference between d(f(Mxi)) and the original image xi.\nOverview. In the following parts of this section, we ﬁrst introduce our preliminary exploration on the\nMTL of MIM and CL tasks in Section 2.2, which reveals the existence of the conﬂicting gradient\ndirection. Afterward, in Section 2.3, we provide a simple separating idea towards mitigating the\nconﬂicts, which further leads to the proposed Layer Grafted Pre-training in Section 2.4.\n2\nPublished as a conference paper at ICLR 2023\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nBlock 9\nBlock 10\nBlock 11\nBlock 12\nBlock Index\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\nCMIM, CL(x)\nFigure 1: The box plot of CMIM,CL(x) across different blocks for MTL combination of MIM and CL.\nThis is measured on training datasets when the network is trained to 100 epochs (total 300 epochs).\nThe red dash line indicates the linear regression of median numbers.\n2.2\nCONFLICTS PREVENT MULTI-TASK LEARNING FROM WORKING\nOur ﬁrst step towards answering the question of whether CL and MIM can complement each other\nis to examine the most straightforward and conceptually simple idea - Multi-Task Learning (MTL)\ncombination. Speciﬁcally, each iteration of MTL is composed of two steps. Firstly, the images\nare augmented twice for computing the CL loss following Moco V3 Chen et al. (2021). Then, the\nimage with minimal augmentation would be utilized for computing MIM loss following MAE He\net al. (2021). These two losses share the same encoder and would be added together as the ﬁnal loss.\nAs summarized in Table 1, MTL only yields a marginal performance improvement of 0.4% on linear\nevaluation compared to the MIM baseline. However, it is still much lower that the CL baseline (-\n8.3%). Moreover, on both 1% few-shot and ﬁne-tuning performance, MTL is even inferior to both\nMIM and CL baselines. Similar observations were also made by Wang et al. (2022a).\nWe further conjecture that the conﬂicts between these two targets in the MTL combination is the\ncause of the bad performance. To verify it, we design a gradient surgery experiment by computing\nthe cosine similarity between gradients of two tasks following Yu et al. (2020). Formally, the cosine\nsimilarity is calculated as follows:\nCMIM,CL(x) = ∇θLMIM (x)T\n∥∇θLMIM (x)∥\n∇θLCL (x)\n∥∇θLCL (x)∥\n(3)\nwhere LMIM and LCL denote the losses for MIM and CL, respectively. x is a batch of input samples.\nWe measure the distribution of CMIM,CL(x) across different layers of a pre-trained MTL model. As\nshown in Figure 1, there always exist negative values for CMIM,CL(x), where the MIM and CL are\noptimized in opposite directions. Moreover, the gradient direction varies across layers - more severe\nas layers go deeper.\nAlso, the conﬂicts can be reﬂected in two losses’ contradictory targets to enforce. The MIM loss, for\ninstance, requires that the reconstruction have the same brightness, color distribution, and positions\nas the input image, therefore the model needs to be sensitive to all these augmentations. Conversely,\nCL loss is designed to ensure that the model remains invariant regardless of different augmentations.\n2.3\nADDRESSING THE CONFLICTS VIA SEPARATING\nGiven the conﬂicts of the MTL combination, we ask the following question: if the two losses conﬂict\nwhen both are placed at the end, how about placing them differently, such as appending them to dif-\nferent layers? Fortunately, recent empirical evidence suggests that CL and MIM may favor different\npre-training methods. For MIM, Wang et al. (2022c) points out that, when only the pre-trained lower\n3\nPublished as a conference paper at ICLR 2023\nLayer Grafted Pre-training\nLearning Rate Decay\nCL loss\nMIM     CL Grafting\nCL     MIM Grafting\nStep 1\nStep 2\nMIM loss\nMIM loss\nCL loss\nCL loss\nMIM loss\nFigure 2: The pipelines of the MIM→CL, CL→MIM Grafting, and Layer Grafted Pre-training. The\nformer two are employed for preliminary experiments. The latter one is the ﬁnal adopt pipeline,\nwhich is the ‘smooth out’ version of MIM→CL Grafting.\nTable 1: Illustration of preliminary study experiments’ performance on ViT-B\/16. Linear, 1% and\nFine-tuning denote linear evaluation, 1% few-shot and ﬁne-tuning performance, respectively. The\nperformance of MIM and CL are from MAE (He et al., 2021) and MocoV3 (Chen et al., 2021), re-\nspectively. MTL combination denotes the Muti-Task Learning (MTL) Combination of MIM and CL.\nMTL combination is pretrained for 300 epochs. For step 1 of MIM→CL and CL→MIM Grafting,\nwe directly adopt the pre-trained model of MAE and MoCo V3, respectively. Step 2 of MIM→CL\nand CL→MIM Grafting is trained for 100 epochs.\nMethod\nLinear\n1%\nFine-tuning\nMIM (MAE)\n68.0\n51.1\n83.6\nCL (Moco V3)\n76.7\n63.4\n83.2\nMTL combination\n68.4\n47.6\n81.0\nCL→MIM Grafting\n65.5\n32.5\n82.5\nMIM→CL Grafting\n74.5\n56.5\n83.6\nlayers are retained while the higher layers are reset to random initialization, most of the gain is still\npreserved for downstream ﬁne-tuning tasks. Based on this observation, the lower layers appear to\nbe a key element in MIM. On the other hand, Chen et al. (2021) ﬁnds CL to be ineffective and even\nunstable for training the projection layer, the earliest layer of ViT (Dosovitskiy et al., 2020). Fixing\nits weight to be random initialization can even yield signiﬁcantly higher performance. Additionally,\nCL excels at semantic concepts, which happen often at higher layers of the neural network.\nDriven by the above analysis, we propose a simple MIM→CL Grafting framework. As shown in\nFigure 2, MIM→CL Grafting can be separated into two steps: (step i) the lower layers are ﬁrst\ntrained with MIM and then ﬁxed, on the top of which (step ii) higher layers continue to learn with\nCL. Despite the simplicity of this framework, it yields promising preliminary results as shown in\nTable 1, exceeding the MTL combination by [6.1%, 8.9%, 2.6%] for [linear evaluation, 1% few-\nshot, Fine-tuning] performance, respectively. In contrast, when the order of two tasks is reversed,\nthe resulting CL→MIM Grafting, would suffer a dramatic drop in performance, which is even lower\nthan the MTL combination by [2.9%, 15.1%] in terms of [linear evaluation, 1% few-shot] perfor-\nmance, respectively. The huge gap between CL→MIM and MIM→CL Grafting further conﬁrms\nthe preference of MIM and CL towards lower and higher layers, respectively.\nThe example discussed at the end of Section 2.2 can also explain why this preference difference\nhappens: The two different prior knowledge types requested by MIM and CL, while seemingly at\nodds, may work together at different layers of the model. For example, the sensitivity to augmenta-\ntions can be helpful for recognizing the local feature with strong color patterns (Xiao et al., 2020)\nin the lower layers (i.e. the fur of leopards). Meanwhile, for a consistent semantic understanding,\n4\nPublished as a conference paper at ICLR 2023\nthe inﬂuence of the lightness difference should be eliminated when it comes to understanding the\ncontext feature at higher layers.\n2.4\nLAYER GRAFTED PRE-TRAINING\nTo fully unleash the power of Grafting, we ‘smooth out’ the boundary of MIM→CL grafting to\navoid a sudden change in the feature space. Speciﬁcally, rather than ﬁxing the lower layers, we\nassign them a small learning rate. The resultant method, termed as Layer Grafted Pre-training, is\nshown in Figure 2. In Section 3.3, we also explore other LR choices and our results indicate that\nemploying small and large LR for lower and higher layers, respectively, yields the best performance.\nBy effectively capturing the augmentation-sensitive features (i.e. the colors) in the lower layers with\nMIM while learning semantic alignment in the higher layers with CL. The proposed Layer Grafted\nPre-training enables the learning of strong visual representations. It not only provides strong inter-\nclass variance that helps to cluster but also beneﬁts the intra-class variance by keeping the diversity\nof samples in the early features.\n3\nEXPERIMENT\n3.1\nSETTING\nGeneral. We conduct all the experiments on ImageNet-1k (Deng et al., 2009) with Nvidia V100\nGPUs. The code is implemented with Pytorch (Paszke et al., 2019).\nBackbone. We adopt the standard ViT-B and ViT-L architecture (Dosovitskiy et al., 2020) with the\ntoken size of 16×16. The ViT-B is by default employed unless speciﬁed. When training with CL\nloss, we employ the projection and prediction head following Moco V3 (Chen et al., 2021). The\nsettings for pre-training and evaluation protocols can be found at Appendix A.5.\n3.2\nCOMPARISON WITH STATE-OF-THE-ART METHODS\nWe start by verifying the effectiveness of the proposed Layer Grafted Pre-training by comparing it\nwith state-of-the-art methods. As shown in Table 2, in ViT-B\/16, compared to the employed MIM\nand CL baselines, the proposed Layer Grafted Pre-training leads to a consistent improvement. For\ninstance, it improves MAE and Moco V3 by [9.7%, 14.4%, 4.5%] and [1.0%, 2.1%, 2.4%] for\n[linear evaluation, 1% few-shot, 10% few-shot], respectively.\nCompared to close competitors which also attempt to combine MIM and CL, the proposed Layer\nGrafted Pre-training surpasses iBoT by 1.5% for linear evaluation performance. Compared to SIM,\nthe proposed Layer Grafted Pre-training yields an improvement of 1.1% and 0.2% for linear evalu-\nation and 1% few-shot learning performance, respectively.\nTable 2: Comparison with State-of-The-Arts on ViT-B\/16 and ViT-L\/16. Linear, 1% and 10% denote\nthe top-1 accuracy (%) of linear evaluation, 1% and 10% few-shot learning, respectively. †: We\nemploy the result of iBoT without augmentations from Zhou et al. (2021) for fair comparison.\nBackBone\nMethod\nLinear\n1%\n10%\nViT-B\/16\nMAE (He et al., 2021)\n68.0\n51.1\n73.3\nMoco V3 (Chen et al., 2021)\n76.7\n63.4\n75.4\niBoT† (Zhou et al., 2021)\n76.0\n-\n-\nSIM (Tao et al., 2022)\n76.4\n65.3\n-\nC-MAE (Huang et al., 2022)\n73.9\n65.3\n77.3\nMimCo (Zhou et al., 2022)\n70.2\n62.7\n-\nLayer Grafted Pre-training (Ours)\n77.7\n65.5\n77.8\nViT-L\/16\nMAE (He et al., 2021)\n75.8\n55.2\n78.7\nMoco V3 (Chen et al., 2021)\n77.6\n-\n-\nLayer Grafted Pre-training (Ours)\n81.0\n69.3\n80.1\n5\nPublished as a conference paper at ICLR 2023\n(a) Moco V3\n(b) Layer Grafted Pre-training\nFigure 3: t-SNE (Van der Maaten & Hinton, 2008) visualization for feature distribution of Moco V3\nand Layer Grafted Pre-training. Different colors represent different classes. Best viewed in color.\nOur method also demonstrates good scalability toward larger models size. For instance, when scal-\ning from ViT-B\/16 to ViT-L\/16, Layer Grafted Pre-training further improves accuracy by [3.3%,\n3.8%, 2.3%] in terms of [linear evaluation, 1% few-shot, 10% few-shot], respectively. Remarkably,\nthe gap above Moco V3 in linear evaluation performance also increases from 1.0% to 3.4%.\nWe further qualitatively evaluate the representations learned by the proposed Layer Grafted Pre-\ntraining using t-SNE (Van der Maaten & Hinton, 2008). As shown in Figure 3b, the proposed Layer\nGrafted Pre-training shows better inter-class variance. For example, the categories represented by\npink (•) and light blue (•) points are hard to separate given they are very close with each other in\nFigure 3a. In contrast, for representation of the proposed Layer Grafted Pre-training, they form two\nclusters with a clear boundary in Figure 3b. Besides, the proposed Layer Grafted Pre-training also\n(a) Linear, 3rd Stage LR: 1.5e-4\n(b) Linear, 3rd Stage LR: 1.5e-5\n(c) Linear, 3rd Stage LR: 1.5e-6\n(d) Tuning, 3rd Stage LR: 1.5e-4\n(e) Tuning, 3rd Stage LR: 1.5e-5\n(f) Tuning, 3rd Stage LR: 1.5e-6\nFigure 4: Illustration of the LR grid search results for different stages in terms of linear evaluations\nand ﬁne-tuning performance. The grid is [1.5e-6,1.5e-5,1.5e-4] for each stage. [(a), (b), (c)] and\n[(d), (e), (f)] denotes the linear evaluation and ﬁne-tuning performance with third stage LR of [1.5e-\n4, 1.5e-5, 1.5e-6], respectively. [(a), (b)] and [(e), (f)] share the same color bar with (c) and (g),\nrespectively. That of (d), (e) and (f) are also the same. The tested points are highlighted with blue\ndots in each plot. Best view in color.\n6\nPublished as a conference paper at ICLR 2023\nshows better intra-variance: the red (•), green (•) and yellow (•) points of Moco V3 collapse to a\nsmaller region than the proposed Layer Grafted Pre-training.\n3.3\nLR SEARCH LAYER GRAFTED PRE-TRAINING\nWe further verify if small and large LR work the best for the proposed Layer Grafted Pre-training.\nSpeciﬁcally, we study the performance with different LR settings on the three stages on ViT-B\/16\n(Refer to ﬁne-tuning part of Appendix A.5 for the deﬁnition of stages), where each stage is searched\nwith a grid of [1.5e-6,1.5e-5,1.5e-4]. As demonstrated in Figure 4, when the LR increase from\n1.5e-6 to 1.5e-4 for the third stage, both linear evaluation and ﬁne-tuning performance achieve an\nimprovement. Taking linear evaluation as an example, as shown in Figure 4a, 4b and 4c, when\nthe LR of the third stage increase from 1.5e-6 to 1.5e-5 and 1.5e-4, the performance range could\nimprove from 2.5%-63.0% to 64.8%-70.9% and 73.7%-75.1%, respectively. In contrast, a big LR\nof the ﬁrst stage would lead to a drop in performance. For instance, in terms of the linear evalu-\nation performance with third stage LR of 1.5e-4, as shown in Figure 4a, the performance ranges\nare 74.9%-75.1%, 74.8%-75.0% and 73.7%-73.8% for ﬁrst stage LR of 1.5e-6, 1.5e-5 and 1.5e-\n4, respectively. The LR of the second stage is not as sensitive as that of the ﬁrst or third stage.\n0\n2\n4\n6\n8\n10\n12\nThe number of fixing blocks\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\nTop 1 accuracy (%)\n0.3\n9.5\nMAE\nLayer Grafted Pre-training\nFigure 5: Comparison between the pro-\nposed Layer Grafted Pre-training and\nMAE under different numbers of ﬁx-\ning blocks on ViT-B\/16 in terms of\nﬁne-tuning performance. The training\ndataset is the full ImageNet-1k.\nBest\nview in color.\nThe trend of ﬁne-tuning performance is also similar to\nthat of the linear evaluation performance. The preference\nfor larger LR for higher layers indicates that they can ben-\neﬁt by performing CL. Meanwhile, lower layers prefer a\nsmaller LR means that keeping MIM features for these\nlayers can be helpful.\n3.4\nMORE ABLATIONS\nFine-tuning Performance Comparison. We also com-\npare State-of-The-Art methods for ﬁne-tuning perfor-\nmance. As shown in Table 3, the proposed Layer Grafted\nPre-training yields a competitive ﬁne-tuning performance\nof 83.9%, which is 0.3% and 0.7% higher than the em-\nployed MIM (MAE) and CL (Moco V3) baselines, re-\nspectively.\nMoreover, Layer Grafted Pre-training also\nsurpasses SIM by 0.1%.\nPartial Fine-tuning. Follow MAE (He et al., 2021), we\nevaluate the performance of the proposed Layer Grafted\nPre-training with different number of ﬁxing blocks. As\nillustrated in Figure 5, Layer Grafted Pre-training consistently yields higher performance than MAE.\nAnd this gap continue to grow larger when more layers are ﬁxed, indicating the superiority of the\nrepresentations learned by the proposed method.\nVariance-Invariance-Covariance Analysis. A study of the Variance-Invariance-Covariance pat-\ntern for the output of each block is conducted in order to better understand the Layer Grafted Pre-\ntraining. As illustrated in Figure 6, we ﬁnd that the VIC pattern of Layer Grafted Pre-training tends\nTable 3: Top 1 Fine-tuning performance comparison.\nBackBone\nMethod\nFine-tuning\nViT-B\/16\nMAE (He et al., 2021)\n83.6\nMoco V3 (Chen et al., 2021)\n83.2\nSIM (Tao et al., 2022)\n83.8\nConMIM (Yi et al., 2022)\n83.7\nLayer Grafted Pre-training (Ours)\n83.9\nViT-L\/16\nMAE (He et al., 2021)\n85.9\nMoco V3 (Chen et al., 2021)\n84.1\nLayer Grafted Pre-training (Ours)\n85.9\n7\nPublished as a conference paper at ICLR 2023\n0\n2\n4\n6\n8\n10\nBlock Index\n0.026\n0.028\n0.030\n0.032\n0.034\n0.036\n0.038\n0.040\nMAE\nMAE Fine-tune\nLayer Grafted Pre-training\n(a) Variance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.0000\n0.0001\n0.0002\n0.0003\n0.0004\n0.0005\n0.0006\n0.0007\nMAE\nMAE Fine-tune\nLayer Grafted Pre-training\n(b) Invariance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nMAE\nMAE Fine-tune\nLayer Grafted Pre-training\n(c) Covariance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.026\n0.028\n0.030\n0.032\n0.034\n0.036\n0.038\n0.040\nMoco V3\nMoco V3 Fine-tune\nLayer Grafted Pre-training\n(d) Variance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.0000\n0.0001\n0.0002\n0.0003\n0.0004\n0.0005\n0.0006\n0.0007\nMoco V3\nMoco V3 Fine-tune\nLayer Grafted Pre-training\n(e) Invariance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nMoco V3\nMoco V3 Fine-tune\nLayer Grafted Pre-training\n(f) Covariance\nFigure 6: The Variance-Invariance-Covariance (VIC) analysis for different methods. VIC are com-\nputed on ViT-B\/16 following Bardes et al. (2021). The input features are ﬁrst averaged over all\ntokens and then normalized to remove the effect from magnitude. [(a), (d)], [(b), (e)] and [(c), (f)]\nstudy variance, covariance and invariance, respectively. Best view in color.\nTable 4: Layer Grafted Pre-training on ViT-B\/16 with VICReg (Bardes et al., 2021). We train ViT-\nB\/16 with VICReg for 100 epochs as the baseline. For Layer Grafted Pre-training - VICReg, the CL\nloss of stage ii is replaced with VICReg loss.\nMethod\nLinear\nFine-tuning\nVICReg (Bardes et al., 2021)\n70.1%\n81.2%\nLayer Grafted Pre-training - VICReg (Ours)\n74.9%\n83.6%\nto be similar to that of ﬁne-tuning. In the MAE case, the VIC curve of Layer Grafted Pre-training\nclosely matches that of MAE Fine-tune, much closer than MAE itself. The similarity between\nthe proposed Layer Grafted Pre-training and ﬁne-tuning in the VIC pattern also explains the high\nfew-shot performance: weights of the pre-training do not need to change substantially to match the\ndownstream task.\nLayer Grafted Pre-training with VICReg. We further examine the generalizability of the pro-\nposed idea on a different pre-training method - VICRegn (Bardes et al., 2021). As shown in Table 4,\nwhen replacing the CL loss with the VICReg loss, the proposed Layer Grafted Pre-training still\nyields strong performance, surpassing the VICReg baseline by [4.8%, 2.4%] for [linear evaluation,\nﬁne-tuning] performance, respectively.\n4\nRELATED WORKS\nContrastive Learning.\nCL performs instance classiﬁcation tasks by contrasting positive pairs\nagainst negative pairs (Chen et al., 2020b; He et al., 2020; Zhuang et al., 2019; Dosovitskiy et al.,\n2014). Other close works also explore learning without negative samples (Grill et al., 2020; Bardes\net al., 2021; Caron et al., 2021; Zbontar et al., 2021; Chen & He, 2021) and the clustering based\napproachs (Caron et al., 2020).\n8\nPublished as a conference paper at ICLR 2023\nOne common merit of these methods is their strong performance on learning good representa-\ntions, which shows strong clustering pattern (Dwibedi et al., 2021) and leads to state-of-the-art\nfew-shot\/semi-supervised performance (Chen et al., 2020c; Tian et al., 2020; Li et al., 2021; Jiang\net al., 2022; You et al., 2022). However, they contain an implicit assumption that the features should\nbe invariant to heavy augmentations, which, however, could further lead to worse performance when\nthe downstream performance violates it (Xiao et al., 2020). The proposed Layer Grafted Pre-training\naddress this via leveraging MIM for processing the features of lower layers.\nMask Image Modeling. Mask Image Modeling (MIM) is inspired by the success of BERT (Devlin\net al., 2018) in Natural Language Processing (NLP). iGPT (Chen et al., 2020a) begins the exploration\nof this idea in Computer Vision (CV). The emergence of ViT (Dosovitskiy et al., 2020) further\nshrinks the gap of backbones between CV and NLP, motivating more researchers to delve into this\ndirection. Beit (Bao et al., 2021; Peng et al., 2022), MaskFeat (Wei et al., 2022) and Peco (Dong\net al., 2021) focus on predicting tokens. MAE (He et al., 2021) and simMIM (Xie et al., 2022)\nfurther show the possibility of directly reconstructing the original pixels. Following works (Dong\net al., 2022; Chen et al., 2022; Wang et al., 2022b) continue to improve the performance or extend to\nother modalities. However, MIM achieves the most success in ﬁne-tuning with enough data points.\nFor downstream tasks with limited data, MIM fails to surpass CL given the lack of linear separability\nfor its representations (Tao et al., 2022). We address this drawback by employing CL for learning\nthe semantic alignment for higher layers.\nBridging Contrastive Learning and Mask Image Modeling. Only recently, researchers begin\nto explore the potential of combining MIM and CL. iBoT (Zhou et al., 2021), one of the pioneers\nin this direction, proposes to switch the modeling of images to the modeling of features. Some\nconcurrent works also follow this self-distillation paradigm (Tao et al., 2022; Assran et al., 2022).\nHowever, just like CL, this paradigm still relays on the involvement of strong augmentations to avoid\ncollapsing, which could lead to over-suppressing of some features (i.e. color) (Xiao et al., 2020). In\ncontrast, we employ MAE (He et al., 2021), an image modeling framework that is free from strong\naugmentations. Besides, previous combination works treat the network as a whole while we provide\na new layer-wise perspective.\nComparison to other multiple-step pre-training tasks. One may relate the proposed method\nwith previous multiple-step pre-training tasks like intermediate ﬁne-tuning ( e.g., ﬁnetuning a MIM\nmodel using ImageNet22k and transfer to ImageNet1k (Bao et al., 2021)) or self-supervised ﬁne-\ntuning like Reed et al. (2022). The main differences lie in two aspects: (1) The key innovation of the\nproposed method is to reveal and utilize the layerwise difference between MIM and CL. In contrast,\nintermediate ﬁnetuning and self-supervised ﬁne-tuning are treating the model as a whole; and (2)\nWhile intermediate ﬁnetuning and self-supervised are designed for the same pretraining methods\nacross different domains, the proposed method is devised for different pretraining methods in\nthe same domain.\n5\nCONCLUSION\nIn this work, we propose Layer Grafted Pre-training, a simple yet principled method for under-\nstanding and bridging two popular types of self-supervised learning methods - Mask Image Mod-\neling (MIM) and Contrastive Learning (CL). Our work provides a simple remedy to the conﬂicts\nbetween MIM and CL and further reveals the different preferences of these two methods toward\ndifferent parts of the neural network. It advances the quality of the self-supervised representations\nand achieves strong performance on linear evaluation and few-shot performance. Potential future\nwork includes assessing or extending the proposed method to real-world unlabeled data with more\nchallenges such as long-tail distribution or imbalance (Jiang et al., 2021).\nREFERENCES\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent,\nArmand Joulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-efﬁcient\nlearning. arXiv preprint arXiv:2204.07141, 2022.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254, 2021.\n9\nPublished as a conference paper at ICLR 2023\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization\nfor self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. Advances in Neural\nInformation Processing Systems, 33:9912–9924, 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of\nthe IEEE\/CVF International Conference on Computer Vision, pp. 9650–9660, 2021.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International conference on machine learning, pp. 1691–\n1703. PMLR, 2020a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning,\npp. 1597–1607. PMLR, 2020b.\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big\nself-supervised models are strong semi-supervised learners. Advances in neural information pro-\ncessing systems, 33:22243–22255, 2020c.\nXiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han,\nPing Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representa-\ntion learning. arXiv preprint arXiv:2202.03026, 2022.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020d.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\ntransformers. In Proceedings of the IEEE\/CVF International Conference on Computer Vision, pp.\n9640–9649, 2021.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training\ntext encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nXiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen,\nFang Wen, and Nenghai Yu. Peco: Perceptual codebook for bert pre-training of vision transform-\ners. arXiv preprint arXiv:2111.12710, 2021.\nXiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen,\nFang Wen, and Nenghai Yu. Bootstrapped masked autoencoders for vision bert pretraining. In\nECCV, 2022.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discrimi-\nnative unsupervised feature learning with convolutional neural networks. Advances in neural\ninformation processing systems, 27, 2014.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n10\nPublished as a conference paper at ICLR 2023\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With\na little help from my friends: Nearest-neighbor contrastive learning of visual representations. In\nProceedings of the IEEE\/CVF International Conference on Computer Vision, pp. 9588–9597,\n2021.\nPriya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-\ndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\nJean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\net al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural\ninformation processing systems, 33:21271–21284, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for\nunsupervised visual representation learning.\nIn Proceedings of the IEEE\/CVF conference on\ncomputer vision and pattern recognition, pp. 9729–9738, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick.\nMasked\nautoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\nZhicheng Huang, Xiaojie Jin, Chengze Lu, Qibin Hou, Ming-Ming Cheng, Dongmei Fu, Xiaohui\nShen, and Jiashi Feng. Contrastive masked autoencoders are stronger vision learners. arXiv\npreprint arXiv:2207.13532, 2022.\nZiyu Jiang, Tianlong Chen, Bobak J Mortazavi, and Zhangyang Wang. Self-damaging contrastive\nlearning. In International Conference on Machine Learning, pp. 4927–4939. PMLR, 2021.\nZiyu Jiang, Tianlong Chen, Xuxi Chen, Yu Cheng, Luowei Zhou, Lu Yuan, Ahmed Awadallah, and\nZhangyang Wang. Dna: Improving few-shot transfer learning with low-rank decomposition and\nalignment. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n23–27, 2022, Proceedings, Part XX, pp. 239–256. Springer, 2022.\nSuichan Li, Dongdong Chen, Yinpeng Chen, Lu Yuan, Lei Zhang, Qi Chu, Bin Liu, and Nenghai\nYu. Improve unsupervised pretraining for few-label transfer. In Proceedings of the IEEE\/CVF\nInternational Conference on Computer Vision, pp. 10201–10210, 2021.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the\nIEEE\/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance\ndeep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran\nAssociates, Inc., 2019.\nZhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling\nwith vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022.\nColorado J Reed, Xiangyu Yue, Ani Nrusimha, Sayna Ebrahimi, Vivek Vijaykumar, Richard Mao,\nBo Li, Shanghang Zhang, Devin Guillory, Sean Metzger, et al. Self-supervised pretraining im-\nproves self-supervised pretraining. In Proceedings of the IEEE\/CVF Winter Conference on Ap-\nplications of Computer Vision, pp. 2584–2594, 2022.\n11\nPublished as a conference paper at ICLR 2023\nChenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, and Jifeng Dai. Siamese image\nmodeling for self-supervised vision representation learning. arXiv preprint arXiv:2206.01204,\n2022.\nYonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking\nfew-shot image classiﬁcation: a good embedding is all you need? In European Conference on\nComputer Vision, pp. 266–282. Springer, 2020.\nHugo Touvron, Matthieu Cord, and Herv´e J´egou. Deit iii: Revenge of the vit. arXiv preprint\narXiv:2204.07118, 2022.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\nLuya Wang, Feng Liang, Yangguang Li, Wanli Ouyang, Honggang Zhang, and Jing Shao. Repre:\nImproving self-supervised vision transformer with reconstructive pre-training.\narXiv preprint\narXiv:2201.06857, 2022a.\nRui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang\nJiang, Luowei Zhou, and Lu Yuan. Bevt: Bert pretraining of video transformers. In IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR 2022), 2022b.\nShaoru Wang, Jin Gao, Zeming Li, Jian Sun, and Weiming Hu. A closer look at self-supervised\nlightweight vision transformers. arXiv preprint arXiv:2205.14443, 2022c.\nChen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichten-\nhofer. Masked feature prediction for self-supervised visual pre-training. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 14668–14678, 2022.\nTete Xiao, Xiaolong Wang, Alexei A Efros, and Trevor Darrell. What should not be contrastive in\ncontrastive learning. arXiv preprint arXiv:2008.05659, 2020.\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu.\nSimmim: A simple framework for masked image modeling. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition, pp. 9653–9663, 2022.\nKun Yi, Yixiao Ge, Xiaotong Li, Shusheng Yang, Dian Li, Jianping Wu, Ying Shan, and Xiaohu\nQie. Masked image modeling with denoising contrast. arXiv preprint arXiv:2205.09616, 2022.\nChenyu You, Weicheng Dai, Fenglin Liu, Haoran Su, Xiaoran Zhang, Lawrence Staib, and James S\nDuncan. Mine your own anatomy: Revisiting medical image segmentation with extremely limited\nlabels. arXiv preprint arXiv:2209.13476, 2022.\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.\nGradient surgery for multi-task learning. Advances in Neural Information Processing Systems,\n33:5824–5836, 2020.\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St´ephane Deny. Barlow twins: Self-supervised\nlearning via redundancy reduction. In International Conference on Machine Learning, pp. 12310–\n12320. PMLR, 2021.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot:\nImage bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.\nQiang Zhou, Chaohui Yu, Hao Luo, Zhibin Wang, and Hao Li. Mimco: Masked image modeling\npre-training with contrastive teacher. In Proceedings of the 30th ACM International Conference\non Multimedia, pp. 4487–4495, 2022.\nChengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning\nof visual embeddings. In Proceedings of the IEEE\/CVF International Conference on Computer\nVision, pp. 6002–6012, 2019.\n12\nPublished as a conference paper at ICLR 2023\nTable 5: Comparison between MIM (MAE) and CL (Moco V3) for frozen features from blocks\n[6,9,12], on top of which we employ various numbers of blocks (#Blocks) for ﬁne-tuning. 0 block\nindicates that only a linear classiﬁcation head is employed (identical to linear evaluation). Top 1\naccuracy on ImageNet-1K is reported and the best performance under each setting is highlighted\nwith bold text.\n#Blocks\nThe block index of the feature\n6\n9\n12\nMIM\nCL\nMIM\nCL\nMIM\nCL\n0\n38.9%\n43.6%\n59.3%\n65.5%\n68.0%\n76.7%\n1\n70.1%\n72.7%\n77.8%\n78.1%\n78.9%\n79.1%\n2\n76.3%\n76.8%\n80.2%\n79.2%\n80.6%\n79.7%\n4\n78.5%\n78.1%\n81.2%\n79.4%\n81.4%\n79.2%\nA\nAPPENDIX\nThis appendix contains the following details that we could not include in the main paper due to\nspace restrictions.\nA.1\nMORE ANALYSIS FOR THE LAYER-WISE DIFFERENCE BETWEEN MIM AND CL\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nThe blocks index\n40\n50\n60\n70\n80\n90\n100\n110\nMIM\nCL\nLayer Grafted Pre-training\nFigure 7: Demonstration of average attention dis-\ntance (Dosovitskiy et al., 2020) for MIM (MAE),\nCL (Moco V3) and Graft in terms of the across all\nattention heads.\nWe provide more analysis to further understand\nthe layerwise difference between MIM and CL.\nWe start by analyzing the average attention dis-\ntance across different layers. As shown in Fig-\nure 7, on the one hand, for the deep layers (i.e.\nfrom 8th to 12th blocks), the average attention\ndistance of CL would keep increasing, where\nthe aggregation of local features is likely to\nhappen. In contrast, the average attention dis-\ntance of MIM keeps the same level for the deep\nlayers. On the other hand, the average attn dis-\ntance of shallow layers (i.e. 1st and 2nd blocks)\nof CL is much larger than that of MIM, which\nmay distract the model from extracting local\nfeatures. The proposed method combines the\nlower and higher layers’ patterns of MIM and\nCL, respectively, forming a gradually increas-\ning attention distance pattern. Remarkably, this\npattern echos the philosophy of gradually in-\ncreasing receptive ﬁeld for designing network architecture He et al. (2016); Liu et al. (2021).\nSecondly, we study the different properties of features across different layers. Speciﬁcally, in Im-\nageNet 1K, we turn several random initialized blocks on top of features from different layers. As\ndemonstrated in Table 5, when only ﬁne-tuning the classiﬁcation head (#Block = 0), the perfor-\nmance of MIM is much lower than CL, indicating that the feature of CL is more close to semantic\nrepresentation. By contrast, when increasing the turnable blocks, the performance of MIM would\nsigniﬁcantly increase and even surpass CL, demonstrating the features of MIM better encode the\nlocal features. The potential of these local features can be stimulated by adding enough modules to\naggregate them. The proposed Layer Grafted Pre-training employs MIM for producing high-quality\nearly features while utilizing the CL in higher layers for aggregation.\nA.2\nDISCUSSION AND COMPARISON WITH CONCURRENT WORKS\nIn this section, we discuss the difference between the proposed method with four concurrent\nworks (Tao et al., 2022; Huang et al., 2022; Zhou et al., 2022; Yi et al., 2022) and provide more\n13\nPublished as a conference paper at ICLR 2023\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nBlock 9\nBlock 10\nBlock 11\nBlock 12\nBlock Index\n0.10\n0.05\n0.00\n0.05\n0.10\nCMIM, CL(x)\n(a) 200 epochs\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nBlock 9\nBlock 10\nBlock 11\nBlock 12\nBlock Index\n0.075\n0.050\n0.025\n0.000\n0.025\n0.050\n0.075\nCMIM, CL(x)\n(b) 300 epochs\nFigure 8: The box plot of CMIM,CL(x) across different blocks for MTL combination of MIM and\nCL. This is measured on training datasets when the network is trained to (a) 200 epochs and (b) 300\nepochs (total 300 epochs). The red dash line indicates the linear regression of median numbers.\ncomparisons. To combine the strength of MIM and CL, SIM (Tao et al., 2022) proposes to pre-\ndict the dense representations of an augmented view for enforcing semantic alignment and spatial\nsensitivity simultaneously. CMAE (Huang et al., 2022) proposes two new components: pixel shift\nand feature decoder. MimCo (Zhou et al., 2022) utilizes the CL pre-trained model as a teacher\nand performs patch-level and image-level reconstruction tasks. ConMIM (Yi et al., 2022) utilizes\ncontrastive constraints to produce a dynamic masked prediction target. The difference between the\nproposed Layer Grafted Pre-training and these works are as follows:\n• While all the concurrent works are treating the network as a whole, we reveal the different\npreferences of MIM and CL towards their internal different layers, which motivates us a\ndesign a novel layerwise method.\n• Our method employs the original design of MIM and CL, which not only is simple but also\nenables an apple-to-apple comparison. In contrast, It’s non-straightforward to tell whether\nthe improvements of the concurrent works are from the newly introduced module or the\noriginal MIM\/CL design.\n• The proposed Layer Grafted Pre-training provides an in-depth analysis of the reason why\nMIM and CL cannot be directly combined together through gradient analysis.\nAlso, here we further analyze why the proposed method fails to surpass the concurrent work C-\nMAE (Huang et al., 2022) in terms of ﬁne-tuning performance. One possible reason lies in whether\nthe masked view is employed for contrastive learning. C-MAE is contrasting a masked and a full\nimage while the proposed method is contrasting two full images following Moco V3. The difference\nin design further leads to different strengths: On the one hand, empirical results highlight that the\n14\nPublished as a conference paper at ICLR 2023\nTable 6: ADE20K semantic segmentation comparison using UperNet with different pre-training\nmethods.\nMethod\nmIoU\nMAE (He et al., 2021)\n48.1\nMoco V3 (Chen et al., 2021)\n47.3\nLayer Grafted Pre-training (Ours)\n48.7\nmasked view can beneﬁt the downstream ﬁne-tuning task (Touvron et al., 2022; He et al., 2021),\nwhich may be because it helps to learn the correlation between sparse patches that cannot be built\nunder full view. On the other hand, contrasting full images leads to a smaller gap with downstream\ntasks and thus beneﬁting the downstream few-shot and linear evaluation tasks.\nA.3\nMORE GRADIENT ANALYSIS RESULTS\nWe measure the distribution of CMIM,CL(x) across different training epochs and conﬁrmed that the\nstatistical results persist the same across the entire training process, rather than just a few speciﬁc\nepochs. As two examples, Figures 8a and 8b show that a considerable part of values is negative in\nthe 200 and 300 epochs.\nA.4\nTRANSFER LEARNING RESULTS\nWe further evaluate the proposed method for the standard transfer semantic segmentation task. As\nshown in Table 6, on ADE20K with UperNet, the proposed method achieves higher performance\nthan both MAE and Moco V3.\nA.5\nMORE SETTINGS\nPre-training. We by default adopt MAE (He et al., 2021) and Moco V3 (Chen et al., 2021) as our\nMIM and CL frameworks, respectively. For the ﬁrst step of Layer Grafted Pre-training, since it\nidentically follows the original MIM pipeline, we directly adopt the pre-trained model of MAE (He\net al., 2021) and conduct the second step, where we initialize the network with the MIM pre-trained\nmodel and train with Moco V3 (Chen et al., 2021) for 300 epochs. For LR, We ﬁrst split the network\ninto three stages. Each of them contains the same number of blocks. (i.e. 4 and 8 blocks for ViT-B\nand ViT-L, respectively.) Then, the base LR of the ﬁrst and second stages (corresponding to lower\nlayers) is assigned as 1.5e-5 while the third stage is set as 1.5e-4 by default. In the second stage of\nViT-L, we further ensure the early layers of the resultant model are close to the MIM pre-training\nby minimizing the l2 distance between the ﬁrst 12 layers between them (Refer Section A.6 for more\nablations). Other settings are identically followed from Moco V3 (Chen et al., 2021).\nFine-tuning. For ﬁne-tuning, we train with AdamW (Loshchilov & Hutter, 2017) for 100 epochs\nfollowing MAE (He et al., 2021). We employ a base LR of 5e-4 with linear scaling rule (Goyal\net al., 2017). The layer-wise LR decay ratio is set as 0.6 (Clark et al., 2020). For other settings such\nas data augmentation, LR scheduling or weight decay, we identically follow He et al. (2021).\nFew-shot Learning. We conduct few-shot learning with 1% or 10% available labels. The sub-\nsampling splits are adopted from Chen et al. (2020c). For 1% few-shot evaluation, following Caron\net al. (2021), we ﬁrst generate frozen features on training images without data augmentation, on top\nof which a logistic regression classiﬁer is trained for prediction. For 10% semi-supervised learning,\nwe train from the ﬁrst layer of the projection head following Chen et al. (2020c). We train for 400\nepochs with an initial base LR of 3e-5. Other settings are identical to the ﬁne-tuning.\nLinear Evaluation. For linear evaluation, we train a linear classiﬁer on top of frozen pre-train\nfeatures to measure the quality of the visual representations following common practices (Chen\net al., 2020b). Following Moco V3 (Chen et al., 2021), the classiﬁer is trained for 90 epochs with\nSGD optimizer and weight decay of 0. The LR is swept for each case.\n15\nPublished as a conference paper at ICLR 2023\nTable 7: Ablation study for l2 regularization on ViT-L. Top 1 accuracy on ImageNet-1K is reported\nand the best performance under each setting is highlighted with bold text.\nl2 regularization\nLinear\n1%\n10%\nFinetune\n\u0017\n80.5\n68.9\n79.8\n85.7\n✓\n81.0\n69.3\n80.1\n85.9\nA.6\nABLATION STUDY FOR l2 REGULARIZATION\nIn this section, we ablation study the effectiveness of l2 regularization mentioned at section A.5. As\nshown in Table 7, employing the l2 regularization can help to preserve the Mask Image Modeling\nfeatures of the lower layers and yield consistent improvement across multiple benchmarks.\n16\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Layer Grafted Pre-training：结合对比学习和掩码图像建模，提升标签效率\n\n## 📌 背景痛点\/本文动机\n近年来，对比学习（CL）和掩码图像建模（MIM）都证明了自监督学习在视觉表征学习中的强大能力。然而，简单地结合这两种方法并没有取得理想的效果。本文作者通过实验发现，将CL和MIM的损失函数同时优化会导致梯度方向冲突，并且随着网络层数的加深，冲突变得更加严重。这促使作者重新思考如何更好地结合这两种方法。\n\n## 🚀 核心方法\n💡 创新点1：层级嫁接预训练\n本文提出了层级嫁接预训练（Layer Grafted Pre-training）框架，该框架将MIM和CL分别应用于网络的不同层级。具体来说，首先使用MIM损失函数训练网络的低层，然后在这些层的基础上，使用CL损失函数继续训练网络的高层。这种“顺序级联”的方式有效地避免了MIM和CL损失函数之间的冲突，并使每个损失函数都能在其最合适的层级上进行预训练。\n\n💡 创新点2：平滑嫁接\n为了进一步优化层级嫁接预训练，本文提出了“平滑嫁接”的策略。具体来说，在第二步中，不仅训练高层，还允许低层以较小的学习率进行微调。这种策略可以避免特征空间中突然的变化，并使低层能够更好地保留MIM学习到的特征。\n\n## 📈 实验结果\n在ImageNet-1k数据集上，层级嫁接预训练在1%少样本学习任务中取得了65.5%的Top-1准确率，比MIM和CL基线分别提高了14.4%和2.1%。此外，层级嫁接预训练在10%少样本学习和线性评估任务中也取得了显著的性能提升。\n\n## 💬 可借鉴之处\n层级嫁接预训练框架为结合对比学习和掩码图像建模提供了一种简单而有效的方法。该方法可以应用于各种视觉表征学习任务，并有望提升标签效率。此外，本文提出的“平滑嫁接”策略也为其他多阶段预训练任务提供了新的思路。","llm_summary_res_status":200}
{"title":"Vision-Language Models Provide Promptable Representations for Reinforcement Learning","authors":"William Chen, Oier Mees, Aviral Kumar, Sergey Levine","summary":"Humans can quickly learn new behaviors by leveraging background world\nknowledge. In contrast, agents trained with reinforcement learning (RL)\ntypically learn behaviors from scratch. We thus propose a novel approach that\nuses the vast amounts of general and indexable world knowledge encoded in\nvision-language models (VLMs) pre-trained on Internet-scale data for embodied\nRL. We initialize policies with VLMs by using them as promptable\nrepresentations: embeddings that encode semantic features of visual\nobservations based on the VLM's internal knowledge and reasoning capabilities,\nas elicited through prompts that provide task context and auxiliary\ninformation. We evaluate our approach on visually-complex, long horizon RL\ntasks in Minecraft and robot navigation in Habitat. We find that our policies\ntrained on embeddings from off-the-shelf, general-purpose VLMs outperform\nequivalent policies trained on generic, non-promptable image embeddings. We\nalso find our approach outperforms instruction-following methods and performs\ncomparably to domain-specific embeddings. Finally, we show that our approach\ncan use chain-of-thought prompting to produce representations of common-sense\nsemantic reasoning, improving policy performance in novel scenes by 1.5 times.","url":"http:\/\/arxiv.org\/abs\/2402.02651v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.02651v3","published":1707094136000,"comment":null,"pdf_text":"Vision-Language Models Provide Promptable\nRepresentations for Reinforcement Learning\nWilliam Chen\nU.C. Berkeley\nOier Mees\nU.C. Berkeley\nAviral Kumar\nGoogle DeepMind\nSergey Levine\nU.C. Berkeley\nAbstract\nHumans can quickly learn new behaviors by leveraging background world knowl-\nedge. In contrast, agents trained with reinforcement learning (RL) typically learn\nbehaviors from scratch. We thus propose a novel approach that uses the vast\namounts of general and indexable world knowledge encoded in vision-language\nmodels (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize\npolicies with VLMs by using them as promptable representations: embeddings\nthat encode semantic features of visual observations based on the VLM’s internal\nknowledge and reasoning capabilities, as elicited through prompts that provide task\ncontext and auxiliary information. We evaluate our approach on visually-complex,\nlong horizon RL tasks in Minecraft and robot navigation in Habitat. We find that\nour policies trained on embeddings from off-the-shelf, general-purpose VLMs out-\nperform equivalent policies trained on generic, non-promptable image embeddings.\nWe also find our approach outperforms instruction-following methods and performs\ncomparably to domain-specific embeddings. Finally, we show that our approach\ncan use chain-of-thought prompting to produce representations of common-sense\nsemantic reasoning, improving policy performance in novel scenes by 1.5 times.\n1\nIntroduction\nEmbodied decision-making often requires representations informed by world knowledge for per-\nceptual grounding, planning, and control. Humans rapidly learn to perform sensorimotor tasks by\ndrawing on prior knowledge, which might be high-level and abstract (“If I’m cooking something\nthat needs milk, the milk is probably in the refrigerator”) or grounded and low-level (e.g., what\nrefrigerators and milk look like). These capabilities would be highly beneficial for reinforcement\nlearning (RL) too: we aim for our agents to interpret tasks in terms of concepts that can be reasoned\nabout with relevant prior knowledge and grounded with previously-learned representations, thus\nenabling more efficient learning. However, doing so requires a condensed source of vast amounts of\ngeneral-purpose world knowledge, captured in a form that allows us to specifically index into and\naccess task-relevant information. Therefore, we need representations that are contextual, such that\nagents can use a concise task context to draw out relevant background knowledge, abstractions, and\ngrounded features that aid it in acquiring a new behavior.\nAn approach to facilitate this involves integrating RL agents with the prior knowledge and reasoning\nabilities of pre-trained foundation models. Transformer-based language models (LMs) and vision-\nlanguage models (VLMs) are trained on Internet-scale data to enable generalization in downstream\ntasks requiring facts or common sense. Moreover, in-context learning [8], chain-of-thought reason-\ning (CoT) [71], and instruction fine-tuning [50] have provided better ways to index into (V)LMs’\nknowledge and steer their capabilities based on user needs. These successes have seen some transfer\nto embodied control, with (V)LMs being used to reason about goals to produce executable plans\nCorrespondence to: William Chen <verityw@berkeley.edu>. Website: pr2l.github.io.\narXiv:2402.02651v3  [cs.LG]  23 May 2024\nPrompt\n“Spiders in Minecraft \nare black. Is there a \nspider in this image?”\nVLM\nTask \nEnv\nLearned\nPolicy\n“Yes, there is a spider.”\nActions\nObservations\nPrompt\n“Would a toilet be \nfound here? Why or \nwhy not?”\nVLM\nTask \nEnv\nLearned\nPolicy\n“No, as it’s a bedroom. Toilets \nare usually found in bathrooms.”\nActions\nObservations\nMinecraft Task: Combat Spider\nHabitat Task: Find Toilet\nFigure 1: Example instantiations of PR2L for tasks in Minecraft and Habitat. We query a VLM with a\ntask-relevant prompt about observations to produce promptable representations, which we train a policy on\nvia RL. Rather than directly asking for actions or specifying the task, the prompt enables indexing into the\nVLM’s prior world knowledge to access task-relevant information. This prompt also allows us to inject auxiliary\ninformation and elicit chain-of-thought reasoning.\n[2] or as encoders of useful information (like instructions [40] or feedback [61]) that the control\npolicy utilizes. Both these paradigms have major limitations: actions generated by LMs are often not\nappropriately grounded, unless the tasks and scenes are amenable to being expressed or captioned in\nlanguage. Even then, (V)LMs are often only suited to producing subtask plans, not low-level control\nsignals. On the other hand, using (V)LMs to simply encode inputs under-utilizes their knowledge and\nreasoning abilities, instead focusing on producing embeddings that reflect the compositionality of\nlanguage (e.g., so an instruction-following policy may generalize). This motivates the development\nof an algorithm for learning to produce low-level actions that are grounded and leverage (V)LMs’\nknowledge and reasoning.\nTo this end, we introduce Promptable Representations for Reinforcement Learning (PR2L): a flexible\nframework for steering VLMs into producing semantic features, which (i) integrate observations\nwith prior task knowledge and (ii) are grounded into actions via RL (see Figure 1). Specifically,\nwe ask a VLM questions about observations that are related to the given control task, priming it to\nattend to task-relevant features in the image based on both its internal world knowledge, reasoning\ncapabilities, and any supplemental information injected via prompting. The VLM then encodes this\ninformation in decoded text, which is discarded, and associated embeddings, which serve as inputs\nto a learned policy. In contrast to the standard approach of using pre-trained image encoders to\nconvert visual inputs into generic features for downstream learning, our method yields task-specific\nfeatures capturing information particularly conducive to learning a considered task. Thus, the VLM\ndoes not just produce an un-grounded encoding of instructions, but embeddings containing semantic\ninformation relevant to the task, that is both grounded and informed by the VLM’s prior knowledge.\nTo the best our knowledge, we introduce the first approach for initializing RL policies with generative\nVLM representations. We demonstrate our approach on tasks in Minecraft [19] and Habitat [58], as\nthey present semantically-rich problems representative of many practical, realistic, and challenging\napplications of RL. We find that PR2L outperforms equivalent policies trained on vision-only\nembeddings or with instruction-conditioning, popular ways of using pre-trained image models and\nVLMs respectively for control. We also show that promptable representations extracted from general-\npurpose VLMs are competitive with domain-specific representations. Our results highlight how\nvisually-complex control tasks can benefit from accessing the knowledge captured within VLMs via\nprompting in both online and offline RL settings.\n2\nRelated Works\nVision-language models. In this work, we utilize generative VLMs (like [33, 34, 14, 29]): models\nthat generate language in response to an image and a text prompt passed as input. This is in contrast to\nother designs of combining vision and language that either generate images or segmentation [57, 30]\nand contrastive representations [52]. Formally, the VLM enables sampling from p(x1:K|I, c), where\nx1:K represents the K tokens of the output, I is the input image(s), c is the prompt, and p is the\ndistribution over natural language responses produced by the VLM on those inputs. Typically, the\nVLM is pre-trained on tasks that require building association between vision and language such as\ncaptioning. All these tasks require learning to attend to certain semantic features of input images\ndepending on the given prompt. For auto-regressive generative VLMs, this distribution is factorized as\nQ\nt p(xt|I, c, x1:t−1). Typical architectures parameterize these distributions using weights that define\na representation ϕt(I, c, x1:t−1), which depends on the image I, the prompt c, and the previously\nemitted tokens, and a decoder p(xt|ϕt(I, c, x1:t−1)), which defines a distribution over the next token.\n2\nEmbodied (V)LM reasoning. Many recent works have leveraged (V)LMs as priors over effective\nplans for a given goal. These works use the model’s language modeling and auto-regressive generation\ncapabilities to extract such priors as textual subtask sequences [2, 24, 60] or code [36, 64, 77, 68],\nthereby using the (V)LM to decompose long-horizon tasks into executable parts. These systems\noften need grounding mechanisms to ensure plan feasibility (e.g., affordance estimators [2], scene\ncaptioners [77], or trajectory labelers [51]). They also often assume access to low-level policies\nthat can execute these subtasks, such as robot pick-and-place skills [2, 36], which is often a strong\nassumption. These methods generally do not address how such policies can be acquired, nor how\nthese low-level skills can themselves benefit from the prior knowledge in (V)LMs. Even works in\nthis area that use RL still use (V)LMs as state-dependent priors over reasonable high-level goals to\nlearn [17]. This is a key difference from our work: instead of considering priors on plans\/goals, we\nrely on VLM’s implicit knowledge of the world to extract representations which encode task-relevant\ninformation. We train a policy to convert these features into low-level actions via standard RL,\nmeaning the VLM does not need to know how to take actions for a task.\nEmbodied (V)LM pre-training. Other works use (V)LMs to embed useful information like instruc-\ntions [40, 45, 42, 44, 49], feedback [61, 9], reward specifications [19], and data for world modeling\n[39, 47]. These works use (V)LMs as encoders of the compositional semantic structure of input text\nand images, which aids in generalization: an instruction-conditioned model may never have learned\nto grasp apples (but can grasp other objects), but by interacting with them in other ways and receiving\nassociated language descriptions, the model might still be able to grasp them zero-shot. In contrast,\nour method produces embeddings that are informed by world knowledge and reasoning, both from\nprompting and pre-training. Rather than just specifying that the task is to acquire an apple, we ask a\nVLM to parse observations into task-relevant features, like whether there is an apple in the image or\nif the observed location likely contains apples – information that is useful even in single-task RL.\nThus, we use VLMs to help RL solve new tasks, not just to follow instructions.\nThese two categories are not mutually exclusive: Brohan et al. [6] use VLMs to understand instruc-\ntions, but also reasoning (e.g., figuring out the “correct bowl” for a strawberry is one that contains\nfruits); Palo et al. [51] use a LM to reason about goal subtasks and a VLM to know when a trajectory\nmatches a subtask, automating the demonstration collection\/labeling of Ahn et al. [2], while Adeniji\net al. [1] use a similar approach to pretrain a language-conditioned RL policy that is transferable to\nlearning other tasks; and Shridhar et al. [63] use CLIP to merge vision and text instructions directly\ninto a form that a Transporter [76] policy can operationalize. Nevertheless, these works primarily\nfocus on instruction-following for robot manipulation. Our approach instead prompts a VLM to\nsupplement RL with representations of world knowledge, not instructions. In addition, except for\nAdeniji et al. [1], these works focus on behavior cloning (BC), assuming access to demonstrations for\npolicy learning, whereas our framework can be used for both online RL and offline RL\/BC.\n3\nPR2L: Promptable Representations for Reinforcement Learning\nWe adopt the standard framework of partially-observed Markov decision process in deep RL, wherein\nthe objective is to find a policy mapping states to actions that maximizes the expected returns. Our\ngoal is to supplement RL with task-relevant information extracted from VLMs containing general-\npurpose knowledge. One way to index into this information is by prompting the model to get it\nto produce semantic information relevant to a given control task. Therefore, our approach, PR2L,\nqueries a VLM with a task-relevant prompt for each visual observation received by the agent, and\nreceives both the decoded text and, critically, the intermediate representations, which we refer to\nas promptable representations. Even though the decoded text might often not be correct or directly\nusable for choosing the action, our key insight is that these VLM embeddings can still provide\nuseful semantic features for training control policies via RL. This recipe enables us to incorporate\nsemantic information without the need of re-training or fine-tuning a VLM to directly output actions,\nas proposed by Brohan et al. [6]. Note that our method is not an instruction-following method, and\nit does not require a task instruction to perform well. Instead, our approach still learns control via\nRL, while benefiting from the incorporation of background context. In this section, we will describe\nvarious components of our approach, accompanied by practical design choices and considerations.\n3.1\nPromptable Representations\nIn principle, one can directly query a VLM to produce actions for a task given a visual observation.\nWhile this may work when high-level goals or subtasks are sufficient, VLMs are empirically poor at\nyielding the low-level actions used commonly in RL [23]. As VLMs are trained to follow instructions\n3\nPolicy Network\nFrozen LLM Transformer Layers\nImage Encoder\nTokenizer\nDetokenizer\nPrompt\n“Spiders in Minecraft are black. Is \nthere a spider in this image?”\nDecoded Text\n“Yes, there is a spider.”\nLearned Transformer\nEncoder\nCLS\nDecoder\nNon-visual Observations\nAction\nSummary \nEmbed\nVision-Language Model\nPolicy\n…\nFigure 2: Schematic of how we extract task-relevant features from the VLM and use them in a policy\ntrained with RL. These representations can incorporate task context from the prompt, while generic image\nembeddings cannot. As generative VLM’s embeddings can be variable length, the policy has a Transformer layer\nthat takes in these embeddings and a “CLS” token, thereby condensing all inputs into a single summary vector.\nand answer questions about images, it is more appropriate to use these models to extract and reason\nabout semantic features about observations that are conducive to being linked to actions. We thus\nelicit features that are useful for the downstream task by querying these VLMs with task-relevant\nprompts that provide contextual task information, thereby causing the VLM to attend to and interpret\nappropriate parts of observed images. Extracting these features naïvely by only using the VLM’s\ndecoded text has its own challenges: such models often suffer from hallucinations [26] and an inability\nto report what they “know” in language, even when their embeddings contain such information\n[27, 21]. However, even when the text is bad, the underlying representations still contain valuable\ngranular world information that is potentially lost in the projection to language [32, 72, 22, 35]. Thus,\nwe disregard the generated text and instead provide our policy the embeddings produced by the VLM\nin response to prompts asking about relevant semantic features in observations instead.\nWhich parts of the network can be used as promptable representations? The VLMs we consider\nare all based on the Transformer architecture [67], which treats the prompt, input image(s), and\ndecoded text as token sequences. This architecture provides a source of learned representations by\ncomputing embeddings for each token at every layer based on the previous layer’s token embeddings.\nIn terms of the generative VLM formalism introduced prior, a Transformer-based VLM’s repre-\nsentations ϕt(I, c, x1:t−1) consist of N embeddings per token (the outputs of the N self-attention\nlayers) in the input image I, prompt c, and decoded text x1:t−1. The decoder p(xt|ϕt) extracts the\nfinal layer’s embedding of the most recent token xt−1, projecting it to a distribution over the token\nvocabulary and allowing for it to be sampled. When given a visual observation and task prompt,\nthe tokens representing the prompt, image, and answer consequently encode task-relevant semantic\ninformation. Thus, for each observation, we use the VLM to sample a response to the task prompt\nx1:K ∼p(x1:K|I, c). We then use some or all of these token embeddings ϕK(I, c, x1:t−1) as our\npromptable representations and feed them, along with any non-visual observation information, as a\nstate representation into our neural policy trained with RL.\nIn summary, our approach involves creating a task-relevant prompt that provides context and auxiliary\ninformation. This prompt, alongside the current visual observation from the environment, is fed\nto into the VLM to generate tokens. While these tokens are used for decoding, they are ultimately\ndiscarded. Instead, we utilize the representations produced by the VLM (associated with the image,\nprompt, and decoded text) as input for our policy, which is trained via an off-the-shelf online RL\nalgorithm to produce appropriate actions. A schematic of our approach is depicted in Figure 2 and a\ncode snippet example is presented in Appendix I.\n3.2\nDesign Choices for PR2L\nTo instantiate this idea, we need to make some concrete design choices in practice. First, the\nrepresentations of the VLM’s decoded text depend on the chosen decoding scheme: greedy decoding\nis fast and deterministic, but may yield low-probability decoded tokens; beam search improves on this\nby considering multiple “branches” of decoded text, at the cost of requiring more compute time (for\npotentially small improvements); lastly, sampling-based decoding can quickly yield estimates of the\nmaximum likelihood answer, but at the cost of introducing stochasticity, which may increase variance.\nGiven the inherent high-variance of our tasks (due to sparse rewards and partial observability) and\nthe expense of VLM decoding, we opt for greedy decoding or fixed-seed sampling.\nSecond, one must choose which VLM layers’ embeddings to utilize in the policy. While theoretically,\nall layers of the VLM could be used, pre-trained Transformer models tend to encode valuable high-\n4\nlevel semantic information in their later layers [66, 25]. Thus, we opt to only feed the final few\nlayers’ representations into our policy. As these representation sequences are of variable length, we\nincorporate an encoder-decoder Transformer layer in the policy. At each time step in a trajectory,\nthis layer receives variable-length VLM representations, which are attended to and converted into a\nfixed-length summarization by the embeddings of a learned “CLS” token [15] in the decoder (green\nin Figure 2). We also note that this policy can receive the observed image directly (e.g., after being\nembedded by the image encoder), so as to not lose any visual information from being processed\nby the VLM. However, we do not do this in our experiments in order to more clearly isolate and\ndemonstrate the usefulness of the VLM’s representations in particular.\nFinally, while it is possible to fine-tune the VLM for RL end-to-end with the policy [6],this incurs\nsubstantial compute, memory, and time overhead, particularly with larger VLMs. Nonetheless, we\nfind that our approach performs better than not using the language and prompting components of the\nVLM. This holds true even when the VLM is frozen, and only the policy is trained via RL, or when\nthe decoded text occasionally fails to answer the task-specific prompt correctly.\n3.3\nTask-Relevant Prompt Design\nHow do we design good prompts to elicit useful representations from VLMs? As we aim to\nextract good state representations from the VLM for a downstream policy, we do not use instructions\nor task descriptions, but task-relevant prompts: questions that make the VLM attend to and encode\nsemantic features in the image that are useful for the RL policy learning to solve the task [5]. For\ninstance, if the task is to find a toilet within a house, appropriate prompts include “What room is this?”\nand “Would a toilet be found here?” Intuitively, the answers to these questions help determine good\nactions (e.g., look around the room or explore elsewhere), making the corresponding representations\ngood for representing the state for a policy. Answering the questions will require the VLM to attend to\ntask-relevant features in the scene, relying on the model’s internal conception of what things look like\nand common-sense semantic relations. One can also prompt the VLM to use chain of thought [71]\nto explain its generated text, often requiring it to reason about task-relevant features in the image,\nresulting in further enrichment of the state representations. Finally, prompts can provide helpful\nauxiliary information: e.g., one can describe what certain entities of interest look like, aiding the\nVLM in detecting them even if they were not commonly found in the model’s pre-training data.\nNote that prompts based on instructions or task descriptions do not enjoy the above properties: while\nthe goal of those prior methods is to be able to directly query the VLM for the optimal action, the\ngoal of task-relevant prompts is to produce a useful state representation, such that running RL with\nthem can accelerate learning an optimal policy. While the former is not possible without task-specific\ntraining data for the VLM in the control task, the latter proves beneficial with off-the-shelf VLMs.\nEvaluating and designing prompts for RL. Since the specific representations elicited from the VLM\nare determined by the prompt, we want to design prompts that produce promptable representations\nthat maximize performance on the downstream task. The brute-force approach would involve running\nRL with each candidate prompt to measure its efficacy, but this would be computationally very\nexpensive. In lieu of this, we evaluate candidate prompts on a small dataset of observations labeled\nwith semantic features of interest for the considered task. Example features include whether task-\nrelevant entities are in the image, the relative position of said entities, or even actions (if expert\ndemonstrations are available). We test prompts by querying the VLM and checking how well the\nresulting decoded text for each image matches ground truth labels. As this is only practical for\nsmall, discrete spaces that are easily expressed in words, we see how well a small model can fit the\nVLM’s embeddings to the labels (akin to probing in self-supervised learning [62, 4]). While this does\nnot directly optimize for task performance, it does act as a proxy that ensures a prompt’s resulting\nrepresentations encode certain semantic features which are helpful for the task.\n4\nExperimental Setups\nOur experiments analyze whether promptable representations from VLMs provide benefits to down-\nstream control, thus providing an effective vehicle for transferring Internet-scale knowledge to RL.\nWe aim to show that PR2L is a good source of state representations, even with our current VLMs\nthat are bad at reasoning about actions – as such models become more performant, we expect such\nrepresentations to be even better. We thus design experiments to answer the following: (1) Can\npromptable representations obtained via task-specific prompts enable more performant and sample-\nefficient learning than those of non-promptable image encoders pre-trained for vision or control? (2)\nHow does PR2L compare to approaches that directly “ask” the VLM to generate good actions for a\n5\ntask specified in the prompt? (3) How does PR2L fare against other popular learning approaches or\npurely visual features in our domains of interest?\n4.1\nDomain 1: Minecraft\nWe first conduct experiments in Minecraft, which provides control tasks that require associating\nvisual observations with rich semantic information to succeed. Moreover, since these observations\nare distinct from the images in the the pre-training dataset of the VLM, succeeding on these tasks\nrelies crucially on the efficacy of the task-specific prompt in meaningfully affecting the learned\nrepresentation, enabling us to stress-test our method. E.g., while spiders in Minecraft somewhat\nresemble real-life spiders, they exhibit stylistic exaggerations such as bright red eyes and a large black\nbody. If the task-specific prompt is indeed effective in informing the VLM of these facts, it would\nproduce a representation that is more conducive to policy learning and this would be reflected in\ntask performance. For this domain, we use the half-precision Vicuna-7B version of the InstructBLIP\ninstruction-tuned generative VLM [14, 12] to produce promptable representations.\nMinecraft tasks. We consider all programmatic Minecraft tasks evaluated by Fan et al. [19]: combat\nspider, milk cow, shear sheep, combat zombie, combat enderman, and combat pigman1. The\nremaining tasks considered by Fan et al. [19] are creative tasks, which do not have programmatic\nreward functions or success detectors, so we cannot directly train RL agents on them. We follow the\nMineDojo definitions of observation\/action spaces and reward function structures for these tasks: at\neach time step, the policy observes an egocentric RGB image, its pose, and its previously action;\nthe policy can choose a discrete action to turn the agent by changing the agent’s pitch and\/or yaw in\ndiscrete increments, move, attack, or use a held item. These tasks are long horizon, with a maximum\nepisode length of 500 - 1000 and taking roughly 200 steps for a learned policy to complete them. See\nFigure 3 for example observations and Appendix B.1 for more details.\nComparisons. We compare PR2L to five performant classes of approaches for RL in Minecraft: (a)\nMethods using non-promptable representations of visual observations. This does not use prompting\naltogether, instead using task-agnostic embeddings from the VLM’s image encoder (specifically, the\nViT-g\/14 from InstructBLIP – blue in Figure 2). While these representations are still pre-trained, PR2L\nutilizes prompting to produce task-specific representations. For a fair comparison, we use the exact\nsame policy architecture and hyperparameters for this baseline as in PR2L, ensuring that performance\ndifferences come from prompting for better representations from the VLM. (b) Methods that directly\n“asks” the VLM to output actions to execute on the agent. This adapts the approach of Brohan et al.\n[6] to our setting and directly outputs the action from the VLM. While Brohan et al. [6] also fine-tune\nthe VLM backbone, we are unable to do so using our compute resources. To compensate, we do not\njust execute the action from the VLM, but train an RL policy to map this decoded action to a better\none. Note that if the VLM already decodes good action texts, simply copying over this action via RL\nshould be easy. (c) Methods for efficient RL from pixels via model-based approaches. We choose\nDreamer v3, since it has proven to be successful at learning Minecraft tasks from scratch [20]. (d)\nMethods leveraging pretrained representations specifically useful for embodied control, though which\nare non-promptable and non-Minecraft specific. We choose VC-1 and R3M [43, 46]. (e) Methods\nusing models pre-trained on large-scale Minecraft data. These serve as “oracle” comparisons, as\nthese representations are explicitly fine-tuned on Minecraft YouTube videos, whereas our pre-trained\nVLM is both frozen and not trained on any Minecraft video data. We choose MineCLIP, VPT, and\nSTEVE-1 as our sources of Minecraft-specific representations [19, 3, 37].\nWe use PPO [59] as our base RL algorithm for all non-Dreamer Minecraft policies. We also note that\nwe do not compare against non-RL methods, such as Voyager (which uses LLMs to write high-level\ncode skills, abstracting away low-level control to hand-written APIs that use oracle information). See\nAppendix B.2 for training details and E.1 for further discussion of such non-learned systems.\n4.2\nDomain 2: Habitat\nA major advantage of VLMs pre-trained on Internet-scale data is their reasoning and generalization\ncapabilities. To evaluate this, we run offline BC and RL experiments in the Habitat household\nsimulator. In contrast to Minecraft, tasks in this domain require connecting naturalistic images\nwith real-world common sense about the structure and contents of typical home environments. Our\nexperiments evaluate (1) whether PR2L confers the generalization properties of VLMs to our policies,\n1 Fan et al. [19] also consider hunt cow\/sheep. However, we omit them as we were unable to replicate their\nresults on those tasks; all approaches failed to learn them.\n6\nPR2L Prompt\nRT-2-style Baseline Prompt\nChange Auxiliary Text Ablation Prompt\nCombat Spider\nSpiders in Minecraft are black.\nIs there a spider in this image?\nI want to fight a spider. I can attack,\nmove, or turn. What should I do?\nIs there a spider in this image?\nMilk Cow\nIs there a cow in this image?\nI want to milk a cow. I can use my bucket,\nmove, or turn. What should I do?\nCows in Minecraft are black and white.\nIs there a cow in this image?\nShear Sheep\nIs there a sheep in this image?\nI want to shear a sheep. I can use my shears,\nmove, or turn. What should I do?\nSheep in Minecraft are usually white.\nIs there a sheep in this image?\nOther Combat Tasks\nIs there a [target entity] in this image?\nI want to fight a [target entity]. I can attack,\nmove, or turn. What should I do?\n-\nTable 1: Prompts used in Minecraft for querying the VLM with PR2L, comparison (b), and the change auxiliary\ntext ablation. For the last column, we remove the auxiliary text for combat spider, and add it in for the other two.\n(2) whether PR2L-based policies can leverage the semantic reasoning capabilities of the underlying\nVLM (e.g., via chain-of-thought [71]), and (3) whether PR2L can learn entirely from stale, offline\ndata sources. We use a Llama2-7B Prismatic VLM for the Habitat experiments [29].\nHabitat tasks. We consider the ObjectNav task suite in 3D scanned household scenes from the\nHM3D dataset [58, 73, 54]. These tasks involve a simulated robot traversing a home environment to\nfind an instance of a specified object (toilet, bed, sofa, television, plant, or chair) in the shortest path\npossible. The full benchmark consists of 80 household scenes intended to train the agent and 20 for\nvalidation. We change the observation space to consist of just RGB vision, previous action, pose,\nand target object class, omitting depth images to ensure that observed performance differences come\nfrom the quality of promptable representations vs. unpromptable ones. Like with MineDojo, these\ntasks are long horizon, taking 80 steps for a privileged shortest path follower to succeed and 150+\nfor humans. See Figure 3 for example observations and Appendix C for more details.\nComparisons. To see if PR2L can leverage VLM reasoning capabilities, we train two PR2L policies,\none with and one without chain-of-thought prompting (see Section 4.3). We also train a policy\non Prismatic VLM image encoder embeddings (equivalent to Minecraft approach (a), but with\nDino+SigLIP [11, 78]) on a human demonstration dataset collected from the ObjectNav training\nscenes collected with Habitat-Web [55] and used by past works on large-scale BC on pre-trained\nvisual representations [56, 74, 43]. As it previously achieved state-of-the-art performance among\nthose works, we also compare against two policies using VC-1 as an encoder [43], either using just\nits summarizing CLS token or using a learned Transformer layer to condense its patch embeddings.\nWe adopt the same LSTM-based recurrent architecture used by that work, but replace the image\nembeddings with a learned Transformer layer that condenses our input token embeddings (from the\nVLM, VLM image encoder, or VC-1) into a single summary embedding, as done with Minecraft.\nDue to computational constraints, we train all policies on just under a tenth of the full dataset of\n77k trajectories\/12M steps. In contrast, other works using this dataset train on the entire dataset.\nNevertheless, we evaluate on the unseen validation scenes, thereby testing how well PR2L generalizes.\n4.3\nDesigning Task-Specific Prompts for Minecraft and Habitat\nWe now discuss how to design prompts for PR2L. As noted in Section 3.3, these are not instructions\nor task descriptions, but prompts that force the VLM to encode semantic information useful for the\ntask in its representation. The simplest relevant feature for our Minecraft tasks is the presence of the\ntarget entity in an observation. Thus, we choose “Is there a [target entity] in this image?” as the base\nof our chosen prompt. We also pick two alternate prompts per task that prepend different amounts of\nauxiliary information about the target entity. E.g., for combat spider, one candidate is “Spiders in\nMinecraft are black.” To choose between these candidates, we measure how well the VLM is able\nto decode a correct answer to the prompt question of whether or not the target entity is present in\nthe image on a small annotated dataset. Full details of this prompt evaluation scheme for the first\nthree Minecraft tasks are presented in Appendix A and Table 5. We find that auxiliary text only helps\nwith detecting spiders while systematically and significantly degrading the detection of sheep and\ncows. Our ablations show that this detection success rate metric correlates with performance of the\nRL policy. Additionally, the prompts used for comparison (b) follow the prompt structure prescribed\nby Brohan et al. [6], which motivated this comparison. In these prompts, we also provide a list of\nactions that the VLM can choose from to the policy. All chosen prompts are presented in Table 1.\nFor Habitat, we choose the prompt “Would a [target object] be found here? Why or why not?” As\nopposed to the Minecraft prompts, this does not just identify the presence of a target object in the\nimage, but draws on general knowledge from the VLM to determine if the observed location would\ncontain the target object, even if said object is not in view. The second part of the prompt then leads\nthe VLM to provide a chain of thought (CoT) [71] rationale for its final answer. This CoT draws out\n7\nTask\nPR2L (Ours)\nBaselines\nOracles\nVLM Image Encoder\nRT-2-style\nDreamer\nVC-1\nR3M\nMineCLIP\nVPT\nSTEVE-1\nCombat Spider\n97.6 ± 14.9\n51.2 ± 9.3\n71.5 ± 9.7\n5.4 ± 1.1\n72.2 ± 9.3\n72.9 ± 8.7\n176.9 ± 19.8\n137.2 ± 19.2\n88.8 ± 14.0\nMilk Cow\n223.4 ± 35.4\n95.2 ± 18.7\n128.6 ± 28.9\n24.0 ± 1.2\n96.6 ± 16.3\n100.0 ± 14.1\n194.4 ± 33.3\n85.5 ± 14.5\n75.2 ± 15.4\nShear Sheep\n37.0 ± 4.4\n23.0 ± 3.6\n26.2 ± 3.2\n20.9 ± 1.2\n26.5 ± 4.0\n17.5 ± 2.4\n23.1 ± 3.7\n24.1 ± 2.9\n18.2 ± 2.5\nCombat Zombie\n24.6 ± 1.6\n14.8 ± 2.0\n18.2 ± 2.1\n1.8 ± 0.2\n5.6 ± 1.0\n5.8 ± 1.4\n56.6 ± 8.3\n31.2 ± 3.2\n23.6 ± 3.4\nCombat Enderman\n52.2 ± 5.6\n51.9 ± 6.8\n44.6 ± 5.8\n1.6 ± 0.5\n27.2 ± 2.4\n33.8 ± 3.8\n72.1 ± 7.1\n74.4 ± 13.2\n59.3 ± 6.7\nCombat Pigman\n46.4 ± 3.3\n36.8 ± 3.7\n35.1 ± 2.5\n5.8 ± 1.5\n33.7 ± 4.9\n31.4 ± 4.2\n189.0 ± 7.9\n169.0 ± 7.8\n98.3 ± 8.4\nTable 2: Performance of PR2L, baseline, and oracle approaches in Minecraft tasks. Values reported\nare IQM successes and standard errors. PR2L universally outperforms all baselines. As they are trained on\nMinecraft-specific data, the oracles outperform PR2L in half the comparisons (italicized).\nTask (# Episodes)\nPR2L (Ours)\nVLM Image Encoder\nVC-1 + CLS\nVC-1 + Patch Embeds\nWith CoT\nWithout CoT\n40 Epochs\n120 Epochs\n40 Epochs\n120 Epochs\nAverage (2000)\n41.9%\n27.8%\n11.6%\n6.8%\n8.9%\n13.6%\n15.8%\nToilet (398)\n37.2%\n22.9%\n8.8%\n2.8%\n2.0%\n7.0%\n9.3%\nBed (433)\n45.0%\n28.9%\n12.9%\n6.7%\n9.9%\n14.8%\n19.2%\nSofa (376)\n48.1%\n34.3%\n11.7%\n9.8%\n14.4%\n17.0%\n19.4%\nChair (428)\n51.2%\n40.9%\n17.5%\n11.7%\n15.0%\n22.4%\n23.8%\nTelevision (281)\n26.7%\n10.3%\n5.0%\n2.8%\n3.2%\n4.6%\n4.6%\nPlant (84)\n23.8%\n8.3%\n9.1%\n1.2%\n1.2%\n9.5%\n9.5%\nTable 3: Performance of PR2L and baselines on Habitat ObjectNav tasks. Following prior works, values\nreported are average success rates in unseen validation scenes. PR2L (with or without CoT) does better than\nall other approaches. PR2L with CoT does the best, universally achieving more than double the performance\nof all non-PR2L approaches and 14.7% higher average performance than PR2L without CoT. Note that PR2L\nand image encoder policies were trained for 40 epochs, but VC-1 policies’ performance saturated at 120, so we\nreport their performance at both times.\ntask-relevant VLM world knowledge by explicitly reasoning about visual semantic concepts, that are\nuseful to learning a policy (see Table 4; ObjectNav). To investigate if PR2L enables embodied agents\nto benefit from these VLM common-sense reasoning capabilities (even if they do not directly reason\nabout actions), we train PR2L policies both with and without the second part of the prompt.\n5\nResults\nMinecraft results. We report the interquartile mean (IQM) and standard error number of successes\nover 16 seeds for all Minecraft tasks in Table 2. PR2L uniformly outperforms the non-oracle\napproaches of (a) using non-promptable image embeddings, (b) directly asking the VLM for actions,\n(c) learning from scratch Dreamer, and (d) using non-promptable control-specific embeddings.\nPR2L outperforms (a) the VLM image encoder baseline, even though both approaches receive\nthe same visual features, with PR2L simply transforming those features via prompting an LLM\n(with no additional information from the environment), thus supporting that prompting does shape\nrepresentations in a beneficial way for learning control tasks. We provide an analysis of why PR2L\nstates are better than (b) RT-2-style ones in Appendix H.1. We observe that PR2L embeddings are\nbimodally distributed, with transitions leading to high reward clustered at one mode. This structure\nlikely enables more efficient learning, thereby showing how control tasks can benefit from extracting\nprior knowledge encoded in VLMs by prompting them with task context, even when the VLM does\nnot know how to act. For (c) the model-based comparisons, we find that Dreamer is not as conducive\nat learning our Minecraft tasks. We hypothesize this is because our tasks are comparatively shorter\nthan the ones considered by Hafner et al. [20], so learning a model is less beneficial (while PR2L\nprovides immediately-useful representations). Additionally, we note that all our approaches involve\ninteracting with partially-observable, non-stationary entities, which the Dreamer model may have a\nhard time learning. See Appendix E.2 for further discussion. Finally, (e) the oracles outperform PR2L\nin combat enderman\/pigman, all but STEVE-1 do better in combat spider\/zombie, and none do better\nin shear sheep\/milk cow. We hypothesize this is because endermen and pigmen are Minecraft-specific\nentities, giving rise to comparatively poor representations in the VLM (which is trained exclusively\non natural images). In contrast, Minecraft zombies\/spiders are heavily stylized, but still somewhat\nresemble other depictions of such creatures, while Minecraft cows and sheep are the closest to their\nnaturalistic counterparts, making PR2L more effective. Even though our VLM is not trained on\nMinecraft data, its representations yield better policies in half the oracle comparisons.\nWe provide ablations in Table 8 and Appendix F. We find that (1) PR2L performs worse when it is\nunprompted or does not decode text, (2) our prompt evaluation scheme successfully identified cases\nwhere auxiliary text improves\/degrades performance, and (3) a policy with oracle entity detection\ndoes worse than PR2L, suggesting our prompt is not just eliciting that feature from the VLM.\n8\nTarget\nExample VLM Responses to \"Would a [target] be found here? Why or why not?\"\nToilet\n\"No, a toilet is not found in this room as it is a bedroom. Toilets are typically found in bathrooms .\nThe image shows a large bed with a wooden headboard, nightstands, and lamps .\"\n\"Yes, a toilet would be found in this room because it is a bathroom.\nThe presence of a toilet in the image is expected, as bathrooms are designed to accommodate toilets for personal hygiene .\"\nBed\n\"No, a bed would not be found in this room because it is a dining room.\nThe presence of a dining table and chairs suggests that this room is designed for eating meals and socializing, rather than sleeping \"\n\"Yes, a bed would be found here because the room is described as a bedroom.\nIt has a bed , a television, a lamp, and a dresser , all of which are common furnishings in a bedroom .\"\nSofa\n\"No, a sofa is not found here because it is a kitchen area with appliances and cabinets .\nSofas are typically found in living rooms or other common areas .\"\n\" Yes, there is a black leather sofa in the living room. It has a red pillow on it. It is a large sectional couch .\"\nTable 4: Example VLM responses to the Habitat prompt for various images. Beyond just detecting the target,\nprompting the VLM for CoT elicits relevant common sense, which it semantically relates to other useful visual\nfeatures. By using the underlying VLM embeddings as a state representation, the policy thus integrates the\nVLM’s knowledge and reasoning into its decision-making.\nHabitat results. Following prior works, we report success rates on the ObjectNav validation\nepisodes in Table 3. PR2L with CoT outperforms all other policies on all tasks, including an almost\n4× performance increase over the VLM image encoder baselines – again, suggesting that using\npromptable representations for control improves over the base purely-visual embeddings. While\nPR2L without CoT still does better than all baselines, we find CoT prompting improves policy\nperformance (by 1.5×, from 27.8% success rate to 41.9%), likely because it provides the policy with\nuseful generalizable features: e.g., even if the agent comes across an unfamiliar room while searching\nfor a toilet, it still knows to look elsewhere if the VLM reasons that, due to the presence of a bed, the\nroom is likely a bedroom (which is unlikely to contain toilets). Thus, even if the VLM cannot reason\nabout actions, our results indicate that PR2L provides a promising way of using its ability to reason\nabout image semantics and common sense for control. See Table 4 for CoT examples.\nWhile we do not beat VC-1’s reported SOTA BC performance (60.3% success rate when VC-1 is\nfrozen [43]), we note that said performance is achieved with (1) over ten times more training data and\ngradient steps and (2) image augmentations to prevent overfitting. Our VC-1 policies were trained on\nthe same amount of data as our PR2L agent and for 1-3× as many gradient steps, but perform far\nworse, suggesting that PR2L is significantly more sample- and compute-efficient than VC-1 policies.\nAdditionally, PR2L does not use any explicit countermeasures to overfitting, yet still generalizes well\nto unseen ObjectNav scenes (aided by the VLM’s representations of reasoning).\nFinally, we analyze policies trained with offline RL in a simplified Habitat setting in Appendices D,\nH, where we find that VLM representations align well with the returns of an optimal policy.\n6\nConclusion\nWe propose Promptable Representations for Reinforcement Learning, a method for extracting se-\nmantic features from images by prompting VLMs with task context to leverage their extensive\ngeneral-purpose prior knowledge. We demonstrate PR2L in Minecraft and Habitat, domains that\nbenefit from interpreting observations in terms of semantic concepts that can be related to task context.\nThis framework for using VLMs for control opens new directions. For example, other types of\nfoundation models pre-trained with more sophisticated methods could also be used for PR2L: e.g.,\nones trained on physical interactions might yield features which encode physics or action knowledge,\nrather than just common-sense visual semantics. Developing and using such models with PR2L offers\nan exciting way to transfer diverse prior knowledge to a broad range of control applications.\nA limitation of PR2L is that prompts are currently hand-crafted based on the user’s conception of\nuseful task features. While coming up with good prompts for our tasks was not hard, the process of\nevaluating and improving them could be automated, which we leave to future works. We also find that\nthe quality of representations largely depends on the VLM – e.g., InstructBLIP could not reason well\nabout Habitat scenes, but the more recent Prismatic VLMs are more capable in that regard, enabling\nour CoT experiments. Thus, as VLM capabilities are expected to increase, we expect the quality of\ntheir representations to also improve. Lastly, the size and speed of VLMs can limit their applicability.\nOur policies typically achieve 3-5 Hz inference speeds, comparable to those of robot policies built on\nlarge models [7, 6, 49]. Likewise, our VLM sizes are comparable to models used for policies in prior\nworks [6, 65]. While their inference speeds may hinder online policy learning, we find that offline\napproaches (which can parallelize training and data generation) we used for Habitat help remedy this.\n9","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Vision-Language Models Provide Promptable Representations for Reinforcement Learning.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nVision-Language Models Provide Promptable Representations for Reinforcement Learning\n```\n#### 2. 论文摘要\n```\nHumans can quickly learn new behaviors by leveraging background world\nknowledge. In contrast, agents trained with reinforcement learning (RL)\ntypically learn behaviors from scratch. We thus propose a novel approach that\nuses the vast amounts of general and indexable world knowledge encoded in\nvision-language models (VLMs) pre-trained on Internet-scale data for embodied\nRL. We initialize policies with VLMs by using them as promptable\nrepresentations: embeddings that encode semantic features of visual\nobservations based on the VLM's internal knowledge and reasoning capabilities,\nas elicited through prompts that provide task context and auxiliary\ninformation. We evaluate our approach on visually-complex, long horizon RL\ntasks in Minecraft and robot navigation in Habitat. We find that our policies\ntrained on embeddings from off-the-shelf, general-purpose VLMs outperform\nequivalent policies trained on generic, non-promptable image embeddings. We\nalso find our approach outperforms instruction-following methods and performs\ncomparably to domain-specific embeddings. Finally, we show that our approach\ncan use chain-of-thought prompting to produce representations of common-sense\nsemantic reasoning, improving policy performance in novel scenes by 1.5 times.\n```\n\n#### 3. 论文全文\n```\nVision-Language Models Provide Promptable\nRepresentations for Reinforcement Learning\nWilliam Chen\nU.C. Berkeley\nOier Mees\nU.C. Berkeley\nAviral Kumar\nGoogle DeepMind\nSergey Levine\nU.C. Berkeley\nAbstract\nHumans can quickly learn new behaviors by leveraging background world knowl-\nedge. In contrast, agents trained with reinforcement learning (RL) typically learn\nbehaviors from scratch. We thus propose a novel approach that uses the vast\namounts of general and indexable world knowledge encoded in vision-language\nmodels (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize\npolicies with VLMs by using them as promptable representations: embeddings\nthat encode semantic features of visual observations based on the VLM’s internal\nknowledge and reasoning capabilities, as elicited through prompts that provide task\ncontext and auxiliary information. We evaluate our approach on visually-complex,\nlong horizon RL tasks in Minecraft and robot navigation in Habitat. We find that\nour policies trained on embeddings from off-the-shelf, general-purpose VLMs out-\nperform equivalent policies trained on generic, non-promptable image embeddings.\nWe also find our approach outperforms instruction-following methods and performs\ncomparably to domain-specific embeddings. Finally, we show that our approach\ncan use chain-of-thought prompting to produce representations of common-sense\nsemantic reasoning, improving policy performance in novel scenes by 1.5 times.\n1\nIntroduction\nEmbodied decision-making often requires representations informed by world knowledge for per-\nceptual grounding, planning, and control. Humans rapidly learn to perform sensorimotor tasks by\ndrawing on prior knowledge, which might be high-level and abstract (“If I’m cooking something\nthat needs milk, the milk is probably in the refrigerator”) or grounded and low-level (e.g., what\nrefrigerators and milk look like). These capabilities would be highly beneficial for reinforcement\nlearning (RL) too: we aim for our agents to interpret tasks in terms of concepts that can be reasoned\nabout with relevant prior knowledge and grounded with previously-learned representations, thus\nenabling more efficient learning. However, doing so requires a condensed source of vast amounts of\ngeneral-purpose world knowledge, captured in a form that allows us to specifically index into and\naccess task-relevant information. Therefore, we need representations that are contextual, such that\nagents can use a concise task context to draw out relevant background knowledge, abstractions, and\ngrounded features that aid it in acquiring a new behavior.\nAn approach to facilitate this involves integrating RL agents with the prior knowledge and reasoning\nabilities of pre-trained foundation models. Transformer-based language models (LMs) and vision-\nlanguage models (VLMs) are trained on Internet-scale data to enable generalization in downstream\ntasks requiring facts or common sense. Moreover, in-context learning [8], chain-of-thought reason-\ning (CoT) [71], and instruction fine-tuning [50] have provided better ways to index into (V)LMs’\nknowledge and steer their capabilities based on user needs. These successes have seen some transfer\nto embodied control, with (V)LMs being used to reason about goals to produce executable plans\nCorrespondence to: William Chen <verityw@berkeley.edu>. Website: pr2l.github.io.\narXiv:2402.02651v3  [cs.LG]  23 May 2024\nPrompt\n“Spiders in Minecraft \nare black. Is there a \nspider in this image?”\nVLM\nTask \nEnv\nLearned\nPolicy\n“Yes, there is a spider.”\nActions\nObservations\nPrompt\n“Would a toilet be \nfound here? Why or \nwhy not?”\nVLM\nTask \nEnv\nLearned\nPolicy\n“No, as it’s a bedroom. Toilets \nare usually found in bathrooms.”\nActions\nObservations\nMinecraft Task: Combat Spider\nHabitat Task: Find Toilet\nFigure 1: Example instantiations of PR2L for tasks in Minecraft and Habitat. We query a VLM with a\ntask-relevant prompt about observations to produce promptable representations, which we train a policy on\nvia RL. Rather than directly asking for actions or specifying the task, the prompt enables indexing into the\nVLM’s prior world knowledge to access task-relevant information. This prompt also allows us to inject auxiliary\ninformation and elicit chain-of-thought reasoning.\n[2] or as encoders of useful information (like instructions [40] or feedback [61]) that the control\npolicy utilizes. Both these paradigms have major limitations: actions generated by LMs are often not\nappropriately grounded, unless the tasks and scenes are amenable to being expressed or captioned in\nlanguage. Even then, (V)LMs are often only suited to producing subtask plans, not low-level control\nsignals. On the other hand, using (V)LMs to simply encode inputs under-utilizes their knowledge and\nreasoning abilities, instead focusing on producing embeddings that reflect the compositionality of\nlanguage (e.g., so an instruction-following policy may generalize). This motivates the development\nof an algorithm for learning to produce low-level actions that are grounded and leverage (V)LMs’\nknowledge and reasoning.\nTo this end, we introduce Promptable Representations for Reinforcement Learning (PR2L): a flexible\nframework for steering VLMs into producing semantic features, which (i) integrate observations\nwith prior task knowledge and (ii) are grounded into actions via RL (see Figure 1). Specifically,\nwe ask a VLM questions about observations that are related to the given control task, priming it to\nattend to task-relevant features in the image based on both its internal world knowledge, reasoning\ncapabilities, and any supplemental information injected via prompting. The VLM then encodes this\ninformation in decoded text, which is discarded, and associated embeddings, which serve as inputs\nto a learned policy. In contrast to the standard approach of using pre-trained image encoders to\nconvert visual inputs into generic features for downstream learning, our method yields task-specific\nfeatures capturing information particularly conducive to learning a considered task. Thus, the VLM\ndoes not just produce an un-grounded encoding of instructions, but embeddings containing semantic\ninformation relevant to the task, that is both grounded and informed by the VLM’s prior knowledge.\nTo the best our knowledge, we introduce the first approach for initializing RL policies with generative\nVLM representations. We demonstrate our approach on tasks in Minecraft [19] and Habitat [58], as\nthey present semantically-rich problems representative of many practical, realistic, and challenging\napplications of RL. We find that PR2L outperforms equivalent policies trained on vision-only\nembeddings or with instruction-conditioning, popular ways of using pre-trained image models and\nVLMs respectively for control. We also show that promptable representations extracted from general-\npurpose VLMs are competitive with domain-specific representations. Our results highlight how\nvisually-complex control tasks can benefit from accessing the knowledge captured within VLMs via\nprompting in both online and offline RL settings.\n2\nRelated Works\nVision-language models. In this work, we utilize generative VLMs (like [33, 34, 14, 29]): models\nthat generate language in response to an image and a text prompt passed as input. This is in contrast to\nother designs of combining vision and language that either generate images or segmentation [57, 30]\nand contrastive representations [52]. Formally, the VLM enables sampling from p(x1:K|I, c), where\nx1:K represents the K tokens of the output, I is the input image(s), c is the prompt, and p is the\ndistribution over natural language responses produced by the VLM on those inputs. Typically, the\nVLM is pre-trained on tasks that require building association between vision and language such as\ncaptioning. All these tasks require learning to attend to certain semantic features of input images\ndepending on the given prompt. For auto-regressive generative VLMs, this distribution is factorized as\nQ\nt p(xt|I, c, x1:t−1). Typical architectures parameterize these distributions using weights that define\na representation ϕt(I, c, x1:t−1), which depends on the image I, the prompt c, and the previously\nemitted tokens, and a decoder p(xt|ϕt(I, c, x1:t−1)), which defines a distribution over the next token.\n2\nEmbodied (V)LM reasoning. Many recent works have leveraged (V)LMs as priors over effective\nplans for a given goal. These works use the model’s language modeling and auto-regressive generation\ncapabilities to extract such priors as textual subtask sequences [2, 24, 60] or code [36, 64, 77, 68],\nthereby using the (V)LM to decompose long-horizon tasks into executable parts. These systems\noften need grounding mechanisms to ensure plan feasibility (e.g., affordance estimators [2], scene\ncaptioners [77], or trajectory labelers [51]). They also often assume access to low-level policies\nthat can execute these subtasks, such as robot pick-and-place skills [2, 36], which is often a strong\nassumption. These methods generally do not address how such policies can be acquired, nor how\nthese low-level skills can themselves benefit from the prior knowledge in (V)LMs. Even works in\nthis area that use RL still use (V)LMs as state-dependent priors over reasonable high-level goals to\nlearn [17]. This is a key difference from our work: instead of considering priors on plans\/goals, we\nrely on VLM’s implicit knowledge of the world to extract representations which encode task-relevant\ninformation. We train a policy to convert these features into low-level actions via standard RL,\nmeaning the VLM does not need to know how to take actions for a task.\nEmbodied (V)LM pre-training. Other works use (V)LMs to embed useful information like instruc-\ntions [40, 45, 42, 44, 49], feedback [61, 9], reward specifications [19], and data for world modeling\n[39, 47]. These works use (V)LMs as encoders of the compositional semantic structure of input text\nand images, which aids in generalization: an instruction-conditioned model may never have learned\nto grasp apples (but can grasp other objects), but by interacting with them in other ways and receiving\nassociated language descriptions, the model might still be able to grasp them zero-shot. In contrast,\nour method produces embeddings that are informed by world knowledge and reasoning, both from\nprompting and pre-training. Rather than just specifying that the task is to acquire an apple, we ask a\nVLM to parse observations into task-relevant features, like whether there is an apple in the image or\nif the observed location likely contains apples – information that is useful even in single-task RL.\nThus, we use VLMs to help RL solve new tasks, not just to follow instructions.\nThese two categories are not mutually exclusive: Brohan et al. [6] use VLMs to understand instruc-\ntions, but also reasoning (e.g., figuring out the “correct bowl” for a strawberry is one that contains\nfruits); Palo et al. [51] use a LM to reason about goal subtasks and a VLM to know when a trajectory\nmatches a subtask, automating the demonstration collection\/labeling of Ahn et al. [2], while Adeniji\net al. [1] use a similar approach to pretrain a language-conditioned RL policy that is transferable to\nlearning other tasks; and Shridhar et al. [63] use CLIP to merge vision and text instructions directly\ninto a form that a Transporter [76] policy can operationalize. Nevertheless, these works primarily\nfocus on instruction-following for robot manipulation. Our approach instead prompts a VLM to\nsupplement RL with representations of world knowledge, not instructions. In addition, except for\nAdeniji et al. [1], these works focus on behavior cloning (BC), assuming access to demonstrations for\npolicy learning, whereas our framework can be used for both online RL and offline RL\/BC.\n3\nPR2L: Promptable Representations for Reinforcement Learning\nWe adopt the standard framework of partially-observed Markov decision process in deep RL, wherein\nthe objective is to find a policy mapping states to actions that maximizes the expected returns. Our\ngoal is to supplement RL with task-relevant information extracted from VLMs containing general-\npurpose knowledge. One way to index into this information is by prompting the model to get it\nto produce semantic information relevant to a given control task. Therefore, our approach, PR2L,\nqueries a VLM with a task-relevant prompt for each visual observation received by the agent, and\nreceives both the decoded text and, critically, the intermediate representations, which we refer to\nas promptable representations. Even though the decoded text might often not be correct or directly\nusable for choosing the action, our key insight is that these VLM embeddings can still provide\nuseful semantic features for training control policies via RL. This recipe enables us to incorporate\nsemantic information without the need of re-training or fine-tuning a VLM to directly output actions,\nas proposed by Brohan et al. [6]. Note that our method is not an instruction-following method, and\nit does not require a task instruction to perform well. Instead, our approach still learns control via\nRL, while benefiting from the incorporation of background context. In this section, we will describe\nvarious components of our approach, accompanied by practical design choices and considerations.\n3.1\nPromptable Representations\nIn principle, one can directly query a VLM to produce actions for a task given a visual observation.\nWhile this may work when high-level goals or subtasks are sufficient, VLMs are empirically poor at\nyielding the low-level actions used commonly in RL [23]. As VLMs are trained to follow instructions\n3\nPolicy Network\nFrozen LLM Transformer Layers\nImage Encoder\nTokenizer\nDetokenizer\nPrompt\n“Spiders in Minecraft are black. Is \nthere a spider in this image?”\nDecoded Text\n“Yes, there is a spider.”\nLearned Transformer\nEncoder\nCLS\nDecoder\nNon-visual Observations\nAction\nSummary \nEmbed\nVision-Language Model\nPolicy\n…\nFigure 2: Schematic of how we extract task-relevant features from the VLM and use them in a policy\ntrained with RL. These representations can incorporate task context from the prompt, while generic image\nembeddings cannot. As generative VLM’s embeddings can be variable length, the policy has a Transformer layer\nthat takes in these embeddings and a “CLS” token, thereby condensing all inputs into a single summary vector.\nand answer questions about images, it is more appropriate to use these models to extract and reason\nabout semantic features about observations that are conducive to being linked to actions. We thus\nelicit features that are useful for the downstream task by querying these VLMs with task-relevant\nprompts that provide contextual task information, thereby causing the VLM to attend to and interpret\nappropriate parts of observed images. Extracting these features naïvely by only using the VLM’s\ndecoded text has its own challenges: such models often suffer from hallucinations [26] and an inability\nto report what they “know” in language, even when their embeddings contain such information\n[27, 21]. However, even when the text is bad, the underlying representations still contain valuable\ngranular world information that is potentially lost in the projection to language [32, 72, 22, 35]. Thus,\nwe disregard the generated text and instead provide our policy the embeddings produced by the VLM\nin response to prompts asking about relevant semantic features in observations instead.\nWhich parts of the network can be used as promptable representations? The VLMs we consider\nare all based on the Transformer architecture [67], which treats the prompt, input image(s), and\ndecoded text as token sequences. This architecture provides a source of learned representations by\ncomputing embeddings for each token at every layer based on the previous layer’s token embeddings.\nIn terms of the generative VLM formalism introduced prior, a Transformer-based VLM’s repre-\nsentations ϕt(I, c, x1:t−1) consist of N embeddings per token (the outputs of the N self-attention\nlayers) in the input image I, prompt c, and decoded text x1:t−1. The decoder p(xt|ϕt) extracts the\nfinal layer’s embedding of the most recent token xt−1, projecting it to a distribution over the token\nvocabulary and allowing for it to be sampled. When given a visual observation and task prompt,\nthe tokens representing the prompt, image, and answer consequently encode task-relevant semantic\ninformation. Thus, for each observation, we use the VLM to sample a response to the task prompt\nx1:K ∼p(x1:K|I, c). We then use some or all of these token embeddings ϕK(I, c, x1:t−1) as our\npromptable representations and feed them, along with any non-visual observation information, as a\nstate representation into our neural policy trained with RL.\nIn summary, our approach involves creating a task-relevant prompt that provides context and auxiliary\ninformation. This prompt, alongside the current visual observation from the environment, is fed\nto into the VLM to generate tokens. While these tokens are used for decoding, they are ultimately\ndiscarded. Instead, we utilize the representations produced by the VLM (associated with the image,\nprompt, and decoded text) as input for our policy, which is trained via an off-the-shelf online RL\nalgorithm to produce appropriate actions. A schematic of our approach is depicted in Figure 2 and a\ncode snippet example is presented in Appendix I.\n3.2\nDesign Choices for PR2L\nTo instantiate this idea, we need to make some concrete design choices in practice. First, the\nrepresentations of the VLM’s decoded text depend on the chosen decoding scheme: greedy decoding\nis fast and deterministic, but may yield low-probability decoded tokens; beam search improves on this\nby considering multiple “branches” of decoded text, at the cost of requiring more compute time (for\npotentially small improvements); lastly, sampling-based decoding can quickly yield estimates of the\nmaximum likelihood answer, but at the cost of introducing stochasticity, which may increase variance.\nGiven the inherent high-variance of our tasks (due to sparse rewards and partial observability) and\nthe expense of VLM decoding, we opt for greedy decoding or fixed-seed sampling.\nSecond, one must choose which VLM layers’ embeddings to utilize in the policy. While theoretically,\nall layers of the VLM could be used, pre-trained Transformer models tend to encode valuable high-\n4\nlevel semantic information in their later layers [66, 25]. Thus, we opt to only feed the final few\nlayers’ representations into our policy. As these representation sequences are of variable length, we\nincorporate an encoder-decoder Transformer layer in the policy. At each time step in a trajectory,\nthis layer receives variable-length VLM representations, which are attended to and converted into a\nfixed-length summarization by the embeddings of a learned “CLS” token [15] in the decoder (green\nin Figure 2). We also note that this policy can receive the observed image directly (e.g., after being\nembedded by the image encoder), so as to not lose any visual information from being processed\nby the VLM. However, we do not do this in our experiments in order to more clearly isolate and\ndemonstrate the usefulness of the VLM’s representations in particular.\nFinally, while it is possible to fine-tune the VLM for RL end-to-end with the policy [6],this incurs\nsubstantial compute, memory, and time overhead, particularly with larger VLMs. Nonetheless, we\nfind that our approach performs better than not using the language and prompting components of the\nVLM. This holds true even when the VLM is frozen, and only the policy is trained via RL, or when\nthe decoded text occasionally fails to answer the task-specific prompt correctly.\n3.3\nTask-Relevant Prompt Design\nHow do we design good prompts to elicit useful representations from VLMs? As we aim to\nextract good state representations from the VLM for a downstream policy, we do not use instructions\nor task descriptions, but task-relevant prompts: questions that make the VLM attend to and encode\nsemantic features in the image that are useful for the RL policy learning to solve the task [5]. For\ninstance, if the task is to find a toilet within a house, appropriate prompts include “What room is this?”\nand “Would a toilet be found here?” Intuitively, the answers to these questions help determine good\nactions (e.g., look around the room or explore elsewhere), making the corresponding representations\ngood for representing the state for a policy. Answering the questions will require the VLM to attend to\ntask-relevant features in the scene, relying on the model’s internal conception of what things look like\nand common-sense semantic relations. One can also prompt the VLM to use chain of thought [71]\nto explain its generated text, often requiring it to reason about task-relevant features in the image,\nresulting in further enrichment of the state representations. Finally, prompts can provide helpful\nauxiliary information: e.g., one can describe what certain entities of interest look like, aiding the\nVLM in detecting them even if they were not commonly found in the model’s pre-training data.\nNote that prompts based on instructions or task descriptions do not enjoy the above properties: while\nthe goal of those prior methods is to be able to directly query the VLM for the optimal action, the\ngoal of task-relevant prompts is to produce a useful state representation, such that running RL with\nthem can accelerate learning an optimal policy. While the former is not possible without task-specific\ntraining data for the VLM in the control task, the latter proves beneficial with off-the-shelf VLMs.\nEvaluating and designing prompts for RL. Since the specific representations elicited from the VLM\nare determined by the prompt, we want to design prompts that produce promptable representations\nthat maximize performance on the downstream task. The brute-force approach would involve running\nRL with each candidate prompt to measure its efficacy, but this would be computationally very\nexpensive. In lieu of this, we evaluate candidate prompts on a small dataset of observations labeled\nwith semantic features of interest for the considered task. Example features include whether task-\nrelevant entities are in the image, the relative position of said entities, or even actions (if expert\ndemonstrations are available). We test prompts by querying the VLM and checking how well the\nresulting decoded text for each image matches ground truth labels. As this is only practical for\nsmall, discrete spaces that are easily expressed in words, we see how well a small model can fit the\nVLM’s embeddings to the labels (akin to probing in self-supervised learning [62, 4]). While this does\nnot directly optimize for task performance, it does act as a proxy that ensures a prompt’s resulting\nrepresentations encode certain semantic features which are helpful for the task.\n4\nExperimental Setups\nOur experiments analyze whether promptable representations from VLMs provide benefits to down-\nstream control, thus providing an effective vehicle for transferring Internet-scale knowledge to RL.\nWe aim to show that PR2L is a good source of state representations, even with our current VLMs\nthat are bad at reasoning about actions – as such models become more performant, we expect such\nrepresentations to be even better. We thus design experiments to answer the following: (1) Can\npromptable representations obtained via task-specific prompts enable more performant and sample-\nefficient learning than those of non-promptable image encoders pre-trained for vision or control? (2)\nHow does PR2L compare to approaches that directly “ask” the VLM to generate good actions for a\n5\ntask specified in the prompt? (3) How does PR2L fare against other popular learning approaches or\npurely visual features in our domains of interest?\n4.1\nDomain 1: Minecraft\nWe first conduct experiments in Minecraft, which provides control tasks that require associating\nvisual observations with rich semantic information to succeed. Moreover, since these observations\nare distinct from the images in the the pre-training dataset of the VLM, succeeding on these tasks\nrelies crucially on the efficacy of the task-specific prompt in meaningfully affecting the learned\nrepresentation, enabling us to stress-test our method. E.g., while spiders in Minecraft somewhat\nresemble real-life spiders, they exhibit stylistic exaggerations such as bright red eyes and a large black\nbody. If the task-specific prompt is indeed effective in informing the VLM of these facts, it would\nproduce a representation that is more conducive to policy learning and this would be reflected in\ntask performance. For this domain, we use the half-precision Vicuna-7B version of the InstructBLIP\ninstruction-tuned generative VLM [14, 12] to produce promptable representations.\nMinecraft tasks. We consider all programmatic Minecraft tasks evaluated by Fan et al. [19]: combat\nspider, milk cow, shear sheep, combat zombie, combat enderman, and combat pigman1. The\nremaining tasks considered by Fan et al. [19] are creative tasks, which do not have programmatic\nreward functions or success detectors, so we cannot directly train RL agents on them. We follow the\nMineDojo definitions of observation\/action spaces and reward function structures for these tasks: at\neach time step, the policy observes an egocentric RGB image, its pose, and its previously action;\nthe policy can choose a discrete action to turn the agent by changing the agent’s pitch and\/or yaw in\ndiscrete increments, move, attack, or use a held item. These tasks are long horizon, with a maximum\nepisode length of 500 - 1000 and taking roughly 200 steps for a learned policy to complete them. See\nFigure 3 for example observations and Appendix B.1 for more details.\nComparisons. We compare PR2L to five performant classes of approaches for RL in Minecraft: (a)\nMethods using non-promptable representations of visual observations. This does not use prompting\naltogether, instead using task-agnostic embeddings from the VLM’s image encoder (specifically, the\nViT-g\/14 from InstructBLIP – blue in Figure 2). While these representations are still pre-trained, PR2L\nutilizes prompting to produce task-specific representations. For a fair comparison, we use the exact\nsame policy architecture and hyperparameters for this baseline as in PR2L, ensuring that performance\ndifferences come from prompting for better representations from the VLM. (b) Methods that directly\n“asks” the VLM to output actions to execute on the agent. This adapts the approach of Brohan et al.\n[6] to our setting and directly outputs the action from the VLM. While Brohan et al. [6] also fine-tune\nthe VLM backbone, we are unable to do so using our compute resources. To compensate, we do not\njust execute the action from the VLM, but train an RL policy to map this decoded action to a better\none. Note that if the VLM already decodes good action texts, simply copying over this action via RL\nshould be easy. (c) Methods for efficient RL from pixels via model-based approaches. We choose\nDreamer v3, since it has proven to be successful at learning Minecraft tasks from scratch [20]. (d)\nMethods leveraging pretrained representations specifically useful for embodied control, though which\nare non-promptable and non-Minecraft specific. We choose VC-1 and R3M [43, 46]. (e) Methods\nusing models pre-trained on large-scale Minecraft data. These serve as “oracle” comparisons, as\nthese representations are explicitly fine-tuned on Minecraft YouTube videos, whereas our pre-trained\nVLM is both frozen and not trained on any Minecraft video data. We choose MineCLIP, VPT, and\nSTEVE-1 as our sources of Minecraft-specific representations [19, 3, 37].\nWe use PPO [59] as our base RL algorithm for all non-Dreamer Minecraft policies. We also note that\nwe do not compare against non-RL methods, such as Voyager (which uses LLMs to write high-level\ncode skills, abstracting away low-level control to hand-written APIs that use oracle information). See\nAppendix B.2 for training details and E.1 for further discussion of such non-learned systems.\n4.2\nDomain 2: Habitat\nA major advantage of VLMs pre-trained on Internet-scale data is their reasoning and generalization\ncapabilities. To evaluate this, we run offline BC and RL experiments in the Habitat household\nsimulator. In contrast to Minecraft, tasks in this domain require connecting naturalistic images\nwith real-world common sense about the structure and contents of typical home environments. Our\nexperiments evaluate (1) whether PR2L confers the generalization properties of VLMs to our policies,\n1 Fan et al. [19] also consider hunt cow\/sheep. However, we omit them as we were unable to replicate their\nresults on those tasks; all approaches failed to learn them.\n6\nPR2L Prompt\nRT-2-style Baseline Prompt\nChange Auxiliary Text Ablation Prompt\nCombat Spider\nSpiders in Minecraft are black.\nIs there a spider in this image?\nI want to fight a spider. I can attack,\nmove, or turn. What should I do?\nIs there a spider in this image?\nMilk Cow\nIs there a cow in this image?\nI want to milk a cow. I can use my bucket,\nmove, or turn. What should I do?\nCows in Minecraft are black and white.\nIs there a cow in this image?\nShear Sheep\nIs there a sheep in this image?\nI want to shear a sheep. I can use my shears,\nmove, or turn. What should I do?\nSheep in Minecraft are usually white.\nIs there a sheep in this image?\nOther Combat Tasks\nIs there a [target entity] in this image?\nI want to fight a [target entity]. I can attack,\nmove, or turn. What should I do?\n-\nTable 1: Prompts used in Minecraft for querying the VLM with PR2L, comparison (b), and the change auxiliary\ntext ablation. For the last column, we remove the auxiliary text for combat spider, and add it in for the other two.\n(2) whether PR2L-based policies can leverage the semantic reasoning capabilities of the underlying\nVLM (e.g., via chain-of-thought [71]), and (3) whether PR2L can learn entirely from stale, offline\ndata sources. We use a Llama2-7B Prismatic VLM for the Habitat experiments [29].\nHabitat tasks. We consider the ObjectNav task suite in 3D scanned household scenes from the\nHM3D dataset [58, 73, 54]. These tasks involve a simulated robot traversing a home environment to\nfind an instance of a specified object (toilet, bed, sofa, television, plant, or chair) in the shortest path\npossible. The full benchmark consists of 80 household scenes intended to train the agent and 20 for\nvalidation. We change the observation space to consist of just RGB vision, previous action, pose,\nand target object class, omitting depth images to ensure that observed performance differences come\nfrom the quality of promptable representations vs. unpromptable ones. Like with MineDojo, these\ntasks are long horizon, taking 80 steps for a privileged shortest path follower to succeed and 150+\nfor humans. See Figure 3 for example observations and Appendix C for more details.\nComparisons. To see if PR2L can leverage VLM reasoning capabilities, we train two PR2L policies,\none with and one without chain-of-thought prompting (see Section 4.3). We also train a policy\non Prismatic VLM image encoder embeddings (equivalent to Minecraft approach (a), but with\nDino+SigLIP [11, 78]) on a human demonstration dataset collected from the ObjectNav training\nscenes collected with Habitat-Web [55] and used by past works on large-scale BC on pre-trained\nvisual representations [56, 74, 43]. As it previously achieved state-of-the-art performance among\nthose works, we also compare against two policies using VC-1 as an encoder [43], either using just\nits summarizing CLS token or using a learned Transformer layer to condense its patch embeddings.\nWe adopt the same LSTM-based recurrent architecture used by that work, but replace the image\nembeddings with a learned Transformer layer that condenses our input token embeddings (from the\nVLM, VLM image encoder, or VC-1) into a single summary embedding, as done with Minecraft.\nDue to computational constraints, we train all policies on just under a tenth of the full dataset of\n77k trajectories\/12M steps. In contrast, other works using this dataset train on the entire dataset.\nNevertheless, we evaluate on the unseen validation scenes, thereby testing how well PR2L generalizes.\n4.3\nDesigning Task-Specific Prompts for Minecraft and Habitat\nWe now discuss how to design prompts for PR2L. As noted in Section 3.3, these are not instructions\nor task descriptions, but prompts that force the VLM to encode semantic information useful for the\ntask in its representation. The simplest relevant feature for our Minecraft tasks is the presence of the\ntarget entity in an observation. Thus, we choose “Is there a [target entity] in this image?” as the base\nof our chosen prompt. We also pick two alternate prompts per task that prepend different amounts of\nauxiliary information about the target entity. E.g., for combat spider, one candidate is “Spiders in\nMinecraft are black.” To choose between these candidates, we measure how well the VLM is able\nto decode a correct answer to the prompt question of whether or not the target entity is present in\nthe image on a small annotated dataset. Full details of this prompt evaluation scheme for the first\nthree Minecraft tasks are presented in Appendix A and Table 5. We find that auxiliary text only helps\nwith detecting spiders while systematically and significantly degrading the detection of sheep and\ncows. Our ablations show that this detection success rate metric correlates with performance of the\nRL policy. Additionally, the prompts used for comparison (b) follow the prompt structure prescribed\nby Brohan et al. [6], which motivated this comparison. In these prompts, we also provide a list of\nactions that the VLM can choose from to the policy. All chosen prompts are presented in Table 1.\nFor Habitat, we choose the prompt “Would a [target object] be found here? Why or why not?” As\nopposed to the Minecraft prompts, this does not just identify the presence of a target object in the\nimage, but draws on general knowledge from the VLM to determine if the observed location would\ncontain the target object, even if said object is not in view. The second part of the prompt then leads\nthe VLM to provide a chain of thought (CoT) [71] rationale for its final answer. This CoT draws out\n7\nTask\nPR2L (Ours)\nBaselines\nOracles\nVLM Image Encoder\nRT-2-style\nDreamer\nVC-1\nR3M\nMineCLIP\nVPT\nSTEVE-1\nCombat Spider\n97.6 ± 14.9\n51.2 ± 9.3\n71.5 ± 9.7\n5.4 ± 1.1\n72.2 ± 9.3\n72.9 ± 8.7\n176.9 ± 19.8\n137.2 ± 19.2\n88.8 ± 14.0\nMilk Cow\n223.4 ± 35.4\n95.2 ± 18.7\n128.6 ± 28.9\n24.0 ± 1.2\n96.6 ± 16.3\n100.0 ± 14.1\n194.4 ± 33.3\n85.5 ± 14.5\n75.2 ± 15.4\nShear Sheep\n37.0 ± 4.4\n23.0 ± 3.6\n26.2 ± 3.2\n20.9 ± 1.2\n26.5 ± 4.0\n17.5 ± 2.4\n23.1 ± 3.7\n24.1 ± 2.9\n18.2 ± 2.5\nCombat Zombie\n24.6 ± 1.6\n14.8 ± 2.0\n18.2 ± 2.1\n1.8 ± 0.2\n5.6 ± 1.0\n5.8 ± 1.4\n56.6 ± 8.3\n31.2 ± 3.2\n23.6 ± 3.4\nCombat Enderman\n52.2 ± 5.6\n51.9 ± 6.8\n44.6 ± 5.8\n1.6 ± 0.5\n27.2 ± 2.4\n33.8 ± 3.8\n72.1 ± 7.1\n74.4 ± 13.2\n59.3 ± 6.7\nCombat Pigman\n46.4 ± 3.3\n36.8 ± 3.7\n35.1 ± 2.5\n5.8 ± 1.5\n33.7 ± 4.9\n31.4 ± 4.2\n189.0 ± 7.9\n169.0 ± 7.8\n98.3 ± 8.4\nTable 2: Performance of PR2L, baseline, and oracle approaches in Minecraft tasks. Values reported\nare IQM successes and standard errors. PR2L universally outperforms all baselines. As they are trained on\nMinecraft-specific data, the oracles outperform PR2L in half the comparisons (italicized).\nTask (# Episodes)\nPR2L (Ours)\nVLM Image Encoder\nVC-1 + CLS\nVC-1 + Patch Embeds\nWith CoT\nWithout CoT\n40 Epochs\n120 Epochs\n40 Epochs\n120 Epochs\nAverage (2000)\n41.9%\n27.8%\n11.6%\n6.8%\n8.9%\n13.6%\n15.8%\nToilet (398)\n37.2%\n22.9%\n8.8%\n2.8%\n2.0%\n7.0%\n9.3%\nBed (433)\n45.0%\n28.9%\n12.9%\n6.7%\n9.9%\n14.8%\n19.2%\nSofa (376)\n48.1%\n34.3%\n11.7%\n9.8%\n14.4%\n17.0%\n19.4%\nChair (428)\n51.2%\n40.9%\n17.5%\n11.7%\n15.0%\n22.4%\n23.8%\nTelevision (281)\n26.7%\n10.3%\n5.0%\n2.8%\n3.2%\n4.6%\n4.6%\nPlant (84)\n23.8%\n8.3%\n9.1%\n1.2%\n1.2%\n9.5%\n9.5%\nTable 3: Performance of PR2L and baselines on Habitat ObjectNav tasks. Following prior works, values\nreported are average success rates in unseen validation scenes. PR2L (with or without CoT) does better than\nall other approaches. PR2L with CoT does the best, universally achieving more than double the performance\nof all non-PR2L approaches and 14.7% higher average performance than PR2L without CoT. Note that PR2L\nand image encoder policies were trained for 40 epochs, but VC-1 policies’ performance saturated at 120, so we\nreport their performance at both times.\ntask-relevant VLM world knowledge by explicitly reasoning about visual semantic concepts, that are\nuseful to learning a policy (see Table 4; ObjectNav). To investigate if PR2L enables embodied agents\nto benefit from these VLM common-sense reasoning capabilities (even if they do not directly reason\nabout actions), we train PR2L policies both with and without the second part of the prompt.\n5\nResults\nMinecraft results. We report the interquartile mean (IQM) and standard error number of successes\nover 16 seeds for all Minecraft tasks in Table 2. PR2L uniformly outperforms the non-oracle\napproaches of (a) using non-promptable image embeddings, (b) directly asking the VLM for actions,\n(c) learning from scratch Dreamer, and (d) using non-promptable control-specific embeddings.\nPR2L outperforms (a) the VLM image encoder baseline, even though both approaches receive\nthe same visual features, with PR2L simply transforming those features via prompting an LLM\n(with no additional information from the environment), thus supporting that prompting does shape\nrepresentations in a beneficial way for learning control tasks. We provide an analysis of why PR2L\nstates are better than (b) RT-2-style ones in Appendix H.1. We observe that PR2L embeddings are\nbimodally distributed, with transitions leading to high reward clustered at one mode. This structure\nlikely enables more efficient learning, thereby showing how control tasks can benefit from extracting\nprior knowledge encoded in VLMs by prompting them with task context, even when the VLM does\nnot know how to act. For (c) the model-based comparisons, we find that Dreamer is not as conducive\nat learning our Minecraft tasks. We hypothesize this is because our tasks are comparatively shorter\nthan the ones considered by Hafner et al. [20], so learning a model is less beneficial (while PR2L\nprovides immediately-useful representations). Additionally, we note that all our approaches involve\ninteracting with partially-observable, non-stationary entities, which the Dreamer model may have a\nhard time learning. See Appendix E.2 for further discussion. Finally, (e) the oracles outperform PR2L\nin combat enderman\/pigman, all but STEVE-1 do better in combat spider\/zombie, and none do better\nin shear sheep\/milk cow. We hypothesize this is because endermen and pigmen are Minecraft-specific\nentities, giving rise to comparatively poor representations in the VLM (which is trained exclusively\non natural images). In contrast, Minecraft zombies\/spiders are heavily stylized, but still somewhat\nresemble other depictions of such creatures, while Minecraft cows and sheep are the closest to their\nnaturalistic counterparts, making PR2L more effective. Even though our VLM is not trained on\nMinecraft data, its representations yield better policies in half the oracle comparisons.\nWe provide ablations in Table 8 and Appendix F. We find that (1) PR2L performs worse when it is\nunprompted or does not decode text, (2) our prompt evaluation scheme successfully identified cases\nwhere auxiliary text improves\/degrades performance, and (3) a policy with oracle entity detection\ndoes worse than PR2L, suggesting our prompt is not just eliciting that feature from the VLM.\n8\nTarget\nExample VLM Responses to \"Would a [target] be found here? Why or why not?\"\nToilet\n\"No, a toilet is not found in this room as it is a bedroom. Toilets are typically found in bathrooms .\nThe image shows a large bed with a wooden headboard, nightstands, and lamps .\"\n\"Yes, a toilet would be found in this room because it is a bathroom.\nThe presence of a toilet in the image is expected, as bathrooms are designed to accommodate toilets for personal hygiene .\"\nBed\n\"No, a bed would not be found in this room because it is a dining room.\nThe presence of a dining table and chairs suggests that this room is designed for eating meals and socializing, rather than sleeping \"\n\"Yes, a bed would be found here because the room is described as a bedroom.\nIt has a bed , a television, a lamp, and a dresser , all of which are common furnishings in a bedroom .\"\nSofa\n\"No, a sofa is not found here because it is a kitchen area with appliances and cabinets .\nSofas are typically found in living rooms or other common areas .\"\n\" Yes, there is a black leather sofa in the living room. It has a red pillow on it. It is a large sectional couch .\"\nTable 4: Example VLM responses to the Habitat prompt for various images. Beyond just detecting the target,\nprompting the VLM for CoT elicits relevant common sense, which it semantically relates to other useful visual\nfeatures. By using the underlying VLM embeddings as a state representation, the policy thus integrates the\nVLM’s knowledge and reasoning into its decision-making.\nHabitat results. Following prior works, we report success rates on the ObjectNav validation\nepisodes in Table 3. PR2L with CoT outperforms all other policies on all tasks, including an almost\n4× performance increase over the VLM image encoder baselines – again, suggesting that using\npromptable representations for control improves over the base purely-visual embeddings. While\nPR2L without CoT still does better than all baselines, we find CoT prompting improves policy\nperformance (by 1.5×, from 27.8% success rate to 41.9%), likely because it provides the policy with\nuseful generalizable features: e.g., even if the agent comes across an unfamiliar room while searching\nfor a toilet, it still knows to look elsewhere if the VLM reasons that, due to the presence of a bed, the\nroom is likely a bedroom (which is unlikely to contain toilets). Thus, even if the VLM cannot reason\nabout actions, our results indicate that PR2L provides a promising way of using its ability to reason\nabout image semantics and common sense for control. See Table 4 for CoT examples.\nWhile we do not beat VC-1’s reported SOTA BC performance (60.3% success rate when VC-1 is\nfrozen [43]), we note that said performance is achieved with (1) over ten times more training data and\ngradient steps and (2) image augmentations to prevent overfitting. Our VC-1 policies were trained on\nthe same amount of data as our PR2L agent and for 1-3× as many gradient steps, but perform far\nworse, suggesting that PR2L is significantly more sample- and compute-efficient than VC-1 policies.\nAdditionally, PR2L does not use any explicit countermeasures to overfitting, yet still generalizes well\nto unseen ObjectNav scenes (aided by the VLM’s representations of reasoning).\nFinally, we analyze policies trained with offline RL in a simplified Habitat setting in Appendices D,\nH, where we find that VLM representations align well with the returns of an optimal policy.\n6\nConclusion\nWe propose Promptable Representations for Reinforcement Learning, a method for extracting se-\nmantic features from images by prompting VLMs with task context to leverage their extensive\ngeneral-purpose prior knowledge. We demonstrate PR2L in Minecraft and Habitat, domains that\nbenefit from interpreting observations in terms of semantic concepts that can be related to task context.\nThis framework for using VLMs for control opens new directions. For example, other types of\nfoundation models pre-trained with more sophisticated methods could also be used for PR2L: e.g.,\nones trained on physical interactions might yield features which encode physics or action knowledge,\nrather than just common-sense visual semantics. Developing and using such models with PR2L offers\nan exciting way to transfer diverse prior knowledge to a broad range of control applications.\nA limitation of PR2L is that prompts are currently hand-crafted based on the user’s conception of\nuseful task features. While coming up with good prompts for our tasks was not hard, the process of\nevaluating and improving them could be automated, which we leave to future works. We also find that\nthe quality of representations largely depends on the VLM – e.g., InstructBLIP could not reason well\nabout Habitat scenes, but the more recent Prismatic VLMs are more capable in that regard, enabling\nour CoT experiments. Thus, as VLM capabilities are expected to increase, we expect the quality of\ntheir representations to also improve. Lastly, the size and speed of VLMs can limit their applicability.\nOur policies typically achieve 3-5 Hz inference speeds, comparable to those of robot policies built on\nlarge models [7, 6, 49]. Likewise, our VLM sizes are comparable to models used for policies in prior\nworks [6, 65]. While their inference speeds may hinder online policy learning, we find that offline\napproaches (which can parallelize training and data generation) we used for Habitat help remedy this.\n9\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 利用视觉语言模型为强化学习提供可提示的表示\n\n## 📌 背景痛点\/本文动机\n人类能够快速学习新行为，这得益于他们丰富的背景知识和推理能力。然而，传统的强化学习（RL）训练的智能体通常需要从头开始学习，缺乏利用背景知识的能力。为了解决这个问题，本文提出了一种新的方法，利用在互联网规模数据上预训练的视觉语言模型（VLM）来为具身RL提供丰富的世界知识。\n\n## 🚀 核心方法\n本文提出了一个名为“可提示表示的强化学习”（PR2L）的框架，通过向VLM提供任务相关的提示，使其生成包含语义信息的表示，并将其用于训练RL策略。具体来说，PR2L包含以下几个关键步骤：\n\n1. **提示设计**：设计任务相关的提示，引导VLM关注并编码图像中与任务相关的语义特征。\n2. **表示提取**：使用VLM对提示和图像进行编码，并提取中间层的表示作为状态表示。\n3. **策略训练**：使用RL算法训练策略，将提取的表示转换为低级动作。\n\n## 📈 实验结果\n本文在Minecraft和Habitat两个领域进行了实验，结果表明PR2L在以下方面取得了显著优势：\n\n* **性能提升**：与使用非提示图像嵌入或指令条件的方法相比，PR2L训练的策略在Minecraft和Habitat任务中取得了更好的性能。\n* **样本效率**：PR2L能够利用VLM的先验知识，从而减少训练所需的样本数量。\n* **泛化能力**：PR2L能够利用VLM的推理能力，使策略在新的场景中表现出更好的泛化能力。\n\n## 💬 可借鉴之处\nPR2L为利用VLM进行强化学习提供了一种新的思路，具有以下可借鉴之处：\n\n* **提示设计**：设计有效的提示是PR2L成功的关键，需要根据任务特点进行精心设计。\n* **表示选择**：选择合适的VLM层和表示方法，以提取对任务有用的语义信息。\n* **策略训练**：使用合适的RL算法和策略网络结构，以充分利用VLM提供的表示。\n\n## 🌟 未来展望\n随着VLM能力的不断提升，PR2L有望在更多领域得到应用，并为强化学习带来新的突破。","llm_summary_res_status":200}
{"title":"Reinforcement Learning Friendly Vision-Language Model for Minecraft","authors":"Haobin Jiang, Junpeng Yue, Hao Luo, Ziluo Ding, Zongqing Lu","summary":"One of the essential missions in the AI research community is to build an\nautonomous embodied agent that can achieve high-level performance across a wide\nspectrum of tasks. However, acquiring or manually designing rewards for all\nopen-ended tasks is unrealistic. In this paper, we propose a novel cross-modal\ncontrastive learning framework architecture, CLIP4MC, aiming to learn a\nreinforcement learning (RL) friendly vision-language model (VLM) that serves as\nan intrinsic reward function for open-ended tasks. Simply utilizing the\nsimilarity between the video snippet and the language prompt is not RL-friendly\nsince standard VLMs may only capture the similarity at a coarse level. To\nachieve RL-friendliness, we incorporate the task completion degree into the VLM\ntraining objective, as this information can assist agents in distinguishing the\nimportance between different states. Moreover, we provide neat YouTube datasets\nbased on the large-scale YouTube database provided by MineDojo. Specifically,\ntwo rounds of filtering operations guarantee that the dataset covers enough\nessential information and that the video-text pair is highly correlated.\nEmpirically, we demonstrate that the proposed method achieves better\nperformance on RL tasks compared with baselines. The code and datasets are\navailable at https:\/\/github.com\/PKU-RL\/CLIP4MC.","url":"http:\/\/arxiv.org\/abs\/2303.10571v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2303.10571v2","published":1679203252000,"comment":"ECCV 2024","pdf_text":"Reinforcement Learning Friendly\nVision-Language Model for Minecraft\nHaobin Jiang1⋆, Junpeng Yue1⋆, Hao Luo1 ,\nZiluo Ding2, and Zongqing Lu1,2†\n1 School of Computer Science, Peking University\n2 Beijing Academy of Artificial Intelligence\n{haobin.jiang,zongqing.lu}@pku.edu.cn\nAbstract. One of the essential missions in the AI research community\nis to build an autonomous embodied agent that can achieve high-level\nperformance across a wide spectrum of tasks. However, acquiring or\nmanually designing rewards for all open-ended tasks is unrealistic. In\nthis paper, we propose a novel cross-modal contrastive learning frame-\nwork architecture, CLIP4MC, aiming to learn a reinforcement learn-\ning (RL) friendly vision-language model (VLM) that serves as an in-\ntrinsic reward function for open-ended tasks. Simply utilizing the sim-\nilarity between the video snippet and the language prompt is not RL-\nfriendly since standard VLMs may only capture the similarity at a coarse\nlevel. To achieve RL-friendliness, we incorporate the task completion\ndegree into the VLM training objective, as this information can as-\nsist agents in distinguishing the importance between different states.\nMoreover, we provide neat YouTube datasets based on the large-scale\nYouTube database provided by MineDojo. Specifically, two rounds of\nfiltering operations guarantee that the dataset covers enough essential\ninformation and that the video-text pair is highly correlated. Empiri-\ncally, we demonstrate that the proposed method achieves better perfor-\nmance on RL tasks compared with baselines. The code and datasets are\navailable at https:\/\/github.com\/PKU-RL\/CLIP4MC.\nKeywords: Dataset · Multimodal model · Reinforcement learning\n1\nIntroduction\nTraining reinforcement learning (RL) agents to perform complex tasks in a\nvision-based and open-ended world can be difficult. One main challenge is that\nmanually specifying reward functions in all open-ended tasks is unrealistic [3,21],\nespecially when we cannot access the internal configuration of the environment.\nIn addition, learning a reward model from human feedback is typically expensive.\nTo this end, MineDojo [8] has proposed an internet-scale, multi-modal knowl-\nedge base of YouTube videos to facilitate learning in an open-ended world. With\nthe advent of a such large-scale database, agents are able to harvest practical\n⋆Equal contribution. † Corresponding author.\narXiv:2303.10571v2  [cs.LG]  5 Aug 2024\n2\nH. Jiang et al.\nknowledge encoded in large amounts of media like human beings. Moreover, a\nvision-language model (VLM), MineCLIP [8], is proposed to utilize the internet-\nscale domain knowledge. In more detail, the learned correlation score between\nthe visual observation and the language prompt can be used effectively as an\nopen-vocabulary, massively multi-task reward function for RL training. There-\nfore, no further task-specific reward design is needed for open-ended tasks.\nHowever, the autonomous embodied agent requires the vision-language model\nto provide a more instructive correlation score. Given the partial observations,\ne.g., video snippet, and the language prompt that describes the task, the agent\nneeds to figure out two non-trivial matters to better evaluate the current state.\nOn the one hand, whether the target entities are present within its field of vision?\nMineCLIP tried to address this question. However, the alignment of texts and\nvideos in the YouTube database is totally a catastrophe, which impedes the\nlearning of VLM. On the other hand, what is the relationship between each video\nsnippet and the degree of completion of the task? Normally, in an open-ended\nworld, e.g. Minecraft, the agent explores first, then approaches and interacts\nwith the target object. In other words, the agent requires approaching the target\nobject before it takes trial-and-error, even for the target that needs to be kept\naway. Therefore, it is reasonable to make an assumption, namely, that the higher\nthe level of completion of the task, the closer the agent is to the targets in the\nvideo snippet. We also argue it is important to incorporate the level of completion\nof the task into the reward function.\nIn this paper, we first construct neat YouTube datasets to facilitate the learn-\ning of basic game concepts, mainly the correspondence between the videos and\ntexts. Though a large-scale database is provided by MineDojo, it contains signif-\nicant noise due to its nature as an online resource. In addition, MineDojo only\nclaims the training dataset is randomly sampled from the database, making it\nhard to reproduce. To overcome the catastrophic misalignment of video-text\npairs in the original database, we have done four steps of dataset processing to\nguarantee the dataset is clean. Firstly, transcript cleaning enhances the accuracy\nof transcripts and ensures they are complete sentences. Secondly, keyword filter-\ning ensures that the content of clips is relevant to the key entities in Minecraft,\nthereby facilitating the learning of basic game concepts. Thirdly, video partition-\ning and selection can handle the issue of scene transitions and thereby mitigate\ninterference from other extraneous information. Lastly, correlation filtering can\neffectively address the issue of mismatch between video clips and transcripts.\nWe also propose an upgraded vision-language model, CLIP4MC, to provide a\nmore RL-friendly reward function. In RL, simply utilizing the similarity between\nthe video snippet and the language prompt is not RL-friendly since MineCLIP\ntends to only capture the similarity at the entity level. In other words, VLM\ncan hardly reflect the relationship between each video snippet and the degree of\ncompletion of the task. However, this information can better help agents distin-\nguish the importance between similar states. To achieve this, we incorporate the\ndegree of task completion into the VLM training objective. In more detail, CLIP\nhas exhibited remarkable segmentation capabilities without fine-tuning. After we\nCLIP4MC\n3\nextend pre-trained MineCLIP with modifications inspired by MaskCLIP [35], it\ncan segment the specified object from the image and label the size of the tar-\nget shown in the corresponding video. Intuitively, the closer the agent is to the\ntarget, the larger the target size becomes. During the learning procedure, we\ndynamically control the degree of contrasting positive and negative pairs of in-\nstances based on the target size in this positive video sample. Thus, CLIP4MC\ncan render a more RL-friendly reward signal that instructs the agent to learn\ntasks faster. Our proposed method is trained on our YouTube dataset and eval-\nuated on MineDojo Programmatic tasks, including harvest, hunt, and combat\ntasks. Empirically, our results show that CLIP4MC can provide a more friendly\nreward signal for the RL training procedure.\nTo summarize, our contributions are as follows:\n– Open-sourced datasets: We provide two high-quality datasets. The first\none undergoes data cleaning (Sections 4.1 to 4.3) and global-level correlation\nfiltering (Section 4.4). The VLM trained on this dataset matches the perfor-\nmance of the officially released MineCLIP, which lacks a publicly available\ntraining set.\n– RL-friendly dataset: Our second dataset further incorporates local-level\ncorrelation filtering (Section 4.4), making it more suited for RL. The VLM\ntrained on this dataset outperforms that on the first dataset.\n– RL-friendly VLM: To better evaluate and leverage the advantages of our\nmore RL-friendly dataset, we introduce CLIP4MC, a novel method to train\na VLM that could improve downstream RL performance (Section 5.2).\n2\nRelated Work\nVideo-Text Retrieval. Video-text retrieval plays an essential role in multi-\nmodal research and has been widely used in many real-world web applications.\nRecently, the pre-trained models have dominated this line of research with no-\nticeable results on both zero-shot and fine-tuned retrieval. Especially, BERT [5],\nViT [6], and CLIP [25], are used as the backbones to extract the text or video\nembedding. The cross-modal embeddings are then matched with specific fusion\nnetworks to find the correct video-text pair.\nIn more detail, CLIP4Clip [20] proposes three different similarity modules to\ncalculate the correlation between video and text embeddings. HiT [19] performs\nhierarchical matching at two different levels, i.e. semantic level and feature level.\nNote that semantic level and feature level features are from the transformer\nnetwork’s higher and lower feature layers, respectively. Frozen [1] proposes a dual\nencoder architecture that utilizes the flexibility of a transformer visual encoder\nto train from images or video clips with text captions. Moreover, MDMMT [7]\nadopts several pre-training models as encoders and it shows the CLIP-based\nmodel performs the best. Therefore, our model follows this line of research by\nusing the pre-trained model, CLIP [25], to extract the feature embeddings.\nMinecraft for AI Research. As an open-ended video game with an egocentric\nvision, Minecraft is a noticeable and important domain in RL due to the nature\n4\nH. Jiang et al.\nof the sparse reward, large exploration space, and long-term episodes. Since\nthe release of the Malmo simulator [14] and later the MineDojo simulator [8],\nvarious methods have attempted to train agents to complete tasks in Minecraft\n[10, 12, 18, 30, 31]. Approaches such as model-based RL, hierarchical RL, goal-\nbased RL, and reward shaping have been adopted to alleviate the sparse reward\nand exploration difficulty for the agent in this environment.\nRecently, with the development of large language models (LLM) like GPT-\n4 [24], a series of methods leveraging LLMs for high-level planning in Minecraft\nhave been proposed [16, 22, 32, 34, 36]. These methods have demonstrated re-\nmarkable capabilities in guiding the agent to complete multiple complicated,\nlong-horizon tasks, such as mining diamonds. These LLMs play a crucial role\nin decision-making, determining the sequence of basic skills required to accom-\nplish specific tasks. Their effectiveness is due to their extensive knowledge about\nMinecraft, learned from the Internet, and their ability to reflect on real-time\nfeedback from the game environment.\nIn addition to the use of LLMs, recent research attempts to incorporate Inter-\nnet visual data into basic skill learning in Minecraft, beyond the traditional RL\nmethods. MineRL [11] collected 60M player demonstrations with action labels,\nmotivating some methods [16,29] based on behavior cloning. As well-labeled data\nis limited in quantity, MineCLIP [8] instead uses over 730K narrated Minecraft\nvideos without action labels from YouTube. It aims to learn a vision-language\nmodel providing auxiliary reward signals, utilizing the vast and diverse data\navailable on the Internet. Different from MineCLIP, VPT [2] uses action-labeled\ndata to train an inverse dynamic model to label 70K hours of Internet videos\nand then conduct behavior cloning.\nUnlike the existing approaches, which incorporate the human experience and\nrequire a large number of demonstrations with action labels to train the agent,\nour work follows the line of MineCLIP [8] and focuses on only using the data\nwithout action labels to assist agent learning in Minecraft, which is more friendly\nwith data collection and has the potential to scale in the future.\n3\nBackground\nMineDojo tasks. MineDojo [8] provides thousands of benchmark tasks, which\ncan be used to develop generally capable agents in Minecraft. These tasks can be\ndivided into two categories, Programmatic and Creative tasks. The former has\nground-truth simulator states to assess whether the task has been completed.\nThe latter, however, do not have well-defined success criteria and tend to be\nmore open-ended, but have to be evaluated by humans.\nWe mainly focus on Programmatic tasks since they can be automatically\nassessed. Specifically, MineDojo provides 4 categories of programmatic tasks, in-\ncluding Harvest, Combat, Survival, and Tech Tree, with 1581 template-generated\nnatural language goals to evaluate the agent’s different capabilities. Among these\ntasks, Survival and Tech Tree tasks are harder than Harvest and Combat tasks.\nCurrently, MineCLIP [8] only expresses promising potential in some Harvest and\nCLIP4MC\n5\n... at some point you will lose there\ntoo or also hearts there spider was\nlike being completely blown away I\ndon 't assume anything at ...\n... so here I'm with nick and we've\ngot look at my inventory and his in-\nventory looks similar this is gonna\nbe interesting so four...\n...  give it  I'm a little chicken just\nlaying around by any chance a lot\nof arrows going on down here  I'm \nguessing most of  ...\n irrelavant video content\n mismatched video content\n matched video content\nFig. 1: Illustration of the YouTube video database. The screenshots of video clips\nare on the left and key entities are circled in red. The corresponding transcript clips\nare on the right and key entities are marked in red. We give examples of irrelevant,\nmismatched, and matched video content in the YouTube video database.\nCombat tasks. Harvest means finding, obtaining, cultivating, or manufacturing\nhundreds of materials and objects. Combat means fighting various monsters and\ncreatures that require fast reflexes and martial skills.\nPOMDP. We model the programmatic task as a partially observable Markov\ndecision process (POMDP) [15]. At each timestep t, the agent obtains the partial\nobservation ot from the global state s_t and a language prompt G, takes action\na_{t} following its policy \\pi (a_{t}|o_{t}), and receives a reward r_ { t} = \\Phi (V_{t},G), where Vt\nis the fixed-length sequence of observations till t (thus a video snippet) and \\Phi \nmaps V_t and G to a scalar value. Then the environment transitions to the next\nstate s_{t+1} given the current state and action according to transition probability\nfunction \\ protect \\ mathcal  {T}(s_{t+1}|s_t,a_t). The agent aims to maximize the expected return R =\n\\m\nat\nhbb  {E}_\\pi \\sum _{t=1}^{T}\\gamma ^{t-1}r_{t}, where \\gamma is the discount factor and T is the episode time horizon.\n4\nYouTube Dataset\nThe Internet is rich in Minecraft-related data, containing a wealth of weakly la-\nbeled or even unlabeled Minecraft knowledge, including crucial entities, plausible\nactions, and common-sense event processes. With these multi-modal data, it is\npossible to create dense language-conditioned rewards, making open-ended long-\nterm task learning feasible. MineDojo [8] collected over 730K YouTube videos\nand their corresponding transcripts, totaling 33 years of video content and 2.2B\nwords in transcripts. Around 640K video clips are selected using keyword-based\nfiltering from these videos, and these clips are used to train MineCLIP. However,\nMineDojo open-sourced a 13.8M dataset, and the 640K video clips were ran-\ndomly sampled in their paper. However, as illustrated in Figure 1, most videos\n6\nH. Jiang et al.\nfeature irrelevant game content that is not conducive to learning basic game\nconcepts. Meanwhile, the alignment between the transcripts and videos may not\nalways be precise, leading to temporal or content discrepancies that could hin-\nder the learning of retrieval and RL tasks. Given the low quality of the data, we\nfound it necessary to provide a neat 640k dataset to the community for the train-\ning usage. To address these issues, we adopt the following few steps to acquire\na clean dataset with high quality.\n4.1\nTranscript Cleaning\nDownloading YouTube’s automatic transcripts directly can lead to several issues.\nFirstly, transcript blocks may overlap, resulting in overlapping timestamps in the\ntranscript. Additionally, the caption quality is typically mediocre, with a higher\noccurrence of misidentifications, especially in non-English videos. Moreover, the\ntranscript lacks punctuation, making it less friendly for understanding semantics.\nBased on the aforementioned issues, we implement a pipeline to construct high-\nquality transcripts as follows: (1) Extract audio from the videos and use Whisper\n[26] to obtain high-quality, temporally non-overlapping transcripts. (2) Employ\nFullStop [9] to generate punctuation, resulting in complete sentences of 10-35\nwords in length.\n4.2\nKeyword Filtering\nFollowing MineDojo [8], we also implement keyword-based filtering to ensure\nthat the content in our dataset is pertinent to the key entities in Minecraft,\nthereby facilitating the learning of basic game concepts. As essential components\nof Minecraft, the key entities, such as stones, trees, and sheep, are common across\nmultiple tasks in MineDojo and videos from YouTube. Specifically, we identify\nentity keywords in the transcripts using a keyword list from MineDojo and ex-\ntract transcript clips formed into sentences to encompass as many keywords as\npossible. These extracted transcript clips then serve as the textual component\nof our dataset, determining the location of corresponding video clips.\n4.3\nVideo Partition and Selection\nAfter completing the previous two steps, we obtain transcript clips relevant to\nthe keywords. For each transcript clip, we calculate the central timestamp that\ncorresponds to the clip based on the transcript timestamps and then use this\ncentral timestamp to extract a video clip with a duration of D seconds from\nthe video. This process allows us to obtain temporally-aligned video clips. How-\never, the video clips obtained in this manner still exhibit some issues, including\nscene transitions and discrepancies between video content and transcripts. We\nhandle the former problem through video partition and filtering, while the latter\nproblem is addressed in Section 4.4.\nCLIP4MC\n7\nFig. 2: Examples of how we estimate the size of key entities in video frames. Red\nbounding boxes are generated by our modified MineCLIP visual encoder, following the\napproach proposed in MaskCLIP [35]. These boxes are then used to calculate the size.\nOwing to the informal nature of YouTube content, there is often a lack of\nsemantic congruence between the video clips and their corresponding transcrip-\ntions, as noted in VideoCLIP [33]. Moreover, video clips frequently contain a\nfew different behaviors which cause scene transitions, e.g. Chopping down the\ntree first, then suddenly switching the inventory bar. Since some scene transi-\ntions lead to irrelevant information, we partition the video content into several\nsemantically coherent segments based on the semantic structure of the video.\nThen we select the segment that aligns best with the transcript.\nTo achieve semantic partition of video content, we employ the Bellman k-\nsegmentation algorithm [13]. This algorithm divides a sequence of data points\ninto k distinct and constant-line segments, providing a piece-wise constant ap-\nproximation of the data. To process the video, we first use the officially re-\nleased MineCLIP [8] video encoder to obtain the embedding of each frame, since\nMineCLIP can capture video semantics to some extent. Subsequently, we parti-\ntion these embeddings into k segments and select the segment with the highest\nsimilarity score, as calculated in MineCLIP.\n4.4\nCorrelation Filtering\nWe employ correlation filtering techniques to address disparities that persist\nbetween video content and transcripts. The correlation filtering is done at two\nlevels, the global and the local levels. From the global level, we calculate the co-\nsine similarities between video embeddings and text embeddings via the original\nMineCLIP and then select clips based on these similarities.\nRecent research [17, 35] has demonstrated that CLIP [25], though trained\non whole images, can generate meaningful local features. Inspired by this and\nfollowing MaskCLIP [35], we make modifications to the original MineCLIP visual\nencoder, empowering it with the ability to estimate and label the size of the key\n8\nH. Jiang et al.\nentity, which is mentioned in the corresponding transcript, in each frame of a\nvideo clip without fine-tuning. Then the correlations between video clips and\ntranscripts are calculated as the summation of the sizes across all frames in each\nvideo clip. We consider this correlation at the local level.\nBased on these two criteria, we select the top k% of clips, resulting in the final\ntraining set. We provide an elaboration on our implementation in Appendix A\nand illustrate some examples in Figure 2.\nThe aforementioned four-step approach creates a dataset consisting of 640K\nvideo-text clip pairs, with an additional 4K pairs extracted for validation of\nvideo-text retrieval. Regarding the constants of the approach, D and k are set\nto 16 and 50, respectively. Therefore, our dataset comprises videos with a total\nduration of one week and approximately 0.16B words, significantly smaller in\nscale compared to the original low-quality 13.8M dataset. Importantly, we will\nopen-source our entire YouTube dataset, serving as an upgraded version of the\nunreleased MineCLIP training data.\n5\nCLIP4MC\nGiven a video snippet V and a language prompt G, the vision-language model\noutputs a correlation score, C, that measures the similarity between the video\nsnippet and the language prompt. Ideally, if the agent performs behaviors fol-\nlowing the description of the language prompt, the vision-language model will\ngenerate a higher correlation score, leading to a higher reward. Otherwise, the\nagent will be given a lower reward.\n5.1\nArchitecture\nWe follow the same architecture design as MineCLIP [8], including a video en-\ncoder, a text encoder, and a similarity calculator. All the video frames first go\nthrough the spatial transformer to obtain a sequence of frame features. The tem-\nporal transformer is then utilized to summarize the sequence of frame features\ninto a single video embedding. An adapter further processes the video embedding\nfor better features. Refer to Appendix B for more details about the architecture.\nEmpirically, we found that the video encoder (essentially MineCLIP) can\nprovide a bond between the entities and the language prompts and give a similar\nhigh reward as long as the target entities are present in the video frames, similar\nto observations in [4]. However, such a reward is not instructive enough for RL\ntasks since it does not reflect the behavioral trends of agents.\n5.2\nContrastive Training\nWe aim to minimize the sum of the multi-modal contrastive losses [23], including\nvideo-to-text, and text-to-video:\n  \\\nb\negin {aligne\nd\n} \\ mathcal  {L }  = & -\\sum  _{\n(\nz_v,z_m,z_t)\\in \\mathcal {B}}\\Big (\\log {\\rm NCE}(z_v,z_t)+\\log {\\rm NCE}(z_t,z_v) \\Big ), \\end {aligned} \n(1)\nCLIP4MC\n9\n░\nImage \nEncoder\nText\nEncoder\nv1\nt1\nv2\nv3\nvN\nt2\nt3\ntN\nv1·t1\nv2·t2\nv3·t3\nvN·tN\nv1·t2 v1·t3\nv1·tN\nv2·t3\nv2·t1\nv2·tN\nRandom Swap\nNo Swap\nRandom \nSwap\nRandomly choose a sample \nRandom Swap\nRandom Swap\nNo Swap\nRandom Swap\nswap\nmax ReLU 1(\n)\nb\nb\nH W\nP\nP\nHW\n\n=\n\n−\nswap\nP\nP\n\nswap\nP\nP\n\nv3·t1\nvN·t1\nv3·t2\nvN·t2\nv3·tN\nvN·t3\nPull\nPush\nText Emb.\nNeg. Video Emb.\nPos. Video Emb.\nPull\nPush\nText Emb.\nNeg. Video Emb.\nPos. Video Emb.\nPull\nPush\nText Emb.\nNeg. Video Emb.\nPos. Video Emb.\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\nVideo Clip (16 seconds)\nTranscript (10-35 words)\nFig. 3: Illustration of CLIP4MC training. The upper part shows the concept of con-\ntrastive learning, while the lower part explains the swapping operation.\nwhere \\protect \\mathcal  {B} is the batch containing sampled video-text pairs, and {\\rm N CE}(\\cdot ,\\cdot ) is the\ncontrastive loss that calculates the similarity of two inputs. To illustrate, the\nvideo-to-text contrastive loss is given by\n  {\\rm NCE }\n(z_v,z _ t)\n = \\\nf\nrac {\n\\ exp\n ( z_v \\c d ot z_t^{+}\/\\lambda )}{\\sum _{z \\in \\{z_t^{+},z_t^{-}\\}} \\exp (z_v \\cdot z\/\\lambda )}, \n(2)\nwhere \\lambda is a temperature hyperparameter, z_\nt^{+} \nis the positive text embedding\nmatching with the video embedding z_v, and \\if\nm mode \\lbrace \\else \\textbraceleft \\fi z_t^{-}\\} are negative text embeddings\nthat are implicitly formed by other text clips in the training batch. Contrastive\nloss of text-to-video is defined in the same way.\nIn addition, we also need to incorporate the degree of task completion into\nthe training objective. Specifically, we hope the similarity score between video\nclip and text embeddings could reflect the task completion degree, i.e., higher\ncompletion leads to higher similarity. The sizes of key entities provided by the\nlocal correlation filtering serve as a surrogate for the task completion degree.\nInitially, we try to dynamically adjust the weight of the positive pairs based on\nthe size of the target object. However, it did not work. Essentially, positive and\nnegative pairs will still be separated, even with smaller sample weights.\nTherefore, we instead forcibly change the labels of the positive and negative\nvideo samples, as illustrated in Figure 3, thereby reducing the model’s confidence\nin certain positive pairs. Specifically, during training, a positive video sample is\nswapped with a random negative video sample from the \\protect \\mathcal  {B} based on probability\n10\nH. Jiang et al.\nPswap. This swap only occurs when the labeled target size HbWb in the positive\nvideo sample is below a certain threshold τ, and the smaller the size, the greater\nthe probability,\n  P_{ \\ rm s w ap} \n=\n P _{\\r\nm m\na\nx} *{\\rm ReLU} \\left ( 1- \\frac {H_b W_b}{\\tau HW}\\right ), \\label {equ:clip4mc} \n(3)\nwhere HW represents the size of the image.\nSince contrastive learning brings positive samples closer and pushes nega-\ntive samples away, converting a positive pair to a negative one with a certain\nprobability, i.e., the aforementioned random swap, during training will lower\nthe similarity score. As the decrease in similarity is directly proportional to the\nswapping probability, we set Pswap to be inversely related to task completion,\nensuing similarity score increases as completion increases.\nIn addition, the upper limit Pmax in Equation (3) is set to 0.5, as the swap\nshould not disrupt severely the normal distinction between positive and negative\npairs in most cases. This constraint enables CLIP4MC to preserve the under-\nstanding capability of MineCLIP for general behaviors that may lack explicit\ntarget entities in Minecraft, as discussed in Appendix F.\n5.3\nRL Training\nFor RL training, the first step is reward generation. At timestep t, we concatenate\nthe agent’s latest 16 egocentric RGB frames in a temporal window to form a\nvideo snippet, V_t. CLIP4MC outputs the probability P_{G,t} that calculates the\nsimilarity of V_t to the task prompt, G, against all other negative prompts. To\ncompute the reward, we further process the raw probability as previous work\n[8]r_t = \\max ( P_{G,t} - 1 \/ N_t,0) r\n, where N_t is the number of prompts passed to\nCLIP4MC. Note that CLIP4MC can handle unseen language prompts without\nany further fine-tuning due to the open-vocabulary ability of CLIP [25].\nThe ultimate goal is to train a policy network that takes as input raw pixels\nand other structural data and outputs discrete actions to accomplish the task\nthat is described by the language prompt, G. We use PPO [28] as our RL training\nbackbone and the policy is trained on the CLIP4MC reward together with the\nsparse task reward if any. The policy input contains several modality-specific\ncomponents and more details can be found in Appendix D.\n6\nExperiments\nIn this section, we comprehensively evaluate and analyze our proposed model\nCLIP4MC, utilizing the open-ended platform MineDojo [8], which comprises\nthousands of diverse, open-ended Minecraft tasks designed for embodied agents.\nWe compare CLIP4MC against two baselines: (1) MineCLIP [official], the\nofficially released MineCLIP model [8]. (2) MineCLIP [ours], using the same\narchitecture as MineCLIP [official] but trained on our cleaned YouTube dataset.\nIt also serves as the ablation of CLIP4MC without the swap operation. We train\nCLIP4MC and MineCLIP [ours] for 20 epochs and select the models with the\nCLIP4MC\n11\nhighest performance on RL tasks. Please refer to Appendix C for more training\ndetails. All results are presented in terms of the mean and standard error of four\nruns with different random seeds.\n6.1\nEnvironment Settings\nWe conduct experiments on eight Programmatic tasks, comprising two harvest\ntasks: milk a cow and shear wool, two combat tasks: combat a spider and combat\na zombie, and four hunt tasks: hunt a cow, hunt a sheep, hunt a pig, and hunt a\nchicken. These tasks are all built-in tasks in the MineDojo benchmark.\nHarvest. Milk a cow requires the agent to obtain milk from a cow with an\nempty bucket. Similarly, shear wool requires the agent to obtain wool from a\nsheep with shears. A harvest task is terminated and considered completed when\nthe target item is obtained by the agent with a specified quantity. The prompts\nused to calculate the reward is “obtain milk from a cow in plains with an empty\nbucket”; for shear wool, it is “shear a sheep in plains with shears”.\nCombat. In these tasks, target animals, spiders, and zombies, are hostile and\nwill actively approach and attack the agent. The agent’s goal is to fight and kill\nthe target animal. The prompt for each combat task is “combat a spider\/zombie\nin plains with a diamond sword”.\nHunt. Hunt tasks consist of hunt a cow, hunt a sheep, hunt a pig, and hunt a\nsheep. For each task, the agent’s goal is to kill the target animal as indicated\nin the task name. Different from combat tasks, the target animals will flee from\nthe agent after being attacked. Therefore, these tasks require the agent to keep\nchasing and attacking the target, making them challenging. As noted in [4],\nthe original MineCLIP reward fails in these tasks since it cannot consistently\nincrease when the agent approaches the agent. This observation aligns with our\nassertion that the original MineCLIP model can hardly capture the degree of\ntask completion. The prompt for each task is “hunt a {target} in plains with a\ndiamond sword” where {target} is replaced with the corresponding animal name.\nMore elaborated introduction of the Minecraft environment and settings of\nthese tasks are available in Appendix D. To guarantee a fair comparison, we\nadopt the same RL hyperparameters for all models and tasks. These hyperpa-\nrameters are listed in Appendix E.\n6.2\nRL Results\nThrough our evaluation of CLIP4MC, MineCLIP [ours], and MineCLIP [official]\nacross eight Minecraft tasks, we want to answer two key questions:\n(1) Whether the YouTube dataset we constructed enables MineCLIP, when\ntrained on it, to provide a more effective reward signal for task learning?\n(2) Whether our upgraded model, CLIP4MC, based on our YouTube dataset,\noffers a reward signal that is further friendly for the RL training procedure?\n12\nH. Jiang et al.\nTable 1: Success rates (%) of RL trained with rewards provided by different models\non eight Minecraft tasks. Each mean and standard error of success rates are calculated\non four models after training 1e6 environment steps with different random seeds.\nModels\nHarvest\nCombat\nmilk a cow\nshear wool\ncombat a spider combat a zombie\nCLIP4MC\n84.5±2.0\n74.6±2.1\n85.8±0.9\n70.4±8.3\nMineCLIP[ours]\n84.4±1.1\n71.6±3.5\n75.4±10.1\n63.6±8.7\nMineCLIP[official]\n84.1±0.5\n73.2±1.8\n82.7±2.5\n57.4±3.7\nModels\nHunt\nhunt a cow\nhunt a sheep\nhunt a pig\nhunt a chicken\nCLIP4MC\n39.8±2.5\n45.9±7.2\n30.6±8.4\n26.1±3.5\nMineCLIP[ours]\n17.3±10.6\n33.0±18.1\n14.1±10.6\n15.3±10.6\nMineCLIP[official]\n11.6±11.1\n28.5±16.7\n1.5±0.6\n0.0±0.0\nThese questions are central to verifying the effectiveness of our dataset and\nCLIP4MC model in Minecraft. Table 1 shows the success rates of all methods\non the eight Minecraft tasks.\nIt is noticeable that, in four hunt tasks, three models demonstrate varying\nperformance on RL. Firstly, MineCLIP [ours] consistently achieves better results\ncompared to MineCLIP [official] across all hunt tasks. The superior performance\nof MineCLIP [ours] provides a positive answer to our first question, suggesting\nthat the YouTube dataset we construct indeed enhances the effectiveness of the\nreward signal in MineCLIP for task learning. Note that this is the dataset we\nplan to release for better training the VLM model for RL tasks on Minecraft.\nSecondly, CLIP4MC shows significantly higher success rates on hunt tasks com-\npared to both MineCLIP [ours] and MineCLIP [official], meaning that the answer\nto the second question is also positive. As CLIP4MC provides a reward signal\ntaking into account a surrogate for the degree of task completion, i.e., the size of\nthe target object in our implementation, it becomes more RL-friendly in these\nchallenging tasks.\nIn addition, we notice a practical example of misalignment in the officially\nreleased MineCLIP model. Specifically, we observe that in hunt a chicken, the\nagent trained with MineCLIP [official] tends to keep looking at the sky, indicat-\ning such behavior can provide a high intrinsic reward. This phenomenon suggests\nthat the officially released MineCLIP indeed suffers from the misalignment prob-\nlem in the YouTube dataset. In contrast, our MineCLIP [official] shows promising\nbehaviors on this task, demonstrating that our dataset processing improves the\nalignment between the transcripts and videos.\nIn contrast to hunt tasks, these methods perform comparably on harvest\ntasks and combat tasks, while CLIP4MC still shows marginal advantage over the\nCLIP4MC\n13\n4\n3\n2\n1\nln(size)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n(a) CLIP4MC\n4\n3\n2\n1\nln(size)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n(b) MineCLIP [ours]\n4\n3\n2\n1\nln(size)\n0.0\n0.1\n0.2\n0.3\n0.4\nReward\n(c) MineCLIP [official]\nFig. 4: Scatter plots illustrating the relationship between the entity size and the in-\ntrinsic reward. The red line indicates a linear fit to the data.\nother two methods. This finding aligns with the results reported in [8], where the\nMineCLIP reward already achieves saturated performance on these easy tasks.\nTherefore, further enhancements in the reward signal, like MineCLIP [ours] and\nCLIP4MC, do not significantly improve performance. Unlike hunt tasks, neither\nharvest nor combat tasks require the agent to take multiple rounds of chasing.\nThis result does not detract from the superiority of CLIP4MC, evidenced by its\nperformance on hunt tasks, which are considered more difficult [2].\n6.3\nReward Analysis\nTo quantitatively verify that CLIP4MC captures the size of the target entity\nspecified in the language prompt, we collect 5000 steps in task hunt a cow and\napply the method described in Appendix A to estimate the maximal size of the\ncow in consecutive 16 frames. Then we use CLIP4MC, MineCLIP [ours], and\nMineCLIP [official] to calculate intrinsic rewards respectively. Before calculating\nthe correlation between the size and intrinsic rewards, we transform the size\nvalue using f(x) = ln (x + e−2), focusing on smaller values. The relationship\nbetween the transformed size and intrinsic rewards is visualized in Figure 4. The\ncorresponding Pearson correlation coefficients from left to right are 0.81, 0.66,\nand 0.62, indicating that CLIP4MC reward has a higher correlation with the\nsize, especially when it is relatively small. This is crucial in RL, as the agent\nneeds dense and distinguishing reward signals to guide the learning process,\nparticularly when the target is distant. Such characteristic is the essential benefit\nof CLIP4MC in RL.\n6.4\nAblation on Dataset Filtering\nTo evaluate the influence of the dataset quality on RL training and verify the\neffectiveness of our proposed correlation filter at the local level, we compare\nMineCLIP [ours] with two ablations: (1) MineCLIP [w\/o LCF] is trained\non the dataset processed with only global correlation filtering, omitting the lo-\ncal correlation filtering. (2) MineCLIP [RS] is trained on 640K video clips\nrandomly sampled from the MineDojo released database, consistent with their\nstated dataset construction method [8]. Note that methods evaluated here do not\napply random swap introduced in Section 5.2 since these datasets cannot provide\n14\nH. Jiang et al.\nTable 2: Success rates (%) of RL trained with rewards provided by different models on\nthree Minecraft tasks. Each mean and standard error of success rates are calculated on\nfour models after training 5e5 and 1e6 environment steps with different random seeds.\nModels\nmilk a cow\nshear wool\nhunt a cow\n5e5\n1e6\n5e5\n1e6\n5e5\n1e6\nours\n73.1±0.9\n84.4±1.1\n47.5±6.3\n71.6±3.5\n15.6±8.7\n17.3±10.6\nw\/o LCF\n71.3±1.8\n80.7±1.1\n53.8±3.3\n73.6±1.5\n0.7±0.7\n12.1±12.0\nRS\n65.1±3.9\n81.7±1.0\n21.7±6.5\n68.5±3.6\n1.0±0.5\n4.0±2.6\nofficial\n69.8±1.3\n84.1±0.5\n47.0±13.5\n73.2±1.8\n2.7±1.1\n11.6±11.1\nentity size. The results are presented in Table 2. Given the overall performance\nacross all three presented tasks, firstly, [RS] does not perfectly reproduce the\nperformance of [official], suggesting that misalignment in the original database\nhinders the reproduction of [official]. Secondly, part of our proposed data filter-\ning makes the model, [w\/o LCF], comparable to [official]. In addition, with all\nof our proposed data filtering techniques, [ours] outperforms [official].\n6.5\nVideo-Text Retrieval Results\nTable 3: Results of video-to-text \/ text-to-video re-\ntrieval on the test set. The best results are high-\nlighted in bold.\nModels\nR@1\nR@5\nR@10\nours\n12.4\/13.1\n27.5\/27.8\n35.3\/35.9\nw\/o LCF 12.7\/14.1 29.4\/29.7 37.9\/37.5\nRS\n11.1\/11.6\n24.8\/25.0\n32.4\/32.2\nTable 3 shows the results\nof video-to-text retrieval and\ntext-to-video on the test set\nwith\nthe\nsame\nMineCLIP\nmodel for a fair comparison.\nWe train these models for 20\nepochs and select the models\nwith the highest R@1 value\non the test set, respectively.\nFrom the results, the model\ntrained on the randomly sam-\npled dataset gets the worst\nperformance than those trained on our YouTube dataset. The results demon-\nstrate that our neat dataset indeed can facilitate the learning of basic game\nconcepts. A notable observation is that [ours] achieves higher success rates on\nRL tasks but lower performance on retrieval tasks compared to [w\/o LCF], sug-\ngesting that the video-text alignment objective does not fully align with RL\nrequirements. [w\/o LCF] is trained on the dataset filtered only by global-level\ncorrelation, which is directly conducted at the video level. In contrast, [ours] is\ntrained on the dataset further filtered by local-level correlation, which is object-\ncentric designing for RL training. Note that [w\/o LCF] is the dataset we plan\nto release for better training the VLM model for video-text retrieval tasks on\nMinecraft.\nCLIP4MC\n15\n7\nConclusion\nWe construct a neat YouTube dataset based on the large-scale YouTube database\nprovided by MineDojo. Moreover, we introduce a novel cross-modal contrastive\nlearning framework architecture, CLIP4MC, to learn an RL-friendly VLM that\nserves as an intrinsic reward function for open-ended tasks. Our findings suggest\nthat our dataset enhances the acquisition of fundamental game concepts and\nCLIP4MC delivers a more effective reward signal for RL training.\nAcknowledgements\nThis work was supported by NSFC under grant 62250068. The authors would\nlike to thank the anonymous reviewers for their valuable comments.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Reinforcement Learning Friendly Vision-Language Model for Minecraft.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nReinforcement Learning Friendly Vision-Language Model for Minecraft\n```\n#### 2. 论文摘要\n```\nOne of the essential missions in the AI research community is to build an\nautonomous embodied agent that can achieve high-level performance across a wide\nspectrum of tasks. However, acquiring or manually designing rewards for all\nopen-ended tasks is unrealistic. In this paper, we propose a novel cross-modal\ncontrastive learning framework architecture, CLIP4MC, aiming to learn a\nreinforcement learning (RL) friendly vision-language model (VLM) that serves as\nan intrinsic reward function for open-ended tasks. Simply utilizing the\nsimilarity between the video snippet and the language prompt is not RL-friendly\nsince standard VLMs may only capture the similarity at a coarse level. To\nachieve RL-friendliness, we incorporate the task completion degree into the VLM\ntraining objective, as this information can assist agents in distinguishing the\nimportance between different states. Moreover, we provide neat YouTube datasets\nbased on the large-scale YouTube database provided by MineDojo. Specifically,\ntwo rounds of filtering operations guarantee that the dataset covers enough\nessential information and that the video-text pair is highly correlated.\nEmpirically, we demonstrate that the proposed method achieves better\nperformance on RL tasks compared with baselines. The code and datasets are\navailable at https:\/\/github.com\/PKU-RL\/CLIP4MC.\n```\n\n#### 3. 论文全文\n```\nReinforcement Learning Friendly\nVision-Language Model for Minecraft\nHaobin Jiang1⋆, Junpeng Yue1⋆, Hao Luo1 ,\nZiluo Ding2, and Zongqing Lu1,2†\n1 School of Computer Science, Peking University\n2 Beijing Academy of Artificial Intelligence\n{haobin.jiang,zongqing.lu}@pku.edu.cn\nAbstract. One of the essential missions in the AI research community\nis to build an autonomous embodied agent that can achieve high-level\nperformance across a wide spectrum of tasks. However, acquiring or\nmanually designing rewards for all open-ended tasks is unrealistic. In\nthis paper, we propose a novel cross-modal contrastive learning frame-\nwork architecture, CLIP4MC, aiming to learn a reinforcement learn-\ning (RL) friendly vision-language model (VLM) that serves as an in-\ntrinsic reward function for open-ended tasks. Simply utilizing the sim-\nilarity between the video snippet and the language prompt is not RL-\nfriendly since standard VLMs may only capture the similarity at a coarse\nlevel. To achieve RL-friendliness, we incorporate the task completion\ndegree into the VLM training objective, as this information can as-\nsist agents in distinguishing the importance between different states.\nMoreover, we provide neat YouTube datasets based on the large-scale\nYouTube database provided by MineDojo. Specifically, two rounds of\nfiltering operations guarantee that the dataset covers enough essential\ninformation and that the video-text pair is highly correlated. Empiri-\ncally, we demonstrate that the proposed method achieves better perfor-\nmance on RL tasks compared with baselines. The code and datasets are\navailable at https:\/\/github.com\/PKU-RL\/CLIP4MC.\nKeywords: Dataset · Multimodal model · Reinforcement learning\n1\nIntroduction\nTraining reinforcement learning (RL) agents to perform complex tasks in a\nvision-based and open-ended world can be difficult. One main challenge is that\nmanually specifying reward functions in all open-ended tasks is unrealistic [3,21],\nespecially when we cannot access the internal configuration of the environment.\nIn addition, learning a reward model from human feedback is typically expensive.\nTo this end, MineDojo [8] has proposed an internet-scale, multi-modal knowl-\nedge base of YouTube videos to facilitate learning in an open-ended world. With\nthe advent of a such large-scale database, agents are able to harvest practical\n⋆Equal contribution. † Corresponding author.\narXiv:2303.10571v2  [cs.LG]  5 Aug 2024\n2\nH. Jiang et al.\nknowledge encoded in large amounts of media like human beings. Moreover, a\nvision-language model (VLM), MineCLIP [8], is proposed to utilize the internet-\nscale domain knowledge. In more detail, the learned correlation score between\nthe visual observation and the language prompt can be used effectively as an\nopen-vocabulary, massively multi-task reward function for RL training. There-\nfore, no further task-specific reward design is needed for open-ended tasks.\nHowever, the autonomous embodied agent requires the vision-language model\nto provide a more instructive correlation score. Given the partial observations,\ne.g., video snippet, and the language prompt that describes the task, the agent\nneeds to figure out two non-trivial matters to better evaluate the current state.\nOn the one hand, whether the target entities are present within its field of vision?\nMineCLIP tried to address this question. However, the alignment of texts and\nvideos in the YouTube database is totally a catastrophe, which impedes the\nlearning of VLM. On the other hand, what is the relationship between each video\nsnippet and the degree of completion of the task? Normally, in an open-ended\nworld, e.g. Minecraft, the agent explores first, then approaches and interacts\nwith the target object. In other words, the agent requires approaching the target\nobject before it takes trial-and-error, even for the target that needs to be kept\naway. Therefore, it is reasonable to make an assumption, namely, that the higher\nthe level of completion of the task, the closer the agent is to the targets in the\nvideo snippet. We also argue it is important to incorporate the level of completion\nof the task into the reward function.\nIn this paper, we first construct neat YouTube datasets to facilitate the learn-\ning of basic game concepts, mainly the correspondence between the videos and\ntexts. Though a large-scale database is provided by MineDojo, it contains signif-\nicant noise due to its nature as an online resource. In addition, MineDojo only\nclaims the training dataset is randomly sampled from the database, making it\nhard to reproduce. To overcome the catastrophic misalignment of video-text\npairs in the original database, we have done four steps of dataset processing to\nguarantee the dataset is clean. Firstly, transcript cleaning enhances the accuracy\nof transcripts and ensures they are complete sentences. Secondly, keyword filter-\ning ensures that the content of clips is relevant to the key entities in Minecraft,\nthereby facilitating the learning of basic game concepts. Thirdly, video partition-\ning and selection can handle the issue of scene transitions and thereby mitigate\ninterference from other extraneous information. Lastly, correlation filtering can\neffectively address the issue of mismatch between video clips and transcripts.\nWe also propose an upgraded vision-language model, CLIP4MC, to provide a\nmore RL-friendly reward function. In RL, simply utilizing the similarity between\nthe video snippet and the language prompt is not RL-friendly since MineCLIP\ntends to only capture the similarity at the entity level. In other words, VLM\ncan hardly reflect the relationship between each video snippet and the degree of\ncompletion of the task. However, this information can better help agents distin-\nguish the importance between similar states. To achieve this, we incorporate the\ndegree of task completion into the VLM training objective. In more detail, CLIP\nhas exhibited remarkable segmentation capabilities without fine-tuning. After we\nCLIP4MC\n3\nextend pre-trained MineCLIP with modifications inspired by MaskCLIP [35], it\ncan segment the specified object from the image and label the size of the tar-\nget shown in the corresponding video. Intuitively, the closer the agent is to the\ntarget, the larger the target size becomes. During the learning procedure, we\ndynamically control the degree of contrasting positive and negative pairs of in-\nstances based on the target size in this positive video sample. Thus, CLIP4MC\ncan render a more RL-friendly reward signal that instructs the agent to learn\ntasks faster. Our proposed method is trained on our YouTube dataset and eval-\nuated on MineDojo Programmatic tasks, including harvest, hunt, and combat\ntasks. Empirically, our results show that CLIP4MC can provide a more friendly\nreward signal for the RL training procedure.\nTo summarize, our contributions are as follows:\n– Open-sourced datasets: We provide two high-quality datasets. The first\none undergoes data cleaning (Sections 4.1 to 4.3) and global-level correlation\nfiltering (Section 4.4). The VLM trained on this dataset matches the perfor-\nmance of the officially released MineCLIP, which lacks a publicly available\ntraining set.\n– RL-friendly dataset: Our second dataset further incorporates local-level\ncorrelation filtering (Section 4.4), making it more suited for RL. The VLM\ntrained on this dataset outperforms that on the first dataset.\n– RL-friendly VLM: To better evaluate and leverage the advantages of our\nmore RL-friendly dataset, we introduce CLIP4MC, a novel method to train\na VLM that could improve downstream RL performance (Section 5.2).\n2\nRelated Work\nVideo-Text Retrieval. Video-text retrieval plays an essential role in multi-\nmodal research and has been widely used in many real-world web applications.\nRecently, the pre-trained models have dominated this line of research with no-\nticeable results on both zero-shot and fine-tuned retrieval. Especially, BERT [5],\nViT [6], and CLIP [25], are used as the backbones to extract the text or video\nembedding. The cross-modal embeddings are then matched with specific fusion\nnetworks to find the correct video-text pair.\nIn more detail, CLIP4Clip [20] proposes three different similarity modules to\ncalculate the correlation between video and text embeddings. HiT [19] performs\nhierarchical matching at two different levels, i.e. semantic level and feature level.\nNote that semantic level and feature level features are from the transformer\nnetwork’s higher and lower feature layers, respectively. Frozen [1] proposes a dual\nencoder architecture that utilizes the flexibility of a transformer visual encoder\nto train from images or video clips with text captions. Moreover, MDMMT [7]\nadopts several pre-training models as encoders and it shows the CLIP-based\nmodel performs the best. Therefore, our model follows this line of research by\nusing the pre-trained model, CLIP [25], to extract the feature embeddings.\nMinecraft for AI Research. As an open-ended video game with an egocentric\nvision, Minecraft is a noticeable and important domain in RL due to the nature\n4\nH. Jiang et al.\nof the sparse reward, large exploration space, and long-term episodes. Since\nthe release of the Malmo simulator [14] and later the MineDojo simulator [8],\nvarious methods have attempted to train agents to complete tasks in Minecraft\n[10, 12, 18, 30, 31]. Approaches such as model-based RL, hierarchical RL, goal-\nbased RL, and reward shaping have been adopted to alleviate the sparse reward\nand exploration difficulty for the agent in this environment.\nRecently, with the development of large language models (LLM) like GPT-\n4 [24], a series of methods leveraging LLMs for high-level planning in Minecraft\nhave been proposed [16, 22, 32, 34, 36]. These methods have demonstrated re-\nmarkable capabilities in guiding the agent to complete multiple complicated,\nlong-horizon tasks, such as mining diamonds. These LLMs play a crucial role\nin decision-making, determining the sequence of basic skills required to accom-\nplish specific tasks. Their effectiveness is due to their extensive knowledge about\nMinecraft, learned from the Internet, and their ability to reflect on real-time\nfeedback from the game environment.\nIn addition to the use of LLMs, recent research attempts to incorporate Inter-\nnet visual data into basic skill learning in Minecraft, beyond the traditional RL\nmethods. MineRL [11] collected 60M player demonstrations with action labels,\nmotivating some methods [16,29] based on behavior cloning. As well-labeled data\nis limited in quantity, MineCLIP [8] instead uses over 730K narrated Minecraft\nvideos without action labels from YouTube. It aims to learn a vision-language\nmodel providing auxiliary reward signals, utilizing the vast and diverse data\navailable on the Internet. Different from MineCLIP, VPT [2] uses action-labeled\ndata to train an inverse dynamic model to label 70K hours of Internet videos\nand then conduct behavior cloning.\nUnlike the existing approaches, which incorporate the human experience and\nrequire a large number of demonstrations with action labels to train the agent,\nour work follows the line of MineCLIP [8] and focuses on only using the data\nwithout action labels to assist agent learning in Minecraft, which is more friendly\nwith data collection and has the potential to scale in the future.\n3\nBackground\nMineDojo tasks. MineDojo [8] provides thousands of benchmark tasks, which\ncan be used to develop generally capable agents in Minecraft. These tasks can be\ndivided into two categories, Programmatic and Creative tasks. The former has\nground-truth simulator states to assess whether the task has been completed.\nThe latter, however, do not have well-defined success criteria and tend to be\nmore open-ended, but have to be evaluated by humans.\nWe mainly focus on Programmatic tasks since they can be automatically\nassessed. Specifically, MineDojo provides 4 categories of programmatic tasks, in-\ncluding Harvest, Combat, Survival, and Tech Tree, with 1581 template-generated\nnatural language goals to evaluate the agent’s different capabilities. Among these\ntasks, Survival and Tech Tree tasks are harder than Harvest and Combat tasks.\nCurrently, MineCLIP [8] only expresses promising potential in some Harvest and\nCLIP4MC\n5\n... at some point you will lose there\ntoo or also hearts there spider was\nlike being completely blown away I\ndon 't assume anything at ...\n... so here I'm with nick and we've\ngot look at my inventory and his in-\nventory looks similar this is gonna\nbe interesting so four...\n...  give it  I'm a little chicken just\nlaying around by any chance a lot\nof arrows going on down here  I'm \nguessing most of  ...\n irrelavant video content\n mismatched video content\n matched video content\nFig. 1: Illustration of the YouTube video database. The screenshots of video clips\nare on the left and key entities are circled in red. The corresponding transcript clips\nare on the right and key entities are marked in red. We give examples of irrelevant,\nmismatched, and matched video content in the YouTube video database.\nCombat tasks. Harvest means finding, obtaining, cultivating, or manufacturing\nhundreds of materials and objects. Combat means fighting various monsters and\ncreatures that require fast reflexes and martial skills.\nPOMDP. We model the programmatic task as a partially observable Markov\ndecision process (POMDP) [15]. At each timestep t, the agent obtains the partial\nobservation ot from the global state s_t and a language prompt G, takes action\na_{t} following its policy \\pi (a_{t}|o_{t}), and receives a reward r_ { t} = \\Phi (V_{t},G), where Vt\nis the fixed-length sequence of observations till t (thus a video snippet) and \\Phi \nmaps V_t and G to a scalar value. Then the environment transitions to the next\nstate s_{t+1} given the current state and action according to transition probability\nfunction \\ protect \\ mathcal  {T}(s_{t+1}|s_t,a_t). The agent aims to maximize the expected return R =\n\\m\nat\nhbb  {E}_\\pi \\sum _{t=1}^{T}\\gamma ^{t-1}r_{t}, where \\gamma is the discount factor and T is the episode time horizon.\n4\nYouTube Dataset\nThe Internet is rich in Minecraft-related data, containing a wealth of weakly la-\nbeled or even unlabeled Minecraft knowledge, including crucial entities, plausible\nactions, and common-sense event processes. With these multi-modal data, it is\npossible to create dense language-conditioned rewards, making open-ended long-\nterm task learning feasible. MineDojo [8] collected over 730K YouTube videos\nand their corresponding transcripts, totaling 33 years of video content and 2.2B\nwords in transcripts. Around 640K video clips are selected using keyword-based\nfiltering from these videos, and these clips are used to train MineCLIP. However,\nMineDojo open-sourced a 13.8M dataset, and the 640K video clips were ran-\ndomly sampled in their paper. However, as illustrated in Figure 1, most videos\n6\nH. Jiang et al.\nfeature irrelevant game content that is not conducive to learning basic game\nconcepts. Meanwhile, the alignment between the transcripts and videos may not\nalways be precise, leading to temporal or content discrepancies that could hin-\nder the learning of retrieval and RL tasks. Given the low quality of the data, we\nfound it necessary to provide a neat 640k dataset to the community for the train-\ning usage. To address these issues, we adopt the following few steps to acquire\na clean dataset with high quality.\n4.1\nTranscript Cleaning\nDownloading YouTube’s automatic transcripts directly can lead to several issues.\nFirstly, transcript blocks may overlap, resulting in overlapping timestamps in the\ntranscript. Additionally, the caption quality is typically mediocre, with a higher\noccurrence of misidentifications, especially in non-English videos. Moreover, the\ntranscript lacks punctuation, making it less friendly for understanding semantics.\nBased on the aforementioned issues, we implement a pipeline to construct high-\nquality transcripts as follows: (1) Extract audio from the videos and use Whisper\n[26] to obtain high-quality, temporally non-overlapping transcripts. (2) Employ\nFullStop [9] to generate punctuation, resulting in complete sentences of 10-35\nwords in length.\n4.2\nKeyword Filtering\nFollowing MineDojo [8], we also implement keyword-based filtering to ensure\nthat the content in our dataset is pertinent to the key entities in Minecraft,\nthereby facilitating the learning of basic game concepts. As essential components\nof Minecraft, the key entities, such as stones, trees, and sheep, are common across\nmultiple tasks in MineDojo and videos from YouTube. Specifically, we identify\nentity keywords in the transcripts using a keyword list from MineDojo and ex-\ntract transcript clips formed into sentences to encompass as many keywords as\npossible. These extracted transcript clips then serve as the textual component\nof our dataset, determining the location of corresponding video clips.\n4.3\nVideo Partition and Selection\nAfter completing the previous two steps, we obtain transcript clips relevant to\nthe keywords. For each transcript clip, we calculate the central timestamp that\ncorresponds to the clip based on the transcript timestamps and then use this\ncentral timestamp to extract a video clip with a duration of D seconds from\nthe video. This process allows us to obtain temporally-aligned video clips. How-\never, the video clips obtained in this manner still exhibit some issues, including\nscene transitions and discrepancies between video content and transcripts. We\nhandle the former problem through video partition and filtering, while the latter\nproblem is addressed in Section 4.4.\nCLIP4MC\n7\nFig. 2: Examples of how we estimate the size of key entities in video frames. Red\nbounding boxes are generated by our modified MineCLIP visual encoder, following the\napproach proposed in MaskCLIP [35]. These boxes are then used to calculate the size.\nOwing to the informal nature of YouTube content, there is often a lack of\nsemantic congruence between the video clips and their corresponding transcrip-\ntions, as noted in VideoCLIP [33]. Moreover, video clips frequently contain a\nfew different behaviors which cause scene transitions, e.g. Chopping down the\ntree first, then suddenly switching the inventory bar. Since some scene transi-\ntions lead to irrelevant information, we partition the video content into several\nsemantically coherent segments based on the semantic structure of the video.\nThen we select the segment that aligns best with the transcript.\nTo achieve semantic partition of video content, we employ the Bellman k-\nsegmentation algorithm [13]. This algorithm divides a sequence of data points\ninto k distinct and constant-line segments, providing a piece-wise constant ap-\nproximation of the data. To process the video, we first use the officially re-\nleased MineCLIP [8] video encoder to obtain the embedding of each frame, since\nMineCLIP can capture video semantics to some extent. Subsequently, we parti-\ntion these embeddings into k segments and select the segment with the highest\nsimilarity score, as calculated in MineCLIP.\n4.4\nCorrelation Filtering\nWe employ correlation filtering techniques to address disparities that persist\nbetween video content and transcripts. The correlation filtering is done at two\nlevels, the global and the local levels. From the global level, we calculate the co-\nsine similarities between video embeddings and text embeddings via the original\nMineCLIP and then select clips based on these similarities.\nRecent research [17, 35] has demonstrated that CLIP [25], though trained\non whole images, can generate meaningful local features. Inspired by this and\nfollowing MaskCLIP [35], we make modifications to the original MineCLIP visual\nencoder, empowering it with the ability to estimate and label the size of the key\n8\nH. Jiang et al.\nentity, which is mentioned in the corresponding transcript, in each frame of a\nvideo clip without fine-tuning. Then the correlations between video clips and\ntranscripts are calculated as the summation of the sizes across all frames in each\nvideo clip. We consider this correlation at the local level.\nBased on these two criteria, we select the top k% of clips, resulting in the final\ntraining set. We provide an elaboration on our implementation in Appendix A\nand illustrate some examples in Figure 2.\nThe aforementioned four-step approach creates a dataset consisting of 640K\nvideo-text clip pairs, with an additional 4K pairs extracted for validation of\nvideo-text retrieval. Regarding the constants of the approach, D and k are set\nto 16 and 50, respectively. Therefore, our dataset comprises videos with a total\nduration of one week and approximately 0.16B words, significantly smaller in\nscale compared to the original low-quality 13.8M dataset. Importantly, we will\nopen-source our entire YouTube dataset, serving as an upgraded version of the\nunreleased MineCLIP training data.\n5\nCLIP4MC\nGiven a video snippet V and a language prompt G, the vision-language model\noutputs a correlation score, C, that measures the similarity between the video\nsnippet and the language prompt. Ideally, if the agent performs behaviors fol-\nlowing the description of the language prompt, the vision-language model will\ngenerate a higher correlation score, leading to a higher reward. Otherwise, the\nagent will be given a lower reward.\n5.1\nArchitecture\nWe follow the same architecture design as MineCLIP [8], including a video en-\ncoder, a text encoder, and a similarity calculator. All the video frames first go\nthrough the spatial transformer to obtain a sequence of frame features. The tem-\nporal transformer is then utilized to summarize the sequence of frame features\ninto a single video embedding. An adapter further processes the video embedding\nfor better features. Refer to Appendix B for more details about the architecture.\nEmpirically, we found that the video encoder (essentially MineCLIP) can\nprovide a bond between the entities and the language prompts and give a similar\nhigh reward as long as the target entities are present in the video frames, similar\nto observations in [4]. However, such a reward is not instructive enough for RL\ntasks since it does not reflect the behavioral trends of agents.\n5.2\nContrastive Training\nWe aim to minimize the sum of the multi-modal contrastive losses [23], including\nvideo-to-text, and text-to-video:\n  \\\nb\negin {aligne\nd\n} \\ mathcal  {L }  = & -\\sum  _{\n(\nz_v,z_m,z_t)\\in \\mathcal {B}}\\Big (\\log {\\rm NCE}(z_v,z_t)+\\log {\\rm NCE}(z_t,z_v) \\Big ), \\end {aligned} \n(1)\nCLIP4MC\n9\n░\nImage \nEncoder\nText\nEncoder\nv1\nt1\nv2\nv3\nvN\nt2\nt3\ntN\nv1·t1\nv2·t2\nv3·t3\nvN·tN\nv1·t2 v1·t3\nv1·tN\nv2·t3\nv2·t1\nv2·tN\nRandom Swap\nNo Swap\nRandom \nSwap\nRandomly choose a sample \nRandom Swap\nRandom Swap\nNo Swap\nRandom Swap\nswap\nmax ReLU 1(\n)\nb\nb\nH W\nP\nP\nHW\n\n=\n\n−\nswap\nP\nP\n\nswap\nP\nP\n\nv3·t1\nvN·t1\nv3·t2\nvN·t2\nv3·tN\nvN·t3\nPull\nPush\nText Emb.\nNeg. Video Emb.\nPos. Video Emb.\nPull\nPush\nText Emb.\nNeg. Video Emb.\nPos. Video Emb.\nPull\nPush\nText Emb.\nNeg. Video Emb.\nPos. Video Emb.\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\nVideo Clip (16 seconds)\nTranscript (10-35 words)\nFig. 3: Illustration of CLIP4MC training. The upper part shows the concept of con-\ntrastive learning, while the lower part explains the swapping operation.\nwhere \\protect \\mathcal  {B} is the batch containing sampled video-text pairs, and {\\rm N CE}(\\cdot ,\\cdot ) is the\ncontrastive loss that calculates the similarity of two inputs. To illustrate, the\nvideo-to-text contrastive loss is given by\n  {\\rm NCE }\n(z_v,z _ t)\n = \\\nf\nrac {\n\\ exp\n ( z_v \\c d ot z_t^{+}\/\\lambda )}{\\sum _{z \\in \\{z_t^{+},z_t^{-}\\}} \\exp (z_v \\cdot z\/\\lambda )}, \n(2)\nwhere \\lambda is a temperature hyperparameter, z_\nt^{+} \nis the positive text embedding\nmatching with the video embedding z_v, and \\if\nm mode \\lbrace \\else \\textbraceleft \\fi z_t^{-}\\} are negative text embeddings\nthat are implicitly formed by other text clips in the training batch. Contrastive\nloss of text-to-video is defined in the same way.\nIn addition, we also need to incorporate the degree of task completion into\nthe training objective. Specifically, we hope the similarity score between video\nclip and text embeddings could reflect the task completion degree, i.e., higher\ncompletion leads to higher similarity. The sizes of key entities provided by the\nlocal correlation filtering serve as a surrogate for the task completion degree.\nInitially, we try to dynamically adjust the weight of the positive pairs based on\nthe size of the target object. However, it did not work. Essentially, positive and\nnegative pairs will still be separated, even with smaller sample weights.\nTherefore, we instead forcibly change the labels of the positive and negative\nvideo samples, as illustrated in Figure 3, thereby reducing the model’s confidence\nin certain positive pairs. Specifically, during training, a positive video sample is\nswapped with a random negative video sample from the \\protect \\mathcal  {B} based on probability\n10\nH. Jiang et al.\nPswap. This swap only occurs when the labeled target size HbWb in the positive\nvideo sample is below a certain threshold τ, and the smaller the size, the greater\nthe probability,\n  P_{ \\ rm s w ap} \n=\n P _{\\r\nm m\na\nx} *{\\rm ReLU} \\left ( 1- \\frac {H_b W_b}{\\tau HW}\\right ), \\label {equ:clip4mc} \n(3)\nwhere HW represents the size of the image.\nSince contrastive learning brings positive samples closer and pushes nega-\ntive samples away, converting a positive pair to a negative one with a certain\nprobability, i.e., the aforementioned random swap, during training will lower\nthe similarity score. As the decrease in similarity is directly proportional to the\nswapping probability, we set Pswap to be inversely related to task completion,\nensuing similarity score increases as completion increases.\nIn addition, the upper limit Pmax in Equation (3) is set to 0.5, as the swap\nshould not disrupt severely the normal distinction between positive and negative\npairs in most cases. This constraint enables CLIP4MC to preserve the under-\nstanding capability of MineCLIP for general behaviors that may lack explicit\ntarget entities in Minecraft, as discussed in Appendix F.\n5.3\nRL Training\nFor RL training, the first step is reward generation. At timestep t, we concatenate\nthe agent’s latest 16 egocentric RGB frames in a temporal window to form a\nvideo snippet, V_t. CLIP4MC outputs the probability P_{G,t} that calculates the\nsimilarity of V_t to the task prompt, G, against all other negative prompts. To\ncompute the reward, we further process the raw probability as previous work\n[8]r_t = \\max ( P_{G,t} - 1 \/ N_t,0) r\n, where N_t is the number of prompts passed to\nCLIP4MC. Note that CLIP4MC can handle unseen language prompts without\nany further fine-tuning due to the open-vocabulary ability of CLIP [25].\nThe ultimate goal is to train a policy network that takes as input raw pixels\nand other structural data and outputs discrete actions to accomplish the task\nthat is described by the language prompt, G. We use PPO [28] as our RL training\nbackbone and the policy is trained on the CLIP4MC reward together with the\nsparse task reward if any. The policy input contains several modality-specific\ncomponents and more details can be found in Appendix D.\n6\nExperiments\nIn this section, we comprehensively evaluate and analyze our proposed model\nCLIP4MC, utilizing the open-ended platform MineDojo [8], which comprises\nthousands of diverse, open-ended Minecraft tasks designed for embodied agents.\nWe compare CLIP4MC against two baselines: (1) MineCLIP [official], the\nofficially released MineCLIP model [8]. (2) MineCLIP [ours], using the same\narchitecture as MineCLIP [official] but trained on our cleaned YouTube dataset.\nIt also serves as the ablation of CLIP4MC without the swap operation. We train\nCLIP4MC and MineCLIP [ours] for 20 epochs and select the models with the\nCLIP4MC\n11\nhighest performance on RL tasks. Please refer to Appendix C for more training\ndetails. All results are presented in terms of the mean and standard error of four\nruns with different random seeds.\n6.1\nEnvironment Settings\nWe conduct experiments on eight Programmatic tasks, comprising two harvest\ntasks: milk a cow and shear wool, two combat tasks: combat a spider and combat\na zombie, and four hunt tasks: hunt a cow, hunt a sheep, hunt a pig, and hunt a\nchicken. These tasks are all built-in tasks in the MineDojo benchmark.\nHarvest. Milk a cow requires the agent to obtain milk from a cow with an\nempty bucket. Similarly, shear wool requires the agent to obtain wool from a\nsheep with shears. A harvest task is terminated and considered completed when\nthe target item is obtained by the agent with a specified quantity. The prompts\nused to calculate the reward is “obtain milk from a cow in plains with an empty\nbucket”; for shear wool, it is “shear a sheep in plains with shears”.\nCombat. In these tasks, target animals, spiders, and zombies, are hostile and\nwill actively approach and attack the agent. The agent’s goal is to fight and kill\nthe target animal. The prompt for each combat task is “combat a spider\/zombie\nin plains with a diamond sword”.\nHunt. Hunt tasks consist of hunt a cow, hunt a sheep, hunt a pig, and hunt a\nsheep. For each task, the agent’s goal is to kill the target animal as indicated\nin the task name. Different from combat tasks, the target animals will flee from\nthe agent after being attacked. Therefore, these tasks require the agent to keep\nchasing and attacking the target, making them challenging. As noted in [4],\nthe original MineCLIP reward fails in these tasks since it cannot consistently\nincrease when the agent approaches the agent. This observation aligns with our\nassertion that the original MineCLIP model can hardly capture the degree of\ntask completion. The prompt for each task is “hunt a {target} in plains with a\ndiamond sword” where {target} is replaced with the corresponding animal name.\nMore elaborated introduction of the Minecraft environment and settings of\nthese tasks are available in Appendix D. To guarantee a fair comparison, we\nadopt the same RL hyperparameters for all models and tasks. These hyperpa-\nrameters are listed in Appendix E.\n6.2\nRL Results\nThrough our evaluation of CLIP4MC, MineCLIP [ours], and MineCLIP [official]\nacross eight Minecraft tasks, we want to answer two key questions:\n(1) Whether the YouTube dataset we constructed enables MineCLIP, when\ntrained on it, to provide a more effective reward signal for task learning?\n(2) Whether our upgraded model, CLIP4MC, based on our YouTube dataset,\noffers a reward signal that is further friendly for the RL training procedure?\n12\nH. Jiang et al.\nTable 1: Success rates (%) of RL trained with rewards provided by different models\non eight Minecraft tasks. Each mean and standard error of success rates are calculated\non four models after training 1e6 environment steps with different random seeds.\nModels\nHarvest\nCombat\nmilk a cow\nshear wool\ncombat a spider combat a zombie\nCLIP4MC\n84.5±2.0\n74.6±2.1\n85.8±0.9\n70.4±8.3\nMineCLIP[ours]\n84.4±1.1\n71.6±3.5\n75.4±10.1\n63.6±8.7\nMineCLIP[official]\n84.1±0.5\n73.2±1.8\n82.7±2.5\n57.4±3.7\nModels\nHunt\nhunt a cow\nhunt a sheep\nhunt a pig\nhunt a chicken\nCLIP4MC\n39.8±2.5\n45.9±7.2\n30.6±8.4\n26.1±3.5\nMineCLIP[ours]\n17.3±10.6\n33.0±18.1\n14.1±10.6\n15.3±10.6\nMineCLIP[official]\n11.6±11.1\n28.5±16.7\n1.5±0.6\n0.0±0.0\nThese questions are central to verifying the effectiveness of our dataset and\nCLIP4MC model in Minecraft. Table 1 shows the success rates of all methods\non the eight Minecraft tasks.\nIt is noticeable that, in four hunt tasks, three models demonstrate varying\nperformance on RL. Firstly, MineCLIP [ours] consistently achieves better results\ncompared to MineCLIP [official] across all hunt tasks. The superior performance\nof MineCLIP [ours] provides a positive answer to our first question, suggesting\nthat the YouTube dataset we construct indeed enhances the effectiveness of the\nreward signal in MineCLIP for task learning. Note that this is the dataset we\nplan to release for better training the VLM model for RL tasks on Minecraft.\nSecondly, CLIP4MC shows significantly higher success rates on hunt tasks com-\npared to both MineCLIP [ours] and MineCLIP [official], meaning that the answer\nto the second question is also positive. As CLIP4MC provides a reward signal\ntaking into account a surrogate for the degree of task completion, i.e., the size of\nthe target object in our implementation, it becomes more RL-friendly in these\nchallenging tasks.\nIn addition, we notice a practical example of misalignment in the officially\nreleased MineCLIP model. Specifically, we observe that in hunt a chicken, the\nagent trained with MineCLIP [official] tends to keep looking at the sky, indicat-\ning such behavior can provide a high intrinsic reward. This phenomenon suggests\nthat the officially released MineCLIP indeed suffers from the misalignment prob-\nlem in the YouTube dataset. In contrast, our MineCLIP [official] shows promising\nbehaviors on this task, demonstrating that our dataset processing improves the\nalignment between the transcripts and videos.\nIn contrast to hunt tasks, these methods perform comparably on harvest\ntasks and combat tasks, while CLIP4MC still shows marginal advantage over the\nCLIP4MC\n13\n4\n3\n2\n1\nln(size)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n(a) CLIP4MC\n4\n3\n2\n1\nln(size)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n(b) MineCLIP [ours]\n4\n3\n2\n1\nln(size)\n0.0\n0.1\n0.2\n0.3\n0.4\nReward\n(c) MineCLIP [official]\nFig. 4: Scatter plots illustrating the relationship between the entity size and the in-\ntrinsic reward. The red line indicates a linear fit to the data.\nother two methods. This finding aligns with the results reported in [8], where the\nMineCLIP reward already achieves saturated performance on these easy tasks.\nTherefore, further enhancements in the reward signal, like MineCLIP [ours] and\nCLIP4MC, do not significantly improve performance. Unlike hunt tasks, neither\nharvest nor combat tasks require the agent to take multiple rounds of chasing.\nThis result does not detract from the superiority of CLIP4MC, evidenced by its\nperformance on hunt tasks, which are considered more difficult [2].\n6.3\nReward Analysis\nTo quantitatively verify that CLIP4MC captures the size of the target entity\nspecified in the language prompt, we collect 5000 steps in task hunt a cow and\napply the method described in Appendix A to estimate the maximal size of the\ncow in consecutive 16 frames. Then we use CLIP4MC, MineCLIP [ours], and\nMineCLIP [official] to calculate intrinsic rewards respectively. Before calculating\nthe correlation between the size and intrinsic rewards, we transform the size\nvalue using f(x) = ln (x + e−2), focusing on smaller values. The relationship\nbetween the transformed size and intrinsic rewards is visualized in Figure 4. The\ncorresponding Pearson correlation coefficients from left to right are 0.81, 0.66,\nand 0.62, indicating that CLIP4MC reward has a higher correlation with the\nsize, especially when it is relatively small. This is crucial in RL, as the agent\nneeds dense and distinguishing reward signals to guide the learning process,\nparticularly when the target is distant. Such characteristic is the essential benefit\nof CLIP4MC in RL.\n6.4\nAblation on Dataset Filtering\nTo evaluate the influence of the dataset quality on RL training and verify the\neffectiveness of our proposed correlation filter at the local level, we compare\nMineCLIP [ours] with two ablations: (1) MineCLIP [w\/o LCF] is trained\non the dataset processed with only global correlation filtering, omitting the lo-\ncal correlation filtering. (2) MineCLIP [RS] is trained on 640K video clips\nrandomly sampled from the MineDojo released database, consistent with their\nstated dataset construction method [8]. Note that methods evaluated here do not\napply random swap introduced in Section 5.2 since these datasets cannot provide\n14\nH. Jiang et al.\nTable 2: Success rates (%) of RL trained with rewards provided by different models on\nthree Minecraft tasks. Each mean and standard error of success rates are calculated on\nfour models after training 5e5 and 1e6 environment steps with different random seeds.\nModels\nmilk a cow\nshear wool\nhunt a cow\n5e5\n1e6\n5e5\n1e6\n5e5\n1e6\nours\n73.1±0.9\n84.4±1.1\n47.5±6.3\n71.6±3.5\n15.6±8.7\n17.3±10.6\nw\/o LCF\n71.3±1.8\n80.7±1.1\n53.8±3.3\n73.6±1.5\n0.7±0.7\n12.1±12.0\nRS\n65.1±3.9\n81.7±1.0\n21.7±6.5\n68.5±3.6\n1.0±0.5\n4.0±2.6\nofficial\n69.8±1.3\n84.1±0.5\n47.0±13.5\n73.2±1.8\n2.7±1.1\n11.6±11.1\nentity size. The results are presented in Table 2. Given the overall performance\nacross all three presented tasks, firstly, [RS] does not perfectly reproduce the\nperformance of [official], suggesting that misalignment in the original database\nhinders the reproduction of [official]. Secondly, part of our proposed data filter-\ning makes the model, [w\/o LCF], comparable to [official]. In addition, with all\nof our proposed data filtering techniques, [ours] outperforms [official].\n6.5\nVideo-Text Retrieval Results\nTable 3: Results of video-to-text \/ text-to-video re-\ntrieval on the test set. The best results are high-\nlighted in bold.\nModels\nR@1\nR@5\nR@10\nours\n12.4\/13.1\n27.5\/27.8\n35.3\/35.9\nw\/o LCF 12.7\/14.1 29.4\/29.7 37.9\/37.5\nRS\n11.1\/11.6\n24.8\/25.0\n32.4\/32.2\nTable 3 shows the results\nof video-to-text retrieval and\ntext-to-video on the test set\nwith\nthe\nsame\nMineCLIP\nmodel for a fair comparison.\nWe train these models for 20\nepochs and select the models\nwith the highest R@1 value\non the test set, respectively.\nFrom the results, the model\ntrained on the randomly sam-\npled dataset gets the worst\nperformance than those trained on our YouTube dataset. The results demon-\nstrate that our neat dataset indeed can facilitate the learning of basic game\nconcepts. A notable observation is that [ours] achieves higher success rates on\nRL tasks but lower performance on retrieval tasks compared to [w\/o LCF], sug-\ngesting that the video-text alignment objective does not fully align with RL\nrequirements. [w\/o LCF] is trained on the dataset filtered only by global-level\ncorrelation, which is directly conducted at the video level. In contrast, [ours] is\ntrained on the dataset further filtered by local-level correlation, which is object-\ncentric designing for RL training. Note that [w\/o LCF] is the dataset we plan\nto release for better training the VLM model for video-text retrieval tasks on\nMinecraft.\nCLIP4MC\n15\n7\nConclusion\nWe construct a neat YouTube dataset based on the large-scale YouTube database\nprovided by MineDojo. Moreover, we introduce a novel cross-modal contrastive\nlearning framework architecture, CLIP4MC, to learn an RL-friendly VLM that\nserves as an intrinsic reward function for open-ended tasks. Our findings suggest\nthat our dataset enhances the acquisition of fundamental game concepts and\nCLIP4MC delivers a more effective reward signal for RL training.\nAcknowledgements\nThis work was supported by NSFC under grant 62250068. The authors would\nlike to thank the anonymous reviewers for their valuable comments.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 基于CLIP4MC的强化学习友好型视觉语言模型在Minecraft中的应用\n\n## 📌 背景痛点\/本文动机\n在人工智能研究领域，构建一个能够在各种任务中实现高级性能的自主具身智能体是核心目标之一。然而，为所有开放性任务手动获取或设计奖励函数是不切实际的。为了解决这个问题，本文提出了一种新的跨模态对比学习框架架构CLIP4MC，旨在学习一个强化学习（RL）友好的视觉语言模型（VLM），该模型可以作为开放性任务的内在奖励函数。\n\n## 🚀 核心方法\n💡 创新点1：构建了两个高质量的数据集\n- 第一个数据集经过数据清洗和全局级相关性过滤，训练出的VLM性能与官方发布的MineCLIP相当。\n- 第二个数据集进一步结合了局部级相关性过滤，更适合RL任务，训练出的VLM性能优于第一个数据集。\n\n💡 创新点2：提出CLIP4MC模型\n- CLIP4MC模型通过将任务完成程度纳入VLM训练目标，使模型能够更好地反映每个视频片段与任务完成程度之间的关系，从而为RL训练过程提供更友好的奖励信号。\n- CLIP4MC模型在MineDojo的编程任务上取得了更好的性能，证明了其在RL任务中的有效性。\n\n## 📈 实验结果\n- 在MineDojo的编程任务中，CLIP4MC模型在狩猎任务上取得了显著更高的成功率，证明了其在RL任务中的有效性。\n- 通过相关性分析，CLIP4MC模型的奖励信号与目标实体的尺寸具有更高的相关性，这对于RL训练过程至关重要。\n\n## 💬 可借鉴之处\n- 本文提出的CLIP4MC模型和构建的数据集为RL任务中的奖励函数设计提供了新的思路和方法。\n- 本文的研究结果表明，将任务完成程度纳入VLM训练目标可以有效地提高RL训练性能。\n- 本文的研究成果可以应用于其他开放性任务中的RL训练，例如自动驾驶、机器人控制等。","llm_summary_res_status":200}
{"title":"ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting","authors":"Shaofei Cai, Zihao Wang, Kewei Lian, Zhancun Mu, Xiaojian Ma, Anji Liu, Yitao Liang","summary":"Vision-language models (VLMs) have excelled in multimodal tasks, but adapting\nthem to embodied decision-making in open-world environments presents\nchallenges. One critical issue is bridging the gap between discrete entities in\nlow-level observations and the abstract concepts required for effective\nplanning. A common solution is building hierarchical agents, where VLMs serve\nas high-level reasoners that break down tasks into executable sub-tasks,\ntypically specified using language. However, language suffers from the\ninability to communicate detailed spatial information. We propose\nvisual-temporal context prompting, a novel communication protocol between VLMs\nand policy models. This protocol leverages object segmentation from past\nobservations to guide policy-environment interactions. Using this approach, we\ntrain ROCKET-1, a low-level policy that predicts actions based on concatenated\nvisual observations and segmentation masks, supported by real-time object\ntracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to\ntackle complex tasks that demand spatial reasoning. Experiments in Minecraft\nshow that our approach enables agents to achieve previously unattainable tasks,\nwith a $\\mathbf{76}\\%$ absolute improvement in open-world interaction\nperformance. Codes and demos are now available on the project page:\nhttps:\/\/craftjarvis.github.io\/ROCKET-1.","url":"http:\/\/arxiv.org\/abs\/2410.17856v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.17856v3","published":1729690019000,"comment":null,"pdf_text":"ROCKET-1: Mastering Open-World Interaction\nwith Visual-Temporal Context Prompting\nShaofei Cai1, Zihao Wang1, Kewei Lian1, Zhancun Mu1, Xiaojian Ma3, Anji Liu2 and Yitao Liang B1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis\nVision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied\ndecision-making in open-world environments presents challenges. One critical issue is bridging the\ngap between discrete entities in low-level observations and the abstract concepts required for effective\nplanning. A common solution is building hierarchical agents, where VLMs serve as high-level reasoners\nthat break down tasks into executable sub-tasks, typically specified using language. However, language\nsuffers from the inability to communicate detailed spatial information. We propose visual-temporal\ncontext prompting, a novel communication protocol between VLMs and policy models. This protocol\nleverages object segmentation from past observations to guide policy-environment interactions. Using\nthis approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual\nobservations and segmentation masks, supported by real-time object tracking from SAM-2. Our method\nunlocks the potential of VLMs, enabling them to tackle complex tasks that demand spatial reasoning.\nExperiments in Minecraft show that our approach enables agents to achieve previously unattainable\ntasks, with a 76% absolute improvement in open-world interaction performance. Codes and demos are\nnow available on the project page: https:\/\/craftjarvis.github.io\/ROCKET-1.\nFigure 1 | Our pipeline solves creative tasks, such as get the obsidian in the original Minecraft version, using the\naction space identical to human players (mouse and keyboard). We present a novel instruction interface, visual-\ntemporal context prompting, under which we learn a spatial-sensitive policy, ROCKET-1. VLMs identify regions\nof interest within each observation and guide ROCKET-1 interactions. Different colors in the segmentation\nrepresent distinct interaction types, for example,\n- use,\n- approach,\n- switch,\n- mine block.\n1. Introduction\nPre-trained foundation vision-language models\n(VLMs) (Achiam et al., 2023; Team et al., 2023)\nhave shown impressive performance in reason-\ning, visual question answering, and task planning\n(Brohan et al., 2023; Cheng et al., 2024; Driess\net al., 2023; Wang et al., 2023b), primarily due\nto training on internet-scale multimodal data. Re-\ncently, there has been growing interest in trans-\nCorresponding author(s): Yitao Liang\nShaofei Cai <caishaofei@stu.pku.edu.cn>, Zihao Wang <zhwang@stu.pku.edu.cn>, Kewei Lian <lkwkwl@stu.pku.edu.cn>, Zhancun Mu\n<muzhancun@stu.pku.edu.cn>, Xiaojian Ma <xiaojian.ma@ucla.edu>, Anji Liu <liuanji@cs.ucla.edu>, Yitao Liang <yitaol@pku.edu.cn>\narXiv:2410.17856v3  [cs.CV]  20 Mar 2025\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 2 | Different pipelines in solving embodied decision-making tasks. (a) End-to-end pipeline modeling\ntoken sequences of language, observations, and actions. (b) Language prompting: VLMs decompose instructions\nfor language-conditioned policy execution. (c) Latent prompting: maps discrete behavior tokens to low-level\nactions. (d) Future-image prompting: fine-tunes VLMs and diffusion models for image-conditioned control. (e)\nVisual-temporal prompting: VLMs generate segmentations and interaction cues to guide ROCKET-1.\nferring these capabilities to embodied decision-\nmaking in open-world environments. Existing\napproaches can be broadly categorized into (i)\nend-to-end and (ii) hierarchical approaches. End-\nto-end approaches, such as RT-2 (Brohan et al.,\n2023), Octo (Octo Model Team et al., 2024), LEO\n(Huang et al., 2023), and OpenVLA (Stone et al.,\n2023), aim to enable VLMs to interact with envi-\nronments by collecting robot manipulation trajec-\ntory data annotated with text. This data is then\ntokenized to fine-tune VLMs into vision-language-\naction models (VLAs) in an end-to-end manner,\nas illustrated in Figure 2(a). However, collecting\nsuch annotated trajectory data is difficult to scale.\nMoreover, introducing the action modality risks\ncompromising the foundational abilities of VLMs.\nHierarchical agent architectures typically con-\nsist of a high-level reasoner and a low-level pol-\nicy, which can be trained independently. In this\narchitecture, the “communication protocol” be-\ntween components defines the capability lim-\nits of the agent. Alternative approaches (Driess\net al., 2023; Wang et al., 2023a,b) leverage VLMs’\nreasoning abilities to zero-shot decompose tasks\ninto language-based sub-tasks, with a separate\nlanguage-conditioned policy executing them in\nthe environment, refer to Figure 2(b). However,\nlanguage instructions often fail to effectively con-\nvey spatial information, limiting the tasks agents\ncan solve. For example, when multiple homony-\nmous objects appear in an observation image, dis-\ntinguishing a specific one using language alone\nmay require extensive spatial descriptors, increas-\ning data collection complexity and learning dif-\nficulty for the language-conditioned policy. To\naddress this issue, approaches like STEVE-1 (Lif-\nshitz et al., 2023), GROOT-1 (Cai et al., 2023b),\nand MineDreamer (Zhou et al., 2024) propose us-\ning a purely vision-based interface to convey task\ninformation to the low-level policy. MineDreamer,\nin particular, uses hindsight relabeling to train an\nimage-conditioned policy (Lifshitz et al., 2023)\nfor interaction, while jointly fine-tuning VLMs\nand diffusion models to generate goal images\nthat guide the policy, shown in Figure 2(d). Al-\nthough replacing language with imagined images\nas the task interface simplifies data collection and\npolicy learning, predicting future observations re-\nquires building a world model, which still faces\nchallenges such as hallucinations, temporal incon-\nsistencies, and limited temporal scope.\nIn human task execution, such as object grasp-\ning, people do not pre-imagine holding an object\nbut maintain focus on the target object while\napproaching its affordance. When the object is\nobscured, humans rely on memory to recall its lo-\ncation and connect past and present visual scenes.\nThis use of visual-temporal context enables hu-\nmans to solve tasks effectively in novel environ-\nments. Building on this idea, we propose a novel\ncommunication protocol called visual-temporal\ncontext prompting, as shown in Figure 2(e). This\nallows users\/reasoners to apply object segmenta-\ntion to highlight regions of interest in past visual\n2\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nobservations and convey interaction-type cues via\na set of skill primitives. Based on this, we learn\nROCKET-1, a low-level policy that uses visual\nobservations and reasoner-provided segmenta-\ntions as task prompts to predict actions causally.\nSpecifically, a transformer (Dai et al., 2019) mod-\nels dependencies between observations, essen-\ntial for representing tasks in partially observable\nenvironments. As a bonus feature, ROCKET-1\ncan enhance its object-tracking capabilities dur-\ning inference by integrating the state-of-the-art\nvideo segmentation model, SAM-2 (Ravi et al.,\n2024), in a plug-and-play fashion. Additionally,\nwe propose a backward trajectory relabeling\nmethod, which efficiently generates segmenta-\ntion annotations in reverse temporal order us-\ning SAM-2, facilitating the creation of training\ndatasets for ROCKET-1. Finally, we develop a\nhierarchical agent architecture leveraging visual-\ntemporal context prompting, which perfectly in-\nherits the vision-language reasoning capabilities\nof foundational VLMs. Experiments in Minecraft\ndemonstrate that our pipeline enables agents to\ncomplete tasks previously unattainable by other\nmethods, while the hierarchical architecture ef-\nfectively solves long-horizon tasks.\nOur main contributions are threefold: (1) We\npresent visual-temporal context prompting, a\nnovel protocol that effectively communicates spa-\ntial and interaction cues in hierarchical agent ar-\nchitecture. (2) We learn ROCKET-1, the first\nsegmentation-conditioned policy in Minecraft, ca-\npable of interacting with nearly all the objects.\n(3) We develop backward trajectory relabel-\ning method that can automatically detect and\nsegment desired objects in collected trajectories\nwith pre-trained SAMs for training ROCKET-1.\n2. Preliminaries\nOffline Reinforcement Learning. We model the\nopen-world interaction problem as a Markov De-\ncision Process (MDP) ⟨O, A, P, C, M, R⟩, where\nO and A represent the observation and action\nspaces, P : O×A×O →ℝ+ describes the environ-\nment dynamics, C is the set of interaction types,\nand M is the segmentation mask space. The bi-\nnary reward function R : O×A ×C×M →{0, 1}\ndetermines whether the policy has completed the\nspecified interaction with the object indicated by\nthe segmentation mask at each time step. The\nobjective of reinforcement learning is to learn a\npolicy that maximizes the expected cumulative\nreward, 𝔼\n\u0002Í𝑇\n𝑡=1 𝑟𝑡\n\u0003\n, where 𝑟𝑡is the reward at time\nstep 𝑡. Our proposed backward trajectory relabel-\ning method ensures that each trajectory attains a\npositive reward based on current object segmen-\ntations. This allows us to discard the rewards and\nlearn a conditioned policy 𝜋(𝑎|𝑜, 𝑐, 𝑚) directly us-\ning behavior cloning. In the offline setting, agents\ndo not interact with the environment but rely on\na fixed, limited dataset of trajectories. This set-\nting is harder as it removes the ability to explore\nthe environment and gather additional feedback.\nVision Language Models. Vision-Language Mod-\nels (VLMs) are machine learning models capable\nof processing both image and language modal-\nities. Recent advances in generative pretrain-\ning have led to the emergence of conversational\nmodels like Gemini (Team et al., 2023), GPT-4o\n(Achiam et al., 2023), and Molmo (Deitke et al.,\n2024), which are trained on large-scale multi-\nmodal data and can reason and generate human-\nlike responses based on text and images. Models\nsuch as Palm-E (Driess et al., 2023) have demon-\nstrated strong abilities in embodied question-\nanswering and task planning. However, stan-\ndalone VLMs cannot often interact directly with\nenvironments. Some approaches use VLMs to\ngenerate language instructions for driving low-\nlevel controllers, but these methods struggle with\nexpressing spatial information. This work focuses\non releasing VLMs’ spatial understanding in em-\nbodied decision-making scenarios. Molmo can\naccurately identify correlated objects in images\nusing a list of (𝑥, 𝑦) coordinates, as demonstrated\nin https:\/\/molmo.allenai.org.\nSegment Anything Models. The Segment Any-\nthing Model (SAM, Kirillov et al. (2023)), intro-\nduced by Meta, is a segmentation model capa-\nble of interactively segmenting objects based on\npoint or bounding box prompts, or segmenting\nall objects in an image at once. It demonstrates\nimpressive zero-shot generalization in both real-\nworld and video game environments. Recently,\nMeta introduced SAM-2 (Ravi et al., 2024), ex-\n3\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 3 | ROCKET-1 architecture. ROCKET-1 processes observations (𝑜), object segmentations (𝑚), and\ninteraction types (𝑐) to predict actions (𝑎) using a causal transformer. Observations and segmentations are\nconcatenated and passed through a visual backbone for deep fusion. Interaction types and segmentations are\nrandomly dropped with a pre-defiened probability during training.\ntending segmentation to the temporal domain.\nWith SAM-2, users can prompt object segmen-\ntation with points or bounding boxes on a sin-\ngle video frame, and the model will track the\nobject forward or backward in time, refer to\nhttps:\/\/ai.meta.com\/sam2.\nRemarkably,\nSAM-2 continues tracking even if the object disap-\npears and reappears, making it well-suited for par-\ntially observable open-world environments. In ad-\ndition, we find the SAM models can be equipped\nwith a text prompt module, enabling them to\nground text-based concepts in visual images, as\nseen in grounded SAM (Liu et al., 2023).\n3. Methods\nOverview. Our work focuses on addressing com-\nplex interactive tasks in open-world environ-\nments like Minecraft. We leverage VLMs’ visual-\nlanguage reasoning capabilities to decompose\ntasks into multiple steps and determine object in-\nteractions based on environmental observations.\nFor example, the “build nether portal” task re-\nquires a sequence of block placements at spe-\ncific locations. A controller is also needed to\nmap these steps into low-level actions. To con-\nvey spatial information accurately, we propose a\nvisual-temporal context prompting protocol and\na low-level policy, ROCKET-1. Pretrained VLMs\nprocess a sequence of frames 𝑜1:𝑡and a language-\nbased task description to generate object segmen-\ntations 𝑚1:𝑡and interaction types 𝑐1:𝑡, represent-\ning the interaction steps. The learned ROCKET-1\n𝜋(𝑎𝑡|𝑜1:𝑡, 𝑚1:𝑡, 𝑐1:𝑡) interprets these outputs to in-\nteract with the environment in real-time. In this\nsection, we outline ROCKET-1 ’s architecture\nand training methods, the dataset collection pro-\ncess, and a pipeline integrating ROCKET-1 with\nstate-of-the-art VLMs.\nROCKET-1 Architecture. To train ROCKET-\n1, we prepare interaction trajectory data in the\nformat: 𝜏= (𝑜1:𝑇, 𝑎1:𝑇, 𝑚1:𝑇, 𝑐1:𝑇), where 𝑜𝑡∈\nℝ3×𝐻×𝑊is the visual observation at time 𝑡, 𝑚𝑡∈\n{0, 1}1×𝐻×𝑊is a binary mask highlighting the ob-\nject in 𝑜𝑡for future interaction, 𝑐𝑡∈ℕdenotes\nthe interaction type, and 𝑎𝑡is the action. If both\n𝑚𝑡and 𝑐𝑡are zeros, no region is highlighted at\n𝑜𝑡. As shown in Figure 3, ROCKET-1 is formal-\nized as a conditioned policy, 𝜋(𝑎𝑡|𝑜1:𝑡, 𝑚1:𝑡, 𝑐1:𝑡),\nwhich takes a sequence of observations and object-\nsegmented interaction regions to causally predict\nactions. To effectively encode spatial information,\ninspired by Zhang et al. (2023), we concatenate\nthe observation and object segmentation pixel-\nwise into a 4-channel image, which is processed\nby a visual backbone for deep fusion, followed by\nan self-attention pooling layer:\nℎ𝑡←Backbone([𝑜𝑡, 𝑚𝑡]),\n(1)\n𝑥𝑡←AttentionPooling(ℎ𝑡).\n(2)\nWe extend the input channels of the first convolu-\ntion in the pre-trained visual backbone from 3 to\n4, initializing the new parameters to 0s to mini-\nmize the gap in early training. A TransformerXL\n4\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 4 | Trajectory relabeling pipeline in Minecraft. A bounding box and point selection are applied to the\nimage center in the frame preceding the interaction event to identify the interacting object. SAM-2 is then run\nin reverse temporal order for a specified duration.\n(Baker et al., 2022; Dai et al., 2019) module is\nthen used to model temporal dependencies be-\ntween observations and incorporate interaction\ntype information to predict the next action ˆ𝑎𝑡:\nˆ𝑎𝑡←TransformerXL(𝑐1, 𝑥1, · · · , 𝑐𝑡, 𝑥𝑡).\n(3)\nWe delay the integration of interaction type infor-\nmation 𝑐𝑡until after fusing 𝑚𝑡and 𝑜𝑡, enabling the\nbackbone to share knowledge across interaction\ntypes and mitigating data imbalance. Behavior\ncloning loss is used for optimization. However,\nthis approach risks making 𝑎𝑡overly dependent\non 𝑚𝑡and 𝑐𝑡, reducing the model’s temporal rea-\nsoning capability. To address this, we propose\nrandomly dropping segmentations with a certain\nprobability, forcing the model to infer user intent\nfrom past inputs (visual-temporal context). The\nfinal optimization objective is:\nL = −\n|𝜏|\n∑︁\n𝑡=1\nlog 𝜋(𝑎𝑡|𝑜1:𝑡, 𝑚1:𝑡⊙𝑤1:𝑡, 𝑐1:𝑡⊙𝑤1:𝑡), (4)\nwhere 𝑤𝑡∼Bernoulli(1 −𝑝) represents a mask,\nwith 𝑝denoting the dropping probability, ⊙de-\nnotes the product operation over time dimension.\nBackward Trajectory Relabeling. We seek to\nbuild a dataset for training ROCKET-1. The col-\nlected trajectory data 𝜏typically contains only\nobservations 𝑜1:𝑇and actions 𝑎1:𝑇. To generate ob-\nject segmentations and interaction types for each\nframe, we propose a novel hindsight relabeling\ntechnique (Andrychowicz et al., 2017) combined\nwith an object tracking model (Ravi et al., 2024)\nfor automatic data labeling. We first abstract a\nset of interactions C and identify frames where\ninteraction events occur, detected using a pre-\ntrained vision-language model, such as Achiam\net al. (2023). Then, we traverse the trajectory in\nreverse order, segmenting interacting objects in\nframe 𝑡via an open-vocabulary grounding model,\nsuch as (Liu et al., 2023). Finally, SAM-2 (Ravi\net al., 2024) is used to track and generate seg-\nmentations for frames 𝑡−1, 𝑡−2, . . . , 𝑡−𝑘, where\n𝑘is the window length.\nFor Minecraft, we use contractor data (Baker\net al., 2022) from OpenAI, consisting of 1.6 billion\nframes of human gameplay. This dataset includes\nmeta information for each frame, recording inter-\naction events such as kill entity, mine block, use\nitem, interact, craft, and switch, eliminating the\nneed for vision-language models to detect events.\nWe observed that interacting objects are often\ncentered in the previous frame, allowing the use\nof a fixed-position bounding box and point with\nthe SAM-2 model for segmentation, replacing\nopen-vocabulary grounding models. We also in-\ntroduced an additional interaction type, navigate.\nIf a player’s movement exceeds a set threshold\nover a period, they are considered to be approach-\ning an object. The object they face in the seg-\nment’s final frame is marked as the target, with\nSAM-2 applied in reverse to identify it in earlier\nframes. As shown in Figure 4, the entire labeling\nprocess can be totally automated.\nIntegration with High-level Reasoner. Complet-\ning complex long-horizon tasks in open-world\n5\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 5 | A hierarchical agent structure based on our proposed visual-temporal context prompting. A\nGPT-4o model decomposes complex tasks into steps based on the current observation, while the Molmo model\nidentifies interactive objects by outputting points. SAM-2 segments these objects based on the point prompts,\nand ROCKET-1 uses the object masks and interaction types to make decisions. GPT-4o and Molmo run at low\nfrequencies, while SAM-2 and ROCKET-1 operate at the same frequency as the environment.\nTable 1 | Hyperparameters for training ROCKET-1.\nHyperparameter\nValue\nInput Image Size\n224 × 224\nVisual Backbone\nEfficientNet-B0 (4 channels)\nPolicy Transformer\nTransformerXL\nNumber of Policy Blocks\n4\nHidden Dimension\n1024\nTrajectory Chunk size\n128\nDropout Rate 𝑝\n0.75\nOptimizer\nAdamW\nLearning Rate\n0.00004\nenvironments requires agents to have strong com-\nmonsense knowledge and do visual-language rea-\nsoning, both of which are strengths of modern\nVLMs. As shown in Figure 5, we design a novel\nhierarchical agent architecture comprising GPT-\n4o (Achiam et al., 2023), Molmo (Deitke et al.,\n2024), SAM-2 (Ravi et al., 2024), and ROCKET-1.\nGPT-4o decomposes tasks into object interactions\nbased on an observation 𝑜𝑡−𝑘, leveraging its ex-\ntensive knowledge and reasoning abilities. Since\nGPT-4o cannot directly output the object masks,\nwe use Molmo to generate (𝑥, 𝑦) coordinates for\nthe described objects. SAM-2 then produces the\nobject mask 𝑚𝑡−𝑘from these coordinates and ef-\nficiently tracks objects 𝑚𝑡−𝑘+1:𝑡in subsequent ob-\nservations. ROCKET-1 uses the generated masks\n𝑚𝑡−𝑘:𝑡and interaction types 𝑐𝑡−𝑘:𝑡from GPT-4o to\nengage with the environment. Due to the high\ncomputational cost, GPT-4o and Molmo run at\nlower frequencies, while SAM-2 and ROCKET-1\noperate at the env’s frequency.\n4. Results and Analysis\nFirst, we provide a detailed overview of the exper-\nimental setup, including the benchmarks, base-\nlines, and implementation details. We then ex-\nplore ROCKET-1 ’s performance on basic open-\nworld interactions and long-horizon tasks. Finally,\nwe conduct comprehensive ablation studies to val-\nidate the rationale behind our design choices.\n4.1. Experimental Setup\nImplementation Details.\nBriefly, we present\nROCKET-1 ’s model architecture, hyperparam-\neters, and optimizer configurations in Table 1.\nDuring training, each complete trajectory is di-\nvided into 128-length segments to reduce mem-\nory requirements. During inference, ROCKET-1\ncan access up to 128 frames of past observations.\nMost training parameters follow the settings from\nprior works such as Baker et al. (2022); Cai et al.\n(2023b, 2024b).\nEnvironment and Benchmark. We use the un-\nmodified Minecraft 1.16.5 (Guss et al., 2019; Lin\net al., 2023) as our testing environment, which\naccepts mouse and keyboard inputs as the action\nspace and outputs a 640 × 360 RGB image as the\nobservation. To comprehensively evaluate the\nagent’s interaction capabilities, as shown in Fig-\nure 6, we introduce the Minecraft Interaction\nBenchmark, consisting of six categories and a\ntotal of 12 tasks, including Hunt, Mine, Interact,\nNavigate, Tool, and Place. This benchmark empha-\nsizes object interaction and spatial localization\n6\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 6 | A benchmark for evaluating open-world interaction capabilities of agents.\nThe benchmark\ncontains six interaction types in Minecraft, totaling 12 tasks. Unlike previous benchmarks, these tasks emphasize\ninteracting with objects at specific spatial locations. For example, in “hunt the sheep in the right fence,” the task\nfails if the agent kills the sheep on the left side. Some tasks, such as “place the oak door on the diamond block,”\nnever appear in the training set. It is also designed to evaluate zero-shot generalization capabilities.\nTable 2 | Results on the Minecraft Interaction benchmark. Each task is tested 32 times, and the average\nsuccess rate is reported as the final result. “Human” indicates instructions provided by a human.\nMethod\nPrompt\nHunt\nMine\nInteract\nNavigate\nTool\nPlace\nAvg\nVPT-bc\nN\/A\n0.13\n0.16\n0.00\n0.13\n0.03\n0.31\n0.00\n0.09\n0.00\n0.00\n0.00\n0.00\n0.07\nSTEVE-1\nHuman\n0.00\n0.06\n0.00\n0.69\n0.00\n0.03\n0.00\n0.31\n0.91\n0.06\n0.16\n0.00\n0.19\nGROOT-1\nHuman\n0.09\n0.22\n0.00\n0.06\n0.03\n0.06\n0.00\n0.03\n0.47\n0.13\n0.03\n0.00\n0.09\nROCKET-1\nMolmo\n0.91\n0.84\n0.78\n0.75\n0.81\n0.50\n0.78\n0.97\n0.94\n0.91\n0.72\n0.91\n0.82\nROCKET-1\nHuman\n0.94\n0.91\n0.91\n0.94\n0.94\n0.91\n0.97\n0.97\n0.97\n0.97\n0.94\n0.97\n0.95\nskills. For example, in the “hunt the sheep in the\nright fence” task, success requires the agent to kill\nsheep within the right fence, while doing so in\nthe left fence results in failure. In the “place the\noak door on the diamond block” task, success is\nachieved only if the oak door is adjacent to the\ndiamond block on at least one side.\nBaselines. We compare our methods with the\nfollowing baselines: (1) VPT (Baker et al., 2022):\nA foundational model pre-trained on large-scale\nYouTube data, with three variants—VPT (fd),\nVPT (bc), and VPT (rl)—representing the vanilla\nfoundational model, behavior-cloning finetuned\nmodel, and RL-finetuned model, respectively. In\nthis study, we utilize the VPT (bc) variant. (2)\nSTEVE-1 (Lifshitz et al., 2023): An instruction-\nfollowing agent finetuned from VPT, capable of\nsolving various short-horizon tasks. We select the\ntext-conditioned version of STEVE-1 for compari-\nson. (3) GROOT-1 (Cai et al., 2023b): A reference-\nvideo conditioned policy designed to perform\nopen-ended tasks, trained on 2,000 hours of long-\nform videos using latent variable models.\n4.2. ROCKET-1 Masters Minecraft Interactions\nWe evaluated ROCKET-1 on the Minecraft In-\nteraction Benchmark, with results as illustrated\nin Table 2. Since ROCKET-1 operates as a low-\nlevel policy, it requires a high-level reasoner to\nprovide prompts within a visual-temporal con-\ntext, driving ROCKET-1 ’s interactions with the\nenvironment. We tested two reasoners: (1) A\nskilled Minecraft human player, who can provide\nprompts to ROCKET-1 at any interaction mo-\nment, serving as an oracle reasoner that demon-\nstrates the upper bound of ROCKET-1 ’s capa-\nbilities. (2) A Molmo 72B model (Deitke et al.,\n2024), where a predefined Molmo prompt is set\nfor each task to periodically select points in the\nobservation as prompts, which are then processed\ninto object segmentations by the SAM-2 model\n7\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 7 | Screenshots of our hierarchical agent when completing long-horizon tasks.\nTable 3 | Comparison of hierarchical architectures with different communication protocols. All seven\ntasks require complex reasoning capabilities. The diamond task was run 100 times, while other tasks were run\n20 times, with average success rates reported.\nMethod\nCommunication Protocol\nPolicy\nDEPS\nlanguage\nSTEVE-1\n0.95\n0.75\n0.15\n0.02\n0.15\n0.00\n0.00\nMineDreamer∗\nfuture image\nSTEVE-1\n0.95\n-\n-\n-\n0.00\n0.00\n0.00\nOmniJarvis\nlatent code\nGROOT-1\n0.95\n0.90\n0.20\n0.08\n0.40\n0.00\n0.00\nOurs\nvisual-temporal context\nROCKET-1\n1.00\n1.00\n0.45\n0.25\n0.75\n0.50\n0.70\n(Ravi et al., 2024). Between Molmo’s invoca-\ntions, SAM-2’s tracking capabilities offer object\nsegmentations to guide ROCKET-1. For all base-\nlines, humans provide prompts. We found that\nROCKET-1 + Molmo consistently outperformed\nall baselines, notably achieving a 91% success\nrate in the “place oak door on the diamond block”\ntask that no baseline can solve.\n4.3. ROCKET-1 Supports Long-Horizon Tasks\nWe compared hierarchical agent architectures\nbased on different communication protocols: (1)\nlanguage-based approaches, exemplified by DEPS\n(Wang et al., 2023b); (2) future-image-based\nmethods, represented by MineDreamer (Zhou\net al., 2024); (3) latent-code-based methods, as\nin OmniJarvis (Wang et al., 2024a); and (4) our\nproposed approach based on visual-temporal con-\ntext, as illustrated in the Figure 5. For Mine-\nDreamer, we used the planner provided by DEPS\nand MineDreamer as the controller to complete\nthe long-horizon experiment. We evaluated these\nmethods on seven tasks, each requiring long-\nhorizon planning: obtaining a wooden pickaxe\n(3.6k), furnace (6k), shears (12k), diamond\n(24k), steak (6k), obsidian (24k), and pink wool\n(6k), where the numbers in parentheses repre-\nsent the time limit. In the first five tasks, the\nagent starts from scratch, while for the obsidian\ntask, we provide an empty bucket and a diamond\npickaxe in advance, and for the pink wool task,\nwe provide shears. Taking the obsidian task as\nan example, the player must first locate a nearby\nwater source, fill the bucket, find a nearby lava\npool, pour the water to form obsidian, and finally\nswitch to the diamond pickaxe to mine the obsid-\nian. Our approach significantly improved success\nrates on the first five tasks, particularly achieving\na 35% increase in the steak task. For the last two\ntasks, all previous baseline methods failed, while\nour approach achieved a 70% success rate on the\nwool dyeing task. Figure 7 presents screenshots.\n4.4. What Matters for Learning ROCKET-1?\nWe conduct ablation studies on individual tasks\nof Minecraft Interaction benchmark: “Hunt right\nsheep (\n)” and “Mine emerald (\n)”.\n8\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nTable 4 | Comparison of different condition fusion\nmethods.\nFusion Positions\nHunt (\n)↑\nMine (\n) ↑\nFusion in transformer layer\n0.91\n0.78\nFusion in visual backbone\n0.72\n0.69\nTable 5 | Comparison between different SAM-2 vari-\nants. We studied the impact of SAM-2 models of dif-\nferent sizes on the agent’s object-tracking capability\n(metric: success rate) and inference speed (metric:\nframes per second, FPS). “#Pmt” indicates the num-\nber of frames between prompts generated by Molmo.\nVariants\n#Pmt\nFPS ↑\n↑\n↑\nbaseline (w\/o sam2)\n3\n0.9\n0.84\n0.82\nbaseline (w\/o sam2)\n30\n9.2\n0.00\n0.03\n+sam2_tiny\n30\n5.4\n0.84\n0.69\n+sam2_small\n30\n5.1\n0.88\n0.50\n+sam2_base_plus\n30\n3.0\n0.88\n0.63\n+sam2_large\n30\n2.4\n0.91\n0.78\nCondition Fusion Methods. We modified the vi-\nsual backbone’s input layer from 3 to 4 channels,\nallowing ROCKET-1 to integrate object segmen-\ntation information. For fusing interaction-type\ninformation, we explored two approaches: (1)\nkeeping the object segmentation channel binary\nand encoding interaction types via an embedding\nlayer for fusion in TransformerXL, and (2) di-\nrectly encoding interaction types into the object\nsegmentation for fusion within the visual back-\nbone. As shown in Table 4, the first approach sig-\nnificantly outperformed the second, as it allows\nthe visual backbone to share knowledge across\ndifferent interaction types and focus on recogniz-\ning objects of interest without being affected by\nimbalanced interaction-type distributions.\nSAM-2 Models. The SAM-2 model acts as a proxy\nsegmentation generator when the high-level rea-\nsoner fails to provide timely object segmentations.\nWe evaluate the impact of different SAM-2 model\nsizes on task performance and inference speed,\nas shown in Table 5. Results indicate that with\nlow-frequency prompts from the high-level rea-\nsoner (Molmo 72B) at 1.5 (game frequency is\n20), SAM-2 greatly improves task success rates.\nWhile “sam2_hiera_large” is the best, increasing\nthe SAM-2 model size yields performance gains\nat the cost of higher time.\n5. Related Works\nInstructions for Multi-Task Policy. Most current\napproaches (Brohan et al., 2022, 2023; Cai et al.,\n2023a; Huang et al., 2023; Lynch et al., 2023)\nuse natural language to describe task details and\ncollect large amounts of text-demonstration data\npairs to train a language-conditioned policy for\ninteraction with the environment. Although nat-\nural language can express a wide range of tasks,\nit struggles to represent spatial relationships ef-\nfectively. Additionally, gathering text-annotated\ndemonstration data is costly, limiting the scala-\nbility of these methods. Alternatives, such as Lif-\nshitz et al. (2023); Majumdar et al. (2022); Sun-\ndaresan et al. (2024), use images to drive goal-\nconditioned policies, typically learning through\nhindsight relabeling in a self-supervised manner.\nWhile this reduces the need for annotated data,\nfuture images are often insufficiently expressive,\nmaking it difficult to capture detailed task execu-\ntion processes. Methods like Cai et al. (2023b);\nJang et al. (2022) propose using reference videos\nto describe tasks, offering strong expressiveness\nbut suffering from ambiguity, which may lead to\ninconsistencies between policy interpretation and\nhuman understanding, raising safety concerns.\nGu et al. (2023) suggests representing tasks with\nrough robot arm trajectories, enabling novel task\ncompletion but only in fully observable environ-\nments, limiting its applicability in open-world\nsettings. CLIPort (Shridhar et al., 2022), which\naddresses pick-and-place tasks by controlling the\nrobot’s start and end positions using heatmaps,\nbears some resemblance to our proposed visual-\ntemporal context prompting method. However,\nCLIPort focuses solely on the pick-and-place task\nsolutions in a fully observable environment.\nAgents in Minecraft. Minecraft offers a highly\nopen sandbox environment with complex tasks\nand free exploration, ideal for testing AGI’s adapt-\nability and long-term planning abilities. Its rich\ninteractions and dynamic environment simulate\nreal-world challenges, making it an excellent\ntestbed for AGI. One line of research focuses on\nlow-level control policies in Minecraft. Baker et al.\n(2022) annotated a large YouTube Minecraft\nvideo dataset with actions and trained the first\nfoundation agent in the domain using behavior\n9\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\ncloning, but it lacks instruction-following capabil-\nities. Cai et al. (2023a) employs a goal-sensitive\nbackbone and horizon prediction module to en-\nhance multi-task execution in partially observ-\nable environments, but it only solves tasks seen\nduring training. Fan et al. (2022) fine-tunes a\nvision-language alignment model MineCLIP using\nYouTube video data, and incorporates it into a re-\nward shaping mechanism for training a multi-task\nagent, though task transfer still requires extensive\nenvironment interaction. Lifshitz et al. (2023)\nuses hindsight-relabeling to learn an image-goal-\nconditioned policy and aligns image and text\nspaces via MineCLIP, but this approach is lim-\nited to short-horizon tasks. Another research fo-\ncus integrates vision-language models for long-\nhorizon task planning in Minecraft (Liu et al.,\n2024; Qin et al., 2023; Wang et al., 2024b; Yuan\net al., 2023; Zheng et al., 2023). DEPS (Wang\net al., 2023b), the first to apply large language\nmodels in Minecraft, uses a four-step process to\ndecompose tasks, achieving the diamond mining\nchallenge with minimal training. Voyager (Wang\net al., 2023a) highlights LLM-based agents’ au-\ntonomous exploration and skill-learning abilities.\nJarvis-1 (Wang et al., 2023c) extends DEPS with\nmultimodal memory, improving long-horizon task\nsuccess rates by recalling past experiences. Om-\nniJarvis (Wang et al., 2024a) learns a behavior\ncodebook using self-supervised methods to jointly\nmodel language, images, and actions.\nMine-\nDreamer (Zhou et al., 2024) fine-tunes VLMs and\na diffusion model to generate goal images for task\nexecution, though it faces challenges with image\nquality and consistency.\n6. Conclusions and Limitations\nThis paper presents a novel hierarchical agent ar-\nchitecture for open-world interaction. To address\nspatial communication challenges, we introduce\nvisual-temporal context prompting to convey in-\ntent between the high-level reasoner and low-\nlevel policy. We develop ROCKET-1, an object-\nsegmentation-conditioned policy for real-time ob-\nject interaction, enhanced by SAM-2 for plug-and-\nplay object tracking. Experiments in Minecraft\nshow that our approach effectively leverages\nVLMs’ visual-language reasoning, achieving su-\nperior open-world interaction performance over\nbaselines.\nAlthough ROCKET-1 significantly enhances\ninteraction capabilities in Minecraft, it cannot en-\ngage with objects that are outside its field of view\nor have not been previously encountered. For in-\nstance, if the reasoner instructs ROCKET-1 to\neliminate a sheep that it has not yet seen, the rea-\nsoner must indirectly guide ROCKET-1 ’s explo-\nration by providing segmentations of other known\nobjects. This limitation reduces ROCKET-1 ’s\nefficiency in completing simple tasks and neces-\nsitates frequent interventions from the reasoner,\nleading to increased computational overhead. We\nsolve this problem in ROCKET-2 (Cai et al., 2025).\nThis project is implemented using MineStudio\n(Cai et al., 2024a).\n7. Acknoledgements\nThis work was supported by the National Science\nand Technology Major Project #2022ZD0114902\nand the CCF-Baidu Open Fund. We sincerely ap-\npreciate their generous support, which enabled\nus to conduct this research.\n10\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\n```\n#### 2. 论文摘要\n```\nVision-language models (VLMs) have excelled in multimodal tasks, but adapting\nthem to embodied decision-making in open-world environments presents\nchallenges. One critical issue is bridging the gap between discrete entities in\nlow-level observations and the abstract concepts required for effective\nplanning. A common solution is building hierarchical agents, where VLMs serve\nas high-level reasoners that break down tasks into executable sub-tasks,\ntypically specified using language. However, language suffers from the\ninability to communicate detailed spatial information. We propose\nvisual-temporal context prompting, a novel communication protocol between VLMs\nand policy models. This protocol leverages object segmentation from past\nobservations to guide policy-environment interactions. Using this approach, we\ntrain ROCKET-1, a low-level policy that predicts actions based on concatenated\nvisual observations and segmentation masks, supported by real-time object\ntracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to\ntackle complex tasks that demand spatial reasoning. Experiments in Minecraft\nshow that our approach enables agents to achieve previously unattainable tasks,\nwith a $\\mathbf{76}\\%$ absolute improvement in open-world interaction\nperformance. Codes and demos are now available on the project page:\nhttps:\/\/craftjarvis.github.io\/ROCKET-1.\n```\n\n#### 3. 论文全文\n```\nROCKET-1: Mastering Open-World Interaction\nwith Visual-Temporal Context Prompting\nShaofei Cai1, Zihao Wang1, Kewei Lian1, Zhancun Mu1, Xiaojian Ma3, Anji Liu2 and Yitao Liang B1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis\nVision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied\ndecision-making in open-world environments presents challenges. One critical issue is bridging the\ngap between discrete entities in low-level observations and the abstract concepts required for effective\nplanning. A common solution is building hierarchical agents, where VLMs serve as high-level reasoners\nthat break down tasks into executable sub-tasks, typically specified using language. However, language\nsuffers from the inability to communicate detailed spatial information. We propose visual-temporal\ncontext prompting, a novel communication protocol between VLMs and policy models. This protocol\nleverages object segmentation from past observations to guide policy-environment interactions. Using\nthis approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual\nobservations and segmentation masks, supported by real-time object tracking from SAM-2. Our method\nunlocks the potential of VLMs, enabling them to tackle complex tasks that demand spatial reasoning.\nExperiments in Minecraft show that our approach enables agents to achieve previously unattainable\ntasks, with a 76% absolute improvement in open-world interaction performance. Codes and demos are\nnow available on the project page: https:\/\/craftjarvis.github.io\/ROCKET-1.\nFigure 1 | Our pipeline solves creative tasks, such as get the obsidian in the original Minecraft version, using the\naction space identical to human players (mouse and keyboard). We present a novel instruction interface, visual-\ntemporal context prompting, under which we learn a spatial-sensitive policy, ROCKET-1. VLMs identify regions\nof interest within each observation and guide ROCKET-1 interactions. Different colors in the segmentation\nrepresent distinct interaction types, for example,\n- use,\n- approach,\n- switch,\n- mine block.\n1. Introduction\nPre-trained foundation vision-language models\n(VLMs) (Achiam et al., 2023; Team et al., 2023)\nhave shown impressive performance in reason-\ning, visual question answering, and task planning\n(Brohan et al., 2023; Cheng et al., 2024; Driess\net al., 2023; Wang et al., 2023b), primarily due\nto training on internet-scale multimodal data. Re-\ncently, there has been growing interest in trans-\nCorresponding author(s): Yitao Liang\nShaofei Cai <caishaofei@stu.pku.edu.cn>, Zihao Wang <zhwang@stu.pku.edu.cn>, Kewei Lian <lkwkwl@stu.pku.edu.cn>, Zhancun Mu\n<muzhancun@stu.pku.edu.cn>, Xiaojian Ma <xiaojian.ma@ucla.edu>, Anji Liu <liuanji@cs.ucla.edu>, Yitao Liang <yitaol@pku.edu.cn>\narXiv:2410.17856v3  [cs.CV]  20 Mar 2025\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 2 | Different pipelines in solving embodied decision-making tasks. (a) End-to-end pipeline modeling\ntoken sequences of language, observations, and actions. (b) Language prompting: VLMs decompose instructions\nfor language-conditioned policy execution. (c) Latent prompting: maps discrete behavior tokens to low-level\nactions. (d) Future-image prompting: fine-tunes VLMs and diffusion models for image-conditioned control. (e)\nVisual-temporal prompting: VLMs generate segmentations and interaction cues to guide ROCKET-1.\nferring these capabilities to embodied decision-\nmaking in open-world environments. Existing\napproaches can be broadly categorized into (i)\nend-to-end and (ii) hierarchical approaches. End-\nto-end approaches, such as RT-2 (Brohan et al.,\n2023), Octo (Octo Model Team et al., 2024), LEO\n(Huang et al., 2023), and OpenVLA (Stone et al.,\n2023), aim to enable VLMs to interact with envi-\nronments by collecting robot manipulation trajec-\ntory data annotated with text. This data is then\ntokenized to fine-tune VLMs into vision-language-\naction models (VLAs) in an end-to-end manner,\nas illustrated in Figure 2(a). However, collecting\nsuch annotated trajectory data is difficult to scale.\nMoreover, introducing the action modality risks\ncompromising the foundational abilities of VLMs.\nHierarchical agent architectures typically con-\nsist of a high-level reasoner and a low-level pol-\nicy, which can be trained independently. In this\narchitecture, the “communication protocol” be-\ntween components defines the capability lim-\nits of the agent. Alternative approaches (Driess\net al., 2023; Wang et al., 2023a,b) leverage VLMs’\nreasoning abilities to zero-shot decompose tasks\ninto language-based sub-tasks, with a separate\nlanguage-conditioned policy executing them in\nthe environment, refer to Figure 2(b). However,\nlanguage instructions often fail to effectively con-\nvey spatial information, limiting the tasks agents\ncan solve. For example, when multiple homony-\nmous objects appear in an observation image, dis-\ntinguishing a specific one using language alone\nmay require extensive spatial descriptors, increas-\ning data collection complexity and learning dif-\nficulty for the language-conditioned policy. To\naddress this issue, approaches like STEVE-1 (Lif-\nshitz et al., 2023), GROOT-1 (Cai et al., 2023b),\nand MineDreamer (Zhou et al., 2024) propose us-\ning a purely vision-based interface to convey task\ninformation to the low-level policy. MineDreamer,\nin particular, uses hindsight relabeling to train an\nimage-conditioned policy (Lifshitz et al., 2023)\nfor interaction, while jointly fine-tuning VLMs\nand diffusion models to generate goal images\nthat guide the policy, shown in Figure 2(d). Al-\nthough replacing language with imagined images\nas the task interface simplifies data collection and\npolicy learning, predicting future observations re-\nquires building a world model, which still faces\nchallenges such as hallucinations, temporal incon-\nsistencies, and limited temporal scope.\nIn human task execution, such as object grasp-\ning, people do not pre-imagine holding an object\nbut maintain focus on the target object while\napproaching its affordance. When the object is\nobscured, humans rely on memory to recall its lo-\ncation and connect past and present visual scenes.\nThis use of visual-temporal context enables hu-\nmans to solve tasks effectively in novel environ-\nments. Building on this idea, we propose a novel\ncommunication protocol called visual-temporal\ncontext prompting, as shown in Figure 2(e). This\nallows users\/reasoners to apply object segmenta-\ntion to highlight regions of interest in past visual\n2\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nobservations and convey interaction-type cues via\na set of skill primitives. Based on this, we learn\nROCKET-1, a low-level policy that uses visual\nobservations and reasoner-provided segmenta-\ntions as task prompts to predict actions causally.\nSpecifically, a transformer (Dai et al., 2019) mod-\nels dependencies between observations, essen-\ntial for representing tasks in partially observable\nenvironments. As a bonus feature, ROCKET-1\ncan enhance its object-tracking capabilities dur-\ning inference by integrating the state-of-the-art\nvideo segmentation model, SAM-2 (Ravi et al.,\n2024), in a plug-and-play fashion. Additionally,\nwe propose a backward trajectory relabeling\nmethod, which efficiently generates segmenta-\ntion annotations in reverse temporal order us-\ning SAM-2, facilitating the creation of training\ndatasets for ROCKET-1. Finally, we develop a\nhierarchical agent architecture leveraging visual-\ntemporal context prompting, which perfectly in-\nherits the vision-language reasoning capabilities\nof foundational VLMs. Experiments in Minecraft\ndemonstrate that our pipeline enables agents to\ncomplete tasks previously unattainable by other\nmethods, while the hierarchical architecture ef-\nfectively solves long-horizon tasks.\nOur main contributions are threefold: (1) We\npresent visual-temporal context prompting, a\nnovel protocol that effectively communicates spa-\ntial and interaction cues in hierarchical agent ar-\nchitecture. (2) We learn ROCKET-1, the first\nsegmentation-conditioned policy in Minecraft, ca-\npable of interacting with nearly all the objects.\n(3) We develop backward trajectory relabel-\ning method that can automatically detect and\nsegment desired objects in collected trajectories\nwith pre-trained SAMs for training ROCKET-1.\n2. Preliminaries\nOffline Reinforcement Learning. We model the\nopen-world interaction problem as a Markov De-\ncision Process (MDP) ⟨O, A, P, C, M, R⟩, where\nO and A represent the observation and action\nspaces, P : O×A×O →ℝ+ describes the environ-\nment dynamics, C is the set of interaction types,\nand M is the segmentation mask space. The bi-\nnary reward function R : O×A ×C×M →{0, 1}\ndetermines whether the policy has completed the\nspecified interaction with the object indicated by\nthe segmentation mask at each time step. The\nobjective of reinforcement learning is to learn a\npolicy that maximizes the expected cumulative\nreward, 𝔼\n\u0002Í𝑇\n𝑡=1 𝑟𝑡\n\u0003\n, where 𝑟𝑡is the reward at time\nstep 𝑡. Our proposed backward trajectory relabel-\ning method ensures that each trajectory attains a\npositive reward based on current object segmen-\ntations. This allows us to discard the rewards and\nlearn a conditioned policy 𝜋(𝑎|𝑜, 𝑐, 𝑚) directly us-\ning behavior cloning. In the offline setting, agents\ndo not interact with the environment but rely on\na fixed, limited dataset of trajectories. This set-\nting is harder as it removes the ability to explore\nthe environment and gather additional feedback.\nVision Language Models. Vision-Language Mod-\nels (VLMs) are machine learning models capable\nof processing both image and language modal-\nities. Recent advances in generative pretrain-\ning have led to the emergence of conversational\nmodels like Gemini (Team et al., 2023), GPT-4o\n(Achiam et al., 2023), and Molmo (Deitke et al.,\n2024), which are trained on large-scale multi-\nmodal data and can reason and generate human-\nlike responses based on text and images. Models\nsuch as Palm-E (Driess et al., 2023) have demon-\nstrated strong abilities in embodied question-\nanswering and task planning. However, stan-\ndalone VLMs cannot often interact directly with\nenvironments. Some approaches use VLMs to\ngenerate language instructions for driving low-\nlevel controllers, but these methods struggle with\nexpressing spatial information. This work focuses\non releasing VLMs’ spatial understanding in em-\nbodied decision-making scenarios. Molmo can\naccurately identify correlated objects in images\nusing a list of (𝑥, 𝑦) coordinates, as demonstrated\nin https:\/\/molmo.allenai.org.\nSegment Anything Models. The Segment Any-\nthing Model (SAM, Kirillov et al. (2023)), intro-\nduced by Meta, is a segmentation model capa-\nble of interactively segmenting objects based on\npoint or bounding box prompts, or segmenting\nall objects in an image at once. It demonstrates\nimpressive zero-shot generalization in both real-\nworld and video game environments. Recently,\nMeta introduced SAM-2 (Ravi et al., 2024), ex-\n3\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 3 | ROCKET-1 architecture. ROCKET-1 processes observations (𝑜), object segmentations (𝑚), and\ninteraction types (𝑐) to predict actions (𝑎) using a causal transformer. Observations and segmentations are\nconcatenated and passed through a visual backbone for deep fusion. Interaction types and segmentations are\nrandomly dropped with a pre-defiened probability during training.\ntending segmentation to the temporal domain.\nWith SAM-2, users can prompt object segmen-\ntation with points or bounding boxes on a sin-\ngle video frame, and the model will track the\nobject forward or backward in time, refer to\nhttps:\/\/ai.meta.com\/sam2.\nRemarkably,\nSAM-2 continues tracking even if the object disap-\npears and reappears, making it well-suited for par-\ntially observable open-world environments. In ad-\ndition, we find the SAM models can be equipped\nwith a text prompt module, enabling them to\nground text-based concepts in visual images, as\nseen in grounded SAM (Liu et al., 2023).\n3. Methods\nOverview. Our work focuses on addressing com-\nplex interactive tasks in open-world environ-\nments like Minecraft. We leverage VLMs’ visual-\nlanguage reasoning capabilities to decompose\ntasks into multiple steps and determine object in-\nteractions based on environmental observations.\nFor example, the “build nether portal” task re-\nquires a sequence of block placements at spe-\ncific locations. A controller is also needed to\nmap these steps into low-level actions. To con-\nvey spatial information accurately, we propose a\nvisual-temporal context prompting protocol and\na low-level policy, ROCKET-1. Pretrained VLMs\nprocess a sequence of frames 𝑜1:𝑡and a language-\nbased task description to generate object segmen-\ntations 𝑚1:𝑡and interaction types 𝑐1:𝑡, represent-\ning the interaction steps. The learned ROCKET-1\n𝜋(𝑎𝑡|𝑜1:𝑡, 𝑚1:𝑡, 𝑐1:𝑡) interprets these outputs to in-\nteract with the environment in real-time. In this\nsection, we outline ROCKET-1 ’s architecture\nand training methods, the dataset collection pro-\ncess, and a pipeline integrating ROCKET-1 with\nstate-of-the-art VLMs.\nROCKET-1 Architecture. To train ROCKET-\n1, we prepare interaction trajectory data in the\nformat: 𝜏= (𝑜1:𝑇, 𝑎1:𝑇, 𝑚1:𝑇, 𝑐1:𝑇), where 𝑜𝑡∈\nℝ3×𝐻×𝑊is the visual observation at time 𝑡, 𝑚𝑡∈\n{0, 1}1×𝐻×𝑊is a binary mask highlighting the ob-\nject in 𝑜𝑡for future interaction, 𝑐𝑡∈ℕdenotes\nthe interaction type, and 𝑎𝑡is the action. If both\n𝑚𝑡and 𝑐𝑡are zeros, no region is highlighted at\n𝑜𝑡. As shown in Figure 3, ROCKET-1 is formal-\nized as a conditioned policy, 𝜋(𝑎𝑡|𝑜1:𝑡, 𝑚1:𝑡, 𝑐1:𝑡),\nwhich takes a sequence of observations and object-\nsegmented interaction regions to causally predict\nactions. To effectively encode spatial information,\ninspired by Zhang et al. (2023), we concatenate\nthe observation and object segmentation pixel-\nwise into a 4-channel image, which is processed\nby a visual backbone for deep fusion, followed by\nan self-attention pooling layer:\nℎ𝑡←Backbone([𝑜𝑡, 𝑚𝑡]),\n(1)\n𝑥𝑡←AttentionPooling(ℎ𝑡).\n(2)\nWe extend the input channels of the first convolu-\ntion in the pre-trained visual backbone from 3 to\n4, initializing the new parameters to 0s to mini-\nmize the gap in early training. A TransformerXL\n4\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 4 | Trajectory relabeling pipeline in Minecraft. A bounding box and point selection are applied to the\nimage center in the frame preceding the interaction event to identify the interacting object. SAM-2 is then run\nin reverse temporal order for a specified duration.\n(Baker et al., 2022; Dai et al., 2019) module is\nthen used to model temporal dependencies be-\ntween observations and incorporate interaction\ntype information to predict the next action ˆ𝑎𝑡:\nˆ𝑎𝑡←TransformerXL(𝑐1, 𝑥1, · · · , 𝑐𝑡, 𝑥𝑡).\n(3)\nWe delay the integration of interaction type infor-\nmation 𝑐𝑡until after fusing 𝑚𝑡and 𝑜𝑡, enabling the\nbackbone to share knowledge across interaction\ntypes and mitigating data imbalance. Behavior\ncloning loss is used for optimization. However,\nthis approach risks making 𝑎𝑡overly dependent\non 𝑚𝑡and 𝑐𝑡, reducing the model’s temporal rea-\nsoning capability. To address this, we propose\nrandomly dropping segmentations with a certain\nprobability, forcing the model to infer user intent\nfrom past inputs (visual-temporal context). The\nfinal optimization objective is:\nL = −\n|𝜏|\n∑︁\n𝑡=1\nlog 𝜋(𝑎𝑡|𝑜1:𝑡, 𝑚1:𝑡⊙𝑤1:𝑡, 𝑐1:𝑡⊙𝑤1:𝑡), (4)\nwhere 𝑤𝑡∼Bernoulli(1 −𝑝) represents a mask,\nwith 𝑝denoting the dropping probability, ⊙de-\nnotes the product operation over time dimension.\nBackward Trajectory Relabeling. We seek to\nbuild a dataset for training ROCKET-1. The col-\nlected trajectory data 𝜏typically contains only\nobservations 𝑜1:𝑇and actions 𝑎1:𝑇. To generate ob-\nject segmentations and interaction types for each\nframe, we propose a novel hindsight relabeling\ntechnique (Andrychowicz et al., 2017) combined\nwith an object tracking model (Ravi et al., 2024)\nfor automatic data labeling. We first abstract a\nset of interactions C and identify frames where\ninteraction events occur, detected using a pre-\ntrained vision-language model, such as Achiam\net al. (2023). Then, we traverse the trajectory in\nreverse order, segmenting interacting objects in\nframe 𝑡via an open-vocabulary grounding model,\nsuch as (Liu et al., 2023). Finally, SAM-2 (Ravi\net al., 2024) is used to track and generate seg-\nmentations for frames 𝑡−1, 𝑡−2, . . . , 𝑡−𝑘, where\n𝑘is the window length.\nFor Minecraft, we use contractor data (Baker\net al., 2022) from OpenAI, consisting of 1.6 billion\nframes of human gameplay. This dataset includes\nmeta information for each frame, recording inter-\naction events such as kill entity, mine block, use\nitem, interact, craft, and switch, eliminating the\nneed for vision-language models to detect events.\nWe observed that interacting objects are often\ncentered in the previous frame, allowing the use\nof a fixed-position bounding box and point with\nthe SAM-2 model for segmentation, replacing\nopen-vocabulary grounding models. We also in-\ntroduced an additional interaction type, navigate.\nIf a player’s movement exceeds a set threshold\nover a period, they are considered to be approach-\ning an object. The object they face in the seg-\nment’s final frame is marked as the target, with\nSAM-2 applied in reverse to identify it in earlier\nframes. As shown in Figure 4, the entire labeling\nprocess can be totally automated.\nIntegration with High-level Reasoner. Complet-\ning complex long-horizon tasks in open-world\n5\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 5 | A hierarchical agent structure based on our proposed visual-temporal context prompting. A\nGPT-4o model decomposes complex tasks into steps based on the current observation, while the Molmo model\nidentifies interactive objects by outputting points. SAM-2 segments these objects based on the point prompts,\nand ROCKET-1 uses the object masks and interaction types to make decisions. GPT-4o and Molmo run at low\nfrequencies, while SAM-2 and ROCKET-1 operate at the same frequency as the environment.\nTable 1 | Hyperparameters for training ROCKET-1.\nHyperparameter\nValue\nInput Image Size\n224 × 224\nVisual Backbone\nEfficientNet-B0 (4 channels)\nPolicy Transformer\nTransformerXL\nNumber of Policy Blocks\n4\nHidden Dimension\n1024\nTrajectory Chunk size\n128\nDropout Rate 𝑝\n0.75\nOptimizer\nAdamW\nLearning Rate\n0.00004\nenvironments requires agents to have strong com-\nmonsense knowledge and do visual-language rea-\nsoning, both of which are strengths of modern\nVLMs. As shown in Figure 5, we design a novel\nhierarchical agent architecture comprising GPT-\n4o (Achiam et al., 2023), Molmo (Deitke et al.,\n2024), SAM-2 (Ravi et al., 2024), and ROCKET-1.\nGPT-4o decomposes tasks into object interactions\nbased on an observation 𝑜𝑡−𝑘, leveraging its ex-\ntensive knowledge and reasoning abilities. Since\nGPT-4o cannot directly output the object masks,\nwe use Molmo to generate (𝑥, 𝑦) coordinates for\nthe described objects. SAM-2 then produces the\nobject mask 𝑚𝑡−𝑘from these coordinates and ef-\nficiently tracks objects 𝑚𝑡−𝑘+1:𝑡in subsequent ob-\nservations. ROCKET-1 uses the generated masks\n𝑚𝑡−𝑘:𝑡and interaction types 𝑐𝑡−𝑘:𝑡from GPT-4o to\nengage with the environment. Due to the high\ncomputational cost, GPT-4o and Molmo run at\nlower frequencies, while SAM-2 and ROCKET-1\noperate at the env’s frequency.\n4. Results and Analysis\nFirst, we provide a detailed overview of the exper-\nimental setup, including the benchmarks, base-\nlines, and implementation details. We then ex-\nplore ROCKET-1 ’s performance on basic open-\nworld interactions and long-horizon tasks. Finally,\nwe conduct comprehensive ablation studies to val-\nidate the rationale behind our design choices.\n4.1. Experimental Setup\nImplementation Details.\nBriefly, we present\nROCKET-1 ’s model architecture, hyperparam-\neters, and optimizer configurations in Table 1.\nDuring training, each complete trajectory is di-\nvided into 128-length segments to reduce mem-\nory requirements. During inference, ROCKET-1\ncan access up to 128 frames of past observations.\nMost training parameters follow the settings from\nprior works such as Baker et al. (2022); Cai et al.\n(2023b, 2024b).\nEnvironment and Benchmark. We use the un-\nmodified Minecraft 1.16.5 (Guss et al., 2019; Lin\net al., 2023) as our testing environment, which\naccepts mouse and keyboard inputs as the action\nspace and outputs a 640 × 360 RGB image as the\nobservation. To comprehensively evaluate the\nagent’s interaction capabilities, as shown in Fig-\nure 6, we introduce the Minecraft Interaction\nBenchmark, consisting of six categories and a\ntotal of 12 tasks, including Hunt, Mine, Interact,\nNavigate, Tool, and Place. This benchmark empha-\nsizes object interaction and spatial localization\n6\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 6 | A benchmark for evaluating open-world interaction capabilities of agents.\nThe benchmark\ncontains six interaction types in Minecraft, totaling 12 tasks. Unlike previous benchmarks, these tasks emphasize\ninteracting with objects at specific spatial locations. For example, in “hunt the sheep in the right fence,” the task\nfails if the agent kills the sheep on the left side. Some tasks, such as “place the oak door on the diamond block,”\nnever appear in the training set. It is also designed to evaluate zero-shot generalization capabilities.\nTable 2 | Results on the Minecraft Interaction benchmark. Each task is tested 32 times, and the average\nsuccess rate is reported as the final result. “Human” indicates instructions provided by a human.\nMethod\nPrompt\nHunt\nMine\nInteract\nNavigate\nTool\nPlace\nAvg\nVPT-bc\nN\/A\n0.13\n0.16\n0.00\n0.13\n0.03\n0.31\n0.00\n0.09\n0.00\n0.00\n0.00\n0.00\n0.07\nSTEVE-1\nHuman\n0.00\n0.06\n0.00\n0.69\n0.00\n0.03\n0.00\n0.31\n0.91\n0.06\n0.16\n0.00\n0.19\nGROOT-1\nHuman\n0.09\n0.22\n0.00\n0.06\n0.03\n0.06\n0.00\n0.03\n0.47\n0.13\n0.03\n0.00\n0.09\nROCKET-1\nMolmo\n0.91\n0.84\n0.78\n0.75\n0.81\n0.50\n0.78\n0.97\n0.94\n0.91\n0.72\n0.91\n0.82\nROCKET-1\nHuman\n0.94\n0.91\n0.91\n0.94\n0.94\n0.91\n0.97\n0.97\n0.97\n0.97\n0.94\n0.97\n0.95\nskills. For example, in the “hunt the sheep in the\nright fence” task, success requires the agent to kill\nsheep within the right fence, while doing so in\nthe left fence results in failure. In the “place the\noak door on the diamond block” task, success is\nachieved only if the oak door is adjacent to the\ndiamond block on at least one side.\nBaselines. We compare our methods with the\nfollowing baselines: (1) VPT (Baker et al., 2022):\nA foundational model pre-trained on large-scale\nYouTube data, with three variants—VPT (fd),\nVPT (bc), and VPT (rl)—representing the vanilla\nfoundational model, behavior-cloning finetuned\nmodel, and RL-finetuned model, respectively. In\nthis study, we utilize the VPT (bc) variant. (2)\nSTEVE-1 (Lifshitz et al., 2023): An instruction-\nfollowing agent finetuned from VPT, capable of\nsolving various short-horizon tasks. We select the\ntext-conditioned version of STEVE-1 for compari-\nson. (3) GROOT-1 (Cai et al., 2023b): A reference-\nvideo conditioned policy designed to perform\nopen-ended tasks, trained on 2,000 hours of long-\nform videos using latent variable models.\n4.2. ROCKET-1 Masters Minecraft Interactions\nWe evaluated ROCKET-1 on the Minecraft In-\nteraction Benchmark, with results as illustrated\nin Table 2. Since ROCKET-1 operates as a low-\nlevel policy, it requires a high-level reasoner to\nprovide prompts within a visual-temporal con-\ntext, driving ROCKET-1 ’s interactions with the\nenvironment. We tested two reasoners: (1) A\nskilled Minecraft human player, who can provide\nprompts to ROCKET-1 at any interaction mo-\nment, serving as an oracle reasoner that demon-\nstrates the upper bound of ROCKET-1 ’s capa-\nbilities. (2) A Molmo 72B model (Deitke et al.,\n2024), where a predefined Molmo prompt is set\nfor each task to periodically select points in the\nobservation as prompts, which are then processed\ninto object segmentations by the SAM-2 model\n7\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 7 | Screenshots of our hierarchical agent when completing long-horizon tasks.\nTable 3 | Comparison of hierarchical architectures with different communication protocols. All seven\ntasks require complex reasoning capabilities. The diamond task was run 100 times, while other tasks were run\n20 times, with average success rates reported.\nMethod\nCommunication Protocol\nPolicy\nDEPS\nlanguage\nSTEVE-1\n0.95\n0.75\n0.15\n0.02\n0.15\n0.00\n0.00\nMineDreamer∗\nfuture image\nSTEVE-1\n0.95\n-\n-\n-\n0.00\n0.00\n0.00\nOmniJarvis\nlatent code\nGROOT-1\n0.95\n0.90\n0.20\n0.08\n0.40\n0.00\n0.00\nOurs\nvisual-temporal context\nROCKET-1\n1.00\n1.00\n0.45\n0.25\n0.75\n0.50\n0.70\n(Ravi et al., 2024). Between Molmo’s invoca-\ntions, SAM-2’s tracking capabilities offer object\nsegmentations to guide ROCKET-1. For all base-\nlines, humans provide prompts. We found that\nROCKET-1 + Molmo consistently outperformed\nall baselines, notably achieving a 91% success\nrate in the “place oak door on the diamond block”\ntask that no baseline can solve.\n4.3. ROCKET-1 Supports Long-Horizon Tasks\nWe compared hierarchical agent architectures\nbased on different communication protocols: (1)\nlanguage-based approaches, exemplified by DEPS\n(Wang et al., 2023b); (2) future-image-based\nmethods, represented by MineDreamer (Zhou\net al., 2024); (3) latent-code-based methods, as\nin OmniJarvis (Wang et al., 2024a); and (4) our\nproposed approach based on visual-temporal con-\ntext, as illustrated in the Figure 5. For Mine-\nDreamer, we used the planner provided by DEPS\nand MineDreamer as the controller to complete\nthe long-horizon experiment. We evaluated these\nmethods on seven tasks, each requiring long-\nhorizon planning: obtaining a wooden pickaxe\n(3.6k), furnace (6k), shears (12k), diamond\n(24k), steak (6k), obsidian (24k), and pink wool\n(6k), where the numbers in parentheses repre-\nsent the time limit. In the first five tasks, the\nagent starts from scratch, while for the obsidian\ntask, we provide an empty bucket and a diamond\npickaxe in advance, and for the pink wool task,\nwe provide shears. Taking the obsidian task as\nan example, the player must first locate a nearby\nwater source, fill the bucket, find a nearby lava\npool, pour the water to form obsidian, and finally\nswitch to the diamond pickaxe to mine the obsid-\nian. Our approach significantly improved success\nrates on the first five tasks, particularly achieving\na 35% increase in the steak task. For the last two\ntasks, all previous baseline methods failed, while\nour approach achieved a 70% success rate on the\nwool dyeing task. Figure 7 presents screenshots.\n4.4. What Matters for Learning ROCKET-1?\nWe conduct ablation studies on individual tasks\nof Minecraft Interaction benchmark: “Hunt right\nsheep (\n)” and “Mine emerald (\n)”.\n8\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nTable 4 | Comparison of different condition fusion\nmethods.\nFusion Positions\nHunt (\n)↑\nMine (\n) ↑\nFusion in transformer layer\n0.91\n0.78\nFusion in visual backbone\n0.72\n0.69\nTable 5 | Comparison between different SAM-2 vari-\nants. We studied the impact of SAM-2 models of dif-\nferent sizes on the agent’s object-tracking capability\n(metric: success rate) and inference speed (metric:\nframes per second, FPS). “#Pmt” indicates the num-\nber of frames between prompts generated by Molmo.\nVariants\n#Pmt\nFPS ↑\n↑\n↑\nbaseline (w\/o sam2)\n3\n0.9\n0.84\n0.82\nbaseline (w\/o sam2)\n30\n9.2\n0.00\n0.03\n+sam2_tiny\n30\n5.4\n0.84\n0.69\n+sam2_small\n30\n5.1\n0.88\n0.50\n+sam2_base_plus\n30\n3.0\n0.88\n0.63\n+sam2_large\n30\n2.4\n0.91\n0.78\nCondition Fusion Methods. We modified the vi-\nsual backbone’s input layer from 3 to 4 channels,\nallowing ROCKET-1 to integrate object segmen-\ntation information. For fusing interaction-type\ninformation, we explored two approaches: (1)\nkeeping the object segmentation channel binary\nand encoding interaction types via an embedding\nlayer for fusion in TransformerXL, and (2) di-\nrectly encoding interaction types into the object\nsegmentation for fusion within the visual back-\nbone. As shown in Table 4, the first approach sig-\nnificantly outperformed the second, as it allows\nthe visual backbone to share knowledge across\ndifferent interaction types and focus on recogniz-\ning objects of interest without being affected by\nimbalanced interaction-type distributions.\nSAM-2 Models. The SAM-2 model acts as a proxy\nsegmentation generator when the high-level rea-\nsoner fails to provide timely object segmentations.\nWe evaluate the impact of different SAM-2 model\nsizes on task performance and inference speed,\nas shown in Table 5. Results indicate that with\nlow-frequency prompts from the high-level rea-\nsoner (Molmo 72B) at 1.5 (game frequency is\n20), SAM-2 greatly improves task success rates.\nWhile “sam2_hiera_large” is the best, increasing\nthe SAM-2 model size yields performance gains\nat the cost of higher time.\n5. Related Works\nInstructions for Multi-Task Policy. Most current\napproaches (Brohan et al., 2022, 2023; Cai et al.,\n2023a; Huang et al., 2023; Lynch et al., 2023)\nuse natural language to describe task details and\ncollect large amounts of text-demonstration data\npairs to train a language-conditioned policy for\ninteraction with the environment. Although nat-\nural language can express a wide range of tasks,\nit struggles to represent spatial relationships ef-\nfectively. Additionally, gathering text-annotated\ndemonstration data is costly, limiting the scala-\nbility of these methods. Alternatives, such as Lif-\nshitz et al. (2023); Majumdar et al. (2022); Sun-\ndaresan et al. (2024), use images to drive goal-\nconditioned policies, typically learning through\nhindsight relabeling in a self-supervised manner.\nWhile this reduces the need for annotated data,\nfuture images are often insufficiently expressive,\nmaking it difficult to capture detailed task execu-\ntion processes. Methods like Cai et al. (2023b);\nJang et al. (2022) propose using reference videos\nto describe tasks, offering strong expressiveness\nbut suffering from ambiguity, which may lead to\ninconsistencies between policy interpretation and\nhuman understanding, raising safety concerns.\nGu et al. (2023) suggests representing tasks with\nrough robot arm trajectories, enabling novel task\ncompletion but only in fully observable environ-\nments, limiting its applicability in open-world\nsettings. CLIPort (Shridhar et al., 2022), which\naddresses pick-and-place tasks by controlling the\nrobot’s start and end positions using heatmaps,\nbears some resemblance to our proposed visual-\ntemporal context prompting method. However,\nCLIPort focuses solely on the pick-and-place task\nsolutions in a fully observable environment.\nAgents in Minecraft. Minecraft offers a highly\nopen sandbox environment with complex tasks\nand free exploration, ideal for testing AGI’s adapt-\nability and long-term planning abilities. Its rich\ninteractions and dynamic environment simulate\nreal-world challenges, making it an excellent\ntestbed for AGI. One line of research focuses on\nlow-level control policies in Minecraft. Baker et al.\n(2022) annotated a large YouTube Minecraft\nvideo dataset with actions and trained the first\nfoundation agent in the domain using behavior\n9\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\ncloning, but it lacks instruction-following capabil-\nities. Cai et al. (2023a) employs a goal-sensitive\nbackbone and horizon prediction module to en-\nhance multi-task execution in partially observ-\nable environments, but it only solves tasks seen\nduring training. Fan et al. (2022) fine-tunes a\nvision-language alignment model MineCLIP using\nYouTube video data, and incorporates it into a re-\nward shaping mechanism for training a multi-task\nagent, though task transfer still requires extensive\nenvironment interaction. Lifshitz et al. (2023)\nuses hindsight-relabeling to learn an image-goal-\nconditioned policy and aligns image and text\nspaces via MineCLIP, but this approach is lim-\nited to short-horizon tasks. Another research fo-\ncus integrates vision-language models for long-\nhorizon task planning in Minecraft (Liu et al.,\n2024; Qin et al., 2023; Wang et al., 2024b; Yuan\net al., 2023; Zheng et al., 2023). DEPS (Wang\net al., 2023b), the first to apply large language\nmodels in Minecraft, uses a four-step process to\ndecompose tasks, achieving the diamond mining\nchallenge with minimal training. Voyager (Wang\net al., 2023a) highlights LLM-based agents’ au-\ntonomous exploration and skill-learning abilities.\nJarvis-1 (Wang et al., 2023c) extends DEPS with\nmultimodal memory, improving long-horizon task\nsuccess rates by recalling past experiences. Om-\nniJarvis (Wang et al., 2024a) learns a behavior\ncodebook using self-supervised methods to jointly\nmodel language, images, and actions.\nMine-\nDreamer (Zhou et al., 2024) fine-tunes VLMs and\na diffusion model to generate goal images for task\nexecution, though it faces challenges with image\nquality and consistency.\n6. Conclusions and Limitations\nThis paper presents a novel hierarchical agent ar-\nchitecture for open-world interaction. To address\nspatial communication challenges, we introduce\nvisual-temporal context prompting to convey in-\ntent between the high-level reasoner and low-\nlevel policy. We develop ROCKET-1, an object-\nsegmentation-conditioned policy for real-time ob-\nject interaction, enhanced by SAM-2 for plug-and-\nplay object tracking. Experiments in Minecraft\nshow that our approach effectively leverages\nVLMs’ visual-language reasoning, achieving su-\nperior open-world interaction performance over\nbaselines.\nAlthough ROCKET-1 significantly enhances\ninteraction capabilities in Minecraft, it cannot en-\ngage with objects that are outside its field of view\nor have not been previously encountered. For in-\nstance, if the reasoner instructs ROCKET-1 to\neliminate a sheep that it has not yet seen, the rea-\nsoner must indirectly guide ROCKET-1 ’s explo-\nration by providing segmentations of other known\nobjects. This limitation reduces ROCKET-1 ’s\nefficiency in completing simple tasks and neces-\nsitates frequent interventions from the reasoner,\nleading to increased computational overhead. We\nsolve this problem in ROCKET-2 (Cai et al., 2025).\nThis project is implemented using MineStudio\n(Cai et al., 2024a).\n7. Acknoledgements\nThis work was supported by the National Science\nand Technology Major Project #2022ZD0114902\nand the CCF-Baidu Open Fund. We sincerely ap-\npreciate their generous support, which enabled\nus to conduct this research.\n10\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | ROCKET-1：掌握开放世界交互的视觉-时间上下文提示\n\n## 📌 背景痛点\/本文动机\n视觉-语言模型（VLMs）在多模态任务中表现出色，但在开放世界环境中进行具身决策时面临挑战。一个关键问题是弥合低级观察中离散实体与有效规划所需的抽象概念之间的差距。一种常见的解决方案是构建分层代理，其中VLMs作为高级推理器，将任务分解为可执行的子任务，通常使用语言指定。然而，语言在传达详细的空间信息方面存在局限性。\n\n## 🚀 核心方法\n💡 创新点1：视觉-时间上下文提示\n本文提出了一种新颖的通信协议，称为视觉-时间上下文提示，用于VLMs和政策模型之间的通信。该协议利用过去观察到的对象分割来指导策略-环境交互。\n\n💡 创新点2：ROCKET-1\n使用这种方法，我们训练了ROCKET-1，这是一个低级策略，它根据连接的视觉观察和分割掩码预测动作，并由SAM-2的实时对象跟踪支持。我们的方法释放了VLMs的潜力，使它们能够处理需要空间推理的复杂任务。\n\n## 📈 实验结果\n在Minecraft中的实验表明，我们的方法使代理能够完成以前无法完成的任务，开放世界交互性能提高了76%。\n\n## 💬 可借鉴之处\n本文提出的视觉-时间上下文提示协议为VLMs和政策模型之间的通信提供了一种新颖的方法，有助于解决开放世界环境中的具身决策问题。ROCKET-1作为一种低级策略，能够有效地处理需要空间推理的复杂任务，并具有实时对象跟踪能力。此外，本文提出的向后轨迹重新标记方法可以自动检测和分割收集的轨迹中的所需对象，为ROCKET-1的训练提供了便利。","llm_summary_res_status":200}
{"title":"GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents","authors":"Shaofei Cai, Bowei Zhang, Zihao Wang, Haowei Lin, Xiaojian Ma, Anji Liu, Yitao Liang","summary":"Developing agents that can follow multimodal instructions remains a\nfundamental challenge in robotics and AI. Although large-scale pre-training on\nunlabeled datasets (no language instruction) has enabled agents to learn\ndiverse behaviors, these agents often struggle with following instructions.\nWhile augmenting the dataset with instruction labels can mitigate this issue,\nacquiring such high-quality annotations at scale is impractical. To address\nthis issue, we frame the problem as a semi-supervised learning task and\nintroduce GROOT-2, a multimodal instructable agent trained using a novel\napproach that combines weak supervision with latent variable models. Our method\nconsists of two key components: constrained self-imitating, which utilizes\nlarge amounts of unlabeled demonstrations to enable the policy to learn diverse\nbehaviors, and human intention alignment, which uses a smaller set of labeled\ndemonstrations to ensure the latent space reflects human intentions. GROOT-2's\neffectiveness is validated across four diverse environments, ranging from video\ngames to robotic manipulation, demonstrating its robust multimodal\ninstruction-following capabilities.","url":"http:\/\/arxiv.org\/abs\/2412.10410v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.10410v1","published":1733550469000,"comment":null,"pdf_text":"GROOT-2: Weakly Supervised Multi-Modal\nInstruction Following Agents\nShaofei Cai†1, Bowei Zhang†1, Zihao Wang1, Haowei Lin1, Xiaojian Ma3, Anji Liu2 and Yitao Liang B1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis\nDeveloping agents that can follow multimodal instructions remains a fundamental challenge in robotics\nand AI. Although large-scale pre-training on unlabeled datasets (no language instruction) has enabled\nagents to learn diverse behaviors, these agents often struggle with following instructions. While augment-\ning the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at\nscale is impractical. To address this issue, we frame the problem as a semi-supervised learning task and\nintroduce GROOT-2, a multimodal instructable agent trained using a novel approach that combines\nweak supervision with latent variable models. Our method consists of two key components: constrained\nself-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn\ndiverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations\nto ensure the latent space reflects human intentions. GROOT-2’s effectiveness is validated across four\ndiverse environments, ranging from video games to robotic manipulation, demonstrating its robust\nmultimodal instruction-following capabilities.\nFigure 1 | By feeding a mixture of demonstrations and some multimodal labels, we learn GROOT-2, a human-\naligned agent capable of understanding multimodal instructions and adaptable to various environments, ranging\nfrom video games to robot manipulation, including Atari, Minecraft, Language Table, and Simpler Env.\n1. Introduction\nDeveloping policies that can follow multimodal instructions to solve open-ended tasks in open-world\nenvironments is a long-standing challenge in robotics and AI research. With the advancement of\nlarge-scale pretraining (Baker et al., 2022; Brohan et al., 2022; Brown et al., 2020), the research\nparadigm for instruction-following policies has shifted from reinforcement learning to supervised\nlearning. In a supervised learning approach, researchers collect large amounts of demonstration data\nand annotate each demonstration with multimodal instructions—such as videos (Duan et al., 2017;\nJang et al., 2022), texts (Lynch et al., 2023; Padalkar et al., 2023), and episode returns (Chen et al.,\n2021)—using hindsight relabeling. In theory, the instruction-following capability of such policies\nCorresponding author(s): Yitao Liang\n† indicates equal contribution\nShaofei Cai<caishaofei@stu.pku.edu.cn>, Bowei Zhang<zhangbowei@stu.pku.edu.cn>, Zihao Wang<zhwang@stu.pku.edu.cn>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Anji Liu<liuanji@cs.ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2412.10410v1  [cs.AI]  7 Dec 2024\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nMechanical Imitation\n𝐵𝐶\n𝐵𝐶+ 𝐾𝐿\nPosterior Collapse\n→→↑… →×\n↑ ←←… ←\n←↑←… ←\n→↑ →… →×\n…\n…\nNaïve Action Copying\nRough Trajectory\nBehavior Semantic\nCondition Agnostic\n0\n1\nFigure 2 | The ELBO Objective of the VAE and Latent Space Spectrum. We define a spectrum based on\n𝑅=\n𝐵𝐶\n𝐵𝐶+𝐾𝐿, where 𝑅= 0 corresponds to “mechanical imitation” and 𝑅= 1 to “posterior collapse.” At low 𝑅,\nlatent vector 𝑧directly outputs action sequences without considering observations ( 𝐵𝐶→0 ). As 𝑅increases,\n𝑧represents high-level task information, such as specific object interactions. At 𝑅= 1, 𝑧provides no beneficial\ninformation for decision-making.\nimproves as the dataset grows. However, annotating demonstrations with high-quality multimodal\nlabels is prohibitively expensive, making it challenging to scale these methods in practice.\nAnother line of work (Ajay et al., 2020; Cai et al., 2023b; Lynch et al., 2020c) avoids the need\nfor additional human annotations by learning from demonstration-only data in a self-supervised\nmanner. These approaches leverage latent variable generative models (Kingma & Welling, 2013) to\njointly learn an encoder and a latent-conditioned policy. The resulting policy is capable of completing\nmultiple tasks specified by a reference video (Cai et al., 2023b). While a reference video is generally\nexpressive enough to represent various tasks, the inherent ambiguity in videos can lead to a learned\nlatent space that is misaligned with human intention. For example, the encoder module may capture\nthe dynamics between adjacent frames in a video, thereby learning a latent representation of the\naction sequence—a process we refer to as “mechanical imitation.” While this latent space accurately\nreconstructs the target action sequence, the resulting latent representation is difficult for human users\nto leverage during policy deployment. Another potential issue is “posterior collapse,” where the latent\nspace collapses to a single point and loses its influence over the policy during inference. We attribute\nthis mismatch between training and inference to the absence of direct supervision for aligning the\nlatent space with human intention. As illustrated in Figure 2, an ideal controllable latent-induced\npolicy space must strike a balance between these two extremes.\nWe present GROOT-2 (refer to Figure 1), a multimodal instructable agent developed using a latent\nvariable model under weak supervision. To unify the training pipeline, we encode instructions from all\nmodalities as distributions over the latent space. The training objectives consist of two key components:\n(1) constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable\nthe latent-conditioned policy to learn diverse behaviors; and (2) human intention alignment, which\nuses relatively small sets of multimodal labels to align the latent space with human intentions.\nSpecifically, we apply the maximum log-likelihood method in the latent space for alignment. The\nunderlying principle is that the latent embedding encoded by multimodal labels should also be sampled\nfrom the distribution learned from the corresponding video. Our approach is both general and flexible,\nas demonstrated through evaluations across four diverse environments—ranging from video games\nto robotic manipulation—including Atari Games (Bellemare et al., 2013), Minecraft (Johnson et al.,\n2016), Language Table Lynch et al. (2023), and Simpler Env (Li et al., 2024). These experiments\nhighlight GROOT-2 ’s robust ability to follow multimodal instructions, with extensive tests showing\nthat scaling up either unlabeled or labeled demonstrations further enhances performance.\n2\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\n2. Background and Problems\n2.1. Latent Variable Models Enable Controllable Behavior Generation\nIn recent years, the GPT series (Brown et al., 2020; Radford, 2018; Radford et al., 2019) has\ndemonstrated impressive capabilities in controllable text generation. Its success can be attributed to\nself-supervised pretraining and the advantageous properties of natural language. A natural language\nparagraph contains rich dependencies between sentences. For instance, the title of an article sets the\ncentral theme for its body, and the response in a question-answer or dialogue is highly correlated\nwith the preceding text. This characteristic enables large language models, trained via next-token\nprediction, to achieve controllable text generation through prompts during inference. Unfortunately,\nsuch strong correlations do not exist between low-level actions. A desired behavior may not have\na necessary preceding trajectory segment. Thus, it isn’t easy to prompt a pre-trained policy model\nto generate a desired behavior. Instead, the generation of actions depends on an underlying latent\nintention variable. A natural approach is to employ latent variable generative models to jointly model\ntrajectory data and the latent variables that drive them, allowing for controllable behavior generation\nby manipulating the latent variables during inference. Next, we will elaborate on how latent variable\nmodels model trajectory data.\nAs a classic latent variable generative model, Variational Autoencoder (VAE, Kingma & Welling\n(2013)) has been widely used in fields such as image generation and text generation. With the\ndevelopment of the offline pretraining paradigm, recent years have seen an increasing number of\nworks utilizing VAE to model trajectory data. Typically, its architectures consist of three components:\na posterior encoder, a prior encoder, and a policy decoder. The posterior encoder, 𝑞(𝑧|𝜏), encodes a\nspecific behavioral trajectory 𝜏= (o1:𝑁, a1:𝑁) and generates a posterior distribution over the latent\nspace. When the action sequence can be accurately inferred from the observation sequence (Baker et al.,\n2022; Zhang et al., 2022)—i.e., when the inverse dynamics model of the environment 𝑝IDM(a1:𝑁|o1:𝑁)\nis easily learned—the action sequence can be excluded from the posterior’s input (Cai et al., 2023b),\nthus reducing the distribution condition to o1:𝑁. The prior encoder, 𝑝(𝑧|o1:𝑘), generates a distribution\nover the latent space based on the history of observations, where 𝑘denotes the length of the observation\nwindow. When 𝑘= 0, the prior distribution is independent of historical observations and is typically\nassumed to follow a standard normal distribution N (0; 1). The decoder, 𝜋(a𝑡|o1:𝑡, 𝑧), is generally a\nlatent-conditioned policy that takes in the environment’s observations along with a specific latent\nvariable to predict the next action to be executed. According to variational inference theory, we can\noptimize the VAE’s modeling capabilities by maximizing the Evidence Lower Bound (ELBO)\nLELBO = 𝔼𝑧∼𝑞(𝑧|o1:𝑁)\n\" 𝑁\n∑︁\n𝑡=𝑘\n−log 𝜋(a𝑡|o1:𝑡, 𝑧)\n#\n+ 𝐷KL(𝑞(𝑧|o1:𝑁) ∥𝑝(𝑧|o≤𝑘)).\n(1)\nThere are generally three main objectives for using VAE to model trajectory data: (1) Modeling\nmultimodal behaviors (Lynch et al., 2020a; Mees et al., 2022): For instance, when trajectory data\nis collected from different individuals, the variations in action sequences across different behavior\nmodes can be substantial. Directly applying a naive behavior cloning algorithm may result in poor\nmodeling performance. Introducing an additional latent variable to differentiate between behavior\nmodes can help mitigate conflicts between them during training. (2) Skill discovery (Gupta et al.,\n2019; Xu et al., 2023): Complex trajectory data is often composed of various skills. A VAE can\nabstract action sequences in a self-supervised manner, enabling skill reuse in downstream tasks, such\nas accelerating the exploration process in reinforcement learning (Ajay et al., 2020; Pertsch et al.,\n2021). (3) Following reference videos to complete open-ended tasks (also known as one-shot\ndemonstration learning, Cai et al. (2023b)): This approach aims to leverage the learned posterior\n3\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 3 | Comparison of Policies with Different Latent Spaces. The reference video depicts digging for\ndiamonds. A policy that mechanically imitates the trajectory falls into lava, while one aligned with human\nintention avoids lava and successfully reaches the diamonds.\nencoder to recognize the underlying intention behind a reference video and encode it as a latent,\nwhich can then drive a policy to complete the specified task in a novel deployment. It points to a way\nto pre-train instruction-following policies using unlabeled trajectory data. We primarily focus on the\nthird objective in the following paragraphs.\n2.2. Modeling Behaviors with VAE Leads to Ambiguous Latent Space\nSeveral studies on VAE (Abeer et al., 2024; Alemi et al., 2018) have pointed out that the Pareto\nfrontier of the ELBO contains an infinite number of solutions for the latent space, a phenomenon we\nrefer to as latent space ambiguity. To facilitate understanding, we provide an informal illustration in\nFigure 2, which shows several possible latent spaces when a VAE is used to model behaviors, all having\nsimilar ELBO values. We differentiate these latent spaces using the ratio 𝑅=\n𝐵𝐶\n𝐵𝐶+𝐾𝐿, where 𝑅= 0\nand 𝑅= 1 represent two extremes of the latent space. When 𝑅approaches 0, the latent condition\ncontains much information, nearly dictating every action of the policy’s behavior. We refer to this as\nmechanical imitation, where the VAE effectively degenerates into an Autoencoder (AE). Conversely,\nwhen 𝑅approaches 1, the latent loses its ability to control the policy’s output, a phenomenon known\nas posterior collapse (Fang et al., 2019; Pagnoni et al., 2018), in which the VAE reduces to an\nAuto-regressive (AR) model. Intuitively, as 𝑅increases, the information encoded in the latent space\nbecomes more high-level, and the policy relies more on environmental feedback (observations) to\nmake decisions that align with the dataset’s distribution. On the other hand, when 𝑅is smaller, the\npolicy tends to down-weight the environment’s observations.\nNot all latent spaces effectively support following a reference video. As shown in Figure 3, the\ngap between the environment state in the reference video and during policy deployment requires\nthe posterior encoder to extract intentions independent of environmental dynamics. For instance, in\nthe Minecraft task “mining a diamond underground,” a reference video may show a player walking\nforward and mining a diamond. If the latent encodes only the trajectory sketch, the policy might fail\nby colliding with obstacles in the deployment environment. This mismatch occurs because humans\ninterpret the video as “mining the diamond” rather than copying specific actions. Aligning the latent\nspace with human intentions is critical for improving policy steerability.\n4\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nEncoder\nPolicy\nEncoder\n𝑜!:#\n𝔼!∼#(!|&!:#)[−log 𝑒(𝑧|𝑜(:*) ]\nEncoder\nEncoder\n𝐷+,(𝑒𝑧𝑜(:* ∥𝑒(𝑧|𝑜())\nHuman Intention Alignment\nConstrained Self-Imitating\nImage Token\nText \/ Returns Token\nAction Token\nSampled Condition Latent\nCondition Distribution\nStop Gradient Operation\nPast Observations\n𝑤!:$\n𝑜!\n𝑜!:#\n𝑜%\n𝑧&\n𝑜!:%'!\n𝑧(\nPolicy\n𝑎%\n𝑜%\n𝑜!:%'!\n−log 𝜋𝑎- 𝑜(:-, 𝑧.)\n𝑎%\n−log 𝜋𝑎- 𝑜(:-, 𝑧&)\nFigure 4 | Pipeline for Constructing a Training Batch for GROOT-2. Each batch includes two sample\ntypes: (1) demonstration-only samples for learning a latent-conditioned policy (Constrained Self-Imitating);\nand (2) labeled samples (text or expected returns) for aligning the latent space with human intentions (Human\nIntention Alignment). The sample ratio varies by dataset distribution.\n3. Aligning Policy Learners with Weak Supervision\nWe explore the development of instructable agents based on latent variable models. To avoid “latent\nspace ambiguity”, we introduce human intention knowledge into the generative pretraining process\nof the policy model to assist in shaping the latent space. As multimodal labels associated with\ndemonstrations carry rich human intention details, we propose a weakly supervised policy learning\nalgorithm to leverage large amounts of unlabeled demonstration data to learn the latent space while\nusing a small amount of multimodal labeled data to align the latent space with human intention.\nUltimately, this enables instructions from all modalities to be unified within the same latent space.\nNext, we will elaborate on the dataset collection, training pipeline, and inference procedure.\nDataset Collection. We can collect two types of training data from the web: a large set of unlabeled\ndemonstrations Ddem = {(o1:𝑁, a1:𝑁)} and a relatively small set of annotated demonstrations Dlab =\n{(o1:𝑁, a1:𝑁, w1:𝑀)}, where o is the image observation provided by the environment, a is the action\ntaken by the policy, w is the word token, 𝑁is the length of a demonstration, 𝑀is the length of an\nannotation sentence. The annotation sentence can be multimodal, such as a language sentence (with\n𝑀≥1) or a scaler of the episode return (with 𝑀= 1), which explains the behavior or outcome of the\ndemonstration from a human’s perspective. Since the annotation data is expensive to collect, we have\n|Dlab| ≪|Ddem|.\nTraining Pipeline. Our goal is to learn a shared latent space Z, per-modal instruction encoders\n𝑒(𝑧|𝑐), and a latent-conditioned policy 𝜋(a𝑡|o≤𝑡, 𝑧). Leveraging past observations is essential for a\npolicy to make decisions in a partially observable environment such as Minecraft (Johnson et al.,\n2016). We call the learned policy model GROOT-2, whose training pipeline is shown in Figure 4. For\nan unlabeled demonstration (o1:𝑁, a1:𝑁), we use the encoder module to produce a prior distribution\n𝑒(𝑧|o1) and a posterior distribution 𝑒(𝑧|o1:𝑁). Using the reparameterization trick (Kingma & Welling,\n2013), we sample the latent 𝑧from the posterior distribution 𝑒(𝑧|o1:𝑁) and train the policy model,\nconditioned on 𝑧and o1:𝑡, to reconstruct the entire action sequence causally. To limit the information\npresented in the latent space, we introduce an auxiliary KL divergence term in the objective:\nLdem(o, a) = 𝔼𝑧∼𝑒(𝑧|o1:𝑁)\n\" 𝑁\n∑︁\n𝑡=1\n−log 𝜋(a𝑡|o1:𝑡, 𝑧)\n#\n+ 𝛽1𝐷KL(𝑒(𝑧|o1:𝑁) ∥𝑒(𝑧|o1)).\n(2)\nThis allows the model to leverage demonstration-only data to enhance the complexity of the la-\ntent space, a process we refer to as “constrained self-imitating.” For a labeled demonstration\n5\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 5 | Diverse visual environments used in the experiments. We test our GROOT-2 on both video games\n(simple Atari games and the complex Minecraft game) and robotic manipulation environments (Language\nTable and Simpler Env). Minecraft is a partially observable open-ended environment, while others are fully\nobservable.\n(o1:𝑁, a1:𝑁, w1:𝑀), we pass the label w1:𝑀through the encoder module to obtain a latent distri-\nbution and train the policy model to reconstruct the action sequence based on the latent 𝑧sampled\nfrom this distribution 𝑒(𝑧|w1:𝑀). This allows human knowledge to be modeled in the latent space.\nFurther, to make the encoder understand demonstration o1:𝑁just like humans, we introduce an auxil-\niary MLE term: maximize the log-likelihood of 𝑒(𝑧|o1:𝑁) given the latent 𝑧sampled from 𝑒(𝑧|w1:𝑀).\nUnlike the prior behavior cloning term, the aligning term can be quickly calculated in closed form.\nThis process is referred to as “human intention alignment”:\nLlab(o, a, w) = 𝔼𝑧∼𝑒(𝑧|w1:𝑀)\n\" 𝑁\n∑︁\n𝑡=1\n−log 𝜋(a𝑡|o1:𝑡, 𝑧)\n#\n−𝛽2𝔼𝑧∼sg[𝑒(𝑧|w1:𝑀)] [log 𝑒(𝑧|o1:𝑁)] ,\n(3)\nwhere sg[·] denotes stop gradient operation. The MLE-based alignment objective ensures that the\nlatent sampled from the label-conditioned distribution 𝑒(𝑧|𝑤1:𝑀) can also be sampled from its video-\nconditioned distribution 𝑒(𝑧|𝑜1:𝑁). The final loss function combines the two objectives:\nL(Ddem, Dlab) = 𝔼(o,a)∼Ddem [Ldem(o, a)] + 𝔼(o,a,w)∼Dlab [Llab(o, a, w)] .\n(4)\nSpecific implementation details, such as the model design choice, can be found in the Appendix A.\nInference Procedure. GROOT-2 supports two types of instructions during inference: (1) visual-based\ninstruction – the user can either retrieve a demonstration from the dataset as a reference video or\nmanually record a reference video to serve as the condition for the policy; (2) label-based instruction\n– the user can input a text sentence or specify an expected return as the condition (depending on the\nlabel modality used during the model’s training). We tested them in the following experiments.\n4. Capabilities and Analysis\nWe aim to address the following questions: (1) How does GROOT-2 perform in open-world video\ngames and robotic manipulation? (2) Can GROOT-2 follow instructions beyond language and video?\n(3) What insights can be gained from visualizing the learned latent space? (4) How does GROOT-2\nscale with labeled and unlabeled trajectories? (5) What is the impact of backbone initialization on\nperformance? (6) How do language and video losses influence performance?\nEnvironment and Benchmarks. We conduct experiments across four types of representative envi-\nronments: classical 2D game-playing benchmarks on Atari (Bellemare et al., 2013), 3D open-world\ngameplaying benchmarks on Minecraft (Johnson et al., 2016; Lin et al., 2023), and Robotics bench-\nmarks on Language Table simulator (Lynch et al., 2023) and Simpler Env simulator(Li et al., 2024),\n6\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nTable 1 | Results on the Open-World Minecraft Benchmark. This benchmark includes 8 task families and\n100 tasks. Each task is evaluated 30 times across three seeds, and the average success rate is calculated per\ntask family. For example, Combat (16) indicates 16 tasks in the Combat family.\nMethods\nPrompt\nCombat\nHunt\nRide\nBreed\nCraft\nMine\nInteract\nPlant\n(16)\n(10)\n(4)\n(8)\n(20)\n(20)\n(10)\n(12)\nVPT\nN\/A\n11±3\n20±4\n7±2\n2±0\n4±1\n7±2\n21±6\n22±7\nSTEVE-1\nlang\n12±3\n9±2\n54±8\n4±2\n5±2\n6±3\n53±9\n33±8\nSTEVE-1\nvisual\n15±4\n10±3\n38±9\n6±2\n6±2\n10±4\n40±8\n43±7\nGROOT-1\nvisual\n18±5\n28±8\n26±6\n12±3\n15±4\n22±7\n57±8\n75±6\nGROOT-2\nlang\n40±7\n43±5\n46±6\n22±6\n18±3\n37±5\n55±4\n75±9\nGROOT-2\nvisual\n37±4\n48±7\n51±4\n20±4\n27±3\n36±7\n63±6\n77±7\nTable 2 | Results on the Language Table Benchmark. We reported success rates (in %) within 200 steps for\neach instruction modality, averaging over 250 rollouts. Results are averaged over 3 seeds with mean and stderr.\n“-” indicates missing data. The percentages in parentheses indicate the proportion of labels used.\nTask Family\nBC-Zero\nLAVA\nRT-1\nGROOT-1\nGROOT-2 (50%)\nGROOT-2 (100%)\nlang\nlang\nlang\nvisual\nlang\nvisual\nlang\nvisual\nblock to block\n-\n90±2\n-\n8±2\n84±9\n78±9\n86±8\n82±7\nblock to absolute loc\n-\n72±4\n-\n10±3\n70±8\n68±8\n76±6\n70±8\nblock to block relative loc\n-\n72±3\n-\n4±1\n74±9\n64±7\n76±8\n62±6\nblock to relative loc\n-\n64±4\n-\n8±2\n82±5\n78±6\n84±6\n80±4\nseparate two blocks\n-\n94±2\n-\n12±2\n98±1\n96±2\n98±0\n98±0\nOverall\n72±3\n78±4\n74±13\n8±2\n82±8\n76±7\n84±6\n78±8\nillustrated in Figure 5. These four simulators are used to evaluate whether GROOT-2 can be effec-\ntively steered by returns (Chen et al., 2021; Mnih et al., 2015), reference videos (Cai et al., 2023b;\nJang et al., 2022), and textual instructions (Brohan et al., 2022, 2023).\nResults on the Open-World Minecraft Benchmark. To evaluate policy models in Minecraft, we\nused the contractor dataset from Baker et al. (2022), containing 160M frames. According to the\nmeta information, labeled trajectories account for approximately 35% of the total data. We extended\nthe Minecraft SkillForge Benchmark (Cai et al., 2023b) from 30 to 100 tasks, grouped into eight\nfamilies: Combat, Hunt, Ride, Breed, Craft, Mine, Interact, and Plant. Details are in the Appendix C.\nWe compared GROOT-2 with three baselines: (1) VPT (Baker et al., 2022), a foundational model\ntrained on YouTube data via imitation learning, lacking instruction-following; (2) STEVE-1 (Lifshitz\net al., 2023), which supports text and future image-conditioned instructions; and (3) GROOT-1 (Cai\net al., 2023b), a self-supervised model using reference videos as instructions. Key findings from\nTable 1 are as follows: (1) GROOT-2 (visual) consistently outperforms GROOT-1 across all task\ncategories, with particularly notable gains in mob interaction tasks like Combat and Hunt. Comparing\ntrajectories on Hunt, GROOT-1 mechanically repeats “attack” actions, while GROOT-2 actively\ntracks objects, showing that text data enhances object-centric understanding and better aligns with\nhuman intentions. (2) GROOT-2 (text) performs similarly to GROOT-2 (visual) across most tasks,\ndemonstrating that language and visual modalities share task knowledge. This enables the model to\nleverage both modalities for improved task completion. This highlights the advantage of combining\nmultimodal data for better alignment with human intentions and improved policy performance.\nResults on the Language Table benchmark. To assess GROOT-2’s multimodal instruction following\n7\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nTable 3 | Results on the Simpler Env Benchmark. We report the success rate (in %) of the video-instruction\nand language-instruction following for each model on 3 task families. The percentages in parentheses indicate\nthe proportion of labels used.\nMethods\nPrompt\nPick Coke Can\nMove Near\nOpen\/Close Drawer\nH-Pose\nV-Pose\nS-Pose\nAvg\nAvg\nOpen\nClose\nAvg\nRT-1-X\nlang\n57\n20\n70\n49\n32\n7\n52\n29\nOcto-base\nlang\n5\n0\n1\n1\n3\n0\n2\n1\nGROOT-2 (50%)\nvisual\n42\n18\n52\n37\n35\n29\n30\n30\nlang\n52\n20\n50\n41\n42\n27\n33\n30\nGROOT-2 (100%)\nvisual\n40\n22\n47\n36\n35\n27\n33\n30\nlang\n53\n23\n52\n42\n45\n27\n35\n31\ncapabilities in the context of Robotic Table Manipulation, we utilized the Google Language Table as our\ntesting platform and compared it with methods such as LAVA (Lynch et al., 2023), RT-1 (Brohan et al.,\n2022), GROOT-1 (Cai et al., 2023b). We utilize a dataset provided by Lynch et al. (2023) comprising\n100M trajectories. We removed the text labels from half of the trajectories in the dataset, creating a\n1 : 1 ratio of labeled to unlabeled trajectories. Given that the Language Table environment comes with\nfive task families, all of which are instructed solely through language, we curated five reference videos\nfor each task with relatively clear intentions to evaluate the model’s ability to comprehend video\ninstructions. Detailed specifics are provided in the appendix D. The experimental results are shown\nin the Table 2. We observed that: (1) GROOT-2 leads by an absolute success rate of 4% following\ntext-based instructions compared, likely due to GROOT-2’s more refined model architecture design.\nWe mark the results of RT-2 in gray here, as it uses significantly more training data than ours. (2) The\nperformance of GROOT-2 in following video instructions dropped by approximately 6% compared\nto text instructions, possibly due to the ambiguity of the reference videos, where a “block to block”\ntype video could be interpreted as a “block to relative location” type task. (3) GROOT-1 struggled to\nunderstand the intentions conveyed by the reference videos. We observed that GROOT-1 imitated a\nreference video’s trajectory sketch rather than their colors and shapes. This further underscores the\nimportance of introducing language annotations for some trajectory data as a crucial method to align\nwith human intentions.\nResults on the Simpler Env Benchmark. We utilized the Simpler Env (Li et al., 2024) simulation\nof the Google Robot environment to evaluate the policy’s capability in controlling complex robotic\narms. GROOT-2 is trained on the OpenX dataset (Collaboration et al., 2023). We erased the text\nlabels from half of the dataset’s trajectories, achieving a 1:1 balance between labeled and unlabeled\ndata. We evaluated three types of tasks: Pick Coke Can, Move Near, and Open\/Close Drawer. Following\nBrohan et al. (2023); Li et al. (2024), we set up multiple variants for each task. For instance, the\nPick Coke Can task involved three different poses for the Coke can; in the Move Near task, the layout\nand types of objects varied; and in the Open\/Close Drawer task, the drawer had three layers from\ntop to bottom. We compared GROOT-2 with baseline methods such as RT-1 (Brohan et al., 2022),\nand Octo (Octo Model Team et al., 2024). Among these, RT-1-X is an efficient language-conditioned\ntransformer-based policy trained on the entire OpenX (Collaboration et al., 2023) dataset, which\ncan be considered the performance boundary that GROOT-2 can achieve. As shown in Table 3, we\nfound that GROOT-2 (lang) and GROOT-2 (visual) achieved comparable performance to the RT-1-X\nmodel across all three tasks. This indicates that our method retains language control capabilities and\nimbues the policy with equivalent visual instruction control abilities.\nCan GROOT-2 Follow Instructions Beyond Language and Video, Like Episode Returns?\nWe evaluated GROOT-2 ’s steerability and performance on four Atari games (Breakout, Demon\n8\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 6 | Comparison of Weakly Supervised (WSL) and Self-Supervised (SSL) Learning on 4 Atari Games.\nPolicies are evaluated under return and reference video conditions. For return conditioning, normalized returns\nare input into the encoder, while for video conditioning, videos with similar returns (error < 0.05) are used.\nFigure 7 | t-SNE Visualization of Learned Latent Spaces on Four Atari Games. The first row shows\nresults under self-supervised learning, while the second row displays GROOT-2 ’s performance under weakly\nsupervised learning. Points represent reference videos, with shapes indicating games and colors denoting\nepisode returns. The first four columns compare individual games, and the last column shows a mixed-game\ncomparison.\nAttack, Hero, and Name This Game). Datasets from Agarwal et al. (2020), containing approximately\n10M frames per game, were used. Episode returns were normalized to 𝜇= 0, 𝜎= 1.\nFor training, we constructed a dataset with 30% labeled trajectories (returns) and 70% unlabeled\ndata. Using this dataset, we trained GROOT (wsl) (weakly supervised learning). For comparison,\nGROOT (ssl) was trained on the same dataset without return labels in a fully self-supervised manner.\nBoth models were jointly trained across the four games. During inference, we evaluated policy perfor-\nmance in following reference videos sampled from the test set with normalized returns {−1, 0, +1},\nusing 20 samples per category. Results (Figure 6) show: (1) GROOT (ssl) can recognize behavioral\nquality in reference videos, constructing a rough intention space even without labeled guidance. (2)\nLabeled data significantly improved GROOT (wsl)’s ability to understand video instructions, with the\ngreatest gains in Hero and Name This Game. We also evaluated GROOT (wsl) on return-style instruc-\ntions with normalized rewards {−1, 0, +1}. The similarity between video and reward-conditioned\nperformance suggests the video encoder and reward encoder share the same intention space. The\nAtari experiments aim to evaluate GROOT-2 ’s performance on modalities beyond language and\nvideo, rather than maximizing scores, distinguishing it from traditional offline RL methods.\nWhat Does the Visualization of the Learned Latent Space Reveal?\n9\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 8 | Performance Comparison on Unlabeled Demonstrations. Human-normalized task scores are\naveraged over 20 rollouts across 5 Minecraft tasks to evaluate the agent’s reference-video following ability.\nWe applied t-SNE to visualize embeddings from randomly sampled reference videos. Each point\nin Figure 7 represents a unique video, with shapes denoting game environments and colors indicating\nepisode returns. The first row illustrates results for GROOT (ssl), where videos in Breakout, Demon\nAttack, and Name This Game are classified into two categories based on episode return magnitudes,\nsuggesting that the self-supervised algorithm distinguishes only significant score differences. In\ncontrast, GROOT (ssl) shows poor clustering and limited steerability in the Hero game. The second\nrow shows results for GROOT (wsl), which captures continuous variations in video behavior quality\nacross all games. As shown in the fifth column, embeddings from different environments follow\na continuous pattern aligned with reward labels, indicating a shared latent space that promotes\ncross-environment knowledge transfer.\nHow Does Scaling Up Unlabeled Trajectories Impact Performance?\nWe trained four GROOT-2 variants with 0%, 25%, 50%, and 100% unlabeled data in Minecraft.\nPerformance was tested on five Minecraft tasks (Chop Tree, Hunt Animals, Combat Enemies, Open\nChest, Climb Mountain) and scored relative to skilled human players. For example, if a human\ncollects 20.0 logs in 600 steps and GROOT-2 collects 15.0, the score is 0.75. Results (Figure 8) show\nconsistent improvement with more unlabeled data, with the 100% variant achieving a 5× gain in\nthe Climb Mountain task over the 0% version. It is worth noting that the Climb Mountain and Open\nChest tasks do not have language instructions in the training set.\nHow Does Scaling Up Labeled Trajectories Impact Performance?\nFigure 9 | Ablation Study on Labeled\nTrajectories in the Language Table.\nTo evaluate the impact of labeled trajectory proportions\nin the training set on the instruction-following capabilities of\nGROOT-2, we conducted experiments on the Language Table\nbenchmark. The total number of trajectories remained con-\nstant across different dataset configurations, with only the pro-\nportion of trajectories containing text labels varying. Figure 9\nreports the success rate achieved by GROOT-2 conditioned\non language. At low labeled data proportions (0%−25%), the\nsuccess rate rapidly increased from 10% to 65%, indicating\nthat labeled data significantly influences model performance. However, as the labeled data proportion\nincreased to 50% −80%, the success rate plateaued, rising slightly from 82% to 83%, demonstrating\ndiminishing marginal gains from additional labeled data. Therefore, under resource constraints, a\nlabeled data proportion of 50% may represent the optimal balance between performance and cost.\nHow Does Backbone Initialization Affect Performance?\nWe evaluated different initializations for ViT (random, ImageNet, CLIP) and BERT (random,\nBERT, CLIP) on the Language Table Benchmark. For randomly initialized models, both backbones\n10\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nwere unfrozen during training. According to Table 4, CLIP initialization yielded the best results\nfor ViT, followed by ImageNet, with minimal difference between them, while random initialization\nperformed worst. For BERT, CLIP and standard BERT initialization performed similarly, both surpassing\nrandom initialization. Initializing vision and language encoders with CLIP parameters improves policy\nperformance and reduces training time.\nBackbone\nViT\nViT\nViT\/BERT BERT BERT\nWeights\n-\nImageNet\nCLIP\n-\nBERT\nSR (in %) 76±10\n80±11\n82±8\n79±12 81±8\nTable 4 | Ablation study on the backbone initialization.\nVariants\n−Llab baseline −Ldem baseline\nPrompt\nvision\nvision\nlang\nlang\nSR (in %) 10±2\n76±7\n12±3\n82±8\nTable 5 | Ablation on Llab and Ldem objectives.\nHow Does Language and Video Losses Impact Performance?\nThe Llab loss significantly enhances the model’s understanding of reference videos, as observed\nin the Language Table environment. We compared a variant without Llab loss to the full GROOT-2\nmodel, both trained on the same scale of the Language Table dataset, and tested their ability to follow\nreference videos using standard evaluation scripts. As shown in Table 5, the variant without Llab\nloss failed to complete any tasks. Further analysis of its output videos revealed that it mechanically\nmimicked the arm movement trajectories in the reference videos, completely ignoring object colors\nand shapes, which is inconsistent with human understanding of the reference videos.\nThe Ldem loss is indispensable in the GROOT-2 architecture. Removing Ldem causes the pipeline\nto degrade into an autoencoder when processing unlabeled data. Without constraints on the latent\nencoding, the model tends to learn the video encoder as an inverse dynamics model, encoding\nlow-level action sequences in latent z instead of high-level task information, thereby significantly\nreducing the behavior cloning loss. Additionally, Table 5 show that removing Ldem causes the language\nencoder’s latent z to collapse, leading to a dramatic drop in task success rates.\n5. Related Works\nLearning Policies Across Diverse Domains. Developing policies for sequential control tasks in\nreal and virtual environments poses significant challenges. Research spans domains such as robotic\nmanipulation (Lynch et al., 2023; Yu et al., 2019), video games (Bellemare et al., 2013; Guss et al.,\n2019), and embodied navigation (Hong et al., 2020; Huang et al., 2023; Savva et al., 2019), with\napproaches categorized into reinforcement learning (RL) and imitation learning (IL) based on reward\nfunction reliance. For video games with dense rewards (e.g., ALE platform (Bellemare et al., 2013)),\nonline RL algorithms can achieve superhuman performance (Badia et al., 2020; Mnih et al., 2015) but\nsuffer from low efficiency, risky interactions, and limited generalization. These challenges restrict their\napplicability to physical (Padalkar et al., 2023) or embodied environments (Guss et al., 2019), where\nrewards and cheap interactions are unavailable. IL, as a supervised learning paradigm, addresses\nthese issues through batch efficiency and scalability with large datasets, leveraging Transformer\narchitectures (Jang et al., 2022; Pashevich et al., 2021; Zhang & Chai, 2021). The RT-X series (Brohan\net al., 2022, 2023; Padalkar et al., 2023) advances robotic manipulation by training Transformers\non large expert demonstration datasets, achieving strong zero-shot generalization. Similarly, Baker\net al. (2022) developed a Transformer-based policy for Minecraft using internet-scale gameplay data,\nsolving the diamond challenge. Building on this, Schmidhuber (2019) frames RL as supervised\nlearning, while Chen et al. (2021); Lee et al. (2022) introduce “decision transformers” to model joint\ndistributions of rewards, states, and actions from offline data, highlighting the potential for unified\npolicy learning within Transformers.\nLearning Policies to Follow Instructions. Enabling policies to follow instructions is key to building\n11\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\ngeneral-purpose agents. A common approach involves using language annotations from offline\ndemonstrations to train language-conditioned policies (Abramson et al., 2020; Brohan et al., 2022;\nCai et al., 2023a; Huang et al., 2023; Raad et al., 2024; Reed et al., 2022; Wang et al., 2023a,b),\nleveraging the compositionality of natural language for generalization. However, obtaining high-\nquality annotations is costly. An alternative uses anticipated outcomes as instructions. Majumdar\net al. (2022) trained an image-goal conditioned navigation policy via hindsight relabeling (HER)\n(Andrychowicz et al., 2017) and aligned goal spaces with text. Similarly, Lifshitz et al. (2023) used\nthis strategy for open-ended tasks in Minecraft. Generative latent variable models offer another\nsolution, using label-free demonstrations to train plan-conditioned policies (Ajay et al., 2020; Lynch\net al., 2020b). Extending this, Cai et al. (2023b) applied a posterior encoder to interpret reference\nvideos in Minecraft. Policy learning with weak supervision remains less explored. Lynch & Sermanet\n(2020) proposed a shared latent space conditioned on language and HER-generated goal images,\nwhile Jang et al. (2022) replaced goal images with video labels under full supervision. Jain et al.\n(2024) trained robots using human videos as task representations but required extensive paired\nvideo-trajectory data. Myers et al. (2023) combined labeled and unlabeled trajectories, aligning\nstart-goal pairs with language via contrastive learning, effective for Table Manipulation but limited in\nhandling complex tasks or generalizing to partially observable environments like Minecraft.\n6. Conclusions, Limitations and Future Works\nThis paper investigates the joint learning of a latent intention space and a multimodal instruction-\nfollowing policy under weak supervision. We identify the “latent space ambiguity” issue in latent\nvariable generative models when handling text-free trajectory data, arising from the absence of\ndirect human guidance in shaping the latent space. To address this, we propose a weakly supervised\nalgorithm for training GROOT-2. Evaluations across four diverse environments, from video games\nto robotic manipulation, demonstrate GROOT-2 ’s generality and flexibility in following multimodal\ninstructions. However, GROOT-2 ’s reliance on trajectory data for training limits its applicability to\nvideo data, which lacks action labels. Considering the abundance and diversity of video data available\nonline compared to trajectory data, extending the weak supervision framework to leverage both play\nand trajectory data would be a promising avenue for future work.\n12\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\n```\n#### 2. 论文摘要\n```\nDeveloping agents that can follow multimodal instructions remains a\nfundamental challenge in robotics and AI. Although large-scale pre-training on\nunlabeled datasets (no language instruction) has enabled agents to learn\ndiverse behaviors, these agents often struggle with following instructions.\nWhile augmenting the dataset with instruction labels can mitigate this issue,\nacquiring such high-quality annotations at scale is impractical. To address\nthis issue, we frame the problem as a semi-supervised learning task and\nintroduce GROOT-2, a multimodal instructable agent trained using a novel\napproach that combines weak supervision with latent variable models. Our method\nconsists of two key components: constrained self-imitating, which utilizes\nlarge amounts of unlabeled demonstrations to enable the policy to learn diverse\nbehaviors, and human intention alignment, which uses a smaller set of labeled\ndemonstrations to ensure the latent space reflects human intentions. GROOT-2's\neffectiveness is validated across four diverse environments, ranging from video\ngames to robotic manipulation, demonstrating its robust multimodal\ninstruction-following capabilities.\n```\n\n#### 3. 论文全文\n```\nGROOT-2: Weakly Supervised Multi-Modal\nInstruction Following Agents\nShaofei Cai†1, Bowei Zhang†1, Zihao Wang1, Haowei Lin1, Xiaojian Ma3, Anji Liu2 and Yitao Liang B1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis\nDeveloping agents that can follow multimodal instructions remains a fundamental challenge in robotics\nand AI. Although large-scale pre-training on unlabeled datasets (no language instruction) has enabled\nagents to learn diverse behaviors, these agents often struggle with following instructions. While augment-\ning the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at\nscale is impractical. To address this issue, we frame the problem as a semi-supervised learning task and\nintroduce GROOT-2, a multimodal instructable agent trained using a novel approach that combines\nweak supervision with latent variable models. Our method consists of two key components: constrained\nself-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn\ndiverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations\nto ensure the latent space reflects human intentions. GROOT-2’s effectiveness is validated across four\ndiverse environments, ranging from video games to robotic manipulation, demonstrating its robust\nmultimodal instruction-following capabilities.\nFigure 1 | By feeding a mixture of demonstrations and some multimodal labels, we learn GROOT-2, a human-\naligned agent capable of understanding multimodal instructions and adaptable to various environments, ranging\nfrom video games to robot manipulation, including Atari, Minecraft, Language Table, and Simpler Env.\n1. Introduction\nDeveloping policies that can follow multimodal instructions to solve open-ended tasks in open-world\nenvironments is a long-standing challenge in robotics and AI research. With the advancement of\nlarge-scale pretraining (Baker et al., 2022; Brohan et al., 2022; Brown et al., 2020), the research\nparadigm for instruction-following policies has shifted from reinforcement learning to supervised\nlearning. In a supervised learning approach, researchers collect large amounts of demonstration data\nand annotate each demonstration with multimodal instructions—such as videos (Duan et al., 2017;\nJang et al., 2022), texts (Lynch et al., 2023; Padalkar et al., 2023), and episode returns (Chen et al.,\n2021)—using hindsight relabeling. In theory, the instruction-following capability of such policies\nCorresponding author(s): Yitao Liang\n† indicates equal contribution\nShaofei Cai<caishaofei@stu.pku.edu.cn>, Bowei Zhang<zhangbowei@stu.pku.edu.cn>, Zihao Wang<zhwang@stu.pku.edu.cn>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Anji Liu<liuanji@cs.ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2412.10410v1  [cs.AI]  7 Dec 2024\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nMechanical Imitation\n𝐵𝐶\n𝐵𝐶+ 𝐾𝐿\nPosterior Collapse\n→→↑… →×\n↑ ←←… ←\n←↑←… ←\n→↑ →… →×\n…\n…\nNaïve Action Copying\nRough Trajectory\nBehavior Semantic\nCondition Agnostic\n0\n1\nFigure 2 | The ELBO Objective of the VAE and Latent Space Spectrum. We define a spectrum based on\n𝑅=\n𝐵𝐶\n𝐵𝐶+𝐾𝐿, where 𝑅= 0 corresponds to “mechanical imitation” and 𝑅= 1 to “posterior collapse.” At low 𝑅,\nlatent vector 𝑧directly outputs action sequences without considering observations ( 𝐵𝐶→0 ). As 𝑅increases,\n𝑧represents high-level task information, such as specific object interactions. At 𝑅= 1, 𝑧provides no beneficial\ninformation for decision-making.\nimproves as the dataset grows. However, annotating demonstrations with high-quality multimodal\nlabels is prohibitively expensive, making it challenging to scale these methods in practice.\nAnother line of work (Ajay et al., 2020; Cai et al., 2023b; Lynch et al., 2020c) avoids the need\nfor additional human annotations by learning from demonstration-only data in a self-supervised\nmanner. These approaches leverage latent variable generative models (Kingma & Welling, 2013) to\njointly learn an encoder and a latent-conditioned policy. The resulting policy is capable of completing\nmultiple tasks specified by a reference video (Cai et al., 2023b). While a reference video is generally\nexpressive enough to represent various tasks, the inherent ambiguity in videos can lead to a learned\nlatent space that is misaligned with human intention. For example, the encoder module may capture\nthe dynamics between adjacent frames in a video, thereby learning a latent representation of the\naction sequence—a process we refer to as “mechanical imitation.” While this latent space accurately\nreconstructs the target action sequence, the resulting latent representation is difficult for human users\nto leverage during policy deployment. Another potential issue is “posterior collapse,” where the latent\nspace collapses to a single point and loses its influence over the policy during inference. We attribute\nthis mismatch between training and inference to the absence of direct supervision for aligning the\nlatent space with human intention. As illustrated in Figure 2, an ideal controllable latent-induced\npolicy space must strike a balance between these two extremes.\nWe present GROOT-2 (refer to Figure 1), a multimodal instructable agent developed using a latent\nvariable model under weak supervision. To unify the training pipeline, we encode instructions from all\nmodalities as distributions over the latent space. The training objectives consist of two key components:\n(1) constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable\nthe latent-conditioned policy to learn diverse behaviors; and (2) human intention alignment, which\nuses relatively small sets of multimodal labels to align the latent space with human intentions.\nSpecifically, we apply the maximum log-likelihood method in the latent space for alignment. The\nunderlying principle is that the latent embedding encoded by multimodal labels should also be sampled\nfrom the distribution learned from the corresponding video. Our approach is both general and flexible,\nas demonstrated through evaluations across four diverse environments—ranging from video games\nto robotic manipulation—including Atari Games (Bellemare et al., 2013), Minecraft (Johnson et al.,\n2016), Language Table Lynch et al. (2023), and Simpler Env (Li et al., 2024). These experiments\nhighlight GROOT-2 ’s robust ability to follow multimodal instructions, with extensive tests showing\nthat scaling up either unlabeled or labeled demonstrations further enhances performance.\n2\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\n2. Background and Problems\n2.1. Latent Variable Models Enable Controllable Behavior Generation\nIn recent years, the GPT series (Brown et al., 2020; Radford, 2018; Radford et al., 2019) has\ndemonstrated impressive capabilities in controllable text generation. Its success can be attributed to\nself-supervised pretraining and the advantageous properties of natural language. A natural language\nparagraph contains rich dependencies between sentences. For instance, the title of an article sets the\ncentral theme for its body, and the response in a question-answer or dialogue is highly correlated\nwith the preceding text. This characteristic enables large language models, trained via next-token\nprediction, to achieve controllable text generation through prompts during inference. Unfortunately,\nsuch strong correlations do not exist between low-level actions. A desired behavior may not have\na necessary preceding trajectory segment. Thus, it isn’t easy to prompt a pre-trained policy model\nto generate a desired behavior. Instead, the generation of actions depends on an underlying latent\nintention variable. A natural approach is to employ latent variable generative models to jointly model\ntrajectory data and the latent variables that drive them, allowing for controllable behavior generation\nby manipulating the latent variables during inference. Next, we will elaborate on how latent variable\nmodels model trajectory data.\nAs a classic latent variable generative model, Variational Autoencoder (VAE, Kingma & Welling\n(2013)) has been widely used in fields such as image generation and text generation. With the\ndevelopment of the offline pretraining paradigm, recent years have seen an increasing number of\nworks utilizing VAE to model trajectory data. Typically, its architectures consist of three components:\na posterior encoder, a prior encoder, and a policy decoder. The posterior encoder, 𝑞(𝑧|𝜏), encodes a\nspecific behavioral trajectory 𝜏= (o1:𝑁, a1:𝑁) and generates a posterior distribution over the latent\nspace. When the action sequence can be accurately inferred from the observation sequence (Baker et al.,\n2022; Zhang et al., 2022)—i.e., when the inverse dynamics model of the environment 𝑝IDM(a1:𝑁|o1:𝑁)\nis easily learned—the action sequence can be excluded from the posterior’s input (Cai et al., 2023b),\nthus reducing the distribution condition to o1:𝑁. The prior encoder, 𝑝(𝑧|o1:𝑘), generates a distribution\nover the latent space based on the history of observations, where 𝑘denotes the length of the observation\nwindow. When 𝑘= 0, the prior distribution is independent of historical observations and is typically\nassumed to follow a standard normal distribution N (0; 1). The decoder, 𝜋(a𝑡|o1:𝑡, 𝑧), is generally a\nlatent-conditioned policy that takes in the environment’s observations along with a specific latent\nvariable to predict the next action to be executed. According to variational inference theory, we can\noptimize the VAE’s modeling capabilities by maximizing the Evidence Lower Bound (ELBO)\nLELBO = 𝔼𝑧∼𝑞(𝑧|o1:𝑁)\n\" 𝑁\n∑︁\n𝑡=𝑘\n−log 𝜋(a𝑡|o1:𝑡, 𝑧)\n#\n+ 𝐷KL(𝑞(𝑧|o1:𝑁) ∥𝑝(𝑧|o≤𝑘)).\n(1)\nThere are generally three main objectives for using VAE to model trajectory data: (1) Modeling\nmultimodal behaviors (Lynch et al., 2020a; Mees et al., 2022): For instance, when trajectory data\nis collected from different individuals, the variations in action sequences across different behavior\nmodes can be substantial. Directly applying a naive behavior cloning algorithm may result in poor\nmodeling performance. Introducing an additional latent variable to differentiate between behavior\nmodes can help mitigate conflicts between them during training. (2) Skill discovery (Gupta et al.,\n2019; Xu et al., 2023): Complex trajectory data is often composed of various skills. A VAE can\nabstract action sequences in a self-supervised manner, enabling skill reuse in downstream tasks, such\nas accelerating the exploration process in reinforcement learning (Ajay et al., 2020; Pertsch et al.,\n2021). (3) Following reference videos to complete open-ended tasks (also known as one-shot\ndemonstration learning, Cai et al. (2023b)): This approach aims to leverage the learned posterior\n3\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 3 | Comparison of Policies with Different Latent Spaces. The reference video depicts digging for\ndiamonds. A policy that mechanically imitates the trajectory falls into lava, while one aligned with human\nintention avoids lava and successfully reaches the diamonds.\nencoder to recognize the underlying intention behind a reference video and encode it as a latent,\nwhich can then drive a policy to complete the specified task in a novel deployment. It points to a way\nto pre-train instruction-following policies using unlabeled trajectory data. We primarily focus on the\nthird objective in the following paragraphs.\n2.2. Modeling Behaviors with VAE Leads to Ambiguous Latent Space\nSeveral studies on VAE (Abeer et al., 2024; Alemi et al., 2018) have pointed out that the Pareto\nfrontier of the ELBO contains an infinite number of solutions for the latent space, a phenomenon we\nrefer to as latent space ambiguity. To facilitate understanding, we provide an informal illustration in\nFigure 2, which shows several possible latent spaces when a VAE is used to model behaviors, all having\nsimilar ELBO values. We differentiate these latent spaces using the ratio 𝑅=\n𝐵𝐶\n𝐵𝐶+𝐾𝐿, where 𝑅= 0\nand 𝑅= 1 represent two extremes of the latent space. When 𝑅approaches 0, the latent condition\ncontains much information, nearly dictating every action of the policy’s behavior. We refer to this as\nmechanical imitation, where the VAE effectively degenerates into an Autoencoder (AE). Conversely,\nwhen 𝑅approaches 1, the latent loses its ability to control the policy’s output, a phenomenon known\nas posterior collapse (Fang et al., 2019; Pagnoni et al., 2018), in which the VAE reduces to an\nAuto-regressive (AR) model. Intuitively, as 𝑅increases, the information encoded in the latent space\nbecomes more high-level, and the policy relies more on environmental feedback (observations) to\nmake decisions that align with the dataset’s distribution. On the other hand, when 𝑅is smaller, the\npolicy tends to down-weight the environment’s observations.\nNot all latent spaces effectively support following a reference video. As shown in Figure 3, the\ngap between the environment state in the reference video and during policy deployment requires\nthe posterior encoder to extract intentions independent of environmental dynamics. For instance, in\nthe Minecraft task “mining a diamond underground,” a reference video may show a player walking\nforward and mining a diamond. If the latent encodes only the trajectory sketch, the policy might fail\nby colliding with obstacles in the deployment environment. This mismatch occurs because humans\ninterpret the video as “mining the diamond” rather than copying specific actions. Aligning the latent\nspace with human intentions is critical for improving policy steerability.\n4\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nEncoder\nPolicy\nEncoder\n𝑜!:#\n𝔼!∼#(!|&!:#)[−log 𝑒(𝑧|𝑜(:*) ]\nEncoder\nEncoder\n𝐷+,(𝑒𝑧𝑜(:* ∥𝑒(𝑧|𝑜())\nHuman Intention Alignment\nConstrained Self-Imitating\nImage Token\nText \/ Returns Token\nAction Token\nSampled Condition Latent\nCondition Distribution\nStop Gradient Operation\nPast Observations\n𝑤!:$\n𝑜!\n𝑜!:#\n𝑜%\n𝑧&\n𝑜!:%'!\n𝑧(\nPolicy\n𝑎%\n𝑜%\n𝑜!:%'!\n−log 𝜋𝑎- 𝑜(:-, 𝑧.)\n𝑎%\n−log 𝜋𝑎- 𝑜(:-, 𝑧&)\nFigure 4 | Pipeline for Constructing a Training Batch for GROOT-2. Each batch includes two sample\ntypes: (1) demonstration-only samples for learning a latent-conditioned policy (Constrained Self-Imitating);\nand (2) labeled samples (text or expected returns) for aligning the latent space with human intentions (Human\nIntention Alignment). The sample ratio varies by dataset distribution.\n3. Aligning Policy Learners with Weak Supervision\nWe explore the development of instructable agents based on latent variable models. To avoid “latent\nspace ambiguity”, we introduce human intention knowledge into the generative pretraining process\nof the policy model to assist in shaping the latent space. As multimodal labels associated with\ndemonstrations carry rich human intention details, we propose a weakly supervised policy learning\nalgorithm to leverage large amounts of unlabeled demonstration data to learn the latent space while\nusing a small amount of multimodal labeled data to align the latent space with human intention.\nUltimately, this enables instructions from all modalities to be unified within the same latent space.\nNext, we will elaborate on the dataset collection, training pipeline, and inference procedure.\nDataset Collection. We can collect two types of training data from the web: a large set of unlabeled\ndemonstrations Ddem = {(o1:𝑁, a1:𝑁)} and a relatively small set of annotated demonstrations Dlab =\n{(o1:𝑁, a1:𝑁, w1:𝑀)}, where o is the image observation provided by the environment, a is the action\ntaken by the policy, w is the word token, 𝑁is the length of a demonstration, 𝑀is the length of an\nannotation sentence. The annotation sentence can be multimodal, such as a language sentence (with\n𝑀≥1) or a scaler of the episode return (with 𝑀= 1), which explains the behavior or outcome of the\ndemonstration from a human’s perspective. Since the annotation data is expensive to collect, we have\n|Dlab| ≪|Ddem|.\nTraining Pipeline. Our goal is to learn a shared latent space Z, per-modal instruction encoders\n𝑒(𝑧|𝑐), and a latent-conditioned policy 𝜋(a𝑡|o≤𝑡, 𝑧). Leveraging past observations is essential for a\npolicy to make decisions in a partially observable environment such as Minecraft (Johnson et al.,\n2016). We call the learned policy model GROOT-2, whose training pipeline is shown in Figure 4. For\nan unlabeled demonstration (o1:𝑁, a1:𝑁), we use the encoder module to produce a prior distribution\n𝑒(𝑧|o1) and a posterior distribution 𝑒(𝑧|o1:𝑁). Using the reparameterization trick (Kingma & Welling,\n2013), we sample the latent 𝑧from the posterior distribution 𝑒(𝑧|o1:𝑁) and train the policy model,\nconditioned on 𝑧and o1:𝑡, to reconstruct the entire action sequence causally. To limit the information\npresented in the latent space, we introduce an auxiliary KL divergence term in the objective:\nLdem(o, a) = 𝔼𝑧∼𝑒(𝑧|o1:𝑁)\n\" 𝑁\n∑︁\n𝑡=1\n−log 𝜋(a𝑡|o1:𝑡, 𝑧)\n#\n+ 𝛽1𝐷KL(𝑒(𝑧|o1:𝑁) ∥𝑒(𝑧|o1)).\n(2)\nThis allows the model to leverage demonstration-only data to enhance the complexity of the la-\ntent space, a process we refer to as “constrained self-imitating.” For a labeled demonstration\n5\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 5 | Diverse visual environments used in the experiments. We test our GROOT-2 on both video games\n(simple Atari games and the complex Minecraft game) and robotic manipulation environments (Language\nTable and Simpler Env). Minecraft is a partially observable open-ended environment, while others are fully\nobservable.\n(o1:𝑁, a1:𝑁, w1:𝑀), we pass the label w1:𝑀through the encoder module to obtain a latent distri-\nbution and train the policy model to reconstruct the action sequence based on the latent 𝑧sampled\nfrom this distribution 𝑒(𝑧|w1:𝑀). This allows human knowledge to be modeled in the latent space.\nFurther, to make the encoder understand demonstration o1:𝑁just like humans, we introduce an auxil-\niary MLE term: maximize the log-likelihood of 𝑒(𝑧|o1:𝑁) given the latent 𝑧sampled from 𝑒(𝑧|w1:𝑀).\nUnlike the prior behavior cloning term, the aligning term can be quickly calculated in closed form.\nThis process is referred to as “human intention alignment”:\nLlab(o, a, w) = 𝔼𝑧∼𝑒(𝑧|w1:𝑀)\n\" 𝑁\n∑︁\n𝑡=1\n−log 𝜋(a𝑡|o1:𝑡, 𝑧)\n#\n−𝛽2𝔼𝑧∼sg[𝑒(𝑧|w1:𝑀)] [log 𝑒(𝑧|o1:𝑁)] ,\n(3)\nwhere sg[·] denotes stop gradient operation. The MLE-based alignment objective ensures that the\nlatent sampled from the label-conditioned distribution 𝑒(𝑧|𝑤1:𝑀) can also be sampled from its video-\nconditioned distribution 𝑒(𝑧|𝑜1:𝑁). The final loss function combines the two objectives:\nL(Ddem, Dlab) = 𝔼(o,a)∼Ddem [Ldem(o, a)] + 𝔼(o,a,w)∼Dlab [Llab(o, a, w)] .\n(4)\nSpecific implementation details, such as the model design choice, can be found in the Appendix A.\nInference Procedure. GROOT-2 supports two types of instructions during inference: (1) visual-based\ninstruction – the user can either retrieve a demonstration from the dataset as a reference video or\nmanually record a reference video to serve as the condition for the policy; (2) label-based instruction\n– the user can input a text sentence or specify an expected return as the condition (depending on the\nlabel modality used during the model’s training). We tested them in the following experiments.\n4. Capabilities and Analysis\nWe aim to address the following questions: (1) How does GROOT-2 perform in open-world video\ngames and robotic manipulation? (2) Can GROOT-2 follow instructions beyond language and video?\n(3) What insights can be gained from visualizing the learned latent space? (4) How does GROOT-2\nscale with labeled and unlabeled trajectories? (5) What is the impact of backbone initialization on\nperformance? (6) How do language and video losses influence performance?\nEnvironment and Benchmarks. We conduct experiments across four types of representative envi-\nronments: classical 2D game-playing benchmarks on Atari (Bellemare et al., 2013), 3D open-world\ngameplaying benchmarks on Minecraft (Johnson et al., 2016; Lin et al., 2023), and Robotics bench-\nmarks on Language Table simulator (Lynch et al., 2023) and Simpler Env simulator(Li et al., 2024),\n6\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nTable 1 | Results on the Open-World Minecraft Benchmark. This benchmark includes 8 task families and\n100 tasks. Each task is evaluated 30 times across three seeds, and the average success rate is calculated per\ntask family. For example, Combat (16) indicates 16 tasks in the Combat family.\nMethods\nPrompt\nCombat\nHunt\nRide\nBreed\nCraft\nMine\nInteract\nPlant\n(16)\n(10)\n(4)\n(8)\n(20)\n(20)\n(10)\n(12)\nVPT\nN\/A\n11±3\n20±4\n7±2\n2±0\n4±1\n7±2\n21±6\n22±7\nSTEVE-1\nlang\n12±3\n9±2\n54±8\n4±2\n5±2\n6±3\n53±9\n33±8\nSTEVE-1\nvisual\n15±4\n10±3\n38±9\n6±2\n6±2\n10±4\n40±8\n43±7\nGROOT-1\nvisual\n18±5\n28±8\n26±6\n12±3\n15±4\n22±7\n57±8\n75±6\nGROOT-2\nlang\n40±7\n43±5\n46±6\n22±6\n18±3\n37±5\n55±4\n75±9\nGROOT-2\nvisual\n37±4\n48±7\n51±4\n20±4\n27±3\n36±7\n63±6\n77±7\nTable 2 | Results on the Language Table Benchmark. We reported success rates (in %) within 200 steps for\neach instruction modality, averaging over 250 rollouts. Results are averaged over 3 seeds with mean and stderr.\n“-” indicates missing data. The percentages in parentheses indicate the proportion of labels used.\nTask Family\nBC-Zero\nLAVA\nRT-1\nGROOT-1\nGROOT-2 (50%)\nGROOT-2 (100%)\nlang\nlang\nlang\nvisual\nlang\nvisual\nlang\nvisual\nblock to block\n-\n90±2\n-\n8±2\n84±9\n78±9\n86±8\n82±7\nblock to absolute loc\n-\n72±4\n-\n10±3\n70±8\n68±8\n76±6\n70±8\nblock to block relative loc\n-\n72±3\n-\n4±1\n74±9\n64±7\n76±8\n62±6\nblock to relative loc\n-\n64±4\n-\n8±2\n82±5\n78±6\n84±6\n80±4\nseparate two blocks\n-\n94±2\n-\n12±2\n98±1\n96±2\n98±0\n98±0\nOverall\n72±3\n78±4\n74±13\n8±2\n82±8\n76±7\n84±6\n78±8\nillustrated in Figure 5. These four simulators are used to evaluate whether GROOT-2 can be effec-\ntively steered by returns (Chen et al., 2021; Mnih et al., 2015), reference videos (Cai et al., 2023b;\nJang et al., 2022), and textual instructions (Brohan et al., 2022, 2023).\nResults on the Open-World Minecraft Benchmark. To evaluate policy models in Minecraft, we\nused the contractor dataset from Baker et al. (2022), containing 160M frames. According to the\nmeta information, labeled trajectories account for approximately 35% of the total data. We extended\nthe Minecraft SkillForge Benchmark (Cai et al., 2023b) from 30 to 100 tasks, grouped into eight\nfamilies: Combat, Hunt, Ride, Breed, Craft, Mine, Interact, and Plant. Details are in the Appendix C.\nWe compared GROOT-2 with three baselines: (1) VPT (Baker et al., 2022), a foundational model\ntrained on YouTube data via imitation learning, lacking instruction-following; (2) STEVE-1 (Lifshitz\net al., 2023), which supports text and future image-conditioned instructions; and (3) GROOT-1 (Cai\net al., 2023b), a self-supervised model using reference videos as instructions. Key findings from\nTable 1 are as follows: (1) GROOT-2 (visual) consistently outperforms GROOT-1 across all task\ncategories, with particularly notable gains in mob interaction tasks like Combat and Hunt. Comparing\ntrajectories on Hunt, GROOT-1 mechanically repeats “attack” actions, while GROOT-2 actively\ntracks objects, showing that text data enhances object-centric understanding and better aligns with\nhuman intentions. (2) GROOT-2 (text) performs similarly to GROOT-2 (visual) across most tasks,\ndemonstrating that language and visual modalities share task knowledge. This enables the model to\nleverage both modalities for improved task completion. This highlights the advantage of combining\nmultimodal data for better alignment with human intentions and improved policy performance.\nResults on the Language Table benchmark. To assess GROOT-2’s multimodal instruction following\n7\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nTable 3 | Results on the Simpler Env Benchmark. We report the success rate (in %) of the video-instruction\nand language-instruction following for each model on 3 task families. The percentages in parentheses indicate\nthe proportion of labels used.\nMethods\nPrompt\nPick Coke Can\nMove Near\nOpen\/Close Drawer\nH-Pose\nV-Pose\nS-Pose\nAvg\nAvg\nOpen\nClose\nAvg\nRT-1-X\nlang\n57\n20\n70\n49\n32\n7\n52\n29\nOcto-base\nlang\n5\n0\n1\n1\n3\n0\n2\n1\nGROOT-2 (50%)\nvisual\n42\n18\n52\n37\n35\n29\n30\n30\nlang\n52\n20\n50\n41\n42\n27\n33\n30\nGROOT-2 (100%)\nvisual\n40\n22\n47\n36\n35\n27\n33\n30\nlang\n53\n23\n52\n42\n45\n27\n35\n31\ncapabilities in the context of Robotic Table Manipulation, we utilized the Google Language Table as our\ntesting platform and compared it with methods such as LAVA (Lynch et al., 2023), RT-1 (Brohan et al.,\n2022), GROOT-1 (Cai et al., 2023b). We utilize a dataset provided by Lynch et al. (2023) comprising\n100M trajectories. We removed the text labels from half of the trajectories in the dataset, creating a\n1 : 1 ratio of labeled to unlabeled trajectories. Given that the Language Table environment comes with\nfive task families, all of which are instructed solely through language, we curated five reference videos\nfor each task with relatively clear intentions to evaluate the model’s ability to comprehend video\ninstructions. Detailed specifics are provided in the appendix D. The experimental results are shown\nin the Table 2. We observed that: (1) GROOT-2 leads by an absolute success rate of 4% following\ntext-based instructions compared, likely due to GROOT-2’s more refined model architecture design.\nWe mark the results of RT-2 in gray here, as it uses significantly more training data than ours. (2) The\nperformance of GROOT-2 in following video instructions dropped by approximately 6% compared\nto text instructions, possibly due to the ambiguity of the reference videos, where a “block to block”\ntype video could be interpreted as a “block to relative location” type task. (3) GROOT-1 struggled to\nunderstand the intentions conveyed by the reference videos. We observed that GROOT-1 imitated a\nreference video’s trajectory sketch rather than their colors and shapes. This further underscores the\nimportance of introducing language annotations for some trajectory data as a crucial method to align\nwith human intentions.\nResults on the Simpler Env Benchmark. We utilized the Simpler Env (Li et al., 2024) simulation\nof the Google Robot environment to evaluate the policy’s capability in controlling complex robotic\narms. GROOT-2 is trained on the OpenX dataset (Collaboration et al., 2023). We erased the text\nlabels from half of the dataset’s trajectories, achieving a 1:1 balance between labeled and unlabeled\ndata. We evaluated three types of tasks: Pick Coke Can, Move Near, and Open\/Close Drawer. Following\nBrohan et al. (2023); Li et al. (2024), we set up multiple variants for each task. For instance, the\nPick Coke Can task involved three different poses for the Coke can; in the Move Near task, the layout\nand types of objects varied; and in the Open\/Close Drawer task, the drawer had three layers from\ntop to bottom. We compared GROOT-2 with baseline methods such as RT-1 (Brohan et al., 2022),\nand Octo (Octo Model Team et al., 2024). Among these, RT-1-X is an efficient language-conditioned\ntransformer-based policy trained on the entire OpenX (Collaboration et al., 2023) dataset, which\ncan be considered the performance boundary that GROOT-2 can achieve. As shown in Table 3, we\nfound that GROOT-2 (lang) and GROOT-2 (visual) achieved comparable performance to the RT-1-X\nmodel across all three tasks. This indicates that our method retains language control capabilities and\nimbues the policy with equivalent visual instruction control abilities.\nCan GROOT-2 Follow Instructions Beyond Language and Video, Like Episode Returns?\nWe evaluated GROOT-2 ’s steerability and performance on four Atari games (Breakout, Demon\n8\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 6 | Comparison of Weakly Supervised (WSL) and Self-Supervised (SSL) Learning on 4 Atari Games.\nPolicies are evaluated under return and reference video conditions. For return conditioning, normalized returns\nare input into the encoder, while for video conditioning, videos with similar returns (error < 0.05) are used.\nFigure 7 | t-SNE Visualization of Learned Latent Spaces on Four Atari Games. The first row shows\nresults under self-supervised learning, while the second row displays GROOT-2 ’s performance under weakly\nsupervised learning. Points represent reference videos, with shapes indicating games and colors denoting\nepisode returns. The first four columns compare individual games, and the last column shows a mixed-game\ncomparison.\nAttack, Hero, and Name This Game). Datasets from Agarwal et al. (2020), containing approximately\n10M frames per game, were used. Episode returns were normalized to 𝜇= 0, 𝜎= 1.\nFor training, we constructed a dataset with 30% labeled trajectories (returns) and 70% unlabeled\ndata. Using this dataset, we trained GROOT (wsl) (weakly supervised learning). For comparison,\nGROOT (ssl) was trained on the same dataset without return labels in a fully self-supervised manner.\nBoth models were jointly trained across the four games. During inference, we evaluated policy perfor-\nmance in following reference videos sampled from the test set with normalized returns {−1, 0, +1},\nusing 20 samples per category. Results (Figure 6) show: (1) GROOT (ssl) can recognize behavioral\nquality in reference videos, constructing a rough intention space even without labeled guidance. (2)\nLabeled data significantly improved GROOT (wsl)’s ability to understand video instructions, with the\ngreatest gains in Hero and Name This Game. We also evaluated GROOT (wsl) on return-style instruc-\ntions with normalized rewards {−1, 0, +1}. The similarity between video and reward-conditioned\nperformance suggests the video encoder and reward encoder share the same intention space. The\nAtari experiments aim to evaluate GROOT-2 ’s performance on modalities beyond language and\nvideo, rather than maximizing scores, distinguishing it from traditional offline RL methods.\nWhat Does the Visualization of the Learned Latent Space Reveal?\n9\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 8 | Performance Comparison on Unlabeled Demonstrations. Human-normalized task scores are\naveraged over 20 rollouts across 5 Minecraft tasks to evaluate the agent’s reference-video following ability.\nWe applied t-SNE to visualize embeddings from randomly sampled reference videos. Each point\nin Figure 7 represents a unique video, with shapes denoting game environments and colors indicating\nepisode returns. The first row illustrates results for GROOT (ssl), where videos in Breakout, Demon\nAttack, and Name This Game are classified into two categories based on episode return magnitudes,\nsuggesting that the self-supervised algorithm distinguishes only significant score differences. In\ncontrast, GROOT (ssl) shows poor clustering and limited steerability in the Hero game. The second\nrow shows results for GROOT (wsl), which captures continuous variations in video behavior quality\nacross all games. As shown in the fifth column, embeddings from different environments follow\na continuous pattern aligned with reward labels, indicating a shared latent space that promotes\ncross-environment knowledge transfer.\nHow Does Scaling Up Unlabeled Trajectories Impact Performance?\nWe trained four GROOT-2 variants with 0%, 25%, 50%, and 100% unlabeled data in Minecraft.\nPerformance was tested on five Minecraft tasks (Chop Tree, Hunt Animals, Combat Enemies, Open\nChest, Climb Mountain) and scored relative to skilled human players. For example, if a human\ncollects 20.0 logs in 600 steps and GROOT-2 collects 15.0, the score is 0.75. Results (Figure 8) show\nconsistent improvement with more unlabeled data, with the 100% variant achieving a 5× gain in\nthe Climb Mountain task over the 0% version. It is worth noting that the Climb Mountain and Open\nChest tasks do not have language instructions in the training set.\nHow Does Scaling Up Labeled Trajectories Impact Performance?\nFigure 9 | Ablation Study on Labeled\nTrajectories in the Language Table.\nTo evaluate the impact of labeled trajectory proportions\nin the training set on the instruction-following capabilities of\nGROOT-2, we conducted experiments on the Language Table\nbenchmark. The total number of trajectories remained con-\nstant across different dataset configurations, with only the pro-\nportion of trajectories containing text labels varying. Figure 9\nreports the success rate achieved by GROOT-2 conditioned\non language. At low labeled data proportions (0%−25%), the\nsuccess rate rapidly increased from 10% to 65%, indicating\nthat labeled data significantly influences model performance. However, as the labeled data proportion\nincreased to 50% −80%, the success rate plateaued, rising slightly from 82% to 83%, demonstrating\ndiminishing marginal gains from additional labeled data. Therefore, under resource constraints, a\nlabeled data proportion of 50% may represent the optimal balance between performance and cost.\nHow Does Backbone Initialization Affect Performance?\nWe evaluated different initializations for ViT (random, ImageNet, CLIP) and BERT (random,\nBERT, CLIP) on the Language Table Benchmark. For randomly initialized models, both backbones\n10\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nwere unfrozen during training. According to Table 4, CLIP initialization yielded the best results\nfor ViT, followed by ImageNet, with minimal difference between them, while random initialization\nperformed worst. For BERT, CLIP and standard BERT initialization performed similarly, both surpassing\nrandom initialization. Initializing vision and language encoders with CLIP parameters improves policy\nperformance and reduces training time.\nBackbone\nViT\nViT\nViT\/BERT BERT BERT\nWeights\n-\nImageNet\nCLIP\n-\nBERT\nSR (in %) 76±10\n80±11\n82±8\n79±12 81±8\nTable 4 | Ablation study on the backbone initialization.\nVariants\n−Llab baseline −Ldem baseline\nPrompt\nvision\nvision\nlang\nlang\nSR (in %) 10±2\n76±7\n12±3\n82±8\nTable 5 | Ablation on Llab and Ldem objectives.\nHow Does Language and Video Losses Impact Performance?\nThe Llab loss significantly enhances the model’s understanding of reference videos, as observed\nin the Language Table environment. We compared a variant without Llab loss to the full GROOT-2\nmodel, both trained on the same scale of the Language Table dataset, and tested their ability to follow\nreference videos using standard evaluation scripts. As shown in Table 5, the variant without Llab\nloss failed to complete any tasks. Further analysis of its output videos revealed that it mechanically\nmimicked the arm movement trajectories in the reference videos, completely ignoring object colors\nand shapes, which is inconsistent with human understanding of the reference videos.\nThe Ldem loss is indispensable in the GROOT-2 architecture. Removing Ldem causes the pipeline\nto degrade into an autoencoder when processing unlabeled data. Without constraints on the latent\nencoding, the model tends to learn the video encoder as an inverse dynamics model, encoding\nlow-level action sequences in latent z instead of high-level task information, thereby significantly\nreducing the behavior cloning loss. Additionally, Table 5 show that removing Ldem causes the language\nencoder’s latent z to collapse, leading to a dramatic drop in task success rates.\n5. Related Works\nLearning Policies Across Diverse Domains. Developing policies for sequential control tasks in\nreal and virtual environments poses significant challenges. Research spans domains such as robotic\nmanipulation (Lynch et al., 2023; Yu et al., 2019), video games (Bellemare et al., 2013; Guss et al.,\n2019), and embodied navigation (Hong et al., 2020; Huang et al., 2023; Savva et al., 2019), with\napproaches categorized into reinforcement learning (RL) and imitation learning (IL) based on reward\nfunction reliance. For video games with dense rewards (e.g., ALE platform (Bellemare et al., 2013)),\nonline RL algorithms can achieve superhuman performance (Badia et al., 2020; Mnih et al., 2015) but\nsuffer from low efficiency, risky interactions, and limited generalization. These challenges restrict their\napplicability to physical (Padalkar et al., 2023) or embodied environments (Guss et al., 2019), where\nrewards and cheap interactions are unavailable. IL, as a supervised learning paradigm, addresses\nthese issues through batch efficiency and scalability with large datasets, leveraging Transformer\narchitectures (Jang et al., 2022; Pashevich et al., 2021; Zhang & Chai, 2021). The RT-X series (Brohan\net al., 2022, 2023; Padalkar et al., 2023) advances robotic manipulation by training Transformers\non large expert demonstration datasets, achieving strong zero-shot generalization. Similarly, Baker\net al. (2022) developed a Transformer-based policy for Minecraft using internet-scale gameplay data,\nsolving the diamond challenge. Building on this, Schmidhuber (2019) frames RL as supervised\nlearning, while Chen et al. (2021); Lee et al. (2022) introduce “decision transformers” to model joint\ndistributions of rewards, states, and actions from offline data, highlighting the potential for unified\npolicy learning within Transformers.\nLearning Policies to Follow Instructions. Enabling policies to follow instructions is key to building\n11\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\ngeneral-purpose agents. A common approach involves using language annotations from offline\ndemonstrations to train language-conditioned policies (Abramson et al., 2020; Brohan et al., 2022;\nCai et al., 2023a; Huang et al., 2023; Raad et al., 2024; Reed et al., 2022; Wang et al., 2023a,b),\nleveraging the compositionality of natural language for generalization. However, obtaining high-\nquality annotations is costly. An alternative uses anticipated outcomes as instructions. Majumdar\net al. (2022) trained an image-goal conditioned navigation policy via hindsight relabeling (HER)\n(Andrychowicz et al., 2017) and aligned goal spaces with text. Similarly, Lifshitz et al. (2023) used\nthis strategy for open-ended tasks in Minecraft. Generative latent variable models offer another\nsolution, using label-free demonstrations to train plan-conditioned policies (Ajay et al., 2020; Lynch\net al., 2020b). Extending this, Cai et al. (2023b) applied a posterior encoder to interpret reference\nvideos in Minecraft. Policy learning with weak supervision remains less explored. Lynch & Sermanet\n(2020) proposed a shared latent space conditioned on language and HER-generated goal images,\nwhile Jang et al. (2022) replaced goal images with video labels under full supervision. Jain et al.\n(2024) trained robots using human videos as task representations but required extensive paired\nvideo-trajectory data. Myers et al. (2023) combined labeled and unlabeled trajectories, aligning\nstart-goal pairs with language via contrastive learning, effective for Table Manipulation but limited in\nhandling complex tasks or generalizing to partially observable environments like Minecraft.\n6. Conclusions, Limitations and Future Works\nThis paper investigates the joint learning of a latent intention space and a multimodal instruction-\nfollowing policy under weak supervision. We identify the “latent space ambiguity” issue in latent\nvariable generative models when handling text-free trajectory data, arising from the absence of\ndirect human guidance in shaping the latent space. To address this, we propose a weakly supervised\nalgorithm for training GROOT-2. Evaluations across four diverse environments, from video games\nto robotic manipulation, demonstrate GROOT-2 ’s generality and flexibility in following multimodal\ninstructions. However, GROOT-2 ’s reliance on trajectory data for training limits its applicability to\nvideo data, which lacks action labels. Considering the abundance and diversity of video data available\nonline compared to trajectory data, extending the weak supervision framework to leverage both play\nand trajectory data would be a promising avenue for future work.\n12\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | GROOT-2：弱监督多模态指令跟随代理\n\n## 📌 背景痛点\/本文动机\n在机器人学和人工智能领域，开发能够遵循多模态指令的代理仍然是一个基本挑战。尽管在大规模无标签数据集上进行预训练（没有语言指令）已经使代理能够学习多样化的行为，但这些代理在遵循指令时往往遇到困难。虽然通过增加数据集的指令标签可以缓解这个问题，但在大规模上获取高质量注释是不切实际的。为了解决这个问题，我们将问题框架化为半监督学习任务，并引入了GROOT-2，这是一个多模态可指令代理，使用一种结合弱监督和潜在变量模型的新方法进行训练。\n\n## 🚀 核心方法\n💡 创新点1：约束自我模仿\n利用大量未标记的演示来使策略能够学习多样化的行为。\n\n💡 创新点2：人类意图对齐\n使用较小的一组标记演示来确保潜在空间反映人类意图。\n\n## 📈 实验结果\nGROOT-2的有效性在四个不同的环境中得到验证，从视频游戏到机器人操作，展示了其强大的多模态指令跟随能力。\n\n## 💬 可借鉴之处\nGROOT-2在多模态指令跟随方面具有强大的能力，可以应用于各种环境，包括视频游戏和机器人操作。此外，GROOT-2的训练方法可以应用于其他需要弱监督学习的任务。","llm_summary_res_status":200}
{"title":"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents","authors":"Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang","summary":"We investigate the challenge of task planning for multi-task embodied agents\nin open-world environments. Two main difficulties are identified: 1) executing\nplans in an open-world environment (e.g., Minecraft) necessitates accurate and\nmulti-step reasoning due to the long-term nature of tasks, and 2) as vanilla\nplanners do not consider how easy the current agent can achieve a given\nsub-task when ordering parallel sub-goals within a complicated plan, the\nresulting plan could be inefficient or even infeasible. To this end, we propose\n\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and\n$\\underline{S}$elect\" ($\\textbf{DEPS}$), an interactive planning approach based\non Large Language Models (LLMs). DEPS facilitates better error correction on\ninitial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of\nthe plan execution process and providing self-$\\textit{explanation}$ of\nfeedback when encountering failures during the extended planning phases.\nFurthermore, it includes a goal $\\textit{selector}$, which is a trainable\nmodule that ranks parallel candidate sub-goals based on the estimated steps of\ncompletion, consequently refining the initial plan. Our experiments mark the\nmilestone of the first zero-shot multi-task agent that can robustly accomplish\n70+ Minecraft tasks and nearly double the overall performances. Further testing\nreveals our method's general effectiveness in popularly adopted non-open-ended\ndomains as well (i.e., ALFWorld and tabletop manipulation). The ablation and\nexploratory studies detail how our design beats the counterparts and provide a\npromising update on the $\\texttt{ObtainDiamond}$ grand challenge with our\napproach. The code is released at https:\/\/github.com\/CraftJarvis\/MC-Planner.","url":"http:\/\/arxiv.org\/abs\/2302.01560v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2302.01560v3","published":1675404387000,"comment":"NeurIPS 2023","pdf_text":"Describe, Explain, Plan and Select:\nInteractive Planning with Large Language Models\nEnables Open-World Multi-Task Agents\nZihao Wang1,2, Shaofei Cai1,2, Guanzhou Chen3, Anji Liu4, Xiaojian Ma4, Yitao Liang1,5∗\nTeam CraftJarvis\n1Institute for Artificial Intelligence, Peking University\n2School of Intelligence Science and Technology, Peking University\n3School of Computer Science, Beijing University of Posts and Telecommunications\n4Computer Science Department, University of California, Los Angeles\n5Beijing Institute for General Artificial Intelligence (BIGAI)\n{zhwang,caishaofei}@stu.pku.edu.cn,rayment@bupt.edu.cn\nliuanji@cs.ucla.edu,xiaojian.ma@ucla.edu,yitaol@pku.edu.cn\nAbstract\nWe investigate the challenge of task planning for multi-task embodied agents in\nopen-world environments.2 Two main difficulties are identified: 1) executing\nplans in an open-world environment (e.g., Minecraft) necessitates accurate and\nmulti-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners\ndo not consider how easy the current agent can achieve a given sub-task when\nordering parallel sub-goals within a complicated plan, the resulting plan could be\ninefficient or even infeasible. To this end, we propose “Describe, Explain, Plan\nand Select” (DEPS), an interactive planning approach based on Large Language\nModels (LLMs). DEPS facilitates better error correction on initial LLM-generated\nplan by integrating description of the plan execution process and providing self-\nexplanation of feedback when encountering failures during the extended planning\nphases. Furthermore, it includes a goal selector, which is a trainable module that\nranks parallel candidate sub-goals based on the estimated steps of completion,\nconsequently refining the initial plan. Our experiments mark the milestone of the\nfirst zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks\nand nearly double the overall performances. Further testing reveals our method’s\ngeneral effectiveness in popularly adopted non-open-ended domains as well (i.e.,\nALFWorld and tabletop manipulation). The ablation and exploratory studies detail\nhow our design beats the counterparts and provide a promising update on the\nObtainDiamond grand challenge with our approach. The code is released at\nhttps:\/\/github.com\/CraftJarvis\/MC-Planner.\n1\nIntroduction\nDeveloping multi-task agents that can accomplish a vast and diverse suite of tasks in complex domains\nhas been viewed as one of the key milestones towards generally capable artificial intelligence [36, 1,\n5, 10, 25]. To enable such capabilities, earlier works have suggested employing a hierarchical goal\nexecution architecture [2, 4], where a planner generates action plans that would then be executed by\nlow-level goal-conditioned controllers. This architecture has been delivering promising progress in\n∗Corresponding Author.\n2We borrow the term “open world” from the game community. It highlights that the agent can navigate inside\na diverse environment and accomplish open-ended tasks freely.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2302.01560v3  [cs.AI]  8 Jul 2024\nReachable region \nwithin 3 mins\nUnachievable \nroutes\nOptional routes\nChallenge #2: State-dependent Task Feasibility\nChallenge #1: Complex Sub-task Dependency\nMine diamond in Minecraft environment\n99%\n42%\n23%\n80%\n9%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1\n2\n3\nPlanner w\/ Learned Controller\nPlanner w\/ Oracle Controller\nTabletop Environment\nALFWorld\nMinecraft\nSuccess Rate\nManipulation in Tabletop environment\nPlanning success plummet in open worlds due to new challenges\nFigure 1: Planning success rates plummet in open worlds due to new challenges.\nmany robotics domains, including table-top and mobile manipulation [46, 4], 2D shape drawing [20]\nand table rearrangement [17]. However, whether such success can be transferred to a more open-ended\nworld with unlimited exploration areas and internet-scale knowledge remains open [14, 10, 13, 12, 19].\nTo understand the gap, we run Inner Monologue[17], a general and competitive hierarchical goal\nexecution model on a typical open-world domain Minecraft [18, 14, 10] and two classical robotic en-\nvironments ALFWorld [41] and Tabletop environments [40, 4]. The algorithm uses a Large Language\nModel (LLM) based planner that contains domain-specific knowledge for all three environments. In\nall environments, we use either an Oracle goal-conditioned controller or a learned one. Results are\nshown in the bar plot in Figure 1. First, even when the Oracle controller is used, the success rate of\nexecuting Minecraft tasks is much less than that of the other environments. Next, the task failure rate\nbecomes even higher in Minecraft when the learned controller is substituted. Both failures originate\nfrom unique challenges brought by open-world environments, which we identify in the following.\nFirst, compared to canonical environments (e.g., Atari [29] and robotic control suite [40]), open\nworlds have highly abundant object types with complex dependency and relation. As a result, ground-\ntruth plans typically involve a long sequence of sub-goals with strict dependencies. As Figure 1\nchallenge #1 suggests, it requires at least 13 sub-goals executed in proper order to obtain a diamond\nin Minecraft, while in Tabletop a task is typically no more than a few consecutive sub-goals.\nAnother challenge brought by the complicated tasks in an open-ended world is the feasibility of the\nproduced plans. Consider the example shown in Figure 1 (challenge #2). To craft a bed in Minecraft,\nthe fastest way is by either slaughtering a sheep to obtain wool, which can be used to craft beds, or\ncollecting beds from a village. However, since no sheep or village is reachable by the agent within 3\nminutes of gameplay, to craft a bed efficiently, the agent should choose to slaughter a spider and use\nmaterials (e.g., string) it drops to craft wool, and then a bed. That is, when dealing with a task that\ncan be completed by executing multiple possible sequences of sub-goals, the planner should be able\nto select the best route based on the current state of the agent. However, the complex and diverse\nstate distribution of open-world environments makes state awareness hard to achieve.\nTo tackle these problems, we propose “Describe, Explain, Plan and Select” (DEPS), an interactive\nplanning approach based on Large Language Models (LLMs) to alleviate the aforementioned issues.\nThe key to tackling the first challenge is to effectively adjust the generated plan upon failure.\nSpecifically, whenever the controller fails to complete a sub-goal, a descriptor will summarize the\ncurrent situation as text and send it back to the LLM-based planner. We then prompt the LLM\nas an explainer to locate the errors in the previous plan. Finally, a planner will refine the plan\nusing information from the descriptor and explainer. To improve the feasibility of generated plans\nconditioned on the current state, which is the second identified challenge, we use a learned goal-\nselector to choose the most accessible sub-task based on the proximity to each candidate sub-goal.\nOur experiments are conducted on 71 tasks in open-ended Minecraft without any demonstration.\nGiven the goal-conditioned controller for atom sub-tasks (i.e., mine log and mine stone), our zero-\n2\n(Re-)Planner\nLLM*\nController\nGoal-conditioned Policy\nSelector\nHPM\nDescriptor\nVLM\nExplainer\nLLM*\nInstruction\nplan 𝑃!\ngoal 𝑔!\nfeedback\naction\nobs\ndescription 𝑑!\nexplain\nEnvironment\nobs\nTask instruction: Obtain a diamond\nin Minecraft survival mode step-by-step?\nCandidate goals:\nSelected Goal 𝒈𝟏:\n× 4\nThe agent locates in the birch forest,\nwhich only has birch wood.\nDescription 𝒅𝒕: I succeed on goal 1-5. I\nfail on goal 6, mining 3\nwith\n.\nNow my inventory has 5 planks, …\nInitial Plan 𝑷𝟎:\n4\n16\n1\n8\n3\n3\n1\n1\n1\nExplanation: Because mining       needs to\nuse at least\n, which I do not have.\nSo I need to craft\nfirst.\n…\nFinished Plan  𝑃!: \n3\n3\n1\n1\n1\n1\nSelected Goal\n𝑔$\n× 3\nno other choices\nDescription 𝒅𝒕: I succeed on goal …. I \nfail on smelting 3\nfrom 3\n, on\n.\nMy inventory now has 3 iron ore, …\nUpdated Plan 𝑷𝒕: \n3\n1\n3\n1\n1\n3\nExplanation: Because smelting       needs\nto use\nand\n, which I do not have.\nSo I need to craft\nfirst.\nTask Finished !\nAgent\nEnv\n𝑎%\n𝑠%\nMine acacia wood\nCraft acacia planks\nCraft crafting table\nCraft wood pickaxe\nCraft stick\nMine cobblestone\nCraft stone pickaxe\nMine diamond\nMine coal\nMine iron ore\nCraft furnace\nSmelt iron ingot\nCraft iron pickaxe\nMine birch wood\nMine oak wood\nCraft birch planks\nCraft oak planks\nFigure 2: Overview of our proposed interactive planner architecture.\nshot3 LLM-based planner can finish all tasks within a limited horizon (3000-12000 steps for different\ntasks). We find DEPS outperforms all language planner baselines by nearly doubling the overall\nsuccess rate, with the same initial state and goal-conditioned controller. Our ablation and exploratory\nstudies then explain how our approach beats the counterparts and becomes the first planning-based\nagent that accomplishes the challenging ObtainDiamond task. DEPS does not require any planning\ntraining for the environment. Additionally, DEPS achieves between on-par and more than 50% relative\nimprovement over existing or concurrent LLM-based planning methods on non-open-ended robotics\ndomains such as ALFWorld [41] and Tabletop environments [40].\n2\nBackground\nWe aim to develop an agent capable of solving long-horizon goal-reaching tasks using image\nobservations and language goals. To accomplish this, we propose a combined approach involving\ngoal-conditioned policies (termed controllers) and a planner. The goal-conditioned policies are\ntrained to complete sub-goals, while the planner decomposes long-horizon tasks into a series of\nK short-horizon sub-goals, g1, . . . , gK, to be executed by the controller. At each time step t, the\ngoal-conditioned policy π(at | st, gk) generates an action at based on the current state st and the\nspecified sub-goal gk.\nPlanning with Large Language Models\nPrevious works have shown that LLMs such as Instruct-\nGPT [32] and Codex [8] can be used as zero-shot planners to generate sub-goal sequences for various\ntasks in embodied environments [16, 42]. Formally, given the task description T as prompt p, LLM\nacts as a planner to decode T into K sub-goals, g1, . . . , gK, which are then executed one by one by\nthe low-level controller π(at | st, gk) to accomplish the task.\nHowever, the above pipeline suffers from both challenges identified in Section 1. Regarding the first\nchallenge, the probability of generating a flawless plan directly from the task description decreases\nsignificantly as the required number of sub-goals increases. Moreover, even when the LLM generates\na correct plan, it is very likely that the plan is highly inefficient given the agent’s current state\n(challenge #2). Prior works mostly focus on solving the first challenge by providing environmental\nfeedback to the LLM through affordance functions [4], success detector [20] or scene descriptor [17].\nHowever, although these approaches work well on many non-open-ended domains, they still suffer\nfrom high failure rates in open-world environments.\n3Similar to [5, 16], “zero-shot” here means no gradient updates are performed. However we provide some\nrelated demonstrations as prompts during inference time.\n3\n3\nTowards Reliable Planning in Embodied Open-World Environments\nIn this section, we first give an overview of our proposed interactive planning framework “Descibe,\nExplain, Plan, and Select” (DEPS) for solving complex and long-horizon tasks in open-world\nenvironments (Sec. 3.1). Next, in Section 3.2, we elaborate how DEPS iteratively refines its plan\nto combat the first identified challenge. Section 3.3 introduces the selector module that is used to\nidentify efficient plans in response to the second identified challenge.\n3.1\nDEPS Overview\nAs demonstrated in Figure 2, our agent (DEPS) consists of an event-triggered Descriptor, a Large\nLanguage Model (LLM) as Explainer and Planner, a goal Selector based on horizon prediction and\na goal-conditioned controller. In the following, we use Minecraft as a running example to better\nelaborate our agent. Note that DEPS can be directly applied to other (non-)open-ended tasks.\nWe take a large language model (LLM) as a zero-shot planner of the agent to complete tasks. Given\na goal command (e.g., ObtainDiamond) as task T, the LLM-based planner decomposes this\nhigh-level task into a sequence of sub-goals {g1, . . . , gK}, as the initial plan P0. The goals are\ninstructions in natural language, such as mine oak wood\n(in Minecraft), find two cups (in\nALFWorld), put block A on top of block B (in Tabletop Manipulation).\nAs described in Section 2, a controller is then invoked to execute the provided sub-goals sequentially\nthrough a goal-conditioned policy π(a | s, g). However, the initial plan provided by the planner often\ncontains errors, which results in execution failures of the controller. For example, the goal\ncan\nnot be finished only with a wooden pickaxe\nas shown in Figure 2. When failure pops up, the\ndescriptor will summarize the current state st and execution outcome of the most recent goal into text\ndt and send it to the LLM. The LLM will first try to locate the errors in the previous plan Pt−1 by\nself-explanation, e.g., the goal\nneed to be executed with a stone pickaxe\n. Then it will re-plan\nthe current task T and generate a revised plan Pt according to the explanation. In this process, the\nLLM is also treated as an explainer in addition to the planner role. The Descriptor, Explainer, and\nPlanner will be detailed in Section 3.2.\nDescription : dt = fDESC(st−1),\nExplanation : et = fEX(dt),\nPrompt : pt = CONCAT(pt−1, dt, et),\nPlan : Pt = fLM(pt),\nGoal : gt ∼fS(Pt, st−1),\nAction : at ∼π(at | st−1, gt)\n(1)\nAs shown in Equation (1), DEPS will iteratively update the plan Pt until the task is finished, where\nfDESC is the descriptor model, fLM denotes the language model as explainer and planner, fS is the\nselector model, π is goal-conditioned policies from the controller.\nTo filter out inefficient plans, the selector is trained to predict the number of time steps remaining to\nachieve every goal gk in a set of parallel goals given the current state st. When the generated plan\ncontains alternative routes, the selector uses this information to choose a suitable goal as the current\ngoal gt. For example, the horizon predicted by the selector of goal acacia tree\nis less than\ngoal oak tree\nin Savanna biome, which leads to chop acacia tree as current goal gt.\n3.2\nDescribe, Explain and Plan with LLM Generates Executable Plans\nCurrent LLM-based planners usually query the LLM once at the beginning of every episode and use\nthe output plan throughout the episode [16, 42]. However, as demonstrated by Figure 1, such one-shot\nplanning methods often fail on long-horizon tasks that require many sub-goals. This is caused by two\nmajor issues. First, since the correct plan for long-horizon tasks needs to respect various complex\npreconditions, it is extremely hard for the LLM to generate a flawless plan directly from the task\ninstructions, resulting in failure when simply following the initial plan. Additionally, due to the\nunpredictable transition dynamics, some incidents may happen during the execution and make the\ninitial plan non-executable. To remedy these problems, existing methods introduce feedback (e.g.,\n4\nPrompt 1 Planner prompt template, Python-like code\ndef craft_wooden_axe(initial_inventory={}):\n# step 1: mine 3 logs\nmine(obj = {\"log\":3}, tool = None)\n# step 2: craft 12 planks from 3 logs\ncraft(obj = {\"planks\":12}, materials = {\"log\":3}, tool = None)\n# step 3: craft 4 sticks from 2 planks\ncraft(obj = {\"stick\":4}, materials = {\"planks\":2}, tool = None)\n# step 4: craft 1 crafting_table from 4 planks\ncraft(obj = {\"crafting_table\":1}, materials = {\"planks\":4}, tool = None)\n# step 5: craft 1 wooden_axe from 3 planks and 2 sticks on crafting table\ncraft(obj = {\"wooden_axe\":1}, {\"planks\": 3, \"stick\": 2}, tool = \"crafting_table\")\nreturn \"wooden_axe\"\nfrom success detector or scene descriptor) to reflect on the results of previous executions [17, 20, 4].\nHowever, merely informing the LLM whether a sub-goal is completed is often insufficient to correct\nthe planning error.\nTo remedy this, we propose “describe, explain and plan”, a new interactive planning method to\ngenerate more executable and explainable plans. We start with rewriting the prompt into an interactive\ndialogue format as in ChatGPT [32] so that subsequent feedback can be passed to the LLM effectively.\nThe produced plan is also augmented with the preconditions and effects of each goal. The structured\nprompt improves the readability and interpretability of the plan and facilitates error-locating when\nthe execution fails later, as demonstrated in Prompt 1.\nThe descriptor will then collect the feedback generated by the agent during the execution of the\ntask. The feedback can be practically obtained either by a person (human feedback [4]), or by a\npre-trained vision-language model CLIP [35]. While the previous type of feedback needs intensive\nhuman involvement, the latter from the pre-trained model needs to be fine-tuned for the specific\ndomain, which decreases the automation and generalization of the agent. On the contrary, Minecraft\nreturns the ‘info’ and other high-level observations (such as biome, GPS, and compass), we can easily\ntranslate the unstructured information into structured language. Therefore we take the symbolic\ninformation available in the game and translate it into feedback description dt in this work. To avoid\ncarrying unrelated information in the prompt, we further distill plan-related messages (e.g., inventory\ninformation, biome) as final event-level description dt as demonstrated in Figure 2.\nNotably, we also treat the LLM as an explainer to explain why the previous plans Pt−1 failed.\nSpecifically, by analyzing the current state from description dt and precondition of current goal gt,\nthe explainer can identify the reason why the current goal cannot be executed successfully. As shown\nin Figure 2, the reason may be the current goal requires the use of an iron pickaxe, but the tool is\nnot prepared in advance, or the current goal requires the use of 3 planks, but the currently available\nplanks are not enough. To implement this, we provide few-shot demonstrations to the LLM as in\nchain-of-thoughts prompting [45], as shown in Prompt 1. Finally, the LLM goes back to its role as a\nplanner and re-plans the task with the explicit explanation of existing bugs in the previous plan Pt−1,\nultimately generating an updated plan Pt according to the explanation.\n3.3\nHorizon-Predictive Selector Yields Efficient Plans\nDue to the abundance of objects and the compositional nature of their functionalities, there often exist\nmultiple feasible plans to complete a task, i.e., there are usually multiple paths for the completion of a\nparticular goal. However, despite the feasibility of all such plans, most of them are highly inefficient\nto execute in the current episode. For example, as shown in Figure 2, obtaining a wood can be done\nby chopping oak trees\n, birch trees\n, or acacia trees\n. But only oak trees are available in the\nplains biome. So the planner needs to choose oak trees since it is more efficient, as the agent does\nnot need to travel to another biome.\nOn the other hand, there is no strict sequential requirement for some goals in the plan Pt, i.e.,\ngi, gj ∼Pt enjoy the same precondition, which means gi and gj can be executed in any order. As\nshown in Figure 1, the choice of different paths (sequences) may affect the execution efficiency of\nthe plan Pt as one goal might be closer to the agent. Always choosing the closer goal to execute first\ncould yield more efficient plans and improve the final success rate under a limited episode length.\nMoreover, the dynamic nature of open-world environments further amplifies the impact of efficient\n5\nGoal: Meat*3\nCandidate Skill: Kill Sheep\nOR Cow OR Pig\nSelection: Kill Sheep\nExplanation: Meet sheep first.\nGoal: Log*2\nCandidate Skill: Chop Oak\nOR Birch OR Acacia Tree\nSelection: Chop Acacia Tree\nExplanation: Savanna biome\nonly has Acacia tree.\nGoal: Coal*1 AND Iron_Ore*1\nCandidate Skill: Mine Coal AND\nIron_Ore\nSelection: Mine Iron_Ore\nExplanation: Meet iron_ore first.\nGoal: Survive in Night.\nCandidate Skill: Sleep in bed\nOR Dig down.\nSelection: Sleep_in_bed\nExplanation: Village has beds.\nFigure 3: Selection Demonstration from “Selector”. Given parallel sub-goals, i.e. candidate skills, our Selector\nwill determine the sequence in which to carry out these sub-goals based on their current proximity to the agent\nand modify the original plan produced by the LM planner.\nplans on the success rate. For example, in Minecraft, if the agent chooses to execute a further goal\nlike collect wood first, the much closer target sheep may disappear and be hard to find again.\nIn order to improve the efficiency of our plans, we propose to use a selector that selects the most\nefficient path with the highest execution success rate as the final plan. Specifically, we design a\nstate-aware selector to choose the nearest goal under state st as the current goal gt from the candidate\ngoal sets in plan Pt. It predicts the goal distribution p(gt|st, Pt) under the current state st and plan\nPt, where gt ∈Gt, Gt describes all current executable goals in Pt. A straight way to implement\nthe selector is to leverage the semantic similarity between the current state and the goal text using\na vision-language model (VLM) such as CLIP [35]. Nevertheless, this may not exactly reflect the\ndifficulty of completing the goal since VLM lacks practical experience. For example, an “oak tree” in\nfront of the agent could lead to high semantic similarity for the “chopping tree” goal, but it may be\ninefficient to achieve this goal if a canyon is in the middle between the agent and the oak tree.\nTo mitigate this, we implement a horizon-predictive selector that embeds practical task experience to\naccurately rank the goals based on their efficiency and feasibility. Here, we define the horizon of a\ngoal ht(g) := Tg −t as the remaining time steps to complete the given goal, where Tg is the time of\ncompleting goal g. This metric accurately reflects how quickly we can achieve the given goal from\nthe current state. To estimate the horizon, we learn a neural network µ to fit the offline trajectories by\nminimizing the entropy loss −log µ(ht(g) | st, g), where ht is the ground-truth horizon in trajectories\nof completing goal g. Therefore, the goal distribution can be formulated as follows:\nf(gt | st, Pt) =\nexp(−µ(gt, st))\nP\ng∈Gt exp(−µ(g, st)).\n(2)\nWe set goal-sensitive Impala CNN [6] as the backbone of the selector. In practice, the horizon predic-\ntive selector can be jointly trained with the controller policies and share the backbone parameters [6].\n4\nExperiments\nThis section analyzes and evaluates our proposed “describe, explain, plan, and select\" (DEPS) method.\nTo minimize performance variation caused by the low-level controller, we standardize all experiments\nwith one controller learned by behavior cloning. We refer to the details of this controller in Appendix\nC. In Section 4.1, we introduce our testing environments and our evaluation task set, consisting of the\nhardest 71 tasks from MCU SkillForgeChain [22]. In Section 4.2, we report our performance in the\ncontext of existing LLM-based planners. Ablation studies are conducted in Section 4.3. Finally, we\npay close attention to the hardest task, ObtainDiamond, which is long-hailed as a major challenge\nin the community. The experiments on ALFWorld and Tabletop Manipulation environments are\nshown in Appendix A.\n4.1\nExperimental Setup\nEnvironment and Task Setting\nWe first evaluate our proposed method in Minecraft, a popular\nopen-world environment with both challenges discussed in Section 1. For better reflecting the\nperformance of DEPS, we choose three Minecraft environments with different versions for better\n6\nevaluation, including Minedojo [10] with Minecraft 1.11.2, MineRL [3] with Minecraft 1.16.5, and\nMC-TextWorld [22] with Minecraft 1.19.2. Rules and items have something different in the above\nthree Minecraft environments, which can better evaluate the dynamic and interactive planning abilities\nof DEPS.\nTable 1: Attributes of 8 meta tasks covering Task101: We evaluate the algorithm on Minecraft Task101. We\ngroup the consisted 71 task into 8 different meta groups, with each focusing on testing a different aspect of our\nproposed method.\nMeta\nName\nNumber\nExample Task\nMax. Steps\nInitial Inventory\nGiven Tool\nMT1\nBasic\n14\nMake a wooden door.\n3000\nEmpty\nAxe\nMT2\nTool (Simple)\n12\nMake a stone pickaxe.\n3000\nEmpty\nAxe\nMT3\nHunt and Food\n7\nCook the beef.\n6000\nEmpty\nAxe\nMT4\nDig-Down\n6\nMine coal.\n3000\nEmpty\nAxe\nMT5\nEquipment\n9\nEquip the leather helmet.\n6000\nEmpty\nAxe\nMT6\nTool (Complex)\n7\nMake shears and bucket.\n6000\nEmpty\nAxe\nMT7\nIronStage\n13\nObtain an iron sword.\n6000\nEmpty\nAxe\nMT8\nChallenge\n1\nObtain a diamond!\n12000\nEmpty\nAxe\nWe choose 71 tasks from the Minecraft Universe Benchmark SkillForgeChain [22] for evaluation.\nThese tasks are related to items that can be obtained in the Minecraft overworld. To better present the\nresults, we divide the 71 Minecraft tasks into 8 meta groups according to the ingredients and function\nof the tasks, i.e., MT1-MT8. The instruction for every task is written in natural language, e.g., make\na wooden door in MT1 (Basic group) and obtain a diamond in MT8 (Challenge group),\nas illustrated in Table 1. Considering how long it typically takes human players to complete each\ntask as a ballpark [14], we set different maximum episode steps for different meta tasks from 3000\n(for easiest Basic tasks) to 12000 (for the hardest Challenge tasks). The names, number of required\nskills, and functions of all tasks are listed in Appendix B. We give an empty inventory for every task\nin Survival mode and require the agent to obtain every item from the environment by itself. Note that\nour agent will be summoned in different environments randomly for each evaluation. Biomes and\ninitial positions are also different each time. Following the previous work [18], we take the success\nrate as the evaluation metric.\nBaselines\nWe compare DEPS with other language-based planners, including GPT as Zero-shot\nPlanner(GPT) [16], ProgPrompt(PP) [42], Chain-of-Thought(CoT) [45], Inner Monologue(IM) [17],\nand Code as Policies(CaP) [20]. For all baseline models, we use the same demonstration example\nin the prompt, the same LM model from OpenAI, and the same controller in all tasks for a fair\ncomparison. Since these methods were not originally experimented with Minecraft, we reproduce\nthem to conform to the Minecraft specification based on prompt and feedback template design. All\nplanner methods access the LLM model through OpenAI API (text-davinci-03 model [32]\nfor GPT, CoT, and IM, and code-davinci-02 model [8] for PP, CaP, and Ours). All hyper-\nparameters of LLM (including the temperature and best_of, etc.) are kept as default. We also list the\nfull prompt of all different methods in Appendix G.\n4.2\nMain Results\nEvery task is executed 30 times and the average results in Minedojo [10] for every meta task are listed\nin Table 2. Our approach achieves the best performance with all meta tasks. As the complexity of the\ntask increases from MT1-MT8, the planner usually needs to give more accurate task steps (i.e., longer\ngoal sequence) to achieve the final task. Therefore the success rate of all agents decreases with the\nreasoning steps increasing. Starting from MT6, almost all existing LLM-based planners fail (nearly\n0 success rate). DEP (w\/o Selector) already consistently beats existing LLM-based planners in all\nmeta tasks with a significant margin. This validates that “describe, explain and plan” can estimate the\nreason for current plan failure and correct the original flawed plans. Due to the limited maximum\nepisode length and restricted control success rate for a hard goal (e.g., Mine diamond with\niron_pickaxe), the final success rate is still capped.\n7\nTable 2: Success rates of DEPS and existing LLM planners on Minecraft Task101. The full task-by-task list is in\nAppendix F.\nMethods\nMT1\nMT2\nMT3\nMT4\nMT5\nMT6\nMT7\nMT8\nAVG\nGPT[16, 32]\n25.85±24.8\n47.88±31.5\n10.78±14.6\n7.14±9.0\n1.98±5.9\n0.0±0.0\n0.0±0.0\n0.0±0.0\n15.42\nPP[42]\n30.61±23.6\n40.09±30.6\n17.13±19.1\n16.00±17.3\n3.21±4.9\n0.47±1.3\n0.60±2.2\n0.0±0.0\n16.88\nCoT[45]\n40.24±30.8\n55.21±26.8\n6.82±11.6\n4.76±8.2\n1.73±5.2\n0.0±0.0\n0.0±0.0\n0.0±0.0\n18.89\nIM[17]\n46.89±31.4\n53.73±20.8\n3.64±6.9\n18.41±17.4\n4.57±7.4\n0.64±1.7\n1.02±2.5\n0.0±0.0\n21.64\nCaP[20]\n60.08±17.3\n60.11±20.24\n8.72±9.7\n20.33± 21.0\n2.84±4.6\n0.63±1.3\n0.60±2.2\n0.0±0.0\n25.77\nDEP\n75.70±10.4\n66.13±13.4\n45.69±16.2\n43.35±20.2\n15.93±13.9\n5.71±3.7\n4.60±7.1\n0.50±0.5\n39.36\nDEPS\n79.77±8.5\n79.46±10.6\n62.40±17.9\n53.32±29.3\n29.24±27.3\n13.80±8.0\n12.56±13.3\n0.59±0.5\n48.56\nIn addition, selector also greatly improves the final task success rate of the agent (from DEP w\/o\nSelector to DEPS). Hard meta tasks usually require the completion of multiple sub-goals (up to\ndozens of goals), thus bringing more flexibility and providing more candidate goals for the Selector.\nAt the same time, as the agent conducts experiments with limited episode length, it also places high\ndemands on the efficiency of the plan. Therefore, the Selector brings a significant improvement on\nefficiency-sensitive tasks such as MT7 (up to +2.7 times success rate).\nRobustness on different controller and different Minecraft versions\nWe also evaluate DEPS\non MineRL [3] and MC-Textworld [22]. Note that DEPS is a planning method, which needs to\nequip the goal-conditioned controller for interacting with the Minecraft environments. We choose\nMC-Controller [6] and Steve-1 [21] as controllers to interact with Minedojo and MineRL, respectively.\nThese two methods are all control policies that perceive visual partial observations and produce\nmouse and keyboard actions. While MC-Textworld is a text world, which only keeps the Minecraft\ncrafting recipes and mining rules. So MC-Textworld does not require the controller. The DEPS\nresults of the task set MT1-MT8 on different Minecraft environments are shown in Table 3. The\nresults report that DEPS can generate effective plans in various Minecraft environments. The results\non MC-Textworld [22] also show that the performance drops on more difficult task sets from MT6 to\nMT8 are mainly from the controller limitation.\nTable 3: Success rates of DEPS under different Minecraft environments.\nEnvironment\nVersion\nController\nMT1\nMT2\nMT3\nMT4\nMT5\nMT6\nMT7\nMT8\nMineDojo [10]\n1.11.2\n[6]\n79.77\n79.46\n62.40\n53.32\n29.24\n13.80\n12.56\n0.59\nMineRL [3]\n1.16.5\n[21]\n84.05\n80.32\n24.25\n36.21\n9.16\n17.22\n16.79\n1.84\nMC-Textworld [22]\n1.19.2\n-\n100.00\n90.00\n80.00\n56.25\n64.71\n57.14\n69.57\n50.00\n4.3\nAblation Study\nWe conduct ablation experiments to investigate the number of candidate executable goals for different\nSelector models and the specific impact of the rounds of DEPS.\n4.3.1\nAblation on Selector\nWe verify the robustness of our proposed Selector under different parallel goals. The agent is asked\nto complete 2, 3, and 4 candidate goals (the precondition is consistent for all goals), respectively. The\ngoals of the task correspond to different kinds of mobs or materials.\nWe report the final success rate of our method (DEP) with different selector implementations,\nincluding using a fixed sequence of goals, a random sequence of goals, and selecting a goal based on\nMineCLIP [10], CLIP [35], and our horizon-predictive Selector (HPS). As Figure 4 shows, in one\nround of parallel candidate goals, an improvement of ∆=+22.3%, +29.2%, +32.6% is obtained using\nour horizon-predictive Selector compared to not any selector (i.e., fixed plan), respectively.\nAt a limited episode length, e.g., 1000 steps, goal-model shows a greater advantage, which proves that\ngoal-model can improve the execution efficiency of the plan in embodied environments. In addition,\ncompared to using vision-language models such as CLIP [35] and MineCLIP [10] as a goal model,\nhorizon-predictive has the best performance due to better estimation of the horizon information. The\ncurve trend also demonstrates that agents with Selector scale up under large amounts of goals in an\nopen-world environment.\n8\nMaximum Episode Length\nSuccess Rate\nSelector Model\nHPS\nCLIP\nMineCLIP\nFixed\nRandom\nParallel Goals = 2\nParallel Goals = 3\nParallel Goals = 4\nSuccess Rate\nGoals Setting\nBiome: Plains\n2:\n3:\n4:\nFigure 4: The success rates of DEPS with differ-\nent selectors under varying numbers of parallel\ngoals and maximum episode lengths.\nTable 4: Success Rate of DEPS under different maxi-\nmum rounds of re-planning. Round 0 represents the\nvanilla Planner w\/o the re-planning process. ∞rep-\nresents the re-planning process will not end until task\nsuccess or reaching the maximum horizon, which is still\nlimited by the maximum tokens of LLMs. The maxi-\nmum number of rounds for Codex is around 7-8 rounds.\nRounds\n0\n1\n3\n5\n∞\n∆\n(0 →∞)\nMT1\n28.6\n50.6\n68.1\n79.8\n79.8\n+51.2\nMT2\n37.1\n71.2\n71.4\n79.2\n79.5\n+42.4\nMT3\n15.1\n20.1\n40.3\n40.8\n62.4\n+47.3\nMT4\n15.9\n17.4\n48.3\n50.7\n53.3\n+37.4\nMT5\n3.2\n3.2\n3.2\n15.2\n29.2\n+26.0\nMT6\n0.5\n0.5\n1.1\n1.9\n13.8\n+13.3\nMT7\n0.6\n2.3\n2.9\n2.9\n12.6\n+12.0\nMT8\n0.0\n0.0\n0.0\n0.0\n0.6\n+0.6\n4.3.2\nAblation on Re-Planning Rounds\nWe evaluate our agent on all tasks with increasing maximum rounds of DEPS. The round is defined as\na cycle of interactive LLM-based planning with description, explanation, and planning and selecting,\ni.e., an updated plan. All tasks for every maximum round are executed 30 times and the average\nsuccess rate is reported in Table 4. We take the vanilla LLM planner as the baseline, in which the\nmodel takes the initially generated plan as the final execution plan, without involving any description,\nre-planning, or self-explanation processes during the task execution. Our results in the previous\nsubsection utilize the maximum rounds possible under maximum tokens capped by OpenAI. We\nalso report the success rate increment from vanilla planner to DEPS of every meta task in column\n∆in Table 4. This set of experiments demonstrates that DEPS can iteratively improve its plan in\nopen-world environments. More description, self-explanation, and re-planning rounds produce better\nresults, especially for hard tasks.\n4.4\nObtainDiamond Challenge\nMining diamonds in the open-world game Minecraft, i.e. MT8 in Table 2, has been a long-standing\nchallenge for the community [14]. It is challenging because mining diamonds from scratch in\nMinecraft involves acquiring a sequence of difficult-to-obtain items that require complex planning\non goals like mining, inventory management, crafting with and without a crafting table, tool use,\nsmelting iron ingot in a furnace, and mining at the lowest depths. We take the ObtainDiamond\ntask as a bonus experiment to show the capabilities of our zero-shot planner on complex tasks in\nembodied environments. Previous methods’ success rates on this challenge further vouch for its\ndifficulty. [43, 34] leverages domain-secific reward functions and RL fine-tuning to achieve ∽0.1%\nsuccess rate in 15 minutes of game play. VPT further boosts the success rate to 20% within 20\nminutes of play through pre-training on collects ∽70k hours human demonstrations and finetuning\nwith human-designed reward function [3]. DreamerV3 is trained from scratch to collect diamonds in\na modified Minecraft environment (easier to break blocks) with world models to achieve a success\nrate of 2% [15].\nOur DEPS manages to achieve on-par performance in this grand challenge; our agent achieves a\n0.59% success rate within 10 minutes of gameplay. Note our method does not specifically fine-tune\nfor this challenge. It is designed to be multi-task in its nature. Furthermore, considering our planner\noperates with demonstration prompts on a fixed Large Language Model, it can be straightforwardly\nadapted to other open-ended environments with modifications.\n5\nRelated Works\nTask planning with LLMs\nThere have been some methods leveraging the large language model\nto generate action plans for high-level tasks in embodied environments [46, 9, 11]. [16] decompose\nnatural language commands into sequences of executable actions by text completion and semantic\n9\ntranslation, while SayCan generates feasible plans for robots by jointly decoding an LLM weighted\nby skill affordances from value functions [4]. For better executing the plan in embodied environments,\nsome methods use an object detector describing the initial environment into the language prompt to\nproduce environment-suitable plans and adopt success detectors to check that each step is executed\nsuccessfully [17, 20]. [42] and [20] use the pythonic-style prompt to produce more executable plans.\nHowever, all of the above methods assume that the initial plan from the LLM is correct. When there\nare bugs in the initial plan, it’s difficult for the agent to finish the task successfully.\nInteractive Planning with LLMs\nInner Monologue [17] pilots the front of interactive planning\nwith LLMs, which introduces the feedback (including success detection and scene description) to\nthe planner. However, we found it could still suffer from accumulative planning error, especially in\nlong-horizon open-world tasks. Rather, our “Describe, Explain, Plan and Select” (DEPS) method can\nproduce more reliable plans by leveraging chain-of-thought thinking and explanation to locate the\nerrors in previous plans. Moreover, we also propose a goal Selector to further improve the efficiency\nof the plan, thereby yielding much better performances. Readers are encouraged to refer to the\ncomparative results in Section 4.2 between DEPS and these prior arts. There are also some concurrent\nworks on planning with LLMs [39, 26, 23, 33, 47].\nAgents in Minecraft\nSome previous works have employed the hierarchical architecture to solve\nlong-horizon tasks in Minecraft [30, 27, 24]. Recently, based on the internet-scale corpus, [10]\npre-trains a language-conditioned reward function and learns multi-task MineAgent. [3] collects a\nvast amount of human demonstrations to train a behavior cloning agent. More recently, [15] utilized a\nlearned world model to distill a policy that can efficiently explore in Minecraft. There are also some\nworks focus on learning goal-conditioned policies for better instruction-following [6, 7, 21]. While\nthese efforts all focus on improving the low-level controller. Rather, the planner in our architecture\nemphasizes applying domain knowledge to propose and arrange the sub-goals. It significantly\ninfluences the complexity and breadth of tasks that the agent can handle. Moreover, our planner is\nzero-shot, making it possible to generalize to other long-horizon open worlds.\n6\nLimitations\nAlbeit the impressive results of our approach, we believe there are at least two major limitations within\nour approach. First of all, our framework relies on privately-held LLMs like GPT-3 and ChatGPT,\nwhich makes it less accessible to those who cannot afford or access the service. However, we’re fully\ncommitted to ensuring a more democratized method and will explore using open-sourced models\nincluding OPT [48] and BLOOM [38]. Another issue is the explicit step-by-step planning in our\nsystem. Although it brings us superior performances over the baselines, the planning bottleneck can\nalso prevent our model from being further scaled up. A more appealing approach will be amortizing\nthe planning within an end-to-end trainable goal-conditioned policy, which is worth exploring next.\nFurthermore, some previous fundamental challenges in planning (e.g., dead ends) may not prevalent\nin our adopted environments and hence could be inadvertently overlooked by our paper. We are\ndedicated to addressing more fundamental challenges present in building a multi-task generalist agent\nin our series of following work.\n7\nConclusion\nWe investigate the problem of planning in open worlds. We identify two major challenges unique to\nthese environments: 1) long-term planning requires precise and multi-step reasoning, and 2) planning\nefficiency could be compromised since canonical planners do not take the agent’s proximity to parallel\ngoals\/subtasks into consideration. We propose “Describe, Explain, Plan and Select” (DEPS), an\ninteractive approach based on Large Language Models (LLMs) to tackle them both. Our experiments\nin the challenging Minecraft domain verify the advantages of our approach over counterparts by\nmarking the milestone of robustly accomplishing 70+ Minecraft tasks and nearly doubling the overall\nperformances. DEPS also is the first planning-based agent that can reach the diamond in this game.\n10\nAcknowledgements\nThis work is funded in part by the National Key R&D Program of China #2022ZD0160301, a\ngrant from CCF-Tencent Rhino-Bird Open Research Fund, NSF grants #IIS-1943641, #IIS-1956441,\n#CCF-1837129, an SRA from Meta and a research gift from Amazon Alexa AI, and a gift from\nRelationalAI. We thank Dai Zhixiang from NVIDIA and Xu Hongming from BIGAI on training\nLLMs and infrastructure supports, respectively.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nDescribe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\n```\n#### 2. 论文摘要\n```\nWe investigate the challenge of task planning for multi-task embodied agents\nin open-world environments. Two main difficulties are identified: 1) executing\nplans in an open-world environment (e.g., Minecraft) necessitates accurate and\nmulti-step reasoning due to the long-term nature of tasks, and 2) as vanilla\nplanners do not consider how easy the current agent can achieve a given\nsub-task when ordering parallel sub-goals within a complicated plan, the\nresulting plan could be inefficient or even infeasible. To this end, we propose\n\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and\n$\\underline{S}$elect\" ($\\textbf{DEPS}$), an interactive planning approach based\non Large Language Models (LLMs). DEPS facilitates better error correction on\ninitial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of\nthe plan execution process and providing self-$\\textit{explanation}$ of\nfeedback when encountering failures during the extended planning phases.\nFurthermore, it includes a goal $\\textit{selector}$, which is a trainable\nmodule that ranks parallel candidate sub-goals based on the estimated steps of\ncompletion, consequently refining the initial plan. Our experiments mark the\nmilestone of the first zero-shot multi-task agent that can robustly accomplish\n70+ Minecraft tasks and nearly double the overall performances. Further testing\nreveals our method's general effectiveness in popularly adopted non-open-ended\ndomains as well (i.e., ALFWorld and tabletop manipulation). The ablation and\nexploratory studies detail how our design beats the counterparts and provide a\npromising update on the $\\texttt{ObtainDiamond}$ grand challenge with our\napproach. The code is released at https:\/\/github.com\/CraftJarvis\/MC-Planner.\n```\n\n#### 3. 论文全文\n```\nDescribe, Explain, Plan and Select:\nInteractive Planning with Large Language Models\nEnables Open-World Multi-Task Agents\nZihao Wang1,2, Shaofei Cai1,2, Guanzhou Chen3, Anji Liu4, Xiaojian Ma4, Yitao Liang1,5∗\nTeam CraftJarvis\n1Institute for Artificial Intelligence, Peking University\n2School of Intelligence Science and Technology, Peking University\n3School of Computer Science, Beijing University of Posts and Telecommunications\n4Computer Science Department, University of California, Los Angeles\n5Beijing Institute for General Artificial Intelligence (BIGAI)\n{zhwang,caishaofei}@stu.pku.edu.cn,rayment@bupt.edu.cn\nliuanji@cs.ucla.edu,xiaojian.ma@ucla.edu,yitaol@pku.edu.cn\nAbstract\nWe investigate the challenge of task planning for multi-task embodied agents in\nopen-world environments.2 Two main difficulties are identified: 1) executing\nplans in an open-world environment (e.g., Minecraft) necessitates accurate and\nmulti-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners\ndo not consider how easy the current agent can achieve a given sub-task when\nordering parallel sub-goals within a complicated plan, the resulting plan could be\ninefficient or even infeasible. To this end, we propose “Describe, Explain, Plan\nand Select” (DEPS), an interactive planning approach based on Large Language\nModels (LLMs). DEPS facilitates better error correction on initial LLM-generated\nplan by integrating description of the plan execution process and providing self-\nexplanation of feedback when encountering failures during the extended planning\nphases. Furthermore, it includes a goal selector, which is a trainable module that\nranks parallel candidate sub-goals based on the estimated steps of completion,\nconsequently refining the initial plan. Our experiments mark the milestone of the\nfirst zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks\nand nearly double the overall performances. Further testing reveals our method’s\ngeneral effectiveness in popularly adopted non-open-ended domains as well (i.e.,\nALFWorld and tabletop manipulation). The ablation and exploratory studies detail\nhow our design beats the counterparts and provide a promising update on the\nObtainDiamond grand challenge with our approach. The code is released at\nhttps:\/\/github.com\/CraftJarvis\/MC-Planner.\n1\nIntroduction\nDeveloping multi-task agents that can accomplish a vast and diverse suite of tasks in complex domains\nhas been viewed as one of the key milestones towards generally capable artificial intelligence [36, 1,\n5, 10, 25]. To enable such capabilities, earlier works have suggested employing a hierarchical goal\nexecution architecture [2, 4], where a planner generates action plans that would then be executed by\nlow-level goal-conditioned controllers. This architecture has been delivering promising progress in\n∗Corresponding Author.\n2We borrow the term “open world” from the game community. It highlights that the agent can navigate inside\na diverse environment and accomplish open-ended tasks freely.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2302.01560v3  [cs.AI]  8 Jul 2024\nReachable region \nwithin 3 mins\nUnachievable \nroutes\nOptional routes\nChallenge #2: State-dependent Task Feasibility\nChallenge #1: Complex Sub-task Dependency\nMine diamond in Minecraft environment\n99%\n42%\n23%\n80%\n9%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1\n2\n3\nPlanner w\/ Learned Controller\nPlanner w\/ Oracle Controller\nTabletop Environment\nALFWorld\nMinecraft\nSuccess Rate\nManipulation in Tabletop environment\nPlanning success plummet in open worlds due to new challenges\nFigure 1: Planning success rates plummet in open worlds due to new challenges.\nmany robotics domains, including table-top and mobile manipulation [46, 4], 2D shape drawing [20]\nand table rearrangement [17]. However, whether such success can be transferred to a more open-ended\nworld with unlimited exploration areas and internet-scale knowledge remains open [14, 10, 13, 12, 19].\nTo understand the gap, we run Inner Monologue[17], a general and competitive hierarchical goal\nexecution model on a typical open-world domain Minecraft [18, 14, 10] and two classical robotic en-\nvironments ALFWorld [41] and Tabletop environments [40, 4]. The algorithm uses a Large Language\nModel (LLM) based planner that contains domain-specific knowledge for all three environments. In\nall environments, we use either an Oracle goal-conditioned controller or a learned one. Results are\nshown in the bar plot in Figure 1. First, even when the Oracle controller is used, the success rate of\nexecuting Minecraft tasks is much less than that of the other environments. Next, the task failure rate\nbecomes even higher in Minecraft when the learned controller is substituted. Both failures originate\nfrom unique challenges brought by open-world environments, which we identify in the following.\nFirst, compared to canonical environments (e.g., Atari [29] and robotic control suite [40]), open\nworlds have highly abundant object types with complex dependency and relation. As a result, ground-\ntruth plans typically involve a long sequence of sub-goals with strict dependencies. As Figure 1\nchallenge #1 suggests, it requires at least 13 sub-goals executed in proper order to obtain a diamond\nin Minecraft, while in Tabletop a task is typically no more than a few consecutive sub-goals.\nAnother challenge brought by the complicated tasks in an open-ended world is the feasibility of the\nproduced plans. Consider the example shown in Figure 1 (challenge #2). To craft a bed in Minecraft,\nthe fastest way is by either slaughtering a sheep to obtain wool, which can be used to craft beds, or\ncollecting beds from a village. However, since no sheep or village is reachable by the agent within 3\nminutes of gameplay, to craft a bed efficiently, the agent should choose to slaughter a spider and use\nmaterials (e.g., string) it drops to craft wool, and then a bed. That is, when dealing with a task that\ncan be completed by executing multiple possible sequences of sub-goals, the planner should be able\nto select the best route based on the current state of the agent. However, the complex and diverse\nstate distribution of open-world environments makes state awareness hard to achieve.\nTo tackle these problems, we propose “Describe, Explain, Plan and Select” (DEPS), an interactive\nplanning approach based on Large Language Models (LLMs) to alleviate the aforementioned issues.\nThe key to tackling the first challenge is to effectively adjust the generated plan upon failure.\nSpecifically, whenever the controller fails to complete a sub-goal, a descriptor will summarize the\ncurrent situation as text and send it back to the LLM-based planner. We then prompt the LLM\nas an explainer to locate the errors in the previous plan. Finally, a planner will refine the plan\nusing information from the descriptor and explainer. To improve the feasibility of generated plans\nconditioned on the current state, which is the second identified challenge, we use a learned goal-\nselector to choose the most accessible sub-task based on the proximity to each candidate sub-goal.\nOur experiments are conducted on 71 tasks in open-ended Minecraft without any demonstration.\nGiven the goal-conditioned controller for atom sub-tasks (i.e., mine log and mine stone), our zero-\n2\n(Re-)Planner\nLLM*\nController\nGoal-conditioned Policy\nSelector\nHPM\nDescriptor\nVLM\nExplainer\nLLM*\nInstruction\nplan 𝑃!\ngoal 𝑔!\nfeedback\naction\nobs\ndescription 𝑑!\nexplain\nEnvironment\nobs\nTask instruction: Obtain a diamond\nin Minecraft survival mode step-by-step?\nCandidate goals:\nSelected Goal 𝒈𝟏:\n× 4\nThe agent locates in the birch forest,\nwhich only has birch wood.\nDescription 𝒅𝒕: I succeed on goal 1-5. I\nfail on goal 6, mining 3\nwith\n.\nNow my inventory has 5 planks, …\nInitial Plan 𝑷𝟎:\n4\n16\n1\n8\n3\n3\n1\n1\n1\nExplanation: Because mining       needs to\nuse at least\n, which I do not have.\nSo I need to craft\nfirst.\n…\nFinished Plan  𝑃!: \n3\n3\n1\n1\n1\n1\nSelected Goal\n𝑔$\n× 3\nno other choices\nDescription 𝒅𝒕: I succeed on goal …. I \nfail on smelting 3\nfrom 3\n, on\n.\nMy inventory now has 3 iron ore, …\nUpdated Plan 𝑷𝒕: \n3\n1\n3\n1\n1\n3\nExplanation: Because smelting       needs\nto use\nand\n, which I do not have.\nSo I need to craft\nfirst.\nTask Finished !\nAgent\nEnv\n𝑎%\n𝑠%\nMine acacia wood\nCraft acacia planks\nCraft crafting table\nCraft wood pickaxe\nCraft stick\nMine cobblestone\nCraft stone pickaxe\nMine diamond\nMine coal\nMine iron ore\nCraft furnace\nSmelt iron ingot\nCraft iron pickaxe\nMine birch wood\nMine oak wood\nCraft birch planks\nCraft oak planks\nFigure 2: Overview of our proposed interactive planner architecture.\nshot3 LLM-based planner can finish all tasks within a limited horizon (3000-12000 steps for different\ntasks). We find DEPS outperforms all language planner baselines by nearly doubling the overall\nsuccess rate, with the same initial state and goal-conditioned controller. Our ablation and exploratory\nstudies then explain how our approach beats the counterparts and becomes the first planning-based\nagent that accomplishes the challenging ObtainDiamond task. DEPS does not require any planning\ntraining for the environment. Additionally, DEPS achieves between on-par and more than 50% relative\nimprovement over existing or concurrent LLM-based planning methods on non-open-ended robotics\ndomains such as ALFWorld [41] and Tabletop environments [40].\n2\nBackground\nWe aim to develop an agent capable of solving long-horizon goal-reaching tasks using image\nobservations and language goals. To accomplish this, we propose a combined approach involving\ngoal-conditioned policies (termed controllers) and a planner. The goal-conditioned policies are\ntrained to complete sub-goals, while the planner decomposes long-horizon tasks into a series of\nK short-horizon sub-goals, g1, . . . , gK, to be executed by the controller. At each time step t, the\ngoal-conditioned policy π(at | st, gk) generates an action at based on the current state st and the\nspecified sub-goal gk.\nPlanning with Large Language Models\nPrevious works have shown that LLMs such as Instruct-\nGPT [32] and Codex [8] can be used as zero-shot planners to generate sub-goal sequences for various\ntasks in embodied environments [16, 42]. Formally, given the task description T as prompt p, LLM\nacts as a planner to decode T into K sub-goals, g1, . . . , gK, which are then executed one by one by\nthe low-level controller π(at | st, gk) to accomplish the task.\nHowever, the above pipeline suffers from both challenges identified in Section 1. Regarding the first\nchallenge, the probability of generating a flawless plan directly from the task description decreases\nsignificantly as the required number of sub-goals increases. Moreover, even when the LLM generates\na correct plan, it is very likely that the plan is highly inefficient given the agent’s current state\n(challenge #2). Prior works mostly focus on solving the first challenge by providing environmental\nfeedback to the LLM through affordance functions [4], success detector [20] or scene descriptor [17].\nHowever, although these approaches work well on many non-open-ended domains, they still suffer\nfrom high failure rates in open-world environments.\n3Similar to [5, 16], “zero-shot” here means no gradient updates are performed. However we provide some\nrelated demonstrations as prompts during inference time.\n3\n3\nTowards Reliable Planning in Embodied Open-World Environments\nIn this section, we first give an overview of our proposed interactive planning framework “Descibe,\nExplain, Plan, and Select” (DEPS) for solving complex and long-horizon tasks in open-world\nenvironments (Sec. 3.1). Next, in Section 3.2, we elaborate how DEPS iteratively refines its plan\nto combat the first identified challenge. Section 3.3 introduces the selector module that is used to\nidentify efficient plans in response to the second identified challenge.\n3.1\nDEPS Overview\nAs demonstrated in Figure 2, our agent (DEPS) consists of an event-triggered Descriptor, a Large\nLanguage Model (LLM) as Explainer and Planner, a goal Selector based on horizon prediction and\na goal-conditioned controller. In the following, we use Minecraft as a running example to better\nelaborate our agent. Note that DEPS can be directly applied to other (non-)open-ended tasks.\nWe take a large language model (LLM) as a zero-shot planner of the agent to complete tasks. Given\na goal command (e.g., ObtainDiamond) as task T, the LLM-based planner decomposes this\nhigh-level task into a sequence of sub-goals {g1, . . . , gK}, as the initial plan P0. The goals are\ninstructions in natural language, such as mine oak wood\n(in Minecraft), find two cups (in\nALFWorld), put block A on top of block B (in Tabletop Manipulation).\nAs described in Section 2, a controller is then invoked to execute the provided sub-goals sequentially\nthrough a goal-conditioned policy π(a | s, g). However, the initial plan provided by the planner often\ncontains errors, which results in execution failures of the controller. For example, the goal\ncan\nnot be finished only with a wooden pickaxe\nas shown in Figure 2. When failure pops up, the\ndescriptor will summarize the current state st and execution outcome of the most recent goal into text\ndt and send it to the LLM. The LLM will first try to locate the errors in the previous plan Pt−1 by\nself-explanation, e.g., the goal\nneed to be executed with a stone pickaxe\n. Then it will re-plan\nthe current task T and generate a revised plan Pt according to the explanation. In this process, the\nLLM is also treated as an explainer in addition to the planner role. The Descriptor, Explainer, and\nPlanner will be detailed in Section 3.2.\nDescription : dt = fDESC(st−1),\nExplanation : et = fEX(dt),\nPrompt : pt = CONCAT(pt−1, dt, et),\nPlan : Pt = fLM(pt),\nGoal : gt ∼fS(Pt, st−1),\nAction : at ∼π(at | st−1, gt)\n(1)\nAs shown in Equation (1), DEPS will iteratively update the plan Pt until the task is finished, where\nfDESC is the descriptor model, fLM denotes the language model as explainer and planner, fS is the\nselector model, π is goal-conditioned policies from the controller.\nTo filter out inefficient plans, the selector is trained to predict the number of time steps remaining to\nachieve every goal gk in a set of parallel goals given the current state st. When the generated plan\ncontains alternative routes, the selector uses this information to choose a suitable goal as the current\ngoal gt. For example, the horizon predicted by the selector of goal acacia tree\nis less than\ngoal oak tree\nin Savanna biome, which leads to chop acacia tree as current goal gt.\n3.2\nDescribe, Explain and Plan with LLM Generates Executable Plans\nCurrent LLM-based planners usually query the LLM once at the beginning of every episode and use\nthe output plan throughout the episode [16, 42]. However, as demonstrated by Figure 1, such one-shot\nplanning methods often fail on long-horizon tasks that require many sub-goals. This is caused by two\nmajor issues. First, since the correct plan for long-horizon tasks needs to respect various complex\npreconditions, it is extremely hard for the LLM to generate a flawless plan directly from the task\ninstructions, resulting in failure when simply following the initial plan. Additionally, due to the\nunpredictable transition dynamics, some incidents may happen during the execution and make the\ninitial plan non-executable. To remedy these problems, existing methods introduce feedback (e.g.,\n4\nPrompt 1 Planner prompt template, Python-like code\ndef craft_wooden_axe(initial_inventory={}):\n# step 1: mine 3 logs\nmine(obj = {\"log\":3}, tool = None)\n# step 2: craft 12 planks from 3 logs\ncraft(obj = {\"planks\":12}, materials = {\"log\":3}, tool = None)\n# step 3: craft 4 sticks from 2 planks\ncraft(obj = {\"stick\":4}, materials = {\"planks\":2}, tool = None)\n# step 4: craft 1 crafting_table from 4 planks\ncraft(obj = {\"crafting_table\":1}, materials = {\"planks\":4}, tool = None)\n# step 5: craft 1 wooden_axe from 3 planks and 2 sticks on crafting table\ncraft(obj = {\"wooden_axe\":1}, {\"planks\": 3, \"stick\": 2}, tool = \"crafting_table\")\nreturn \"wooden_axe\"\nfrom success detector or scene descriptor) to reflect on the results of previous executions [17, 20, 4].\nHowever, merely informing the LLM whether a sub-goal is completed is often insufficient to correct\nthe planning error.\nTo remedy this, we propose “describe, explain and plan”, a new interactive planning method to\ngenerate more executable and explainable plans. We start with rewriting the prompt into an interactive\ndialogue format as in ChatGPT [32] so that subsequent feedback can be passed to the LLM effectively.\nThe produced plan is also augmented with the preconditions and effects of each goal. The structured\nprompt improves the readability and interpretability of the plan and facilitates error-locating when\nthe execution fails later, as demonstrated in Prompt 1.\nThe descriptor will then collect the feedback generated by the agent during the execution of the\ntask. The feedback can be practically obtained either by a person (human feedback [4]), or by a\npre-trained vision-language model CLIP [35]. While the previous type of feedback needs intensive\nhuman involvement, the latter from the pre-trained model needs to be fine-tuned for the specific\ndomain, which decreases the automation and generalization of the agent. On the contrary, Minecraft\nreturns the ‘info’ and other high-level observations (such as biome, GPS, and compass), we can easily\ntranslate the unstructured information into structured language. Therefore we take the symbolic\ninformation available in the game and translate it into feedback description dt in this work. To avoid\ncarrying unrelated information in the prompt, we further distill plan-related messages (e.g., inventory\ninformation, biome) as final event-level description dt as demonstrated in Figure 2.\nNotably, we also treat the LLM as an explainer to explain why the previous plans Pt−1 failed.\nSpecifically, by analyzing the current state from description dt and precondition of current goal gt,\nthe explainer can identify the reason why the current goal cannot be executed successfully. As shown\nin Figure 2, the reason may be the current goal requires the use of an iron pickaxe, but the tool is\nnot prepared in advance, or the current goal requires the use of 3 planks, but the currently available\nplanks are not enough. To implement this, we provide few-shot demonstrations to the LLM as in\nchain-of-thoughts prompting [45], as shown in Prompt 1. Finally, the LLM goes back to its role as a\nplanner and re-plans the task with the explicit explanation of existing bugs in the previous plan Pt−1,\nultimately generating an updated plan Pt according to the explanation.\n3.3\nHorizon-Predictive Selector Yields Efficient Plans\nDue to the abundance of objects and the compositional nature of their functionalities, there often exist\nmultiple feasible plans to complete a task, i.e., there are usually multiple paths for the completion of a\nparticular goal. However, despite the feasibility of all such plans, most of them are highly inefficient\nto execute in the current episode. For example, as shown in Figure 2, obtaining a wood can be done\nby chopping oak trees\n, birch trees\n, or acacia trees\n. But only oak trees are available in the\nplains biome. So the planner needs to choose oak trees since it is more efficient, as the agent does\nnot need to travel to another biome.\nOn the other hand, there is no strict sequential requirement for some goals in the plan Pt, i.e.,\ngi, gj ∼Pt enjoy the same precondition, which means gi and gj can be executed in any order. As\nshown in Figure 1, the choice of different paths (sequences) may affect the execution efficiency of\nthe plan Pt as one goal might be closer to the agent. Always choosing the closer goal to execute first\ncould yield more efficient plans and improve the final success rate under a limited episode length.\nMoreover, the dynamic nature of open-world environments further amplifies the impact of efficient\n5\nGoal: Meat*3\nCandidate Skill: Kill Sheep\nOR Cow OR Pig\nSelection: Kill Sheep\nExplanation: Meet sheep first.\nGoal: Log*2\nCandidate Skill: Chop Oak\nOR Birch OR Acacia Tree\nSelection: Chop Acacia Tree\nExplanation: Savanna biome\nonly has Acacia tree.\nGoal: Coal*1 AND Iron_Ore*1\nCandidate Skill: Mine Coal AND\nIron_Ore\nSelection: Mine Iron_Ore\nExplanation: Meet iron_ore first.\nGoal: Survive in Night.\nCandidate Skill: Sleep in bed\nOR Dig down.\nSelection: Sleep_in_bed\nExplanation: Village has beds.\nFigure 3: Selection Demonstration from “Selector”. Given parallel sub-goals, i.e. candidate skills, our Selector\nwill determine the sequence in which to carry out these sub-goals based on their current proximity to the agent\nand modify the original plan produced by the LM planner.\nplans on the success rate. For example, in Minecraft, if the agent chooses to execute a further goal\nlike collect wood first, the much closer target sheep may disappear and be hard to find again.\nIn order to improve the efficiency of our plans, we propose to use a selector that selects the most\nefficient path with the highest execution success rate as the final plan. Specifically, we design a\nstate-aware selector to choose the nearest goal under state st as the current goal gt from the candidate\ngoal sets in plan Pt. It predicts the goal distribution p(gt|st, Pt) under the current state st and plan\nPt, where gt ∈Gt, Gt describes all current executable goals in Pt. A straight way to implement\nthe selector is to leverage the semantic similarity between the current state and the goal text using\na vision-language model (VLM) such as CLIP [35]. Nevertheless, this may not exactly reflect the\ndifficulty of completing the goal since VLM lacks practical experience. For example, an “oak tree” in\nfront of the agent could lead to high semantic similarity for the “chopping tree” goal, but it may be\ninefficient to achieve this goal if a canyon is in the middle between the agent and the oak tree.\nTo mitigate this, we implement a horizon-predictive selector that embeds practical task experience to\naccurately rank the goals based on their efficiency and feasibility. Here, we define the horizon of a\ngoal ht(g) := Tg −t as the remaining time steps to complete the given goal, where Tg is the time of\ncompleting goal g. This metric accurately reflects how quickly we can achieve the given goal from\nthe current state. To estimate the horizon, we learn a neural network µ to fit the offline trajectories by\nminimizing the entropy loss −log µ(ht(g) | st, g), where ht is the ground-truth horizon in trajectories\nof completing goal g. Therefore, the goal distribution can be formulated as follows:\nf(gt | st, Pt) =\nexp(−µ(gt, st))\nP\ng∈Gt exp(−µ(g, st)).\n(2)\nWe set goal-sensitive Impala CNN [6] as the backbone of the selector. In practice, the horizon predic-\ntive selector can be jointly trained with the controller policies and share the backbone parameters [6].\n4\nExperiments\nThis section analyzes and evaluates our proposed “describe, explain, plan, and select\" (DEPS) method.\nTo minimize performance variation caused by the low-level controller, we standardize all experiments\nwith one controller learned by behavior cloning. We refer to the details of this controller in Appendix\nC. In Section 4.1, we introduce our testing environments and our evaluation task set, consisting of the\nhardest 71 tasks from MCU SkillForgeChain [22]. In Section 4.2, we report our performance in the\ncontext of existing LLM-based planners. Ablation studies are conducted in Section 4.3. Finally, we\npay close attention to the hardest task, ObtainDiamond, which is long-hailed as a major challenge\nin the community. The experiments on ALFWorld and Tabletop Manipulation environments are\nshown in Appendix A.\n4.1\nExperimental Setup\nEnvironment and Task Setting\nWe first evaluate our proposed method in Minecraft, a popular\nopen-world environment with both challenges discussed in Section 1. For better reflecting the\nperformance of DEPS, we choose three Minecraft environments with different versions for better\n6\nevaluation, including Minedojo [10] with Minecraft 1.11.2, MineRL [3] with Minecraft 1.16.5, and\nMC-TextWorld [22] with Minecraft 1.19.2. Rules and items have something different in the above\nthree Minecraft environments, which can better evaluate the dynamic and interactive planning abilities\nof DEPS.\nTable 1: Attributes of 8 meta tasks covering Task101: We evaluate the algorithm on Minecraft Task101. We\ngroup the consisted 71 task into 8 different meta groups, with each focusing on testing a different aspect of our\nproposed method.\nMeta\nName\nNumber\nExample Task\nMax. Steps\nInitial Inventory\nGiven Tool\nMT1\nBasic\n14\nMake a wooden door.\n3000\nEmpty\nAxe\nMT2\nTool (Simple)\n12\nMake a stone pickaxe.\n3000\nEmpty\nAxe\nMT3\nHunt and Food\n7\nCook the beef.\n6000\nEmpty\nAxe\nMT4\nDig-Down\n6\nMine coal.\n3000\nEmpty\nAxe\nMT5\nEquipment\n9\nEquip the leather helmet.\n6000\nEmpty\nAxe\nMT6\nTool (Complex)\n7\nMake shears and bucket.\n6000\nEmpty\nAxe\nMT7\nIronStage\n13\nObtain an iron sword.\n6000\nEmpty\nAxe\nMT8\nChallenge\n1\nObtain a diamond!\n12000\nEmpty\nAxe\nWe choose 71 tasks from the Minecraft Universe Benchmark SkillForgeChain [22] for evaluation.\nThese tasks are related to items that can be obtained in the Minecraft overworld. To better present the\nresults, we divide the 71 Minecraft tasks into 8 meta groups according to the ingredients and function\nof the tasks, i.e., MT1-MT8. The instruction for every task is written in natural language, e.g., make\na wooden door in MT1 (Basic group) and obtain a diamond in MT8 (Challenge group),\nas illustrated in Table 1. Considering how long it typically takes human players to complete each\ntask as a ballpark [14], we set different maximum episode steps for different meta tasks from 3000\n(for easiest Basic tasks) to 12000 (for the hardest Challenge tasks). The names, number of required\nskills, and functions of all tasks are listed in Appendix B. We give an empty inventory for every task\nin Survival mode and require the agent to obtain every item from the environment by itself. Note that\nour agent will be summoned in different environments randomly for each evaluation. Biomes and\ninitial positions are also different each time. Following the previous work [18], we take the success\nrate as the evaluation metric.\nBaselines\nWe compare DEPS with other language-based planners, including GPT as Zero-shot\nPlanner(GPT) [16], ProgPrompt(PP) [42], Chain-of-Thought(CoT) [45], Inner Monologue(IM) [17],\nand Code as Policies(CaP) [20]. For all baseline models, we use the same demonstration example\nin the prompt, the same LM model from OpenAI, and the same controller in all tasks for a fair\ncomparison. Since these methods were not originally experimented with Minecraft, we reproduce\nthem to conform to the Minecraft specification based on prompt and feedback template design. All\nplanner methods access the LLM model through OpenAI API (text-davinci-03 model [32]\nfor GPT, CoT, and IM, and code-davinci-02 model [8] for PP, CaP, and Ours). All hyper-\nparameters of LLM (including the temperature and best_of, etc.) are kept as default. We also list the\nfull prompt of all different methods in Appendix G.\n4.2\nMain Results\nEvery task is executed 30 times and the average results in Minedojo [10] for every meta task are listed\nin Table 2. Our approach achieves the best performance with all meta tasks. As the complexity of the\ntask increases from MT1-MT8, the planner usually needs to give more accurate task steps (i.e., longer\ngoal sequence) to achieve the final task. Therefore the success rate of all agents decreases with the\nreasoning steps increasing. Starting from MT6, almost all existing LLM-based planners fail (nearly\n0 success rate). DEP (w\/o Selector) already consistently beats existing LLM-based planners in all\nmeta tasks with a significant margin. This validates that “describe, explain and plan” can estimate the\nreason for current plan failure and correct the original flawed plans. Due to the limited maximum\nepisode length and restricted control success rate for a hard goal (e.g., Mine diamond with\niron_pickaxe), the final success rate is still capped.\n7\nTable 2: Success rates of DEPS and existing LLM planners on Minecraft Task101. The full task-by-task list is in\nAppendix F.\nMethods\nMT1\nMT2\nMT3\nMT4\nMT5\nMT6\nMT7\nMT8\nAVG\nGPT[16, 32]\n25.85±24.8\n47.88±31.5\n10.78±14.6\n7.14±9.0\n1.98±5.9\n0.0±0.0\n0.0±0.0\n0.0±0.0\n15.42\nPP[42]\n30.61±23.6\n40.09±30.6\n17.13±19.1\n16.00±17.3\n3.21±4.9\n0.47±1.3\n0.60±2.2\n0.0±0.0\n16.88\nCoT[45]\n40.24±30.8\n55.21±26.8\n6.82±11.6\n4.76±8.2\n1.73±5.2\n0.0±0.0\n0.0±0.0\n0.0±0.0\n18.89\nIM[17]\n46.89±31.4\n53.73±20.8\n3.64±6.9\n18.41±17.4\n4.57±7.4\n0.64±1.7\n1.02±2.5\n0.0±0.0\n21.64\nCaP[20]\n60.08±17.3\n60.11±20.24\n8.72±9.7\n20.33± 21.0\n2.84±4.6\n0.63±1.3\n0.60±2.2\n0.0±0.0\n25.77\nDEP\n75.70±10.4\n66.13±13.4\n45.69±16.2\n43.35±20.2\n15.93±13.9\n5.71±3.7\n4.60±7.1\n0.50±0.5\n39.36\nDEPS\n79.77±8.5\n79.46±10.6\n62.40±17.9\n53.32±29.3\n29.24±27.3\n13.80±8.0\n12.56±13.3\n0.59±0.5\n48.56\nIn addition, selector also greatly improves the final task success rate of the agent (from DEP w\/o\nSelector to DEPS). Hard meta tasks usually require the completion of multiple sub-goals (up to\ndozens of goals), thus bringing more flexibility and providing more candidate goals for the Selector.\nAt the same time, as the agent conducts experiments with limited episode length, it also places high\ndemands on the efficiency of the plan. Therefore, the Selector brings a significant improvement on\nefficiency-sensitive tasks such as MT7 (up to +2.7 times success rate).\nRobustness on different controller and different Minecraft versions\nWe also evaluate DEPS\non MineRL [3] and MC-Textworld [22]. Note that DEPS is a planning method, which needs to\nequip the goal-conditioned controller for interacting with the Minecraft environments. We choose\nMC-Controller [6] and Steve-1 [21] as controllers to interact with Minedojo and MineRL, respectively.\nThese two methods are all control policies that perceive visual partial observations and produce\nmouse and keyboard actions. While MC-Textworld is a text world, which only keeps the Minecraft\ncrafting recipes and mining rules. So MC-Textworld does not require the controller. The DEPS\nresults of the task set MT1-MT8 on different Minecraft environments are shown in Table 3. The\nresults report that DEPS can generate effective plans in various Minecraft environments. The results\non MC-Textworld [22] also show that the performance drops on more difficult task sets from MT6 to\nMT8 are mainly from the controller limitation.\nTable 3: Success rates of DEPS under different Minecraft environments.\nEnvironment\nVersion\nController\nMT1\nMT2\nMT3\nMT4\nMT5\nMT6\nMT7\nMT8\nMineDojo [10]\n1.11.2\n[6]\n79.77\n79.46\n62.40\n53.32\n29.24\n13.80\n12.56\n0.59\nMineRL [3]\n1.16.5\n[21]\n84.05\n80.32\n24.25\n36.21\n9.16\n17.22\n16.79\n1.84\nMC-Textworld [22]\n1.19.2\n-\n100.00\n90.00\n80.00\n56.25\n64.71\n57.14\n69.57\n50.00\n4.3\nAblation Study\nWe conduct ablation experiments to investigate the number of candidate executable goals for different\nSelector models and the specific impact of the rounds of DEPS.\n4.3.1\nAblation on Selector\nWe verify the robustness of our proposed Selector under different parallel goals. The agent is asked\nto complete 2, 3, and 4 candidate goals (the precondition is consistent for all goals), respectively. The\ngoals of the task correspond to different kinds of mobs or materials.\nWe report the final success rate of our method (DEP) with different selector implementations,\nincluding using a fixed sequence of goals, a random sequence of goals, and selecting a goal based on\nMineCLIP [10], CLIP [35], and our horizon-predictive Selector (HPS). As Figure 4 shows, in one\nround of parallel candidate goals, an improvement of ∆=+22.3%, +29.2%, +32.6% is obtained using\nour horizon-predictive Selector compared to not any selector (i.e., fixed plan), respectively.\nAt a limited episode length, e.g., 1000 steps, goal-model shows a greater advantage, which proves that\ngoal-model can improve the execution efficiency of the plan in embodied environments. In addition,\ncompared to using vision-language models such as CLIP [35] and MineCLIP [10] as a goal model,\nhorizon-predictive has the best performance due to better estimation of the horizon information. The\ncurve trend also demonstrates that agents with Selector scale up under large amounts of goals in an\nopen-world environment.\n8\nMaximum Episode Length\nSuccess Rate\nSelector Model\nHPS\nCLIP\nMineCLIP\nFixed\nRandom\nParallel Goals = 2\nParallel Goals = 3\nParallel Goals = 4\nSuccess Rate\nGoals Setting\nBiome: Plains\n2:\n3:\n4:\nFigure 4: The success rates of DEPS with differ-\nent selectors under varying numbers of parallel\ngoals and maximum episode lengths.\nTable 4: Success Rate of DEPS under different maxi-\nmum rounds of re-planning. Round 0 represents the\nvanilla Planner w\/o the re-planning process. ∞rep-\nresents the re-planning process will not end until task\nsuccess or reaching the maximum horizon, which is still\nlimited by the maximum tokens of LLMs. The maxi-\nmum number of rounds for Codex is around 7-8 rounds.\nRounds\n0\n1\n3\n5\n∞\n∆\n(0 →∞)\nMT1\n28.6\n50.6\n68.1\n79.8\n79.8\n+51.2\nMT2\n37.1\n71.2\n71.4\n79.2\n79.5\n+42.4\nMT3\n15.1\n20.1\n40.3\n40.8\n62.4\n+47.3\nMT4\n15.9\n17.4\n48.3\n50.7\n53.3\n+37.4\nMT5\n3.2\n3.2\n3.2\n15.2\n29.2\n+26.0\nMT6\n0.5\n0.5\n1.1\n1.9\n13.8\n+13.3\nMT7\n0.6\n2.3\n2.9\n2.9\n12.6\n+12.0\nMT8\n0.0\n0.0\n0.0\n0.0\n0.6\n+0.6\n4.3.2\nAblation on Re-Planning Rounds\nWe evaluate our agent on all tasks with increasing maximum rounds of DEPS. The round is defined as\na cycle of interactive LLM-based planning with description, explanation, and planning and selecting,\ni.e., an updated plan. All tasks for every maximum round are executed 30 times and the average\nsuccess rate is reported in Table 4. We take the vanilla LLM planner as the baseline, in which the\nmodel takes the initially generated plan as the final execution plan, without involving any description,\nre-planning, or self-explanation processes during the task execution. Our results in the previous\nsubsection utilize the maximum rounds possible under maximum tokens capped by OpenAI. We\nalso report the success rate increment from vanilla planner to DEPS of every meta task in column\n∆in Table 4. This set of experiments demonstrates that DEPS can iteratively improve its plan in\nopen-world environments. More description, self-explanation, and re-planning rounds produce better\nresults, especially for hard tasks.\n4.4\nObtainDiamond Challenge\nMining diamonds in the open-world game Minecraft, i.e. MT8 in Table 2, has been a long-standing\nchallenge for the community [14]. It is challenging because mining diamonds from scratch in\nMinecraft involves acquiring a sequence of difficult-to-obtain items that require complex planning\non goals like mining, inventory management, crafting with and without a crafting table, tool use,\nsmelting iron ingot in a furnace, and mining at the lowest depths. We take the ObtainDiamond\ntask as a bonus experiment to show the capabilities of our zero-shot planner on complex tasks in\nembodied environments. Previous methods’ success rates on this challenge further vouch for its\ndifficulty. [43, 34] leverages domain-secific reward functions and RL fine-tuning to achieve ∽0.1%\nsuccess rate in 15 minutes of game play. VPT further boosts the success rate to 20% within 20\nminutes of play through pre-training on collects ∽70k hours human demonstrations and finetuning\nwith human-designed reward function [3]. DreamerV3 is trained from scratch to collect diamonds in\na modified Minecraft environment (easier to break blocks) with world models to achieve a success\nrate of 2% [15].\nOur DEPS manages to achieve on-par performance in this grand challenge; our agent achieves a\n0.59% success rate within 10 minutes of gameplay. Note our method does not specifically fine-tune\nfor this challenge. It is designed to be multi-task in its nature. Furthermore, considering our planner\noperates with demonstration prompts on a fixed Large Language Model, it can be straightforwardly\nadapted to other open-ended environments with modifications.\n5\nRelated Works\nTask planning with LLMs\nThere have been some methods leveraging the large language model\nto generate action plans for high-level tasks in embodied environments [46, 9, 11]. [16] decompose\nnatural language commands into sequences of executable actions by text completion and semantic\n9\ntranslation, while SayCan generates feasible plans for robots by jointly decoding an LLM weighted\nby skill affordances from value functions [4]. For better executing the plan in embodied environments,\nsome methods use an object detector describing the initial environment into the language prompt to\nproduce environment-suitable plans and adopt success detectors to check that each step is executed\nsuccessfully [17, 20]. [42] and [20] use the pythonic-style prompt to produce more executable plans.\nHowever, all of the above methods assume that the initial plan from the LLM is correct. When there\nare bugs in the initial plan, it’s difficult for the agent to finish the task successfully.\nInteractive Planning with LLMs\nInner Monologue [17] pilots the front of interactive planning\nwith LLMs, which introduces the feedback (including success detection and scene description) to\nthe planner. However, we found it could still suffer from accumulative planning error, especially in\nlong-horizon open-world tasks. Rather, our “Describe, Explain, Plan and Select” (DEPS) method can\nproduce more reliable plans by leveraging chain-of-thought thinking and explanation to locate the\nerrors in previous plans. Moreover, we also propose a goal Selector to further improve the efficiency\nof the plan, thereby yielding much better performances. Readers are encouraged to refer to the\ncomparative results in Section 4.2 between DEPS and these prior arts. There are also some concurrent\nworks on planning with LLMs [39, 26, 23, 33, 47].\nAgents in Minecraft\nSome previous works have employed the hierarchical architecture to solve\nlong-horizon tasks in Minecraft [30, 27, 24]. Recently, based on the internet-scale corpus, [10]\npre-trains a language-conditioned reward function and learns multi-task MineAgent. [3] collects a\nvast amount of human demonstrations to train a behavior cloning agent. More recently, [15] utilized a\nlearned world model to distill a policy that can efficiently explore in Minecraft. There are also some\nworks focus on learning goal-conditioned policies for better instruction-following [6, 7, 21]. While\nthese efforts all focus on improving the low-level controller. Rather, the planner in our architecture\nemphasizes applying domain knowledge to propose and arrange the sub-goals. It significantly\ninfluences the complexity and breadth of tasks that the agent can handle. Moreover, our planner is\nzero-shot, making it possible to generalize to other long-horizon open worlds.\n6\nLimitations\nAlbeit the impressive results of our approach, we believe there are at least two major limitations within\nour approach. First of all, our framework relies on privately-held LLMs like GPT-3 and ChatGPT,\nwhich makes it less accessible to those who cannot afford or access the service. However, we’re fully\ncommitted to ensuring a more democratized method and will explore using open-sourced models\nincluding OPT [48] and BLOOM [38]. Another issue is the explicit step-by-step planning in our\nsystem. Although it brings us superior performances over the baselines, the planning bottleneck can\nalso prevent our model from being further scaled up. A more appealing approach will be amortizing\nthe planning within an end-to-end trainable goal-conditioned policy, which is worth exploring next.\nFurthermore, some previous fundamental challenges in planning (e.g., dead ends) may not prevalent\nin our adopted environments and hence could be inadvertently overlooked by our paper. We are\ndedicated to addressing more fundamental challenges present in building a multi-task generalist agent\nin our series of following work.\n7\nConclusion\nWe investigate the problem of planning in open worlds. We identify two major challenges unique to\nthese environments: 1) long-term planning requires precise and multi-step reasoning, and 2) planning\nefficiency could be compromised since canonical planners do not take the agent’s proximity to parallel\ngoals\/subtasks into consideration. We propose “Describe, Explain, Plan and Select” (DEPS), an\ninteractive approach based on Large Language Models (LLMs) to tackle them both. Our experiments\nin the challenging Minecraft domain verify the advantages of our approach over counterparts by\nmarking the milestone of robustly accomplishing 70+ Minecraft tasks and nearly doubling the overall\nperformances. DEPS also is the first planning-based agent that can reach the diamond in this game.\n10\nAcknowledgements\nThis work is funded in part by the National Key R&D Program of China #2022ZD0160301, a\ngrant from CCF-Tencent Rhino-Bird Open Research Fund, NSF grants #IIS-1943641, #IIS-1956441,\n#CCF-1837129, an SRA from Meta and a research gift from Amazon Alexa AI, and a gift from\nRelationalAI. We thank Dai Zhixiang from NVIDIA and Xu Hongming from BIGAI on training\nLLMs and infrastructure supports, respectively.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 基于大型语言模型的交互式规划，助力开放世界多任务智能体\n\n## 📌 背景痛点\/本文动机\n在开放世界环境中，多任务智能体面临着两大挑战：1）执行计划需要精确的多步推理，因为任务具有长期性；2）传统的规划器在排序复杂的计划中的并行子目标时，没有考虑当前智能体完成给定子任务的难易程度，导致生成的计划可能效率低下甚至不可行。\n\n## 🚀 核心方法\n本文提出了“描述、解释、规划和选择”（DEPS）的交互式规划方法，基于大型语言模型（LLMs）来解决上述挑战。\n\n💡 创新点1：描述、解释和规划\nDEPS 通过集成计划执行过程的描述和提供自我解释的反馈，更好地纠正初始 LLM 生成的计划中的错误。当遇到失败时，描述器会总结当前情况并发送给 LLM，LLM 作为解释器定位错误，然后根据描述器和解释器的信息更新计划。\n\n💡 创新点2：目标选择器\nDEPS 包含一个可训练的目标选择器模块，该模块根据完成每个并行候选子目标的估计步骤对它们进行排序，从而细化初始计划。选择器使用预测剩余时间步数来完成每个目标，并根据当前状态选择最接近的目标。\n\n## 📈 实验结果\n实验结果表明，DEPS 在开放世界环境（如 Minecraft）中取得了显著的成果，能够稳健地完成 70 多个任务，并且整体性能几乎翻倍。此外，DEPS 在非开放世界环境（如 ALFWorld 和桌面操作）中也表现出良好的效果。\n\n## 💬 可借鉴之处\nDEPS 的交互式规划方法为开放世界多任务智能体的开发提供了新的思路。通过集成描述、解释和规划，以及使用目标选择器，DEPS 能够生成更可靠和高效的计划，从而提高智能体在开放世界环境中的任务完成能力。","llm_summary_res_status":200}
{"title":"LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation","authors":"Shuo Cheng, Danfei Xu","summary":"To assist with everyday human activities, robots must solve complex\nlong-horizon tasks and generalize to new settings. Recent deep reinforcement\nlearning (RL) methods show promise in fully autonomous learning, but they\nstruggle to reach long-term goals in large environments. On the other hand,\nTask and Motion Planning (TAMP) approaches excel at solving and generalizing\nacross long-horizon tasks, thanks to their powerful state and action\nabstractions. But they assume predefined skill sets, which limits their\nreal-world applications. In this work, we combine the benefits of these two\nparadigms and propose an integrated task planning and skill learning framework\nnamed LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages the\nsymbolic interface of a task planner to guide RL-based skill learning and\ncreates abstract state space to enable skill reuse. More importantly, LEAGUE\nlearns manipulation skills in-situ of the task planning system, continuously\ngrowing its capability and the set of tasks that it can solve. We evaluate\nLEAGUE on four challenging simulated task domains and show that LEAGUE\noutperforms baselines by large margins. We also show that the learned skills\ncan be reused to accelerate learning in new tasks domains and transfer to a\nphysical robot platform.","url":"http:\/\/arxiv.org\/abs\/2210.12631v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2210.12631v2","published":1666508225000,"comment":"Accepted to RA-L 2023","pdf_text":"IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\n1\nLEAGUE: Guided Skill Learning and Abstraction\nfor Long-Horizon Manipulation\nShuo Cheng1 and Danfei Xu1\nAbstract—To assist with everyday human activities, robots\nmust solve complex long-horizon tasks and generalize to new\nsettings. Recent deep reinforcement learning (RL) methods show\npromise in fully autonomous learning, but they struggle to reach\nlong-term goals in large environments. On the other hand, Task\nand Motion Planning (TAMP) approaches excel at solving and\ngeneralizing across long-horizon tasks, thanks to their powerful\nstate and action abstractions. But they assume predefined skill\nsets, which limits their real-world applications. In this work, we\ncombine the benefits of these two paradigms and propose an\nintegrated task planning and skill learning framework named\nLEAGUE (Learning and Abstraction with Guidance). LEAGUE\nleverages the symbolic interface of a task planner to guide RL-\nbased skill learning and creates abstract state space to enable skill\nreuse. More importantly, LEAGUE learns manipulation skills\nin-situ of the task planning system, continuously growing its\ncapability and the set of tasks that it can solve. We evaluate\nLEAGUE on four challenging simulated task domains and show\nthat LEAGUE outperforms baselines by large margins. We also\nshow that the learned skills can be reused to accelerate learning\nin new tasks domains and transfer to a physical robot platform.\nIndex Terms—Reinforcement Learning; Task and Motion Plan-\nning; Continual Learning\nI. INTRODUCTION\nD\nEVELOPING robots that can autonomously learn to\nwork in everyday human environments, such as house-\nholds, has been a long-standing challenge. Deep Reinforce-\nment Learning (DRL) methods have shown promise in allow-\ning robots to acquire skills with limited supervision [9, 16], but\nthey are still far from enabling home robots on their own. Two\nsignificant challenges stand out: 1) real-world tasks are often\nlong-horizon, requiring the learning agent to explore a vast\nspace of possible action sequences that grows exponentially\nwith the task duration, and 2) home robots must perform\ndiverse tasks in varying environments, requiring the learner\nto either generalize or quickly adapt to new situations.\nTo better learn long-horizon tasks, many DRL methods pro-\npose to use domain knowledge and structural prior [2, 22, 28].\nAutomatic goal generation in curriculum learning guides a\nlearning process using intermediate subgoals, enabling an\nagent to explore and make incremental progress toward a long-\nhorizon goal [22]. Other methods use skill primitives or learn\nhierarchical policies to enable temporally-extended decision-\nmaking\n[2, 19]. Although these approaches can outperform\nManuscript received: June, 23, 2023; Accepted August, 7, 2023.\nThis paper was recommended for publication by Jens Kober upon evaluation\nof the Associate Editor and Reviewers’ comments.\n1Georgia Institute of Technology, correspondence: shuocheng@gatech.edu\nDigital Object Identifier (DOI): see top of this page.\nTask Planner\nGoal: And[In(peg1, hole1),\nIn(peg2, hole2)]\nPick (?peg)\nPRE: {P4(?peg), …}\nEFF+: {P2(?peg), …}\n…\nInsert (?peg, ?hole)\nPRE: {P1(?peg), …}\nEFF+: {P3(?hole), …}\n…\nSymbolic Skill Ops\nstate abstraction \n& rewards\nSkill Library\nSkill Learning\nSymbolic Task Plan\nInsert (peg1, hole1)\nPRE: {P1(peg1), …}\nEFF+: {P3(hole1), …}\n…\nPick (peg2)\nPRE: {P4(peg2), …}\nEFF+: {P2(peg2), …}\n…\nstate abstraction & \nskill instantiation\nTask Execution\nFeedback Curriculum\nPick\n(a) Skill Learning and Abstraction \nwith Symbolic Operator Guidance\n(b) Task and Skill Planning\nreused across \ntasks and domains\nInsert\nPlace\nFig. 1: Overview of the LEAGUE framework. We present an\nintegrated task planning and skill learning framework. (a) The\nsystem uses the symbolic operator interface of a TAMP-like\nsystem as guidance to learn reusable manipulation skills (Alg.\n1). (b) A task planner composes the learned skills to solve\nlong-horizon tasks (Alg. 2). As an integrated system, the task\nplanner acts as a feedback curriculum (bottom) to guide skill\nlearning, and the RL-based skill learner continuously grows\nthe set of tasks that the system can solve.\nvanilla DRL, they still suffer from low sample efficiency, lack\nof interpretability, and fragile generalization [2, 28]. Most\nimportantly, the learned policies are often task-specific and\nfall short in cross-task and cross-domain generalization.\nIn the meantime, more established paradigms in robotics\nhave long sought to address these challenges. In particular,\nTask and Motion Planning (TAMP) [8, 14] leverages symbolic\naction abstractions to enable tractable planning and strong gen-\neralization. Specifically, the symbolic action operators divide a\nlarge planning problem into pieces that are each easier to solve.\nAnd the “lifted” action abstraction allows skill reuse across\ntasks and even domains. For example, a grasp skill operator\nand its underlying implementation can be easily adapted to\nsolve a new task in a new domain. At the same time, most\nTAMP-style approaches assume access to a complete set of\nskills before deployment. This is impractical for two reasons.\nFirst, it is hard to prepare skills for all possible tasks. A\nrobot must be able to grow its skill set on demand. Second,\nit is hard to hand-engineer manipulation skills for complex or\ncontact-rich tasks (e.g., insertion). The challenges make TAMP\nmethods difficult to deploy in real-world settings.\nIn this work, we introduce LEAGUE (LEarning and\nAbstraction with GUidancE), an integrated task planning and\nskill learning framework that learns to solve and generalize\nacross long-horizon tasks (See Fig. 1). LEAGUE harnesses\narXiv:2210.12631v2  [cs.AI]  22 Aug 2023\n2\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\nthe merits of the two research paradigms discussed above.\nStarting with a task planner that is equipped with skills that are\neasy to implement (e.g., reaching), LEAGUE continuously\ngrows the skill set in-situ using a DRL-based learner. The\nintermediate goals in a task plan are prescribed as rewards for\nthe learner to acquire and refine skills, and the mastered skills\nare used to reach the initial states of the new skills. Moreover,\nLEAGUE leverages the action operator definition, i.e., the\npreconditions and the effects, to determine a reduced state\nspace for each learned skill, akin to the concept of information\nhiding in feudal learning [28]. The key idea is to abstract away\ntask-irrelevant features to make the learned skills modular and\nreusable. Together, the result is a virtuous cycle where the task\nplanner guides skill learning and abstraction, and the learner\ncontinuously expands the set of tasks that the system can solve.\nWe conduct empirical studies on four challenging long-\nhorizon manipulation tasks built on the Robosuite simula-\ntion framework [31]. We show that LEAGUE is able to\noutperform state-of-the-art hierarchical reinforcement learning\nmethods [19] by a large margin. We also highlight that our\nmethod can achieve strong generalization to new task goals\nand even domains by reusing and adapting learned skills. As\na result, LEAGUE can solve a challenging simulated coffee-\nmaking task where competitive baselines fall flat. We also\ndemonstrate a LEAGUE system trained in simulation on a\nphysical Franka Emika Panda robot.\nIn summary, our primary contributions are: 1) we leverage\nthe state and action abstractions readily available in a TAMP\nsystem to learn reusable skills, 2) we instantiate the synergies\nbetween the task planner and the skill learner as an integrated\ntask planning and skill learning framework, and 3) we show\nthat the framework can progressively learn skills to solve\ncomplex long-horizon tasks and generalize the learned skills\nto new task goals and domains.\nII. RELATED WORK\nTAMP and Learning for TAMP. Task and Motion Planning\n(TAMP) [8, 14] is a powerful paradigm to solve long-horizon\nmanipulation tasks. The key idea is to break a challenging\nplanning problem into a set of symbolic-continuous search\nproblems that are individually easier to solve. However, TAMP\nmethods require high-level skills and their kinematics or dy-\nnamics models a priori. The assumptions preclude domains for\nwhich hand-engineering manipulation skills is difficult, such as\ncontact-rich tasks. Recent works proposed to learn dynamics\nmodels for TAMP by characterizing skill preconditions and\neffects [15, 17, 24]. For example, Konidaris et al. [15] learns\ncompact symbolic models of an environment through trial-\nand-error. Liang et al. [17] uses graph neural networks to\nmodel skill effects. However, these works still require hand-\nengineering complete skill sets that can solve the target task,\nwhich may not be feasible in real-world applications. Our\nidea of learning skills to augment TAMP systems is closely\nrelated to Silver et al. [24], which proposed to learn neural-\nsymbolic skills via imitation. But they require access to hard-\ncoded demonstration policies that can readily solve the target\ntasks. Our work instead aims to progressively grow TAMP\nskill libraries via guided reinforcement learning to solve long-\nhorizon contact-rich manipulation tasks.\nCurriculum for RL. Our idea to guide skill learning with\na task planner is connected to curriculum-based RL, which\nis to expose an agent to incrementally more difficult in-\ntermediate tasks before mastering a target task [18]. The\nintermediate tasks can take the form of environments [6]\nand subgoals [22, 27]. For example, VaPRL [22] starts with\nnear-success initialization and moves the initial states further\naway. While effective at accelerating task learning, existing\ncurricula focus on teaching tasks or domain-specific policies.\nIn contrast, our method leverages the symbolic abstraction of\na task planner to learn a repertoire of modular and composable\nskills. We show that we can compose learned skills to achieve\nnew goals and even transfer skills to new task domains.\nState and Action Abstractions. State and action abstractions\nare crucial for learning tasks in a large environment [1]. State\nabstraction allows agents to focus on task-relevant features\nof the environment. Action abstraction enables temporally-\nextended decision-making for long-horizon tasks. There exists\na large body of work on learning either or both types of\nabstractions [1, 3, 5, 13, 15, 30]. For example, Jonschkowski\net al. [13] explores different representation learning objectives\nfor effective state abstraction. Abel et al. [1] introduces a\ntheory for value-preserving state-action abstraction. However,\nautonomously discovering suitable abstractions remains an\nopen challenge. Our key insight is that a TAMP framework\nprovides powerful state and action abstractions that can readily\nguide skill learning. Specifically, the symbolic interface of an\naction operator defines both the precondition and the effect\n(action abstraction) and the state subspace that is relevant to\nthe action (state abstraction). The abstractions allow us to train\nskills that are compatible with the task planner and prevent\nthe learned skills from being distracted by irrelevant objects,\nmaking skill reuse across tasks and domains possible.\nHierarchical Modeling in Robot Learning. Our method\ninherits the bi-level hierarchy of a TAMP framework. Hier-\narchical modeling has a rich history in robotics. In addition to\nTAMP, various general frameworks including hierarchical task\nnetworks [11, 20, 29], logical-geometric programming [26],\nand hierarchical reinforcement learning (HRL) [2, 28] have\nbeen proposed to exploit the hierarchical nature of common\nrobotics tasks. In the context of HRL, a small number of works\nhave explored symbolic planner-guided HRL [12]. However,\nthese methods require tabular state representations and are thus\nlimited to simple grid-world domains. In robotics domains, a\nclosely related research thread is to use behavior primitives\nin RL [4, 19]. For example, MAPLE [19] trains a high-\nlevel policy that chooses hand-engineered behavior primitives\nand atomic actions. Our method instead leverages a symbolic\nplanner to serve as the high-level controller to compose learned\nskills, allowing us to continuously extend the skill set while\nalso leading to better generalization.\nIII. METHOD\nWe seek to enable robots to solve and generalize across\nlong-horizon tasks. Our primary contribution is a novel in-\ntegrated task planning and skill learning framework named\nCHENG et al.: LEAGUE\n3\nLEAGUE. Here, we first provide the necessary background in\nSec. III-A, and describe how LEAGUE (1) learns reusable\nskills guided by the symbolic operators of a task planner\nin Sec. III-B and (2) uses planner-generated task plans as\nan autonomous curriculum to continuously learn skills and\nexpand the capability of the overall system in Sec. III-C.\nA. Background\nMDP. We consider a Markov Decision Process (MDP) <\nX, A, R(x, a), T (x′|x, a), p(x(0)), γ >, with continuous state\nspace X, continuous action space A, reward function R,\nand environment transition model T . p(x(0)) denotes the\ndistribution of the initial states, x(H) denotes terminal state,\nand γ is the discount factor. The objective for RL train-\ning is to maximize the expected total reward of the policy\nπ(a|x) that the agent uses to interact with the environment:\nJ = Ex(0),a(0),...,x(H)∼π,p(x(0))\n\u0002P\nt γtR(x(t), a(t))\n\u0003\n.\nTask planning space. To support task planning, we assume\nthe environment is augmented with a symbolic interface\n< O, Λ, ¯Ψ, ¯Ω, G >, where O denotes the object set and Λ\ndenotes a finite set of object types. Each object entity o ∈O\n(e.g., peg1) has a specific type λ ∈Λ (e.g., peg) and a tuple\nof dim(λ)-dimensional feature containing information such as\nposes and joint angles, and the environment state x ∈X is a\nmapping from object entities to features: x(o) ∈Rdim(type(o)).\nPredicates ¯Ψ describe the relationships among multiple ob-\njects. Each predicate ¯ψ (e.g., Holding(?object:peg))\nis characterized by a tuple of object types (λ1, ..., λm) and\na binary classifier that determines whether the relationship\nholds: c ¯\nψ : X ×Om →{True, False}, where each substitute\nentity oi ∈O is restricted to have type λi ∈Λ. Evaluating\na predicate on the state by substituting corresponding object\nentities will result in a ground atom (e.g., Holding(peg1)).\nA task goal g ∈G is represented as a set of ground atoms,\nwhere a symbolic state xΨ can be obtained by evaluating a\nset of predicates ¯Ψ and keeping all positive ground atoms:\nxΨ = PARSE(x, O, ¯Ψ)\n△= {ψ : c ¯\nψ(x, O\n¯\nψ) = True, ∀O\n¯\nψ ⊆O, ∀¯ψ ∈¯Ψ}\n(1)\nwhere O ¯\nψ is a subset of object entities that each entity oi\nhas the same object type λi specified by the predicate ¯ψ.\nSymbolic skill operators. Following prior works [8], we\ncharacterize lifted skill operator ¯ω\n∈\n¯Ωby a tuple <\nPAR, PRE, EFF+, EFF−>, where PRE denotes the precon-\ndition of the operator, which is a set of lifted atoms defining\nthe condition that the operator is executable. EFF+ and EFF−\nare lifted atoms that describe the expected effects (changes\nin conditions) upon successful skill execution. PAR is an\nordered parameter list that defines all object types used in\nPRE, EFF+, and EFF−. A ground skill operator ω substi-\ntutes lifted atoms with object instances: ω = <¯ω, δ>\n△=<\nPRE, EFF+, EFF−>, where δ : Λ →O. Given a task goal,\na symbolic task plan is a list of ground operators that, when\nthe instantiated skills are executed successfully, lead to an\nenvironment state that satisfies the goal condition.\nAs a running example, consider a short task of inserting\na peg (peg1) into the target hole (hole1). The applicable\noperators for this task are defined as:\nPick(?object)\nPAR: [?object:peg]\nPRE: {HandEmpty(),OnTable(?object)}\nEFF−: {HandEmpty(),OnTable(?object)}\nEFF+: {Holding(?object)}\nInsert(?object,?hole)\nPAR: [?object:peg,?hole:hole]\nPRE: {Holding(?object),IsClear(?hole)}\nEFF−: {Holding(?object),IsClear(?hole)}\nEFF+: {HandEmpty(),In(?object,?hole)}\nThe environment starts with peg1 on the table. Evaluating\nthe\nPARSE\nfunction\n(Eq.\n1)\nyields\na\nsymbolic\nstate\n{HandEmpty(),IsClear(hole1),OnTable(peg1)},\na set of grounded atoms that satisfies the preconditions of the\ngrounded operator Pick(peg1). This grounded operator,\nif executed successfully, should reach the symbolic state of\n{Holding(peg1),IsClear(hole1)}, which is an inter-\nmediate subgoal for the final task goal that is characterized by\nthe grounded atoms {HandEmpty(),In(peg1,hole1)}.\nThe\nsymbolic\ntask\nplan\nis\ntherefore\nP\n=\n[Pick(peg1),Insert(peg1,hole1)].\nWe are interested in learning primitive manipulation skills\nfor accomplishing individual subgoals induced by the expected\neffects of the corresponding operators – the building blocks\nthat constitute a symbolic task plan. In our setting, each lifted\noperator ¯ω will have a corresponding skill policy π to be\nlearned, while during execution the ground operators belong\nto the same lifted operator ¯ω share the same skill policy. We\nassume access to the predicates ¯Ψ and the lifted operators ¯Ωof\nthe environments and focus on efficiently learning the skills\nfor achieving the effects. Note that it is possible to invent\nand learn predicates and operators [23, 25], but the topics are\nbeyond the scope of this work.\nB. Skill Learning and Abstraction with Operator Guidance\nAction and state abstractions are fundamental to TAMP’s\nabilities to solve and generalize across long-horizon tasks [8].\nOur key insight is that these abstractions, in the form of sym-\nbolic action operators, can readily guide RL-trained policies\nto gain similar abilities. Specifically, for action abstraction,\nwe train temporally-extended skills to reach desired effects\nof a skill operator by prescribing the effect condition as\nshaped reward. For state abstraction, we take inspiration from\nthe idea of information hiding in feudal learning [28] and\nuse the precondition and effect signature of an operator to\ndetermine a skill-relevant state space for its corresponding\nlearned policy. This allows the policy to be robust against\ndomains shift and achieve generalization, especially in large\nenvironments where most elements are impertinent to a given\nskill. To further accelerate skill learning, we leverage the\nexisting motion planning capability of a TAMP system to\naugment the learned skill with a transition primitive. Below\nwe describe each component in detail.\nSymbolic operators as reward guidance. Our skill learner\nleverages the existing RL method that supports continuous\naction space. In this work, we use Soft Actor-Critic (SAC) [10]\n4\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\nas the basis for skill learning. SAC leverages entropy regular-\nization to enhance exploration. Given the ground operator ω\nof a skill, we can define an operator-guided reward RΨ for\neach individual skill based on continuous environment state x\nand the action a produced by the corresponding policy π that\ntakes in skill-related state ˆx (which will be described later),\nthe objective for our skill learning is therefore rewritten as:\nJ = Ex(0),a(0),...,x(H)∼π,p(x(0))\n\n\nX\nt\nγt(RΨ(x(t), a(t), ω)\n+αH(π(·|ˆx(t)))\n\n\n(2)\nwhere RΨ(·) 7→[0, 1], H(·) is the entropy term introduced\nby SAC. While it is possible to learn directly from sparse\nreward defined by the symbolic state, in practice we associate\neach operator-guided reward with a dense reward function im-\nplemented in the Robosuite [31] benchmark for better learning\nefficiency. Continuing our running example, the shared reward\nfor Pick is defined as 1 −tanh(10.0 ∗d), where d is the\ndistance between the gripper center and target object center,\nand the target object is identified by the task planner.\nEnhance skill reuse with feudal state abstraction. With the\nprecondition and effect signature of a ground operator ω, we\ncan determine a skill-relevant state space to further prevent the\nlearned policy from being distracted by task-irrelevant objects:\nˆx = EXTRACT(x, ω, O)\n△= {x(o) : o ∈PAR, ∀o ∈O}\n(3)\nwhere PAR is the parameter list of the ground operator. In our\nrunning example, the skill-related state ˆx for Pick(peg1)\nincludes the 6D pose of peg1 and the state of the robot. This\ndesign echoes previous works that learn to impose constraints\non states [3], except that here the constraints are directly\ninformed by the task planner.\nAccelerate learning with transition motion primitives. A\nkey to our method is learning modular manipulation skills\nthat can be composed to solve long tasks. However, for\ncomplex manipulation problems, even learning such short\nskills can be challenging. On the other hand, although TAMP\nsystems fall short when facing contact-rich manipulation, they\nexcel at finding collision-free paths. To this end, we propose\nto augment our policy with motion planner-based transition\nprimitives. The key idea is to first approach the skill-relevant\nobject (per the skill operator) using an off-the-shelf motion\nplanner, before convening RL-based skill learning. For the\ntarget of motion planning, we simply set the goal position to be\n0.04m higher than the object or placement position of interest\nthat was identified by the task planning. The component\nsignificantly speeds up the exploration while still allowing the\nsystem to learn closed-loop contact-rich manipulation skills.\nC. Integrated Task Planning and Skill Learning\nSo far, we have described a recipe for learning reusable\nskills using symbolic skill operators as guidance. But these\nskills are not learned in silos. A key to LEAGUE’s success is\nto learn skills in-situ of a task planning system. The integrated\nplanning and learning scheme ensures that the learned skills\nare compatible with the planner, and the skill learner can con-\ntinuously extend the capability of the overall system to solve\nmore tasks. Here we first describe how LEAGUE performs\ntask planning and execution at inference time, and then we\nintroduce an algorithm that uses task plans as an autonomous\ncurriculum to schedule skill learning.\nTask planning and skill execution. To plan for task goal g,\nwe first PARSE (Eq. 1) the continuous environment state x for\nobtaining symbolic state xΨ, which affords symbolic search\nwith ground operators. We then ground each lifted operator\n¯ω ∈¯Ωon the object set O by substituting object entities\nin preconditions and effects, leading to ground operators\nω =< PRE, EFF+, EFF−> that support operating with\nsymbolic states. A ground operator is considered executable\nonly when its preconditions are satisfied: PRE ⊆xΨ. The\noperators induce an abstract transition model F(xΨ, ω) that\nallows planning in symbolic space:\nx′\nΨ = F(xΨ, ω)\n△= (xΨ \\ EFF−) ∪EFF+\n(4)\nWe use PDDL [7] to build the symbolic planner and use A∗\nsearch for generating high-level plans.\nWith the generated task plan, we sequentially invoke the\ncorresponding skill π∗to reach the subgoal that complies\nwith the effects of each ground operator ω in the plan. We\nrollout each skill controller until it fulfills the effects of the\noperator or a maximum skill horizon H is reached. To verify\nwhether the l-th skill is executed successfully, we first obtain\nthe corresponding symbolic state xl\nΨ by parsing the ending\nenvironment state x∗. The execution is considered successful\nonly when the environment state x∗conforms to the expected\neffects: F(xl−1\nΨ , ωl) ⊆xl\nΨ. We keep track of the failed skills\nand the starting simulator info s∗to inform the learning\ncurriculum.\nTask planner as an automated curriculum. To efficiently\nacquire all necessary skills for a given multi-step task, we\nleverage the task planner as an automated curriculum to learn\nskills in a progressive manner. The key idea is to use more\nproficient skills to reach the preconditions of skills that require\nadditional learning (See Fig. 1). The algorithm is sketched in\nAlg. 1 and Alg. 2. On a high level, we repeat task planning\nand skill learning until convergence. We keep track of failed\nskills during N task executions and adopt strict scheduling\ncriteria, where a skill is scheduled for learning (Sec. III-B)\nif it ever fails during the N episodes. Notably, we share the\nreplay buffers for different skill instances (e.g., Pick(peg1)\nand Pick(peg2)) that belong to the same lifted operator, so\nthat the relevant experience can be reused to further improve\nthe learning efficiency and generalization.\nIV. EXPERIMENTS\nOur experiments aim to show that 1) LEAGUE can progres-\nsively learn and refine skills to solve long-horizon tasks and 2)\nour novel operator-guided skill learning and abstraction algo-\nrithm produces composable and reusable skills, enabling quick\nadaptation to new tasks and domains. Finally, we demonstrate\ntransferring a trained LEAGUE system to a physical robot.\nCHENG et al.: LEAGUE\n5\n\u0007\u0006\u0007\n\u0007\u0006\t\n\u0007\u0006\n\u0007\u0006\f\n\u0007\u0006\r\n\b\u0006\u0007\n\b\u0006\t\n\b\u0006\n\b\u0006\f\n\b\u0006\r\n\u000e\u001a!\u0003\u0011\u001f\u0014\u001c\u001e\u0003\u0004\"\b\u000f\u0005\n\u0007\u0006\u0007\n\u0007\u0006\u000b\n\b\u0006\u0007\n\u0011\u0018\u0017\u0019\u0019\u0003\u0010\u001d\u001b\u0015\u0017\u0013\u0017\u0014\u001a\u0013#\n\u0010 \u0019\u0019\n\u0010\u0017\u0013\u0018\n\u0010\u0019\u0012\u0013\u0014\n\u0010 \u001e\u0016\nFig. 2: Visualizing skill learning progress. The plot shows the proficiency level of each skill throughout the process of\nlearning a simplified StowHammer task. The skill proficiency is the average normalized reward a skill receives at an iteration.\nAlgorithm 1 SKILLCURRICULUM\nhyperparameters:\nNumber of training iterations K\ninput:\nenv\n▷task environment\ng\n▷symbolic task goal\n¯Ψ\n▷state predicates\n¯Ω\n▷lifted operators\nstart\nΠ ←[π(0)\n1\n, ..., π(0)\n|¯\nΩ|]\n▷initialize all skill policies\nt ←0\nwhile Not Converged do\nD ←∅\n▷buffer for failed skills\nfor i ←[1, ..., N] do\nD ←D ∪TRYSOLVETASK(env, g, ¯Ψ, ¯Ω, Π)\nend for\nfor i, s, ω ←D do\nπ(t)\ni\n←Π[i]\nfor k ←[1, ..., K] do\nπ(t+k)\ni\n←SAC(env, s, π(t+k−1)\ni\n, ω)\n▷RL training\nend for\nΠ[i] ←π(t+K)\ni\nend for\nt ←t + K\nend while\nreturn Π\nAlgorithm 2 TRYSOLVETASK\nhyperparameters:\nMaximal skill horizon H\ninput:\nenv\n▷task environment\ng\n▷symbolic task goal\n¯Ψ\n▷state predicates\n¯Ω\n▷lifted operators\nΠ\n▷skill policies\nstart\nO, x(0) ←env.get_state()\nx(0)\nΨ\n←PARSE(x(0), O, ¯Ψ)\n▷continuous state to symbolic state\nΩ←GROUND(O, ¯Ω)\n▷get grounded operators\n[ω1, ..., ωL] ←SEARCH(x(0)\nΨ , g, Ω)\n▷found plan with length L\nD, l ←[], 0\nwhile l < L do\ni ←LOOKUPSKILL(ωl)\nπ∗←Π[i]\ns∗←env.get_sim()\n▷get simulator state\nx∗←ROLLOUT(env, π∗, H)\nif ISSUCCESS(x∗, ωl) then\nl ←l + 1\n▷advance to the next skill\ncontinue\nelse\nD ←D ∪(i, s∗, ωl)\n▷collect failed skills and states\nend if\nend while\nreturn D\nA. Experimental Setup\nWe conduct evaluations in four simulated domains, in which\nwe devise tasks that require multi-step reasoning, contact-rich\nmanipulation, and long-horizon interactions (See Fig. 3).\nStackAtTarget is to stack two cubes on a tight target region\nwith a specific order. The applicable skill operators are Pick\nand Place. Since the cubes are randomly placed in the scene,\nthe top cube may occupy the target region, in which situation\nthe robot must first remove the top cube before stacking.\nStowHammer requires the robot to stow two hammers\ninto different closed cabinets. It involves four skills: Pick,\nPlace, Pull, Push. Since the workspace is tight, the robot\nneeds to close an opened cabinet before being able to open\nthe other one, which requires multi-step reasoning.\nPegInHole is to pick up and insert two pegs into two\nhorizontal holes. The applicable operators are Pick and\nInsert. This task challenges the robot with contact-rich\nmanipulations and multi-step planning.\nMakeCoffee is to pick up a coffee pod from a closed cabi-\nnet, insert it into the holder of the coffee machine, and finally\nclose both the lid and the cabinet, The applicable operators\nare Pick, Pull, Push, CloseLid, and InsertHolder.\nThe environments are built on Robosuite [31] simulator. We\nuse a Franka Emika Panda robot arm that is controlled at\n20Hz with an operational space controller (OSC), which has\n5 degrees of freedom: end-effector position and the yaw angle\nand the position of the gripper. See Fig. 3 for an illustration.\nB. Visualize the Progressive Skill Learning Process\nBefore discussing quantitative comparisons, we seek to\ngain intuitive understanding of our progressive skill learning\nscheme (Sec. III-C), where the learning curriculum adjusts\nbased on the proficiencies of the skills. In Fig. 2, we visualize\nthe proficiency level of each skill throughout the process of\nlearning a simplified StowHammer task, where the goal is\nto stow away one hammer instead of two. The y axis is\nthe average normalized reward a skill receives. Note that we\nonly visualize a subset of skills scheduled for training at an\niteration. The corresponding behavior of each skill at a certain\nstage is visualized in the snapshots on top of the plot.\nAt the beginning of the training, the system can only reach\nthe\nprecondition\nfor\nexecuting\nthe\nPull(?cabinet)\n6\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\ncube0\ncube1\ntarget0\nAND[OnTarget(cube0, target0),\nOn(cube1, cube0)]\nTraining goal\ncabinet1\ncabinet0\nhammer1\nhammer0\nAND[InCabinet(hammer0, cabinet0),\nInCabinet(hammer1, cabinet1),\nIsCabinetClose(cabinet0),\nIsCabinetClose(cabinet1)]\nTraining goal\nhole1\npeg2\npeg1\nhole2\nAND[In(peg2, hole2),\nIn(peg1, hole1)]\nTraining goal\nholder\nlid\npod\ncabinet\nAND[In(pod, holder),\nIsLidClose(lid),\nIsCabinetClose(cabinet)]\nTraining goal\nFig. 3: Simulation task setup. We show the initialization\nof the simulation setups for the four tasks: StackAtTarget,\nStowHammer, PegInHole and MakeCoffee.\nskill\nbut\nnot\nother\nskills,\nthus\nthe\nexperience\nof\nPull(?cabinet) skill is collected and it is repeatedly\nselected for training. Until the agent is able to open one of\nthe cabinets, the second planned skill Pick(?object) is\nthen instantiated for learning and execution. Finally at the\nend of the training, all skills become proficient to be used\nto execute the entire task. The result qualitatively shows that\nLEAGUE’s automated curriculum is effective at progressively\nlearning skills to achieve long-horizon task goals.\nC. Quantitative Evaluation\nHere, we seek to highlight various aspects of our solution\nparadigm through quantitatively comparing LEAGUE with a\nnumber of strong baseline methods. Below we describe the\nbaselines and discuss the results.\n• RL (SAC): We adopt SAC [10] as a strong RL baseline.\nTo facilitate a fair comparison, we extend the vanilla task\nreward function to staged rewards using an oracle task\nplan, where the reward at each step is the summation\nof achieved rewards for each completed subgoal and the\nreward for the current subgoal.\n• Curriculum RL (CRL): We follow the main idea of\nstate-of-the-art curriculum RL approaches [22, 27], which\nstarts the training with near-success initializations and\ngradually move the initial states back to the true en-\nvironment initial states. To facilitate a fair comparison,\nwe sample the curriculum’s initial states based on the\nsubgoals of an oracle task plan (in reverse) and adopt the\nsame staged reward described above.\n• Hierarchical RL (HRL): This baseline adapts the recent\nprimitive-based HRL frameworks [4, 19] for our tasks.\nThe key idea is to train a high-level meta controller\nto compose parameterized skill primitives and atomic\nactions. We base our implementation on MAPLE [19]\nand use the oracle task plan to identify the target objects\nfor defining the affordance to guide the exploration.\n• Symb+MP: An open-loop baseline that resembles a\nvanilla TAMP framework, which greedily generates a\nmotion plan for each skill in a task plan. The robot then\nexecutes the plan through a trajectory controller.\n• Symb+RL: An ablation baseline of LEAGUE that re-\nmoves the state abstraction ( III-B) and retains all other\nfeatures including the symbolic plan-based curriculum.\nThe multi-stage nature of our evaluation tasks makes de-\nsigning smooth task-level metrics difficult. Thus we adopt task\nprogress as our metric, which is defined as the summed reward\nof all task stages normalized to [0, 1]. Below we discuss the\nmain findings based on Fig. 4.\nHigh-level reasoning is critical for solving long-horizon\ntasks. We observe that in StackAtTarget, a long-horizon task\nwith relatively simple manipulation steps, methods equipped\nwith a task planner (LEAGUE, Symb+MP, and Symb+RL)\nsignificantly outperforms all other baselines. The most com-\npetitive HRL baseline occasionally learns to move the bottom\ncube to the target region. This shows the value of explicit high-\nlevel reasoning, in particular as a plan-informed automated\ncurriculum in LEAGUE. Notably, the open loop Symb+MP\nperforms on par with LEAGUE because simple picking and\nplacing can readily be solved by open-loop trajectories.\nLEAGUE can solve long-horizon, contact-rich manip-\nulation tasks. LEAGUE significantly outperforms all other\nbaselines in StowHammer and PegInHole, which are both\nlong-horizon and require contact-rich manipulation. Notably,\nmost baselines cannot advance beyond opening the cabinet in\nStowHammer and picking up the first peg in PegInHole.\nSkill reuse is critical to learning structured tasks. Com-\nmon multi-step tasks have repeating structures, which can be\nleveraged by methods that explicitly reuse learned skills. We\nnote that both LEAGUE and Symb+RL perform competitively\nin StackAtTarget that involve repeating steps (i.e., stack two\ncubes). On the other hand, HammerPlace and PegInHole\ninvolve more objects, most of which are not relevant to a\ngiven skill. This prevents na¨ıve skill reuse — a policy may\nlearn spurious correlation to these irrelevant features — and\nnecessitates state abstraction, which we will discuss next.\nState abstraction facilitates skill reuse in complex envi-\nronments. We observe that LEAGUE outperforms Symb+RL\nin both HammerPlace and PegInHole. This shows that state\nabstraction can further improve skill reuse in complex envi-\nronments by ignoring features that are irrelevant to a skill.\nWe will also show in Sec. IV-D that skill reuse enables our\nmethod to generalize to novel task goals and domains.\nOther observations. We observe that without explicit prior\nstructures such as motion primitive, SAC baseline is able to\nexploit environment artifacts and learn shortcut behaviors. For\nexample, in the StowHammer task, SAC agent learns to grip\nthe head of the hammer to prevent slipping, but the grasping\npose precludes it from fitting the hammer to the drawer.\nMoreover, our analysis found that the CRL agent often failed\nto reach the final goal from some intermediate states due to the\nstrong sequential dependency of our evaluation tasks: the robot\nmust succeed in one stage to reach the pre-condition of the\nnext. And because the environment steps budget is distributed\nto multiple stages, CRL often underperforms other baselines\nCHENG et al.: LEAGUE\n7\n\u0006\n\u0007\n\b\n\t\n\n\u000b\n\u0010!(\u0003\u0018&\u001c#%\n\u0007\u001c\f\n\u0006\u0005\u0006\n\u0006\u0005\b\n\u0006\u0005\n\u0006\u0005\f\n\u0006\u0005\r\n\u0007\u0005\u0006\n\u0019\u001a%\u001e\u0003\u0016$\"\u001d$\u001c%%\n\u0016\u001c\u001d\u0012!\u0011\"\u001f\u001c\n\u0015'$%\n\u0018) \u001b\u0004\u0017\u0013\n\u0011\u0017\u0013\n\u000f\u0017\u0013\n\u0018\u000e\u000f\n\u0018) \u001b\u0004\u0014\u0016\n\u0006\n\u0007\n\b\n\t\n\n\u000b\n\u0010\u001f&\u0003\u0017$\u001b!#\n\u0007\u001b\f\n\u0006\u0005\u0006\n\u0006\u0005\b\n\u0006\u0005\n\u0006\u0005\f\n\u0006\u0005\r\n\u0007\u0005\u0006\n\u0018\u0019#\u001d\u0003\u0015\" \u001c\"\u001b##\n\u0017$ '\u0011\u0019\u001e\u001e\u001b\"\n\u0014%\"#\n\u0017(\u001e\u001a\u0004\u0016\u0012\n\u0011\u0016\u0012\n\u000f\u0016\u0012\n\u0017\u000e\u000f\n\u0017(\u001e\u001a\u0004\u0013\u0015\n\u0006\n\u0007\n\b\n\t\n\u000f\u001f&\u0003\u0016$\u001b!#\n\u0007\u001b\u000b\n\u0006\u0005\u0006\n\u0006\u0005\b\n\u0006\u0005\n\u0006\u0005\u000b\n\u0006\u0005\f\n\u0007\u0005\u0006\n\u0017\u0018#\u001d\u0003\u0014\" \u001c\"\u001b##\n\u0016$\u0018\u001a\u001d\r$\u0017\u0018\"\u001c\u001b$\n\u0013%\"#\n\u0016'\u001e\u0019\u0004\u0015\u0011\n\u0010\u0015\u0011\n\u000e\u0015\u0011\n\u0016\r\u000e\n\u0016'\u001e\u0019\u0004\u0012\u0014\nFig. 4: Baseline comparison. We compare relevant methods on three task domains. The plot shows the corresponding\naverage task progress during evaluation throughout training, which is measured as the summation of achieved rewards of\neach successfully executed skill in the task plan and normalized to 1. The results are reported using 5 random seeds, with the\nstandard deviation shown as the shaded area.\nTABLE I: We report the performance of applying our method\nto new task goals in the StowHammer and the PegInHole\ndomains without additional learning.\nTraining Goal\nTest Goal1\nTest Goal2\nStowHammer\n0.94 ± 0.21\n0.90 ± 0.12\n0.73 ± 0.31\nPegInHole\n0.87 ± 0.23\n0.53 ± 0.05\n1.00 ± 0.00\n(e.g., SAC) in completing the initial stages of a task.\nD. Generalization to New Tasks and Domains\nTo validate that our method can effectively generalize to\nnew task goals and even new task domains by reusing learned\nskills, we present the following experiments.\nGeneralize to new task goals. Besides evaluating the training\ngoals (shown in Fig. 3), we directly test our models on\nnew task goals for the StowHammer and the PegInHole\ndomains. For StowHammer domain, the first test goal is to\nswap the hammer-cabinet mapping. The second test goal is\nto place hammer1 into cabinet0 and keep cabinet1\nopen. For PegInHole, the first test goal is to swap the peg-\nhole mapping. The second goal is to only insert peg1 into\nhole2. The results are in Table I. We observe that LEAGUE\nexperiences little performance drop when generalizing to new\ntask goals without additional training, demonstrating strong\ncompositional generalization capability and skill modularity.\nQuick adaptation to new domains. Another exciting pos-\nsibility of LEAGUE is to transfer skills learned from one\ndomain to another. We design an experiment to validate this\nfeature. The target domain is MakeCoffee, which is the most\nchallenging task of the four. We adapt skills Pick, Pull, and\nPush learned in the StowHammer domain for learning the\nMakeCoffee task. As shown in Fig. 6, compared to learning\nfrom scratch, transferring learned skills can significantly accel-\nerate learning (the x-axis is shorter than in Fig. 4) and enables\nthe robot to solve the entire task. This highlights LEAGUE’s\nstrong potential for continual learning.\nE. Real World Demonstration\nWe demonstrate transferring simulation-trained LEAGUE\nsystem to two real-world task domains: StackThreeAtTarget\nand StowObject. For the StackThreeAtTarget task, we ran-\ndomly place three cubes and a target region on the table. The\ntask is to stack the cubes at the target region. We directly\nreuse the skills trained in StackAtTarget in simulation to\ndemonstrate generalization to different number of objects and\ninitial conditions. The StowObject is to stow two objects into\ntwo cabins. Similar to StowHammer, the task also requires\nthe robot to operate the cabinets. We reuse skills trained in\nthe simulated StowHammer domain.\nOur system uses a Franka Emika Panda robot. We take\nRGBD images from an Intel RealSense D435 camera and use\nAprilTag [21] to detect the 6D poses of task-relevant objects.\nOur system performs state estimation prior to each skill execu-\ntion, synchronizes the states to a simulated environment, and\nexecutes each skill generated by LEAGUE from the simulated\nenvironment through open-loop control.\nFig. 5 shows the key frames of three task execution pro-\ncesses and the corresponding task goals. Our system achieves\nan 8\/10 success rate for the StackThreeAtTarget task, and a\n6\/10 success rate for the StowObject task. The failure mode\nfor the StackThreeAtTarget task is that the AprilTags getting\noccluded from the camera in some initial configurations. The\nfailure mode for StowObject task is that sometimes the learned\npolicy is not able to generate a valid motion for operating the\ndrawer, and the objects slipping from the gripper.\nV. CONCLUSIONS, LIMITATIONS, AND FUTURE WORKS\nWe introduced LEAGUE, an integrated task planning and\nskill learning framework that represents a virtuous-cycle sys-\ntem: It leverages the high-level reasoning ability and abstrac-\ntion of a TAMP framework to facilitate the exploration and\ngeneralization of an RL skill learner, which in turn expands\nthe capability of the overall system. Through challenging\nmanipulation tasks in both simulation and the real world,\nwe demonstrated that LEAGUE is effective at solving long-\nhorizon tasks and generalizing to new tasks and domains.\nWhile empirically effective, our method does have a number\nof limitations. As we discussed in Sec. III-A, we assume\naccess to a library of skill operators that serve as the basis\nfor skill learning. Relatedly, our assumptions for skill-relevant\nstate abstraction, although effective, may not hold in certain\ncases (e.g. unintended consequences during exploration). A\npossible path to address both challenges is to learn skill\noperators with sparse transition models from unstructured\nexperiences [23, 25]. Second, our skill learning process re-\nlies on the environment-provided dense reward function. RL\nalgorithms that can better learn from sparse reward would\n8\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\nGoal: AND[OnTarget(cube2, target0), On(cube1, cube2), On(cube0, cube1)]\nGoal: AND[OnTarget(cube0, target0), On(cube1, cube0), On(cube2, cube1)]\nGoal: AND[InCabinet(hammer0, cabinet0), InCabinet(marker0, cabinet1), IsCabinetClosed(cabinet0), IsCabinetClosed(cabinet1)]\nFig. 5: Real robot demonstration. Key frames of three task execution processes (bottom) and their final task goals (top).\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nEnv Steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTask Progress\nMakeCoffee\nOurs - skill reusing\nOurs - from scratch\nFig. 6: Generalization to new domain. For MakeCoffee task,\nwe compare (a) learning the task from scratch and (b) learning\nby adapting the skills (Pick, Pull, and Push) learned from\nthe StowHammer domain.\nallow LEAGUE to build a tighter connection with the symbolic\nspace. Finally, in the real-world setting, LEAGUE is limited\nby the capability of the off-the-shelf perception algorithms. We\nplan to explore learning visuomotor control policies to make\nLEAGUE easier to deploy in the real world.\nREFERENCES\n[1] David Abel, Nate Umbanhowar, Khimya Khetarpal, Dilip Arumugam,\nDoina Precup, and Michael Littman.\nValue preserving state-action\nabstractions. In AISTATS, 2020.\n[2] Pierre-Luc Bacon, Jean Harb, and Doina Precup.\nThe option-critic\narchitecture. In AAAI, 2017.\n[3] Rohan Chitnis, Tom Silver, Beomjoon Kim, Leslie Kaelbling, and\nTomas Lozano-Perez. Camps: Learning context-specific abstractions for\nefficient planning in factored mdps. In CoRL, 2021.\n[4] Murtaza Dalal, Deepak Pathak, and Russ R Salakhutdinov. Accelerat-\ning robotic reinforcement learning via parameterized action primitives.\nNeurIPS, 2021.\n[5] Scott Emmons, Ajay Jain, Misha Laskin, Thanard Kurutach, Pieter\nAbbeel, and Deepak Pathak.\nSparse graphical memory for robust\nplanning. NeurIPS, 2020.\n[6] Kuan Fang, Yuke Zhu, Silvio Savarese, and L Fei-Fei.\nAdaptive\nprocedural task generation for hard-exploration problems.\nIn ICLR,\n2020.\n[7] Maria Fox and Derek Long.\nPddl2. 1: An extension to pddl for\nexpressing temporal planning domains. JAIR, 2003.\n[8] Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim,\nTom Silver, Leslie Pack Kaelbling, and Tom´as Lozano-P´erez. Integrated\ntask and motion planning. Annu. Rev. Control Robot. Auton. Syst., 2021.\n[9] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep\nreinforcement learning for robotic manipulation with asynchronous off-\npolicy updates. In ICRA, 2017.\n[10] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft\nactor-critic: Off-policy maximum entropy deep reinforcement learning\nwith a stochastic actor. In ICML, 2018.\n[11] Bradley Hayes and Brian Scassellati.\nAutonomously constructing\nhierarchical task networks for planning and human-robot collaboration.\nIn ICRA, 2016.\n[12] Le´on Illanes, Xi Yan, Rodrigo Toro Icarte, and Sheila A McIlraith.\nSymbolic plans as high-level instructions for reinforcement learning.\nIn ICAPS, 2020.\n[13] Rico Jonschkowski and Oliver Brock.\nLearning state representations\nwith robotic priors. Auton. Robots, 2015.\n[14] Leslie Pack Kaelbling and Tom´as Lozano-P´erez. Hierarchical task and\nmotion planning in the now. In ICRA, 2011.\n[15] George Konidaris, Leslie Pack Kaelbling, and Tomas Lozano-Perez.\nFrom skills to symbols: Learning symbolic representations for abstract\nhigh-level planning. JAIR, 2018.\n[16] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-\nto-end training of deep visuomotor policies. JMLR, 2016.\n[17] Jacky Liang, Mohit Sharma, Alex LaGrassa, Shivam Vats, Saumya\nSaxena, and Oliver Kroemer. Search-based task planning with learned\nskill effect models for lifelong robotic manipulation. In ICRA, 2022.\n[18] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E\nTaylor, and Peter Stone. Curriculum learning for reinforcement learning\ndomains: A framework and survey. JMLR, 2020.\n[19] Soroush Nasiriany, Huihan Liu, and Yuke Zhu. Augmenting reinforce-\nment learning with behavior primitives for diverse manipulation tasks.\nIn ICRA, 2022.\n[20] Negin Nejati, Pat Langley, and Tolga Konik. Learning hierarchical task\nnetworks by observation. In ICML, 2006.\n[21] Edwin Olson. AprilTag: A robust and flexible visual fiducial system. In\nICRA, 2011.\n[22] Archit Sharma, Abhishek Gupta, Sergey Levine, Karol Hausman, and\nChelsea Finn. Autonomous reinforcement learning via subgoal curricula.\nNeurIPS, 2021.\n[23] Tom Silver, Rohan Chitnis, Joshua Tenenbaum, Leslie Pack Kaelbling,\nand Tom´as Lozano-P´erez.\nLearning symbolic operators for task and\nmotion planning. In IROS, 2021.\n[24] Tom Silver, Ashay Athalye, Joshua B Tenenbaum, Tom´as Lozano-P´erez,\nand Leslie Pack Kaelbling. Learning neuro-symbolic skills for bilevel\nplanning. In CoRL, 2022.\n[25] Tom Silver, Rohan Chitnis, Nishanth Kumar, Willie McClinton, Tomas\nLozano-Perez, Leslie Pack Kaelbling, and Joshua Tenenbaum. Predicate\ninvention for bilevel planning. In AAAI, 2023.\n[26] Marc Toussaint. Logic-geometric programming: An optimization-based\napproach to combined task and motion planning. In IJCAI, 2015.\n[27] Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan,\nJos´ephine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao\nJiao, et al. Jump-start reinforcement learning. arXiv, 2022.\n[28] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas\nHeess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal\nnetworks for hierarchical reinforcement learning. In ICML, 2017.\n[29] Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei,\nand Silvio Savarese. Neural task programming: Learning to generalize\nacross hierarchical tasks. In ICRA, 2018.\n[30] Danfei Xu, Ajay Mandlekar, Roberto Mart´ın-Mart´ın, Yuke Zhu, Silvio\nSavarese, and Li Fei-Fei. Deep affordance foresight: Planning through\nwhat can be done in the future. In ICRA, 2021.\n[31] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart´ın-Mart´ın.\nrobosuite: A modular simulation framework and benchmark for robot\nlearning. In arXiv, 2020.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nLEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation\n```\n#### 2. 论文摘要\n```\nTo assist with everyday human activities, robots must solve complex\nlong-horizon tasks and generalize to new settings. Recent deep reinforcement\nlearning (RL) methods show promise in fully autonomous learning, but they\nstruggle to reach long-term goals in large environments. On the other hand,\nTask and Motion Planning (TAMP) approaches excel at solving and generalizing\nacross long-horizon tasks, thanks to their powerful state and action\nabstractions. But they assume predefined skill sets, which limits their\nreal-world applications. In this work, we combine the benefits of these two\nparadigms and propose an integrated task planning and skill learning framework\nnamed LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages the\nsymbolic interface of a task planner to guide RL-based skill learning and\ncreates abstract state space to enable skill reuse. More importantly, LEAGUE\nlearns manipulation skills in-situ of the task planning system, continuously\ngrowing its capability and the set of tasks that it can solve. We evaluate\nLEAGUE on four challenging simulated task domains and show that LEAGUE\noutperforms baselines by large margins. We also show that the learned skills\ncan be reused to accelerate learning in new tasks domains and transfer to a\nphysical robot platform.\n```\n\n#### 3. 论文全文\n```\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\n1\nLEAGUE: Guided Skill Learning and Abstraction\nfor Long-Horizon Manipulation\nShuo Cheng1 and Danfei Xu1\nAbstract—To assist with everyday human activities, robots\nmust solve complex long-horizon tasks and generalize to new\nsettings. Recent deep reinforcement learning (RL) methods show\npromise in fully autonomous learning, but they struggle to reach\nlong-term goals in large environments. On the other hand, Task\nand Motion Planning (TAMP) approaches excel at solving and\ngeneralizing across long-horizon tasks, thanks to their powerful\nstate and action abstractions. But they assume predefined skill\nsets, which limits their real-world applications. In this work, we\ncombine the benefits of these two paradigms and propose an\nintegrated task planning and skill learning framework named\nLEAGUE (Learning and Abstraction with Guidance). LEAGUE\nleverages the symbolic interface of a task planner to guide RL-\nbased skill learning and creates abstract state space to enable skill\nreuse. More importantly, LEAGUE learns manipulation skills\nin-situ of the task planning system, continuously growing its\ncapability and the set of tasks that it can solve. We evaluate\nLEAGUE on four challenging simulated task domains and show\nthat LEAGUE outperforms baselines by large margins. We also\nshow that the learned skills can be reused to accelerate learning\nin new tasks domains and transfer to a physical robot platform.\nIndex Terms—Reinforcement Learning; Task and Motion Plan-\nning; Continual Learning\nI. INTRODUCTION\nD\nEVELOPING robots that can autonomously learn to\nwork in everyday human environments, such as house-\nholds, has been a long-standing challenge. Deep Reinforce-\nment Learning (DRL) methods have shown promise in allow-\ning robots to acquire skills with limited supervision [9, 16], but\nthey are still far from enabling home robots on their own. Two\nsignificant challenges stand out: 1) real-world tasks are often\nlong-horizon, requiring the learning agent to explore a vast\nspace of possible action sequences that grows exponentially\nwith the task duration, and 2) home robots must perform\ndiverse tasks in varying environments, requiring the learner\nto either generalize or quickly adapt to new situations.\nTo better learn long-horizon tasks, many DRL methods pro-\npose to use domain knowledge and structural prior [2, 22, 28].\nAutomatic goal generation in curriculum learning guides a\nlearning process using intermediate subgoals, enabling an\nagent to explore and make incremental progress toward a long-\nhorizon goal [22]. Other methods use skill primitives or learn\nhierarchical policies to enable temporally-extended decision-\nmaking\n[2, 19]. Although these approaches can outperform\nManuscript received: June, 23, 2023; Accepted August, 7, 2023.\nThis paper was recommended for publication by Jens Kober upon evaluation\nof the Associate Editor and Reviewers’ comments.\n1Georgia Institute of Technology, correspondence: shuocheng@gatech.edu\nDigital Object Identifier (DOI): see top of this page.\nTask Planner\nGoal: And[In(peg1, hole1),\nIn(peg2, hole2)]\nPick (?peg)\nPRE: {P4(?peg), …}\nEFF+: {P2(?peg), …}\n…\nInsert (?peg, ?hole)\nPRE: {P1(?peg), …}\nEFF+: {P3(?hole), …}\n…\nSymbolic Skill Ops\nstate abstraction \n& rewards\nSkill Library\nSkill Learning\nSymbolic Task Plan\nInsert (peg1, hole1)\nPRE: {P1(peg1), …}\nEFF+: {P3(hole1), …}\n…\nPick (peg2)\nPRE: {P4(peg2), …}\nEFF+: {P2(peg2), …}\n…\nstate abstraction & \nskill instantiation\nTask Execution\nFeedback Curriculum\nPick\n(a) Skill Learning and Abstraction \nwith Symbolic Operator Guidance\n(b) Task and Skill Planning\nreused across \ntasks and domains\nInsert\nPlace\nFig. 1: Overview of the LEAGUE framework. We present an\nintegrated task planning and skill learning framework. (a) The\nsystem uses the symbolic operator interface of a TAMP-like\nsystem as guidance to learn reusable manipulation skills (Alg.\n1). (b) A task planner composes the learned skills to solve\nlong-horizon tasks (Alg. 2). As an integrated system, the task\nplanner acts as a feedback curriculum (bottom) to guide skill\nlearning, and the RL-based skill learner continuously grows\nthe set of tasks that the system can solve.\nvanilla DRL, they still suffer from low sample efficiency, lack\nof interpretability, and fragile generalization [2, 28]. Most\nimportantly, the learned policies are often task-specific and\nfall short in cross-task and cross-domain generalization.\nIn the meantime, more established paradigms in robotics\nhave long sought to address these challenges. In particular,\nTask and Motion Planning (TAMP) [8, 14] leverages symbolic\naction abstractions to enable tractable planning and strong gen-\neralization. Specifically, the symbolic action operators divide a\nlarge planning problem into pieces that are each easier to solve.\nAnd the “lifted” action abstraction allows skill reuse across\ntasks and even domains. For example, a grasp skill operator\nand its underlying implementation can be easily adapted to\nsolve a new task in a new domain. At the same time, most\nTAMP-style approaches assume access to a complete set of\nskills before deployment. This is impractical for two reasons.\nFirst, it is hard to prepare skills for all possible tasks. A\nrobot must be able to grow its skill set on demand. Second,\nit is hard to hand-engineer manipulation skills for complex or\ncontact-rich tasks (e.g., insertion). The challenges make TAMP\nmethods difficult to deploy in real-world settings.\nIn this work, we introduce LEAGUE (LEarning and\nAbstraction with GUidancE), an integrated task planning and\nskill learning framework that learns to solve and generalize\nacross long-horizon tasks (See Fig. 1). LEAGUE harnesses\narXiv:2210.12631v2  [cs.AI]  22 Aug 2023\n2\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\nthe merits of the two research paradigms discussed above.\nStarting with a task planner that is equipped with skills that are\neasy to implement (e.g., reaching), LEAGUE continuously\ngrows the skill set in-situ using a DRL-based learner. The\nintermediate goals in a task plan are prescribed as rewards for\nthe learner to acquire and refine skills, and the mastered skills\nare used to reach the initial states of the new skills. Moreover,\nLEAGUE leverages the action operator definition, i.e., the\npreconditions and the effects, to determine a reduced state\nspace for each learned skill, akin to the concept of information\nhiding in feudal learning [28]. The key idea is to abstract away\ntask-irrelevant features to make the learned skills modular and\nreusable. Together, the result is a virtuous cycle where the task\nplanner guides skill learning and abstraction, and the learner\ncontinuously expands the set of tasks that the system can solve.\nWe conduct empirical studies on four challenging long-\nhorizon manipulation tasks built on the Robosuite simula-\ntion framework [31]. We show that LEAGUE is able to\noutperform state-of-the-art hierarchical reinforcement learning\nmethods [19] by a large margin. We also highlight that our\nmethod can achieve strong generalization to new task goals\nand even domains by reusing and adapting learned skills. As\na result, LEAGUE can solve a challenging simulated coffee-\nmaking task where competitive baselines fall flat. We also\ndemonstrate a LEAGUE system trained in simulation on a\nphysical Franka Emika Panda robot.\nIn summary, our primary contributions are: 1) we leverage\nthe state and action abstractions readily available in a TAMP\nsystem to learn reusable skills, 2) we instantiate the synergies\nbetween the task planner and the skill learner as an integrated\ntask planning and skill learning framework, and 3) we show\nthat the framework can progressively learn skills to solve\ncomplex long-horizon tasks and generalize the learned skills\nto new task goals and domains.\nII. RELATED WORK\nTAMP and Learning for TAMP. Task and Motion Planning\n(TAMP) [8, 14] is a powerful paradigm to solve long-horizon\nmanipulation tasks. The key idea is to break a challenging\nplanning problem into a set of symbolic-continuous search\nproblems that are individually easier to solve. However, TAMP\nmethods require high-level skills and their kinematics or dy-\nnamics models a priori. The assumptions preclude domains for\nwhich hand-engineering manipulation skills is difficult, such as\ncontact-rich tasks. Recent works proposed to learn dynamics\nmodels for TAMP by characterizing skill preconditions and\neffects [15, 17, 24]. For example, Konidaris et al. [15] learns\ncompact symbolic models of an environment through trial-\nand-error. Liang et al. [17] uses graph neural networks to\nmodel skill effects. However, these works still require hand-\nengineering complete skill sets that can solve the target task,\nwhich may not be feasible in real-world applications. Our\nidea of learning skills to augment TAMP systems is closely\nrelated to Silver et al. [24], which proposed to learn neural-\nsymbolic skills via imitation. But they require access to hard-\ncoded demonstration policies that can readily solve the target\ntasks. Our work instead aims to progressively grow TAMP\nskill libraries via guided reinforcement learning to solve long-\nhorizon contact-rich manipulation tasks.\nCurriculum for RL. Our idea to guide skill learning with\na task planner is connected to curriculum-based RL, which\nis to expose an agent to incrementally more difficult in-\ntermediate tasks before mastering a target task [18]. The\nintermediate tasks can take the form of environments [6]\nand subgoals [22, 27]. For example, VaPRL [22] starts with\nnear-success initialization and moves the initial states further\naway. While effective at accelerating task learning, existing\ncurricula focus on teaching tasks or domain-specific policies.\nIn contrast, our method leverages the symbolic abstraction of\na task planner to learn a repertoire of modular and composable\nskills. We show that we can compose learned skills to achieve\nnew goals and even transfer skills to new task domains.\nState and Action Abstractions. State and action abstractions\nare crucial for learning tasks in a large environment [1]. State\nabstraction allows agents to focus on task-relevant features\nof the environment. Action abstraction enables temporally-\nextended decision-making for long-horizon tasks. There exists\na large body of work on learning either or both types of\nabstractions [1, 3, 5, 13, 15, 30]. For example, Jonschkowski\net al. [13] explores different representation learning objectives\nfor effective state abstraction. Abel et al. [1] introduces a\ntheory for value-preserving state-action abstraction. However,\nautonomously discovering suitable abstractions remains an\nopen challenge. Our key insight is that a TAMP framework\nprovides powerful state and action abstractions that can readily\nguide skill learning. Specifically, the symbolic interface of an\naction operator defines both the precondition and the effect\n(action abstraction) and the state subspace that is relevant to\nthe action (state abstraction). The abstractions allow us to train\nskills that are compatible with the task planner and prevent\nthe learned skills from being distracted by irrelevant objects,\nmaking skill reuse across tasks and domains possible.\nHierarchical Modeling in Robot Learning. Our method\ninherits the bi-level hierarchy of a TAMP framework. Hier-\narchical modeling has a rich history in robotics. In addition to\nTAMP, various general frameworks including hierarchical task\nnetworks [11, 20, 29], logical-geometric programming [26],\nand hierarchical reinforcement learning (HRL) [2, 28] have\nbeen proposed to exploit the hierarchical nature of common\nrobotics tasks. In the context of HRL, a small number of works\nhave explored symbolic planner-guided HRL [12]. However,\nthese methods require tabular state representations and are thus\nlimited to simple grid-world domains. In robotics domains, a\nclosely related research thread is to use behavior primitives\nin RL [4, 19]. For example, MAPLE [19] trains a high-\nlevel policy that chooses hand-engineered behavior primitives\nand atomic actions. Our method instead leverages a symbolic\nplanner to serve as the high-level controller to compose learned\nskills, allowing us to continuously extend the skill set while\nalso leading to better generalization.\nIII. METHOD\nWe seek to enable robots to solve and generalize across\nlong-horizon tasks. Our primary contribution is a novel in-\ntegrated task planning and skill learning framework named\nCHENG et al.: LEAGUE\n3\nLEAGUE. Here, we first provide the necessary background in\nSec. III-A, and describe how LEAGUE (1) learns reusable\nskills guided by the symbolic operators of a task planner\nin Sec. III-B and (2) uses planner-generated task plans as\nan autonomous curriculum to continuously learn skills and\nexpand the capability of the overall system in Sec. III-C.\nA. Background\nMDP. We consider a Markov Decision Process (MDP) <\nX, A, R(x, a), T (x′|x, a), p(x(0)), γ >, with continuous state\nspace X, continuous action space A, reward function R,\nand environment transition model T . p(x(0)) denotes the\ndistribution of the initial states, x(H) denotes terminal state,\nand γ is the discount factor. The objective for RL train-\ning is to maximize the expected total reward of the policy\nπ(a|x) that the agent uses to interact with the environment:\nJ = Ex(0),a(0),...,x(H)∼π,p(x(0))\n\u0002P\nt γtR(x(t), a(t))\n\u0003\n.\nTask planning space. To support task planning, we assume\nthe environment is augmented with a symbolic interface\n< O, Λ, ¯Ψ, ¯Ω, G >, where O denotes the object set and Λ\ndenotes a finite set of object types. Each object entity o ∈O\n(e.g., peg1) has a specific type λ ∈Λ (e.g., peg) and a tuple\nof dim(λ)-dimensional feature containing information such as\nposes and joint angles, and the environment state x ∈X is a\nmapping from object entities to features: x(o) ∈Rdim(type(o)).\nPredicates ¯Ψ describe the relationships among multiple ob-\njects. Each predicate ¯ψ (e.g., Holding(?object:peg))\nis characterized by a tuple of object types (λ1, ..., λm) and\na binary classifier that determines whether the relationship\nholds: c ¯\nψ : X ×Om →{True, False}, where each substitute\nentity oi ∈O is restricted to have type λi ∈Λ. Evaluating\na predicate on the state by substituting corresponding object\nentities will result in a ground atom (e.g., Holding(peg1)).\nA task goal g ∈G is represented as a set of ground atoms,\nwhere a symbolic state xΨ can be obtained by evaluating a\nset of predicates ¯Ψ and keeping all positive ground atoms:\nxΨ = PARSE(x, O, ¯Ψ)\n△= {ψ : c ¯\nψ(x, O\n¯\nψ) = True, ∀O\n¯\nψ ⊆O, ∀¯ψ ∈¯Ψ}\n(1)\nwhere O ¯\nψ is a subset of object entities that each entity oi\nhas the same object type λi specified by the predicate ¯ψ.\nSymbolic skill operators. Following prior works [8], we\ncharacterize lifted skill operator ¯ω\n∈\n¯Ωby a tuple <\nPAR, PRE, EFF+, EFF−>, where PRE denotes the precon-\ndition of the operator, which is a set of lifted atoms defining\nthe condition that the operator is executable. EFF+ and EFF−\nare lifted atoms that describe the expected effects (changes\nin conditions) upon successful skill execution. PAR is an\nordered parameter list that defines all object types used in\nPRE, EFF+, and EFF−. A ground skill operator ω substi-\ntutes lifted atoms with object instances: ω = <¯ω, δ>\n△=<\nPRE, EFF+, EFF−>, where δ : Λ →O. Given a task goal,\na symbolic task plan is a list of ground operators that, when\nthe instantiated skills are executed successfully, lead to an\nenvironment state that satisfies the goal condition.\nAs a running example, consider a short task of inserting\na peg (peg1) into the target hole (hole1). The applicable\noperators for this task are defined as:\nPick(?object)\nPAR: [?object:peg]\nPRE: {HandEmpty(),OnTable(?object)}\nEFF−: {HandEmpty(),OnTable(?object)}\nEFF+: {Holding(?object)}\nInsert(?object,?hole)\nPAR: [?object:peg,?hole:hole]\nPRE: {Holding(?object),IsClear(?hole)}\nEFF−: {Holding(?object),IsClear(?hole)}\nEFF+: {HandEmpty(),In(?object,?hole)}\nThe environment starts with peg1 on the table. Evaluating\nthe\nPARSE\nfunction\n(Eq.\n1)\nyields\na\nsymbolic\nstate\n{HandEmpty(),IsClear(hole1),OnTable(peg1)},\na set of grounded atoms that satisfies the preconditions of the\ngrounded operator Pick(peg1). This grounded operator,\nif executed successfully, should reach the symbolic state of\n{Holding(peg1),IsClear(hole1)}, which is an inter-\nmediate subgoal for the final task goal that is characterized by\nthe grounded atoms {HandEmpty(),In(peg1,hole1)}.\nThe\nsymbolic\ntask\nplan\nis\ntherefore\nP\n=\n[Pick(peg1),Insert(peg1,hole1)].\nWe are interested in learning primitive manipulation skills\nfor accomplishing individual subgoals induced by the expected\neffects of the corresponding operators – the building blocks\nthat constitute a symbolic task plan. In our setting, each lifted\noperator ¯ω will have a corresponding skill policy π to be\nlearned, while during execution the ground operators belong\nto the same lifted operator ¯ω share the same skill policy. We\nassume access to the predicates ¯Ψ and the lifted operators ¯Ωof\nthe environments and focus on efficiently learning the skills\nfor achieving the effects. Note that it is possible to invent\nand learn predicates and operators [23, 25], but the topics are\nbeyond the scope of this work.\nB. Skill Learning and Abstraction with Operator Guidance\nAction and state abstractions are fundamental to TAMP’s\nabilities to solve and generalize across long-horizon tasks [8].\nOur key insight is that these abstractions, in the form of sym-\nbolic action operators, can readily guide RL-trained policies\nto gain similar abilities. Specifically, for action abstraction,\nwe train temporally-extended skills to reach desired effects\nof a skill operator by prescribing the effect condition as\nshaped reward. For state abstraction, we take inspiration from\nthe idea of information hiding in feudal learning [28] and\nuse the precondition and effect signature of an operator to\ndetermine a skill-relevant state space for its corresponding\nlearned policy. This allows the policy to be robust against\ndomains shift and achieve generalization, especially in large\nenvironments where most elements are impertinent to a given\nskill. To further accelerate skill learning, we leverage the\nexisting motion planning capability of a TAMP system to\naugment the learned skill with a transition primitive. Below\nwe describe each component in detail.\nSymbolic operators as reward guidance. Our skill learner\nleverages the existing RL method that supports continuous\naction space. In this work, we use Soft Actor-Critic (SAC) [10]\n4\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\nas the basis for skill learning. SAC leverages entropy regular-\nization to enhance exploration. Given the ground operator ω\nof a skill, we can define an operator-guided reward RΨ for\neach individual skill based on continuous environment state x\nand the action a produced by the corresponding policy π that\ntakes in skill-related state ˆx (which will be described later),\nthe objective for our skill learning is therefore rewritten as:\nJ = Ex(0),a(0),...,x(H)∼π,p(x(0))\n\n\nX\nt\nγt(RΨ(x(t), a(t), ω)\n+αH(π(·|ˆx(t)))\n\n\n(2)\nwhere RΨ(·) 7→[0, 1], H(·) is the entropy term introduced\nby SAC. While it is possible to learn directly from sparse\nreward defined by the symbolic state, in practice we associate\neach operator-guided reward with a dense reward function im-\nplemented in the Robosuite [31] benchmark for better learning\nefficiency. Continuing our running example, the shared reward\nfor Pick is defined as 1 −tanh(10.0 ∗d), where d is the\ndistance between the gripper center and target object center,\nand the target object is identified by the task planner.\nEnhance skill reuse with feudal state abstraction. With the\nprecondition and effect signature of a ground operator ω, we\ncan determine a skill-relevant state space to further prevent the\nlearned policy from being distracted by task-irrelevant objects:\nˆx = EXTRACT(x, ω, O)\n△= {x(o) : o ∈PAR, ∀o ∈O}\n(3)\nwhere PAR is the parameter list of the ground operator. In our\nrunning example, the skill-related state ˆx for Pick(peg1)\nincludes the 6D pose of peg1 and the state of the robot. This\ndesign echoes previous works that learn to impose constraints\non states [3], except that here the constraints are directly\ninformed by the task planner.\nAccelerate learning with transition motion primitives. A\nkey to our method is learning modular manipulation skills\nthat can be composed to solve long tasks. However, for\ncomplex manipulation problems, even learning such short\nskills can be challenging. On the other hand, although TAMP\nsystems fall short when facing contact-rich manipulation, they\nexcel at finding collision-free paths. To this end, we propose\nto augment our policy with motion planner-based transition\nprimitives. The key idea is to first approach the skill-relevant\nobject (per the skill operator) using an off-the-shelf motion\nplanner, before convening RL-based skill learning. For the\ntarget of motion planning, we simply set the goal position to be\n0.04m higher than the object or placement position of interest\nthat was identified by the task planning. The component\nsignificantly speeds up the exploration while still allowing the\nsystem to learn closed-loop contact-rich manipulation skills.\nC. Integrated Task Planning and Skill Learning\nSo far, we have described a recipe for learning reusable\nskills using symbolic skill operators as guidance. But these\nskills are not learned in silos. A key to LEAGUE’s success is\nto learn skills in-situ of a task planning system. The integrated\nplanning and learning scheme ensures that the learned skills\nare compatible with the planner, and the skill learner can con-\ntinuously extend the capability of the overall system to solve\nmore tasks. Here we first describe how LEAGUE performs\ntask planning and execution at inference time, and then we\nintroduce an algorithm that uses task plans as an autonomous\ncurriculum to schedule skill learning.\nTask planning and skill execution. To plan for task goal g,\nwe first PARSE (Eq. 1) the continuous environment state x for\nobtaining symbolic state xΨ, which affords symbolic search\nwith ground operators. We then ground each lifted operator\n¯ω ∈¯Ωon the object set O by substituting object entities\nin preconditions and effects, leading to ground operators\nω =< PRE, EFF+, EFF−> that support operating with\nsymbolic states. A ground operator is considered executable\nonly when its preconditions are satisfied: PRE ⊆xΨ. The\noperators induce an abstract transition model F(xΨ, ω) that\nallows planning in symbolic space:\nx′\nΨ = F(xΨ, ω)\n△= (xΨ \\ EFF−) ∪EFF+\n(4)\nWe use PDDL [7] to build the symbolic planner and use A∗\nsearch for generating high-level plans.\nWith the generated task plan, we sequentially invoke the\ncorresponding skill π∗to reach the subgoal that complies\nwith the effects of each ground operator ω in the plan. We\nrollout each skill controller until it fulfills the effects of the\noperator or a maximum skill horizon H is reached. To verify\nwhether the l-th skill is executed successfully, we first obtain\nthe corresponding symbolic state xl\nΨ by parsing the ending\nenvironment state x∗. The execution is considered successful\nonly when the environment state x∗conforms to the expected\neffects: F(xl−1\nΨ , ωl) ⊆xl\nΨ. We keep track of the failed skills\nand the starting simulator info s∗to inform the learning\ncurriculum.\nTask planner as an automated curriculum. To efficiently\nacquire all necessary skills for a given multi-step task, we\nleverage the task planner as an automated curriculum to learn\nskills in a progressive manner. The key idea is to use more\nproficient skills to reach the preconditions of skills that require\nadditional learning (See Fig. 1). The algorithm is sketched in\nAlg. 1 and Alg. 2. On a high level, we repeat task planning\nand skill learning until convergence. We keep track of failed\nskills during N task executions and adopt strict scheduling\ncriteria, where a skill is scheduled for learning (Sec. III-B)\nif it ever fails during the N episodes. Notably, we share the\nreplay buffers for different skill instances (e.g., Pick(peg1)\nand Pick(peg2)) that belong to the same lifted operator, so\nthat the relevant experience can be reused to further improve\nthe learning efficiency and generalization.\nIV. EXPERIMENTS\nOur experiments aim to show that 1) LEAGUE can progres-\nsively learn and refine skills to solve long-horizon tasks and 2)\nour novel operator-guided skill learning and abstraction algo-\nrithm produces composable and reusable skills, enabling quick\nadaptation to new tasks and domains. Finally, we demonstrate\ntransferring a trained LEAGUE system to a physical robot.\nCHENG et al.: LEAGUE\n5\n\u0007\u0006\u0007\n\u0007\u0006\t\n\u0007\u0006\n\u0007\u0006\f\n\u0007\u0006\r\n\b\u0006\u0007\n\b\u0006\t\n\b\u0006\n\b\u0006\f\n\b\u0006\r\n\u000e\u001a!\u0003\u0011\u001f\u0014\u001c\u001e\u0003\u0004\"\b\u000f\u0005\n\u0007\u0006\u0007\n\u0007\u0006\u000b\n\b\u0006\u0007\n\u0011\u0018\u0017\u0019\u0019\u0003\u0010\u001d\u001b\u0015\u0017\u0013\u0017\u0014\u001a\u0013#\n\u0010 \u0019\u0019\n\u0010\u0017\u0013\u0018\n\u0010\u0019\u0012\u0013\u0014\n\u0010 \u001e\u0016\nFig. 2: Visualizing skill learning progress. The plot shows the proficiency level of each skill throughout the process of\nlearning a simplified StowHammer task. The skill proficiency is the average normalized reward a skill receives at an iteration.\nAlgorithm 1 SKILLCURRICULUM\nhyperparameters:\nNumber of training iterations K\ninput:\nenv\n▷task environment\ng\n▷symbolic task goal\n¯Ψ\n▷state predicates\n¯Ω\n▷lifted operators\nstart\nΠ ←[π(0)\n1\n, ..., π(0)\n|¯\nΩ|]\n▷initialize all skill policies\nt ←0\nwhile Not Converged do\nD ←∅\n▷buffer for failed skills\nfor i ←[1, ..., N] do\nD ←D ∪TRYSOLVETASK(env, g, ¯Ψ, ¯Ω, Π)\nend for\nfor i, s, ω ←D do\nπ(t)\ni\n←Π[i]\nfor k ←[1, ..., K] do\nπ(t+k)\ni\n←SAC(env, s, π(t+k−1)\ni\n, ω)\n▷RL training\nend for\nΠ[i] ←π(t+K)\ni\nend for\nt ←t + K\nend while\nreturn Π\nAlgorithm 2 TRYSOLVETASK\nhyperparameters:\nMaximal skill horizon H\ninput:\nenv\n▷task environment\ng\n▷symbolic task goal\n¯Ψ\n▷state predicates\n¯Ω\n▷lifted operators\nΠ\n▷skill policies\nstart\nO, x(0) ←env.get_state()\nx(0)\nΨ\n←PARSE(x(0), O, ¯Ψ)\n▷continuous state to symbolic state\nΩ←GROUND(O, ¯Ω)\n▷get grounded operators\n[ω1, ..., ωL] ←SEARCH(x(0)\nΨ , g, Ω)\n▷found plan with length L\nD, l ←[], 0\nwhile l < L do\ni ←LOOKUPSKILL(ωl)\nπ∗←Π[i]\ns∗←env.get_sim()\n▷get simulator state\nx∗←ROLLOUT(env, π∗, H)\nif ISSUCCESS(x∗, ωl) then\nl ←l + 1\n▷advance to the next skill\ncontinue\nelse\nD ←D ∪(i, s∗, ωl)\n▷collect failed skills and states\nend if\nend while\nreturn D\nA. Experimental Setup\nWe conduct evaluations in four simulated domains, in which\nwe devise tasks that require multi-step reasoning, contact-rich\nmanipulation, and long-horizon interactions (See Fig. 3).\nStackAtTarget is to stack two cubes on a tight target region\nwith a specific order. The applicable skill operators are Pick\nand Place. Since the cubes are randomly placed in the scene,\nthe top cube may occupy the target region, in which situation\nthe robot must first remove the top cube before stacking.\nStowHammer requires the robot to stow two hammers\ninto different closed cabinets. It involves four skills: Pick,\nPlace, Pull, Push. Since the workspace is tight, the robot\nneeds to close an opened cabinet before being able to open\nthe other one, which requires multi-step reasoning.\nPegInHole is to pick up and insert two pegs into two\nhorizontal holes. The applicable operators are Pick and\nInsert. This task challenges the robot with contact-rich\nmanipulations and multi-step planning.\nMakeCoffee is to pick up a coffee pod from a closed cabi-\nnet, insert it into the holder of the coffee machine, and finally\nclose both the lid and the cabinet, The applicable operators\nare Pick, Pull, Push, CloseLid, and InsertHolder.\nThe environments are built on Robosuite [31] simulator. We\nuse a Franka Emika Panda robot arm that is controlled at\n20Hz with an operational space controller (OSC), which has\n5 degrees of freedom: end-effector position and the yaw angle\nand the position of the gripper. See Fig. 3 for an illustration.\nB. Visualize the Progressive Skill Learning Process\nBefore discussing quantitative comparisons, we seek to\ngain intuitive understanding of our progressive skill learning\nscheme (Sec. III-C), where the learning curriculum adjusts\nbased on the proficiencies of the skills. In Fig. 2, we visualize\nthe proficiency level of each skill throughout the process of\nlearning a simplified StowHammer task, where the goal is\nto stow away one hammer instead of two. The y axis is\nthe average normalized reward a skill receives. Note that we\nonly visualize a subset of skills scheduled for training at an\niteration. The corresponding behavior of each skill at a certain\nstage is visualized in the snapshots on top of the plot.\nAt the beginning of the training, the system can only reach\nthe\nprecondition\nfor\nexecuting\nthe\nPull(?cabinet)\n6\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\ncube0\ncube1\ntarget0\nAND[OnTarget(cube0, target0),\nOn(cube1, cube0)]\nTraining goal\ncabinet1\ncabinet0\nhammer1\nhammer0\nAND[InCabinet(hammer0, cabinet0),\nInCabinet(hammer1, cabinet1),\nIsCabinetClose(cabinet0),\nIsCabinetClose(cabinet1)]\nTraining goal\nhole1\npeg2\npeg1\nhole2\nAND[In(peg2, hole2),\nIn(peg1, hole1)]\nTraining goal\nholder\nlid\npod\ncabinet\nAND[In(pod, holder),\nIsLidClose(lid),\nIsCabinetClose(cabinet)]\nTraining goal\nFig. 3: Simulation task setup. We show the initialization\nof the simulation setups for the four tasks: StackAtTarget,\nStowHammer, PegInHole and MakeCoffee.\nskill\nbut\nnot\nother\nskills,\nthus\nthe\nexperience\nof\nPull(?cabinet) skill is collected and it is repeatedly\nselected for training. Until the agent is able to open one of\nthe cabinets, the second planned skill Pick(?object) is\nthen instantiated for learning and execution. Finally at the\nend of the training, all skills become proficient to be used\nto execute the entire task. The result qualitatively shows that\nLEAGUE’s automated curriculum is effective at progressively\nlearning skills to achieve long-horizon task goals.\nC. Quantitative Evaluation\nHere, we seek to highlight various aspects of our solution\nparadigm through quantitatively comparing LEAGUE with a\nnumber of strong baseline methods. Below we describe the\nbaselines and discuss the results.\n• RL (SAC): We adopt SAC [10] as a strong RL baseline.\nTo facilitate a fair comparison, we extend the vanilla task\nreward function to staged rewards using an oracle task\nplan, where the reward at each step is the summation\nof achieved rewards for each completed subgoal and the\nreward for the current subgoal.\n• Curriculum RL (CRL): We follow the main idea of\nstate-of-the-art curriculum RL approaches [22, 27], which\nstarts the training with near-success initializations and\ngradually move the initial states back to the true en-\nvironment initial states. To facilitate a fair comparison,\nwe sample the curriculum’s initial states based on the\nsubgoals of an oracle task plan (in reverse) and adopt the\nsame staged reward described above.\n• Hierarchical RL (HRL): This baseline adapts the recent\nprimitive-based HRL frameworks [4, 19] for our tasks.\nThe key idea is to train a high-level meta controller\nto compose parameterized skill primitives and atomic\nactions. We base our implementation on MAPLE [19]\nand use the oracle task plan to identify the target objects\nfor defining the affordance to guide the exploration.\n• Symb+MP: An open-loop baseline that resembles a\nvanilla TAMP framework, which greedily generates a\nmotion plan for each skill in a task plan. The robot then\nexecutes the plan through a trajectory controller.\n• Symb+RL: An ablation baseline of LEAGUE that re-\nmoves the state abstraction ( III-B) and retains all other\nfeatures including the symbolic plan-based curriculum.\nThe multi-stage nature of our evaluation tasks makes de-\nsigning smooth task-level metrics difficult. Thus we adopt task\nprogress as our metric, which is defined as the summed reward\nof all task stages normalized to [0, 1]. Below we discuss the\nmain findings based on Fig. 4.\nHigh-level reasoning is critical for solving long-horizon\ntasks. We observe that in StackAtTarget, a long-horizon task\nwith relatively simple manipulation steps, methods equipped\nwith a task planner (LEAGUE, Symb+MP, and Symb+RL)\nsignificantly outperforms all other baselines. The most com-\npetitive HRL baseline occasionally learns to move the bottom\ncube to the target region. This shows the value of explicit high-\nlevel reasoning, in particular as a plan-informed automated\ncurriculum in LEAGUE. Notably, the open loop Symb+MP\nperforms on par with LEAGUE because simple picking and\nplacing can readily be solved by open-loop trajectories.\nLEAGUE can solve long-horizon, contact-rich manip-\nulation tasks. LEAGUE significantly outperforms all other\nbaselines in StowHammer and PegInHole, which are both\nlong-horizon and require contact-rich manipulation. Notably,\nmost baselines cannot advance beyond opening the cabinet in\nStowHammer and picking up the first peg in PegInHole.\nSkill reuse is critical to learning structured tasks. Com-\nmon multi-step tasks have repeating structures, which can be\nleveraged by methods that explicitly reuse learned skills. We\nnote that both LEAGUE and Symb+RL perform competitively\nin StackAtTarget that involve repeating steps (i.e., stack two\ncubes). On the other hand, HammerPlace and PegInHole\ninvolve more objects, most of which are not relevant to a\ngiven skill. This prevents na¨ıve skill reuse — a policy may\nlearn spurious correlation to these irrelevant features — and\nnecessitates state abstraction, which we will discuss next.\nState abstraction facilitates skill reuse in complex envi-\nronments. We observe that LEAGUE outperforms Symb+RL\nin both HammerPlace and PegInHole. This shows that state\nabstraction can further improve skill reuse in complex envi-\nronments by ignoring features that are irrelevant to a skill.\nWe will also show in Sec. IV-D that skill reuse enables our\nmethod to generalize to novel task goals and domains.\nOther observations. We observe that without explicit prior\nstructures such as motion primitive, SAC baseline is able to\nexploit environment artifacts and learn shortcut behaviors. For\nexample, in the StowHammer task, SAC agent learns to grip\nthe head of the hammer to prevent slipping, but the grasping\npose precludes it from fitting the hammer to the drawer.\nMoreover, our analysis found that the CRL agent often failed\nto reach the final goal from some intermediate states due to the\nstrong sequential dependency of our evaluation tasks: the robot\nmust succeed in one stage to reach the pre-condition of the\nnext. And because the environment steps budget is distributed\nto multiple stages, CRL often underperforms other baselines\nCHENG et al.: LEAGUE\n7\n\u0006\n\u0007\n\b\n\t\n\n\u000b\n\u0010!(\u0003\u0018&\u001c#%\n\u0007\u001c\f\n\u0006\u0005\u0006\n\u0006\u0005\b\n\u0006\u0005\n\u0006\u0005\f\n\u0006\u0005\r\n\u0007\u0005\u0006\n\u0019\u001a%\u001e\u0003\u0016$\"\u001d$\u001c%%\n\u0016\u001c\u001d\u0012!\u0011\"\u001f\u001c\n\u0015'$%\n\u0018) \u001b\u0004\u0017\u0013\n\u0011\u0017\u0013\n\u000f\u0017\u0013\n\u0018\u000e\u000f\n\u0018) \u001b\u0004\u0014\u0016\n\u0006\n\u0007\n\b\n\t\n\n\u000b\n\u0010\u001f&\u0003\u0017$\u001b!#\n\u0007\u001b\f\n\u0006\u0005\u0006\n\u0006\u0005\b\n\u0006\u0005\n\u0006\u0005\f\n\u0006\u0005\r\n\u0007\u0005\u0006\n\u0018\u0019#\u001d\u0003\u0015\" \u001c\"\u001b##\n\u0017$ '\u0011\u0019\u001e\u001e\u001b\"\n\u0014%\"#\n\u0017(\u001e\u001a\u0004\u0016\u0012\n\u0011\u0016\u0012\n\u000f\u0016\u0012\n\u0017\u000e\u000f\n\u0017(\u001e\u001a\u0004\u0013\u0015\n\u0006\n\u0007\n\b\n\t\n\u000f\u001f&\u0003\u0016$\u001b!#\n\u0007\u001b\u000b\n\u0006\u0005\u0006\n\u0006\u0005\b\n\u0006\u0005\n\u0006\u0005\u000b\n\u0006\u0005\f\n\u0007\u0005\u0006\n\u0017\u0018#\u001d\u0003\u0014\" \u001c\"\u001b##\n\u0016$\u0018\u001a\u001d\r$\u0017\u0018\"\u001c\u001b$\n\u0013%\"#\n\u0016'\u001e\u0019\u0004\u0015\u0011\n\u0010\u0015\u0011\n\u000e\u0015\u0011\n\u0016\r\u000e\n\u0016'\u001e\u0019\u0004\u0012\u0014\nFig. 4: Baseline comparison. We compare relevant methods on three task domains. The plot shows the corresponding\naverage task progress during evaluation throughout training, which is measured as the summation of achieved rewards of\neach successfully executed skill in the task plan and normalized to 1. The results are reported using 5 random seeds, with the\nstandard deviation shown as the shaded area.\nTABLE I: We report the performance of applying our method\nto new task goals in the StowHammer and the PegInHole\ndomains without additional learning.\nTraining Goal\nTest Goal1\nTest Goal2\nStowHammer\n0.94 ± 0.21\n0.90 ± 0.12\n0.73 ± 0.31\nPegInHole\n0.87 ± 0.23\n0.53 ± 0.05\n1.00 ± 0.00\n(e.g., SAC) in completing the initial stages of a task.\nD. Generalization to New Tasks and Domains\nTo validate that our method can effectively generalize to\nnew task goals and even new task domains by reusing learned\nskills, we present the following experiments.\nGeneralize to new task goals. Besides evaluating the training\ngoals (shown in Fig. 3), we directly test our models on\nnew task goals for the StowHammer and the PegInHole\ndomains. For StowHammer domain, the first test goal is to\nswap the hammer-cabinet mapping. The second test goal is\nto place hammer1 into cabinet0 and keep cabinet1\nopen. For PegInHole, the first test goal is to swap the peg-\nhole mapping. The second goal is to only insert peg1 into\nhole2. The results are in Table I. We observe that LEAGUE\nexperiences little performance drop when generalizing to new\ntask goals without additional training, demonstrating strong\ncompositional generalization capability and skill modularity.\nQuick adaptation to new domains. Another exciting pos-\nsibility of LEAGUE is to transfer skills learned from one\ndomain to another. We design an experiment to validate this\nfeature. The target domain is MakeCoffee, which is the most\nchallenging task of the four. We adapt skills Pick, Pull, and\nPush learned in the StowHammer domain for learning the\nMakeCoffee task. As shown in Fig. 6, compared to learning\nfrom scratch, transferring learned skills can significantly accel-\nerate learning (the x-axis is shorter than in Fig. 4) and enables\nthe robot to solve the entire task. This highlights LEAGUE’s\nstrong potential for continual learning.\nE. Real World Demonstration\nWe demonstrate transferring simulation-trained LEAGUE\nsystem to two real-world task domains: StackThreeAtTarget\nand StowObject. For the StackThreeAtTarget task, we ran-\ndomly place three cubes and a target region on the table. The\ntask is to stack the cubes at the target region. We directly\nreuse the skills trained in StackAtTarget in simulation to\ndemonstrate generalization to different number of objects and\ninitial conditions. The StowObject is to stow two objects into\ntwo cabins. Similar to StowHammer, the task also requires\nthe robot to operate the cabinets. We reuse skills trained in\nthe simulated StowHammer domain.\nOur system uses a Franka Emika Panda robot. We take\nRGBD images from an Intel RealSense D435 camera and use\nAprilTag [21] to detect the 6D poses of task-relevant objects.\nOur system performs state estimation prior to each skill execu-\ntion, synchronizes the states to a simulated environment, and\nexecutes each skill generated by LEAGUE from the simulated\nenvironment through open-loop control.\nFig. 5 shows the key frames of three task execution pro-\ncesses and the corresponding task goals. Our system achieves\nan 8\/10 success rate for the StackThreeAtTarget task, and a\n6\/10 success rate for the StowObject task. The failure mode\nfor the StackThreeAtTarget task is that the AprilTags getting\noccluded from the camera in some initial configurations. The\nfailure mode for StowObject task is that sometimes the learned\npolicy is not able to generate a valid motion for operating the\ndrawer, and the objects slipping from the gripper.\nV. CONCLUSIONS, LIMITATIONS, AND FUTURE WORKS\nWe introduced LEAGUE, an integrated task planning and\nskill learning framework that represents a virtuous-cycle sys-\ntem: It leverages the high-level reasoning ability and abstrac-\ntion of a TAMP framework to facilitate the exploration and\ngeneralization of an RL skill learner, which in turn expands\nthe capability of the overall system. Through challenging\nmanipulation tasks in both simulation and the real world,\nwe demonstrated that LEAGUE is effective at solving long-\nhorizon tasks and generalizing to new tasks and domains.\nWhile empirically effective, our method does have a number\nof limitations. As we discussed in Sec. III-A, we assume\naccess to a library of skill operators that serve as the basis\nfor skill learning. Relatedly, our assumptions for skill-relevant\nstate abstraction, although effective, may not hold in certain\ncases (e.g. unintended consequences during exploration). A\npossible path to address both challenges is to learn skill\noperators with sparse transition models from unstructured\nexperiences [23, 25]. Second, our skill learning process re-\nlies on the environment-provided dense reward function. RL\nalgorithms that can better learn from sparse reward would\n8\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\nGoal: AND[OnTarget(cube2, target0), On(cube1, cube2), On(cube0, cube1)]\nGoal: AND[OnTarget(cube0, target0), On(cube1, cube0), On(cube2, cube1)]\nGoal: AND[InCabinet(hammer0, cabinet0), InCabinet(marker0, cabinet1), IsCabinetClosed(cabinet0), IsCabinetClosed(cabinet1)]\nFig. 5: Real robot demonstration. Key frames of three task execution processes (bottom) and their final task goals (top).\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nEnv Steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTask Progress\nMakeCoffee\nOurs - skill reusing\nOurs - from scratch\nFig. 6: Generalization to new domain. For MakeCoffee task,\nwe compare (a) learning the task from scratch and (b) learning\nby adapting the skills (Pick, Pull, and Push) learned from\nthe StowHammer domain.\nallow LEAGUE to build a tighter connection with the symbolic\nspace. Finally, in the real-world setting, LEAGUE is limited\nby the capability of the off-the-shelf perception algorithms. We\nplan to explore learning visuomotor control policies to make\nLEAGUE easier to deploy in the real world.\nREFERENCES\n[1] David Abel, Nate Umbanhowar, Khimya Khetarpal, Dilip Arumugam,\nDoina Precup, and Michael Littman.\nValue preserving state-action\nabstractions. In AISTATS, 2020.\n[2] Pierre-Luc Bacon, Jean Harb, and Doina Precup.\nThe option-critic\narchitecture. In AAAI, 2017.\n[3] Rohan Chitnis, Tom Silver, Beomjoon Kim, Leslie Kaelbling, and\nTomas Lozano-Perez. Camps: Learning context-specific abstractions for\nefficient planning in factored mdps. In CoRL, 2021.\n[4] Murtaza Dalal, Deepak Pathak, and Russ R Salakhutdinov. Accelerat-\ning robotic reinforcement learning via parameterized action primitives.\nNeurIPS, 2021.\n[5] Scott Emmons, Ajay Jain, Misha Laskin, Thanard Kurutach, Pieter\nAbbeel, and Deepak Pathak.\nSparse graphical memory for robust\nplanning. NeurIPS, 2020.\n[6] Kuan Fang, Yuke Zhu, Silvio Savarese, and L Fei-Fei.\nAdaptive\nprocedural task generation for hard-exploration problems.\nIn ICLR,\n2020.\n[7] Maria Fox and Derek Long.\nPddl2. 1: An extension to pddl for\nexpressing temporal planning domains. JAIR, 2003.\n[8] Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim,\nTom Silver, Leslie Pack Kaelbling, and Tom´as Lozano-P´erez. Integrated\ntask and motion planning. Annu. Rev. Control Robot. Auton. Syst., 2021.\n[9] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep\nreinforcement learning for robotic manipulation with asynchronous off-\npolicy updates. In ICRA, 2017.\n[10] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft\nactor-critic: Off-policy maximum entropy deep reinforcement learning\nwith a stochastic actor. In ICML, 2018.\n[11] Bradley Hayes and Brian Scassellati.\nAutonomously constructing\nhierarchical task networks for planning and human-robot collaboration.\nIn ICRA, 2016.\n[12] Le´on Illanes, Xi Yan, Rodrigo Toro Icarte, and Sheila A McIlraith.\nSymbolic plans as high-level instructions for reinforcement learning.\nIn ICAPS, 2020.\n[13] Rico Jonschkowski and Oliver Brock.\nLearning state representations\nwith robotic priors. Auton. Robots, 2015.\n[14] Leslie Pack Kaelbling and Tom´as Lozano-P´erez. Hierarchical task and\nmotion planning in the now. In ICRA, 2011.\n[15] George Konidaris, Leslie Pack Kaelbling, and Tomas Lozano-Perez.\nFrom skills to symbols: Learning symbolic representations for abstract\nhigh-level planning. JAIR, 2018.\n[16] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-\nto-end training of deep visuomotor policies. JMLR, 2016.\n[17] Jacky Liang, Mohit Sharma, Alex LaGrassa, Shivam Vats, Saumya\nSaxena, and Oliver Kroemer. Search-based task planning with learned\nskill effect models for lifelong robotic manipulation. In ICRA, 2022.\n[18] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E\nTaylor, and Peter Stone. Curriculum learning for reinforcement learning\ndomains: A framework and survey. JMLR, 2020.\n[19] Soroush Nasiriany, Huihan Liu, and Yuke Zhu. Augmenting reinforce-\nment learning with behavior primitives for diverse manipulation tasks.\nIn ICRA, 2022.\n[20] Negin Nejati, Pat Langley, and Tolga Konik. Learning hierarchical task\nnetworks by observation. In ICML, 2006.\n[21] Edwin Olson. AprilTag: A robust and flexible visual fiducial system. In\nICRA, 2011.\n[22] Archit Sharma, Abhishek Gupta, Sergey Levine, Karol Hausman, and\nChelsea Finn. Autonomous reinforcement learning via subgoal curricula.\nNeurIPS, 2021.\n[23] Tom Silver, Rohan Chitnis, Joshua Tenenbaum, Leslie Pack Kaelbling,\nand Tom´as Lozano-P´erez.\nLearning symbolic operators for task and\nmotion planning. In IROS, 2021.\n[24] Tom Silver, Ashay Athalye, Joshua B Tenenbaum, Tom´as Lozano-P´erez,\nand Leslie Pack Kaelbling. Learning neuro-symbolic skills for bilevel\nplanning. In CoRL, 2022.\n[25] Tom Silver, Rohan Chitnis, Nishanth Kumar, Willie McClinton, Tomas\nLozano-Perez, Leslie Pack Kaelbling, and Joshua Tenenbaum. Predicate\ninvention for bilevel planning. In AAAI, 2023.\n[26] Marc Toussaint. Logic-geometric programming: An optimization-based\napproach to combined task and motion planning. In IJCAI, 2015.\n[27] Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan,\nJos´ephine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao\nJiao, et al. Jump-start reinforcement learning. arXiv, 2022.\n[28] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas\nHeess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal\nnetworks for hierarchical reinforcement learning. In ICML, 2017.\n[29] Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei,\nand Silvio Savarese. Neural task programming: Learning to generalize\nacross hierarchical tasks. In ICRA, 2018.\n[30] Danfei Xu, Ajay Mandlekar, Roberto Mart´ın-Mart´ın, Yuke Zhu, Silvio\nSavarese, and Li Fei-Fei. Deep affordance foresight: Planning through\nwhat can be done in the future. In ICRA, 2021.\n[31] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart´ın-Mart´ın.\nrobosuite: A modular simulation framework and benchmark for robot\nlearning. In arXiv, 2020.\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | LEAGUE：基于引导的技能学习和抽象，助力机器人解决长期操作任务\n\n## 📌 背景痛点\/本文动机\n随着人工智能技术的不断发展，机器人已经逐渐走进我们的日常生活，并在各种场景中发挥着重要作用。然而，要让机器人真正实现自主学习和操作，仍然面临着许多挑战。其中，长期操作任务（long-horizon tasks）的解决和泛化能力是机器人领域的一大难题。现有的深度强化学习（DRL）方法在自主学习方面表现出色，但在大型环境中实现长期目标仍然存在困难。另一方面，任务和运动规划（TAMP）方法擅长解决和泛化长期任务，但由于其依赖于预定义的技能集，限制了其在现实世界中的应用。\n\n## 🚀 核心方法\n为了克服上述挑战，本文提出了LEAGUE（Learning and Abstraction with Guidance）框架，该框架结合了DRL和TAMP的优势，实现了长期操作任务的解决和泛化。LEAGUE的核心创新点包括：\n\n💡 创新点1：利用任务规划器的符号接口指导基于强化学习的技能学习，并创建抽象状态空间以实现技能复用。\n💡 创新点2：在任务规划系统中学习操作技能，不断扩展其能力和可解决的任务集。\n\n## 📈 实验结果\n本文在四个具有挑战性的模拟任务领域对LEAGUE进行了评估，结果表明LEAGUE在性能上显著优于基线方法。此外，本文还展示了学习到的技能可以复用于加速新任务领域的学习，并迁移到物理机器人平台。\n\n## 💬 可借鉴之处\nLEAGUE框架为机器人解决长期操作任务提供了一种新的思路，其核心思想可以应用于其他领域，例如自动驾驶、游戏AI等。此外，LEAGUE框架中的技能学习和抽象方法也可以为其他强化学习算法提供借鉴。","llm_summary_res_status":200}
{"title":"Learning from Visual Observation via Offline Pretrained State-to-Go Transformer","authors":"Bohan Zhou, Ke Li, Jiechuan Jiang, Zongqing Lu","summary":"Learning from visual observation (LfVO), aiming at recovering policies from\nonly visual observation data, is promising yet a challenging problem. Existing\nLfVO approaches either only adopt inefficient online learning schemes or\nrequire additional task-specific information like goal states, making them not\nsuited for open-ended tasks. To address these issues, we propose a two-stage\nframework for learning from visual observation. In the first stage, we\nintroduce and pretrain State-to-Go (STG) Transformer offline to predict and\ndifferentiate latent transitions of demonstrations. Subsequently, in the second\nstage, the STG Transformer provides intrinsic rewards for downstream\nreinforcement learning tasks where an agent learns merely from intrinsic\nrewards. Empirical results on Atari and Minecraft show that our proposed method\noutperforms baselines and in some tasks even achieves performance comparable to\nthe policy learned from environmental rewards. These results shed light on the\npotential of utilizing video-only data to solve difficult visual reinforcement\nlearning tasks rather than relying on complete offline datasets containing\nstates, actions, and rewards. The project's website and code can be found at\nhttps:\/\/sites.google.com\/view\/stgtransformer.","url":"http:\/\/arxiv.org\/abs\/2306.12860v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2306.12860v1","published":1687439699000,"comment":"19 pages","pdf_text":"Learning from Visual Observation via Offline\nPretrained State-to-Go Transformer\nBohan Zhou12\nKe Li2\nJiechuan Jiang12\nZongqing Lu12†\n1PKU\n2BAAI\nAbstract\nLearning from visual observation (LfVO), aiming at recovering policies from\nonly visual observation data, is promising yet a challenging problem. Existing\nLfVO approaches either only adopt inefficient online learning schemes or require\nadditional task-specific information like goal states, making them not suited for\nopen-ended tasks. To address these issues, we propose a two-stage framework for\nlearning from visual observation. In the first stage, we introduce and pretrain State-\nto-Go (STG) Transformer offline to predict and differentiate latent transitions of\ndemonstrations. Subsequently, in the second stage, the STG Transformer provides\nintrinsic rewards for downstream reinforcement learning tasks where an agent\nlearns merely from intrinsic rewards. Empirical results on Atari and Minecraft show\nthat our proposed method outperforms baselines and in some tasks even achieves\nperformance comparable to the policy learned from environmental rewards. These\nresults shed light on the potential of utilizing video-only data to solve difficult\nvisual reinforcement learning tasks rather than relying on complete offline datasets\ncontaining states, actions, and rewards. The project’s website and code can be\nfound at https:\/\/sites.google.com\/view\/stgtransformer.\nSTG Transformer\nEncoder\nTemporally-aligned\nRepresentation Learning\nPolicy\nEncoder\nIntrinsic\nObserve\nTransfer\nPPO\nStage 1:Offline Pretraining\nStage 2:Online RL\nInput\nInput\nTransition \nDiscrimination\nFigure 1: A two-stage framework for learning from visual observation. The first stage involves three\nconcurrently pretrained components. A feature encoder is trained in a self-supervised manner to\nprovide easily predicted and temporally aligned representations for stacked-image states. State-to-Go\n(STG) Transformer is trained in an adversarial way to accurately predict transitions in latent space.\nA discriminator is updated simultaneously to distinguish state transitions of prediction from expert\ndemonstrations, which provides high-quality intrinsic rewards for downstream online reinforcement\nlearning in the next stage.\n†Correspondence to Zongqing Lu <zongqing.lu@pku.edu.cn>\nPreprint. Under review.\narXiv:2306.12860v1  [cs.LG]  22 Jun 2023\n1\nIntroduction\nReinforcement learning (RL) from scratch imposes significant challenges due to sample inefficiency\nand hard exploration in environments with sparse rewards. This has led to increased interest in\nimitation learning (IL). IL agents learn policies by imitating expert demonstrations in a data-driven\nmanner rather than through trial-and-error processes. It has been proven effective in various domains,\nincluding games [1] and robotics [2].\nHowever, acquiring demonstrated actions can be expensive or impractical, e.g., from videos that\nare largely available though, leading to the development of learning from observation (LfO) [3, 4,\n5, 6, 7, 8, 9]. This line of research utilizes observation-only data about agent behaviors and state\ntransitions for policy learning. Humans naturally learn from visual observation without requiring\nexplicit action guidance, such as beginners in video games improving their skills by watching skilled\nplayers’ recordings. However, LfO agents face challenges in extracting useful features from raw\nvisual observations and using them to train a policy due to the lack of explicit action information.\nThus, further study of learning from visual observation (LfVO) has the potential to grant agents\nhuman-like learning capabilities.\nIn this paper, we investigate a reinforcement learning setting in which agents learn from visual\nobservation to play challenging video games, such as Atari and Minecraft. Many existing LfO\napproaches [4, 5, 6, 7, 8] only apply to vector-observation environments, such as MuJoCo, while\nothers explicitly consider or can be applied to high-dimensional visual observations. Among them,\nrepresentation-learning methods [9, 10, 11] learn effective visual representations and recover an\nintrinsic reward function from them. However, most of these methods only excel in continuous\ncontrol tasks, exhibiting certain limitations when applied to video games as we show in experiments\nlater. Adversarial methods [3, 12, 13] learn an expert-agent observation discriminator online to\ndirectly indicate visual differences. However, noises or local changes in visual observations may\neasily cause misclassification [14]. In [12, 13], additional proprioceptive features (e.g., joint angles)\nare used to train a discriminator, which are unavailable in environments that only provide visual\nobservations. Moreover, as these methods require online training, sample efficiency is much lower\ncompared to offline learning. Goal-oriented methods, like [15], evaluate the proximity of each visual\nobservation to expert demonstrations or predefined goals. However, defining explicit goals is often\nimpractical in open-ended tasks [16]. Furthermore, the continuity of observation sequences in video\ngames cannot be guaranteed due to respawn settings or unanticipated events.\nTo address these limitations and hence enable RL agents to effectively learn from visual observation,\nwe propose a general two-stage framework that leverages visual observations of expert demonstrations\nto guide online RL. In the first stage, unlike existing online adversarial methods, we introduce and\npretrain State-to-Go (STG) Transformer, a variant of Decision Transformer (DT) [17], for offline\npredicting transitions in latent space. In the meanwhile, temporally-aligned and predictable visual\nrepresentations are learned. Together, a discriminator is trained to differentiate expert transitions,\ngenerating intrinsic rewards to guide downstream online RL training in the second stage. That said,\nin the second stage, agents learn merely from generated intrinsic rewards without environmental\nreward signals. Our empirical evaluation reveals significant improvements in both sample efficiency\nand overall performance across various video games, demonstrating the effectiveness of the proposed\nframework.\nOur main contributions are as follows:\n• We propose a general two-stage framework, providing a novel way to enable agents to\neffectively learn from visual observation. We introduce State-to-Go Transformer, which\nis pretrained offline merely on visual observations and then employed to guide online\nreinforcement learning without environmental rewards.\n• We simultaneously learn a discriminator and a temporal distance regressor for temporally-\naligned embeddings while predicting latent transitions. We demonstrate that the jointly\nlearned representations lead to enhanced performance in downstream RL tasks.\n• Through extensive experiments in Atari and Minecraft, we demonstrate that the proposed\nmethod substantially outperforms baselines and even achieves performance comparable to\nthe policies learned from environmental rewards in some games, underscoring the potential\nof leveraging offline video-only data for reinforcement learning.\n2\n2\nRelated Work\nLearning from Observation (LfO) is a more challenging setting than imitation learning (IL),\nin which an agent learns from a set of demonstrated observations to complete a task without the\nassistance of action or reward guidance. Many existing works [5, 18, 19, 20] attempt to train\nan inverse dynamic model to label observation-only demonstrations with expert actions, enabling\nbehavior cloning. However, these methods often suffer from compounding error [21]. On the other\nhand, [4] learns a latent policy and a latent forward model, but the latent actions can sometimes\nbe ambiguous and may not correspond accurately with real actions. GAIfO [3], inspired by [22],\nis an online adversarial framework that couples the process of learning from expert observations\nwith RL training. GAIfO learns a discriminator to evaluate the similarity between online-collected\nobservations and expert demonstrations. Although helpful in mitigating compounding error [3], it\nshows limited applicability in environments with high-dimensional observations. Follow-up methods\n[12, 13] pay more attention to visual-observation environments, but require vector-state in expert\nobservations to either learn a feasible policy or proper visual representations. More importantly,\nlearning a discriminator online is less sample-efficient, compared to LfO via offline pretraining. A\nrecent attempt [23] demonstrates some progress in action-free offline pretraining, but return-to-gos\nare indispensable in addition to observations because of upside down reinforcement learning (UDRL)\nframework [24]. Moreover, it only shows satisfactory results in vector-observation environments\nlike MuJoCo. In this work, we focus on reinforcement learning from offline pretraining on visual\nobservations, which is a more general and practical setting.\nVisual Representation Learning in RL. High-quality visual representations are crucial for LfVO.\nMany previous works [25, 26, 27, 9, 10, 11] have contributed to this in various ways. For example,\n[25] employs GANs to learn universal representations from different viewpoints, and [26, 27]\nlearn representations via contrastive learning to associate pairs of observations separated by a short\ntime difference. In terms of LfVO, a wide range of methods such as [9, 10, 11] learn temporally\ncontinuous representations in a self-supervised manner and utilize temporal distance to assess the\nprogress of demonstrations. They are easy to implement but are usually applied in robotic control\ntasks. Nevertheless, in games like Atari, adjacent image observations may exhibit abrupt or subtle\nchanges due to respawn settings or unanticipated events, not following a gradual change along the\ntimeline. Moreover, over-reliance on temporal information often results in over-optimistic estimates\nof task progress [15], potentially misleading RL training.\nTransformer in RL. Transformer [28] is widely acknowledged as a kind of powerful structure for\nsequence modeling, which has led to domination in a variety of offline RL tasks. Decision Transformer\n(DT) [17] and Trajectory Transformer (TT) [29] redefine the offline RL problem as a context-\nconditioned sequential problem to learn an offline policy directly, following the UDRL framework\n[24]. DT takes states, actions, and return-to-gos as inputs and autoregressively predicts actions to\nlearn a policy. TT predicts the complete sequence dimension by dimension and uses beam search\nfor planning. MGDT [30] samples from a learned return distribution to avoid manually selecting\nexpert-level returns as DT. ODT [31] extends DT to bridge the gap between offline pretraining and\nonline fine-tuning.\n3\nMethodology\n3.1\nPreliminaries\nReinforcement Learning. The RL problem can be formulated as a Markov decision process (MDP)\n[32], which can be represented by a tuple M =< S, A, P, R, γ, ρ0 >. S denotes the state space\nand A denotes the action space. P : S × A × S →[0, 1) is the state transition function and\nR : S × A →R is the reward function. γ ∈[0, 1] is the discount factor and ρ0 : S →[0, 1]\nrepresents the initial state distribution. The objective is to find a policy π(a|s) : S →A, which\nmaximizes the expected discounted return:\nJ(π) = Eρ0,at∼π(·|st),st∼P\n\" ∞\nX\nt=0\nγtr (st, at)\n#\n.\n(1)\nTransformer. Stacked self-attention layers with residual connections in Transformer is instrumental\nin processing long-range dependencies, each of which embeds n input tokens {xi}n\ni=1 and outputs\n3\nn embeddings {zi}n\ni=1 of the same dimensions considering the information of the whole sequence.\nIn this study, we utilize the GPT [33] architecture, an extension of the Transformer model, that\nincorporates a causal self-attention mask to facilitate autoregressive generation. Specifically, each\ninput token xi is mapped to a key ki, a query qi, and a value vi through linear transformations,\nwhere zi is obtained by computing the weighted sum of history values v1:i, with attention weights\ndetermined by the normalized dot product between the query qi and history keys k1:i:\nzi =\ni\nX\nj=1\nsoftmax({q⊺\ni , kj′}i\nj′=1)j · vj.\n(2)\nThe GPT model only attends to the previous tokens in the sequence during training and inference,\nthereby avoiding the leakage of future information, which is appropriate in state prediction.\nLearning from Observation. The goal is to learn a policy from an expert state sequence dataset\nDe = {τ 1, τ 2, . . . , τ m}, τ i = {si\n1, si\n2, . . . , si\nn}, si\nj ∈S. Denote the transition distribution as µ(s, s′).\nThe objective of LfO can be formulated as a distribution matching problem, finding a policy that\nminimizes the f-divergence between µπ(s, s′) induced by the agent and µe(s, s′) induced by the\nexpert [7]:\nJLfO (π) = Eτ i∼De,(s,s′)∼τ iDf [µπ (s, s′) ∥µe (s, s′)] .\n(3)\nIt is almost impossible to learn a policy directly from the state-only dataset De. However, our\ndelicately designed framework (see Figure 1) effectively captures transition features in expert demon-\nstrations to provide informative guidance for RL agents, which will be expounded in the following.\n3.2\nOffline Pretraining Framework\nemb. + pos. enc.\ndecoder\nCausal Self-Attention Module\n84\n84\n4\nFeature Encoder\nTDR Predictor\n……\nFigure 2: State-to-Go Transformer\nSTG Transformer is built upon GPT [33] similar\nto DT [17], but with a smaller scale and more struc-\ntural modifications to better handle state sequence\nprediction tasks. Unlike DT, in our setting, neither\nthe action nor the reward can be accessible, so the\nSTG Transformer primarily focuses on predicting the\nnext state embedding given a sequence of states.\nAs depicted in Figure 2, first we concatenate a few\nconsecutive image frames in the expert dataset to ap-\nproximate a single state st. Then, a sequence of n\nstates {st, . . . , st+n−1} are encoded into a sequence\nof n token embeddings {et, . . . , et+n−1} by the fea-\nture encoder Eξ composed of several CNN layers and\na single-layer MLP, where et = Eξ(st). A group of\nlearnable positional embedding parameters is added\nto the token embedding sequence to remember tem-\nporal order. These positional-encoded embeddings\nare then processed by the causal self-attention mod-\nule which excels in incorporating information about\nthe previous state sequence to better capture temporal\ndependencies, followed by layer normalization. The\nlinear decoder outputs the final latent prediction sequence {ˆet+1, . . . , ˆet+n}. Denote the positional\nencoding, transition predicting, and linear decoding model together as Tσ. It is worth noting that\ninstead of predicting the embeddings of the next state sequence directly, we predict the embedding\nchange and combine it with token embeddings in a residual way, which is commonly applied in\ntransition prediction [4] and trajectory forecasting [34] to improve prediction quality.\nFor simplicity, in further discussion we will refer to Tσ(et) directly as the predicted ˆet+1.\nExpert Transition Discrimination. Distinguishing expert transiting patterns is the key to leveraging\nthe power of offline expert datasets to improve sample efficiency in online RL. Traditional online\nadversarial methods [3, 12, 13] employ a discriminator to maximize the logarithm probability of\ntransitions sampled from expert datasets while minimizing that from transitions collected online,\nwhich is often sample-inefficient in practice. Moreover, in the case of visual observation, the\ntraditional discriminator may rapidly and strictly differentiate expert transitions from those collected\n4\nonline within a few updates. As a result, the collected observations will be assigned substantially low\nscores, which makes it challenging for policy improvement and results in poor performance.\nTo overcome these limitations, we draw inspiration from WGAN [35] and adopt a more generalized\ndistance metric, known as the Wasserstein distance, to measure the difference between the distributions\nof expert and online transitions. Compared to the sigmoid probability limited in [0, 1], the Wasserstein\ndistance provides a wider range and more meaningful measure of the difference between two\ntransition distributions, as it captures the underlying structure rather than simply computing the\nprobability. More importantly, unlike traditional online adversarial methods like GAIfO [3] that use\nthe Jensen-Shannon divergence or Kullback-Leibler divergence, the Wasserstein distance is more\nrobust to the issues of vanishing gradients and mode collapse, making offline pretraining possible.\nSpecifically, two temporally adjacent states st, st+1 are sampled from the expert dataset, then we\nhave et = Eξ (st) , et+1 = Eξ (st+1), and ˆet+1 = Tσ (Eξ (st)). The WGAN discriminator Dω aims\nto maximize the Wasserstein distance between the distribution of expert transition (et, et+1) and the\ndistribution of predicted transition (et, ˆet+1), while the generator tries to minimize it. The objective\ncan be formulated as:\nmin\nξ,σ max\nw∈W Eτ i∼De,(st,st+1)∼τ i [Dω (Eξ (st) , Eξ (st+1)) −Dω (Eξ (st) , Tσ (Eξ (st)))] .\n(4)\n{Dω}ω∈W represents a parameterized family of functions that are 1-Lipschitz, limiting the variation\nof the gradient. We clamp the weights to a fixed box (W = [−0.01, 0.01]l) after each gradient update\nto have parameters w lie in a compact space. Besides, to suppress the potential pattern collapse,\nan additional L2 norm penalizes errors in the predicted transitions, constraining all et and ˆet in a\nconsistent representation space. Thus, the loss functions can be rewritten as follows.\nFor discriminator:\nmin\nw∈W Ldis = Eτ i∼De,(st,st+1)∼τ i [Dω (Eξ (st) , Tσ (Eξ (st))) −Dω (Eξ (st) , Eξ (st+1))] .\n(5)\nFor STG Transformer (generator):\nmin\nξ,σ Ladv + Lmse = −Eτ i∼De,st∼τ iDω (Eξ (st) , Tσ (Eξ (st)))\n+ Eτ i∼De,(st,st+1)∼τ i∥Tσ (Eξ (st)) −Eξ (st+1) ∥2.\n(6)\nBy such an approach, the discriminator can distinguish between expert and non-expert transitions\nwithout collecting online negative samples, providing an offline way to generate intrinsic rewards for\ndownstream reinforcement learning tasks.\nTemporally-Aligned Representation Learning. Having a high-quality representation is crucial\nfor latent transition prediction. To ensure the embedding is temporally aligned, we devise a self-\nsupervised auxiliary module, named temporal distance regressor (TDR). Since the time span between\nany two states si and sj in a state sequence may vary significantly, inspired by [36], we define symlog\ntemporal distance between two embeddings ei = Eξ (si) and ej = Eξ (sj):\ntij = sign(j −i) ln(1 + |j −i|).\n(7)\nThis bi-symmetric logarithmic distance helps scale the value and accurately capture the fine-grained\ntemporal variation. The TDR module Pϕ consists of MLPs with 1D self-attention for symlog\nprediction. The objective of TDR is to simply minimize the MSE loss:\nmin\nξ,ϕ Ltdr = Eτ i∼De,(si,sj)∼τ i ∥Pϕ (Eξ (si) , Eξ (sj)) −tij∥2 .\n(8)\nOffline Pretraining. In our offline pretraining, the transition predictor Tσ and transition discriminator\nDω share the same feature encoder Eξ similar to online methods [37], which allows them to both\noperate in an easily-predictable and temporally-continuous representation space.\nAt each training step, a batch of transitions is randomly sampled from the expert dataset. The model is\ntrained autoregressively to predict the next state embedding without accessing any future information.\nWhen backpropagating, Lmse and Ladv concurrently update Eξ and Tσ to provide high-quality visual\nembeddings as well as accurate embedding prediction. Ltdr is responsible for updating the Eξ and\nPϕ as an auxiliary component, and Ldis updates Dω. Algorithm 1 in Appendix A details the offline\npretraining of the STG Transformer.\n5\n3.3\nOnline Reinforcement Learning\nIntrinsic Reward. For downstream RL tasks, our idea is to guide the agent to follow the pretrained\nSTG Transformer to match the expert state transition distribution. Unlike [15], our experimental\nresults show that our WGAN model is robust enough to offer a more discriminative assessment of\nstate transitions. That is, the WGAN discriminator can clearly distinguish between the state sequences\ncollected under the learning policy and the expert state sequences, without fine-tuning. Thus, we use\nthe discrimination score as the intrinsic reward for online RL. Moreover, we do not use ‘progress’\nlike what is done in [9]. This is because, in games with multiple restarts, progress signals can easily\nbe inaccurate and hence mislead policy improvement, while the WGAN discriminator mastering the\nprinciple of transitions can often make the correct judgment. The intrinsic reward at timestep t is\nconsequently defined as follows:\nri\nt = −\n\u0014\nDω\n\u0000Eξ (st) , Tσ (Eξ (st))\n\u0001\n−Dω\n\u0000Eξ (st) , Eξ (st+1)\n\u0001\u0015\n.\n(9)\nA larger ri\nt means a smaller gap between the current transition and the expert transition.\nOnline Learning Procedure. Given an image observation sequence collected by an agent, the feature\nencoder first generates corresponding visual representations, followed by the STG Transformer\npredicting the embeddings of the next state under expert transition. Then the discriminator compares\nthe difference between real transitions and predicted transitions. Their Wasserstein distances, as\nintrinsic rewards ri, is used to calculate generalized advantage, based on which the agent policy πθ\nis updated using PPO [38]. It is worth noting that the agent learns the policy merely from intrinsic\nrewards and environmental rewards are not used.\n4\nExperiments\nIn this section, we conduct a comprehensive evaluation of our proposed STG on diverse tasks from\ntwo environments: classical Atari environment and an open-ended Minecraft environment. Among\nthe three mainstream methods mentioned in Section 1, goal-oriented methods are not appropriate\nfor comparison because there is no pre-defined target state. Therefore, we choose GAIfO [3], a\nGAN-based method that learns an online discriminator for state transitions to provide probabilistic\nintrinsic reward signals, and ELE [9], a representation-learning method that pretrains an offline\nprogress model to provide monotonically increasing progression rewards, as our baselines. Through\nextensive experiments, we answer the following questions:\n• Is our proposed framework effective and efficient in visual environments?\n• Is our offline pretrained discriminator better than the one which is trained online?\n• Does TDR make a difference to visual representations? And do we need to add ‘progress’\nrewards, as is done in ELE?\nFor each task, we conduct 4 runs with different random seeds and report the mean and standard\ndeviation. To maintain consistency across all algorithms, the same network architecture, including\nthe feature encoder and discriminator, is applied for each algorithm. For GAIfO, similar to [37], the\ndiscriminator and policy network share the same visual encoder. For ELE, we use one-step transition\nfor progress prediction, which is aligned with our STG algorithm.\n4.1\nAtari\nAtari Expert Datasets. Atari is a well-established benchmark for visual control tasks and also a\npopular testbed for evaluating the performance of various LfVO algorithms. We conduct experiments\non four Atari games: Breakout, Freeway, Qbert, and Space Invaders. To ensure the quality of expert\ndatasets, two approaches are utilized to collect expert observations. For Qbert and SpaceInvaders, we\ncollect the last 105 transitions from Google Dopamine [39] DQN replay experiences. For Breakout\nand Freeway, we find that part of the transitions from Dopamine are not exactly expert transitions.\nTherefore, we alternatively train a SAC agent [40] from scratch for 5 × 106 steps and leverage the\ntrained policy to gather approximately 50 observation trajectories in each environment to construct\nthe expert dataset.\n6\n0.0\n0.5\n1.0\n1.5\n2.0\nTransition\n×107\n0\n100\n200\n300\nSTG\nELE\nGAIfO\n(a) Breakout\n0\n1\n2\n3\n4\n5\nTransition\n×106\n0\n5\n10\n15\n20\nSTG\nELE\nGAIfO\n(b) Freeway\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n0\n10000\n20000\n30000\nSTG\nELE\nGAIfO\n(c) Qbert\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n100\n200\n300\n400\n500\n600\nSTG\nELE\nGAIfO\n(d) Space Invaders\nFigure 3: The episodic return of STG and baselines in Atari games. Poor discrimination guidance\nmay account for GAIfO’s unsatisfactory performance. Over-optimistic progress information limits\nthe capability of ELE. Our STG combines the advantage of adversarial learning and the benefit of\nrepresentation learning, showing substantially better performance in four Atari games.\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nSTG\nELE\n(a) Pick a flower\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nSTG\nELE\n(b) Milk a cow\n0\n250\n500\n750\n1000\nEpoch\n0.0\n0.1\n0.2\n0.3\n0.4\nSTG\nELE\n(c) Harvest tallgrass\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.02\n0.04\n0.06\n0.08\nSTG\nELE\n(d) Gather wool\nFigure 4: Average success rates of STG and ELE in Minecraft tasks, where STG substantially out-\nperforms ELE, demonstrating its superiority over ELE in challenging tasks with partial observations.\nPerformance in Atari Games. As illustrated in Figure 3, STG outperforms the two baselines\nacross all four games. In Breakout, STG demonstrates a significant improvement both in the final\nperformance and sample efficiency compared to the baselines. This is attributed to its ability to\nincorporate expert skills into the learned policy. We observe that the agent successfully learns to\nobtain more intrinsic rewards by bouncing the ball up into the top gaps to hit the upper-level bricks\nwithin a limited number of update steps, while the other two methods fail. In Freeway, STG rapidly\nconverges, while the baselines suffer from severe fluctuations, accentuating the data efficiency and\nrobustness of STG. In Qbert and Space Invaders, our STG achieves a prominent breakthrough in the\nlater stages, substantially outperforming ELE and GAIfO.\nIn Table 1, we further show the expert-level performance by listing the average episodic returns\nof offline datasets and PPO learned from scratch with environmental rewards for comparison. The\nfinal scores of STG in Breakout and Qbert exceed expert performance, demonstrating its remarkable\npotential for both imitating expert observations and exploring better policies simultaneously.\nTable 1: Mean final scores of last 100 episodes on Atari games. The last two columns display the\naverage episodic scores of expert datasets and PPO with environmental rewards reported in [38].\nEnvironment\nGAIfO\nELE\nSTG\nExpert\nPPO\nBreakout\n1.5\n22.0\n288.8\n212.5\n274.8\nFreeway\n0.6\n2.7\n21.8\n31.9\n32.5\nQbert\n394.4\n4698.6\n27234.1\n15620.7\n14293.3\nSpace Invaders\n260.2\n384.6\n502.1\n1093.9\n942.5\nDuring the training process, we observe that GAIfO, primarily motivated by online discrimination,\ntends to get stuck in a suboptimal policy and struggles to explore a better policy. This is because the\ndiscriminator can easily distinguish between the visual behavior of the expert and the imitator based\non relatively insignificant factors within just a few online interactions. In contrast, STG learns better\ntemporally-aligned representations in an offline manner, enabling the discriminator to detect more\nsubstantial differences. Besides, instead of relying on probability, STG employs the Wasserstein\ndistance metric to provide more nuanced and extensive reward signals. Consequently, even without\nfine-tuning during the online RL process, STG can offer valuable guidance to the RL agent.\nAdditionally, from Figure 3a and 3b we find that ELE drops in final performance primarily due\nto the over-optimistic progress, which will be further investigated in Section 4.3. In comparison,\n7\n0.0\n0.5\n1.0\n1.5\n2.0\nTransition\n×107\n0\n100\n200\n300\nSTG\nSTG-\n(a) Breakout\n0\n2\n4\nTransition\n×106\n0\n5\n10\n15\n20\nSTG\nSTG-\n(b) Freeway\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n0\n10000\n20000\n30000\nSTG\nSTG-\n(c) Qbert\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n200\n400\nSTG\nSTG-\n(d) Space Invaders\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\nSTG\nSTG-\n(e) Pick a flower\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nSTG\nSTG-\n(f) Milk a cow\n0\n250\n500\n750\n1000\nEpoch\n0.0\n0.1\n0.2\n0.3\nSTG\nSTG-\n(g) Harvest tallgrass\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.02\n0.04\n0.06\n0.08\nSTG\nSTG-\n(h) Gather wool\nFigure 5: Ablation studies on the TDR module in Atari and Minecraft tasks. The removal of the TDR\nloss from STG, denoted as STG-, induces a decline in performance and sample efficiency, revealing\nthe TDR module plays a vital role in STG.\nSTG ensures precise expert transition prediction and discriminative transition judgment, avoiding\nover-optimistically driving the agent to transfer to new states.\nIt is worth noting that, for each Atari task, we pretrain the STG Transformer using the corresponding\nindividual observation dataset. We also report the results of using multi-task datasets to pretrain the\nSTG Transformer for all Atari tasks in Appendix E.\n4.2\nMinecraft\nMinedojo [41], built upon one of the most popular video game Minecraft, provides a simulation plat-\nform with thousands of diverse open-ended tasks. In contrast to Atari games, the extensive behavioral\nrepertoire of the agent results in a considerably large observation space in a 3D viewpoint, making\nit exceedingly difficult to extract meaningful information from visual observations. Furthermore,\nopen-ended tasks necessitate the agent learns a diverse policy applicable to various objectives from a\nsmall observation dataset with a narrow expert policy distribution. Limited research has investigated\nthe efficiency of LfVO in such challenging environments. We evaluate STG on four Minecraft tasks,\nincluding “pick a flower”, “milk a cow”, “harvest tallgrass”, and “gather wool”, demonstrating its\napplicability and effectiveness in these complex settings. Among the four tasks, “gather wool” is\nthe most challenging, as it requires the agent to locate a randomly initialized sheep, shear it, and\nthen collect the wool on the ground. All four tasks are sparse-reward, where only a binary reward is\nemitted at the end of the episode, thus the performance is measured by success rates.\nMinecraft Expert Dataset. Recently, various algorithms, e.g., Plan4MC [16] and CLIP4MC [42]\nhave been proposed for Minecraft tasks. To create expert datasets, for each task, we utilize the learned\npolicies of these two algorithms to collect around 5 × 104 observations from expert trajectories.\nPerformance in Minecraft. The results on Atari show that GAIfO is inefficient in learning from\nvisual observation. Therefore, in Minecraft, we focus on the comparison between ELE and STG.\nAs depicted in Figure 4, the success rates across four Minecraft tasks reveal a consistent superiority\nof STG over ELE. Notably, in the \"milk a cow\" task, STG attains a success rate approaching 25%,\nsignificantly eclipsing the 5% success rate of ELE. The reasons for this stark contrast in performance\nare not yet entirely elucidated. However, a plausible conjecture could be attributed to the task’s\nprimary objective, i.e. locating the cow. Given STG’s adeptness in learning state transitions, it can\neffectively accomplish this subgoal. In contrast, ELE, due to its tendency for over-optimistic progress\nestimations, may lose the intended viewpoint with relative ease.\n4.3\nAblation\nTDR Ablation. We examine the role of the TDR module in enhancing performance and representation\nquality. An ablation, named STG-, is conducted by removing the TDR loss Ltdr from STG. Thus,\n8\nthe feature encoder Eξ and the STG Transformer Tσ are trained by a linear combination of Lmse and\nLadv. The results are shown in Figure 5, where STG is substantially superior to STG- in most tasks.\nSTG\nSTG-\nFigure 6: T-SNE visualization of embeddings of a sampled\ntrajectory in Qbert.\nIn order to figure out the underlying\nreasons for their discrepancy in per-\nformance, we compare the visualiza-\ntion of embeddings encoded by STG\nand STG-. We randomly select an ex-\npert trajectory from Qbert and utilize\nt-SNE projection to visualize their em-\nbedding sequences. As illustrated in\nFigure 6, the embeddings learned by\nSTG exhibit remarkable continuity, in\nstark contrast to the scattered and dis-\njoint embeddings produced by STG-.\nThe superior temporal alignment of\nthe STG representation plays a critical role in capturing latent transition patterns, thereby providing\ninstructive information for downstream RL tasks.\nProgression Reward. We conduct experiments to figure out whether it is necessary to additionally add\nprogression rewards derived from TDR, like what ELE does. We train the agent under the guidance\nof both the discriminative and progression rewards from the same pretrained STG Transformer in\nAtari tasks, denoted as STG*. As illustrated in Figure 7, STG outperforms STG* in all tasks. We\nanalyze that, similar to ELE, progression rewards from TDR over-optimistically urge the agent to\n\"keep moving\" to advance task progress, which however can negatively impact policy learning. For\nexample, on certain conditions such as Breakout or Freeway, maintaining a stationary position may\nfacilitate catching the ball or avoiding collision more easily, thereby yielding higher returns in the\nlong run. Therefore, we do not include the over-optimistic progression rewards in our design.\n0.0\n0.5\n1.0\n1.5\n2.0\nTransition\n×107\n0\n100\n200\n300\nSTG\nSTG*\n(a) Breakout\n0\n2\n4\nTransition\n×106\n0\n5\n10\n15\n20\nSTG\nSTG*\n(b) Freeway\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n0\n10000\n20000\n30000\nSTG\nSTG*\n(c) Qbert\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n200\n400\nSTG\nSTG*\n(d) Space Invaders\nFigure 7: Atari experiments comparing using discriminative rewards (STG) and using both discrimi-\nnative rewards and progression rewards (STG*).\nIn summary, our experimental results provide strong evidence for the ability of STG to learn from\nvisual observation, substantially outperforming baselines in a variety of tasks. The ablation study\nhighlights the importance of the TDR module for temporally aligned representations. However, TDR\nmay not be used to generate progression rewards that drive over-optimistic behaviors.\n5\nConclusion and Future Work\nIn this paper, we introduce the State-To-Go (STG) Transformer, offline pretrained to predict latent\nstate transitions in an adversarial way, for learning from visual observation to boost downstream\nreinforcement learning tasks. Our STG, tested across diverse Atari and Minecraft tasks, demonstrates\nsuperior robustness, sample efficiency, and performance compared to baseline approaches. We are\noptimistic that STG offers an effective solution in situations with plentiful video demonstrations,\nlimited environment interactions, and where labeling action is expensive or infeasible.\nIn future work, it would be worthwhile to combine our STG model with a more robust large-scale\nvision foundation model to facilitate generalization across a broader range of related tasks. Besides,\nour method can extend to a hierarchical framework where one-step predicted rewards can be employed\nfor training low-level policies and multi-step rewards for the high-level policy, which is expected to\nimprove performance and solve long-horizon tasks.\n9","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Learning from Visual Observation via Offline Pretrained State-to-Go Transformer.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nLearning from Visual Observation via Offline Pretrained State-to-Go Transformer\n```\n#### 2. 论文摘要\n```\nLearning from visual observation (LfVO), aiming at recovering policies from\nonly visual observation data, is promising yet a challenging problem. Existing\nLfVO approaches either only adopt inefficient online learning schemes or\nrequire additional task-specific information like goal states, making them not\nsuited for open-ended tasks. To address these issues, we propose a two-stage\nframework for learning from visual observation. In the first stage, we\nintroduce and pretrain State-to-Go (STG) Transformer offline to predict and\ndifferentiate latent transitions of demonstrations. Subsequently, in the second\nstage, the STG Transformer provides intrinsic rewards for downstream\nreinforcement learning tasks where an agent learns merely from intrinsic\nrewards. Empirical results on Atari and Minecraft show that our proposed method\noutperforms baselines and in some tasks even achieves performance comparable to\nthe policy learned from environmental rewards. These results shed light on the\npotential of utilizing video-only data to solve difficult visual reinforcement\nlearning tasks rather than relying on complete offline datasets containing\nstates, actions, and rewards. The project's website and code can be found at\nhttps:\/\/sites.google.com\/view\/stgtransformer.\n```\n\n#### 3. 论文全文\n```\nLearning from Visual Observation via Offline\nPretrained State-to-Go Transformer\nBohan Zhou12\nKe Li2\nJiechuan Jiang12\nZongqing Lu12†\n1PKU\n2BAAI\nAbstract\nLearning from visual observation (LfVO), aiming at recovering policies from\nonly visual observation data, is promising yet a challenging problem. Existing\nLfVO approaches either only adopt inefficient online learning schemes or require\nadditional task-specific information like goal states, making them not suited for\nopen-ended tasks. To address these issues, we propose a two-stage framework for\nlearning from visual observation. In the first stage, we introduce and pretrain State-\nto-Go (STG) Transformer offline to predict and differentiate latent transitions of\ndemonstrations. Subsequently, in the second stage, the STG Transformer provides\nintrinsic rewards for downstream reinforcement learning tasks where an agent\nlearns merely from intrinsic rewards. Empirical results on Atari and Minecraft show\nthat our proposed method outperforms baselines and in some tasks even achieves\nperformance comparable to the policy learned from environmental rewards. These\nresults shed light on the potential of utilizing video-only data to solve difficult\nvisual reinforcement learning tasks rather than relying on complete offline datasets\ncontaining states, actions, and rewards. The project’s website and code can be\nfound at https:\/\/sites.google.com\/view\/stgtransformer.\nSTG Transformer\nEncoder\nTemporally-aligned\nRepresentation Learning\nPolicy\nEncoder\nIntrinsic\nObserve\nTransfer\nPPO\nStage 1:Offline Pretraining\nStage 2:Online RL\nInput\nInput\nTransition \nDiscrimination\nFigure 1: A two-stage framework for learning from visual observation. The first stage involves three\nconcurrently pretrained components. A feature encoder is trained in a self-supervised manner to\nprovide easily predicted and temporally aligned representations for stacked-image states. State-to-Go\n(STG) Transformer is trained in an adversarial way to accurately predict transitions in latent space.\nA discriminator is updated simultaneously to distinguish state transitions of prediction from expert\ndemonstrations, which provides high-quality intrinsic rewards for downstream online reinforcement\nlearning in the next stage.\n†Correspondence to Zongqing Lu <zongqing.lu@pku.edu.cn>\nPreprint. Under review.\narXiv:2306.12860v1  [cs.LG]  22 Jun 2023\n1\nIntroduction\nReinforcement learning (RL) from scratch imposes significant challenges due to sample inefficiency\nand hard exploration in environments with sparse rewards. This has led to increased interest in\nimitation learning (IL). IL agents learn policies by imitating expert demonstrations in a data-driven\nmanner rather than through trial-and-error processes. It has been proven effective in various domains,\nincluding games [1] and robotics [2].\nHowever, acquiring demonstrated actions can be expensive or impractical, e.g., from videos that\nare largely available though, leading to the development of learning from observation (LfO) [3, 4,\n5, 6, 7, 8, 9]. This line of research utilizes observation-only data about agent behaviors and state\ntransitions for policy learning. Humans naturally learn from visual observation without requiring\nexplicit action guidance, such as beginners in video games improving their skills by watching skilled\nplayers’ recordings. However, LfO agents face challenges in extracting useful features from raw\nvisual observations and using them to train a policy due to the lack of explicit action information.\nThus, further study of learning from visual observation (LfVO) has the potential to grant agents\nhuman-like learning capabilities.\nIn this paper, we investigate a reinforcement learning setting in which agents learn from visual\nobservation to play challenging video games, such as Atari and Minecraft. Many existing LfO\napproaches [4, 5, 6, 7, 8] only apply to vector-observation environments, such as MuJoCo, while\nothers explicitly consider or can be applied to high-dimensional visual observations. Among them,\nrepresentation-learning methods [9, 10, 11] learn effective visual representations and recover an\nintrinsic reward function from them. However, most of these methods only excel in continuous\ncontrol tasks, exhibiting certain limitations when applied to video games as we show in experiments\nlater. Adversarial methods [3, 12, 13] learn an expert-agent observation discriminator online to\ndirectly indicate visual differences. However, noises or local changes in visual observations may\neasily cause misclassification [14]. In [12, 13], additional proprioceptive features (e.g., joint angles)\nare used to train a discriminator, which are unavailable in environments that only provide visual\nobservations. Moreover, as these methods require online training, sample efficiency is much lower\ncompared to offline learning. Goal-oriented methods, like [15], evaluate the proximity of each visual\nobservation to expert demonstrations or predefined goals. However, defining explicit goals is often\nimpractical in open-ended tasks [16]. Furthermore, the continuity of observation sequences in video\ngames cannot be guaranteed due to respawn settings or unanticipated events.\nTo address these limitations and hence enable RL agents to effectively learn from visual observation,\nwe propose a general two-stage framework that leverages visual observations of expert demonstrations\nto guide online RL. In the first stage, unlike existing online adversarial methods, we introduce and\npretrain State-to-Go (STG) Transformer, a variant of Decision Transformer (DT) [17], for offline\npredicting transitions in latent space. In the meanwhile, temporally-aligned and predictable visual\nrepresentations are learned. Together, a discriminator is trained to differentiate expert transitions,\ngenerating intrinsic rewards to guide downstream online RL training in the second stage. That said,\nin the second stage, agents learn merely from generated intrinsic rewards without environmental\nreward signals. Our empirical evaluation reveals significant improvements in both sample efficiency\nand overall performance across various video games, demonstrating the effectiveness of the proposed\nframework.\nOur main contributions are as follows:\n• We propose a general two-stage framework, providing a novel way to enable agents to\neffectively learn from visual observation. We introduce State-to-Go Transformer, which\nis pretrained offline merely on visual observations and then employed to guide online\nreinforcement learning without environmental rewards.\n• We simultaneously learn a discriminator and a temporal distance regressor for temporally-\naligned embeddings while predicting latent transitions. We demonstrate that the jointly\nlearned representations lead to enhanced performance in downstream RL tasks.\n• Through extensive experiments in Atari and Minecraft, we demonstrate that the proposed\nmethod substantially outperforms baselines and even achieves performance comparable to\nthe policies learned from environmental rewards in some games, underscoring the potential\nof leveraging offline video-only data for reinforcement learning.\n2\n2\nRelated Work\nLearning from Observation (LfO) is a more challenging setting than imitation learning (IL),\nin which an agent learns from a set of demonstrated observations to complete a task without the\nassistance of action or reward guidance. Many existing works [5, 18, 19, 20] attempt to train\nan inverse dynamic model to label observation-only demonstrations with expert actions, enabling\nbehavior cloning. However, these methods often suffer from compounding error [21]. On the other\nhand, [4] learns a latent policy and a latent forward model, but the latent actions can sometimes\nbe ambiguous and may not correspond accurately with real actions. GAIfO [3], inspired by [22],\nis an online adversarial framework that couples the process of learning from expert observations\nwith RL training. GAIfO learns a discriminator to evaluate the similarity between online-collected\nobservations and expert demonstrations. Although helpful in mitigating compounding error [3], it\nshows limited applicability in environments with high-dimensional observations. Follow-up methods\n[12, 13] pay more attention to visual-observation environments, but require vector-state in expert\nobservations to either learn a feasible policy or proper visual representations. More importantly,\nlearning a discriminator online is less sample-efficient, compared to LfO via offline pretraining. A\nrecent attempt [23] demonstrates some progress in action-free offline pretraining, but return-to-gos\nare indispensable in addition to observations because of upside down reinforcement learning (UDRL)\nframework [24]. Moreover, it only shows satisfactory results in vector-observation environments\nlike MuJoCo. In this work, we focus on reinforcement learning from offline pretraining on visual\nobservations, which is a more general and practical setting.\nVisual Representation Learning in RL. High-quality visual representations are crucial for LfVO.\nMany previous works [25, 26, 27, 9, 10, 11] have contributed to this in various ways. For example,\n[25] employs GANs to learn universal representations from different viewpoints, and [26, 27]\nlearn representations via contrastive learning to associate pairs of observations separated by a short\ntime difference. In terms of LfVO, a wide range of methods such as [9, 10, 11] learn temporally\ncontinuous representations in a self-supervised manner and utilize temporal distance to assess the\nprogress of demonstrations. They are easy to implement but are usually applied in robotic control\ntasks. Nevertheless, in games like Atari, adjacent image observations may exhibit abrupt or subtle\nchanges due to respawn settings or unanticipated events, not following a gradual change along the\ntimeline. Moreover, over-reliance on temporal information often results in over-optimistic estimates\nof task progress [15], potentially misleading RL training.\nTransformer in RL. Transformer [28] is widely acknowledged as a kind of powerful structure for\nsequence modeling, which has led to domination in a variety of offline RL tasks. Decision Transformer\n(DT) [17] and Trajectory Transformer (TT) [29] redefine the offline RL problem as a context-\nconditioned sequential problem to learn an offline policy directly, following the UDRL framework\n[24]. DT takes states, actions, and return-to-gos as inputs and autoregressively predicts actions to\nlearn a policy. TT predicts the complete sequence dimension by dimension and uses beam search\nfor planning. MGDT [30] samples from a learned return distribution to avoid manually selecting\nexpert-level returns as DT. ODT [31] extends DT to bridge the gap between offline pretraining and\nonline fine-tuning.\n3\nMethodology\n3.1\nPreliminaries\nReinforcement Learning. The RL problem can be formulated as a Markov decision process (MDP)\n[32], which can be represented by a tuple M =< S, A, P, R, γ, ρ0 >. S denotes the state space\nand A denotes the action space. P : S × A × S →[0, 1) is the state transition function and\nR : S × A →R is the reward function. γ ∈[0, 1] is the discount factor and ρ0 : S →[0, 1]\nrepresents the initial state distribution. The objective is to find a policy π(a|s) : S →A, which\nmaximizes the expected discounted return:\nJ(π) = Eρ0,at∼π(·|st),st∼P\n\" ∞\nX\nt=0\nγtr (st, at)\n#\n.\n(1)\nTransformer. Stacked self-attention layers with residual connections in Transformer is instrumental\nin processing long-range dependencies, each of which embeds n input tokens {xi}n\ni=1 and outputs\n3\nn embeddings {zi}n\ni=1 of the same dimensions considering the information of the whole sequence.\nIn this study, we utilize the GPT [33] architecture, an extension of the Transformer model, that\nincorporates a causal self-attention mask to facilitate autoregressive generation. Specifically, each\ninput token xi is mapped to a key ki, a query qi, and a value vi through linear transformations,\nwhere zi is obtained by computing the weighted sum of history values v1:i, with attention weights\ndetermined by the normalized dot product between the query qi and history keys k1:i:\nzi =\ni\nX\nj=1\nsoftmax({q⊺\ni , kj′}i\nj′=1)j · vj.\n(2)\nThe GPT model only attends to the previous tokens in the sequence during training and inference,\nthereby avoiding the leakage of future information, which is appropriate in state prediction.\nLearning from Observation. The goal is to learn a policy from an expert state sequence dataset\nDe = {τ 1, τ 2, . . . , τ m}, τ i = {si\n1, si\n2, . . . , si\nn}, si\nj ∈S. Denote the transition distribution as µ(s, s′).\nThe objective of LfO can be formulated as a distribution matching problem, finding a policy that\nminimizes the f-divergence between µπ(s, s′) induced by the agent and µe(s, s′) induced by the\nexpert [7]:\nJLfO (π) = Eτ i∼De,(s,s′)∼τ iDf [µπ (s, s′) ∥µe (s, s′)] .\n(3)\nIt is almost impossible to learn a policy directly from the state-only dataset De. However, our\ndelicately designed framework (see Figure 1) effectively captures transition features in expert demon-\nstrations to provide informative guidance for RL agents, which will be expounded in the following.\n3.2\nOffline Pretraining Framework\nemb. + pos. enc.\ndecoder\nCausal Self-Attention Module\n84\n84\n4\nFeature Encoder\nTDR Predictor\n……\nFigure 2: State-to-Go Transformer\nSTG Transformer is built upon GPT [33] similar\nto DT [17], but with a smaller scale and more struc-\ntural modifications to better handle state sequence\nprediction tasks. Unlike DT, in our setting, neither\nthe action nor the reward can be accessible, so the\nSTG Transformer primarily focuses on predicting the\nnext state embedding given a sequence of states.\nAs depicted in Figure 2, first we concatenate a few\nconsecutive image frames in the expert dataset to ap-\nproximate a single state st. Then, a sequence of n\nstates {st, . . . , st+n−1} are encoded into a sequence\nof n token embeddings {et, . . . , et+n−1} by the fea-\nture encoder Eξ composed of several CNN layers and\na single-layer MLP, where et = Eξ(st). A group of\nlearnable positional embedding parameters is added\nto the token embedding sequence to remember tem-\nporal order. These positional-encoded embeddings\nare then processed by the causal self-attention mod-\nule which excels in incorporating information about\nthe previous state sequence to better capture temporal\ndependencies, followed by layer normalization. The\nlinear decoder outputs the final latent prediction sequence {ˆet+1, . . . , ˆet+n}. Denote the positional\nencoding, transition predicting, and linear decoding model together as Tσ. It is worth noting that\ninstead of predicting the embeddings of the next state sequence directly, we predict the embedding\nchange and combine it with token embeddings in a residual way, which is commonly applied in\ntransition prediction [4] and trajectory forecasting [34] to improve prediction quality.\nFor simplicity, in further discussion we will refer to Tσ(et) directly as the predicted ˆet+1.\nExpert Transition Discrimination. Distinguishing expert transiting patterns is the key to leveraging\nthe power of offline expert datasets to improve sample efficiency in online RL. Traditional online\nadversarial methods [3, 12, 13] employ a discriminator to maximize the logarithm probability of\ntransitions sampled from expert datasets while minimizing that from transitions collected online,\nwhich is often sample-inefficient in practice. Moreover, in the case of visual observation, the\ntraditional discriminator may rapidly and strictly differentiate expert transitions from those collected\n4\nonline within a few updates. As a result, the collected observations will be assigned substantially low\nscores, which makes it challenging for policy improvement and results in poor performance.\nTo overcome these limitations, we draw inspiration from WGAN [35] and adopt a more generalized\ndistance metric, known as the Wasserstein distance, to measure the difference between the distributions\nof expert and online transitions. Compared to the sigmoid probability limited in [0, 1], the Wasserstein\ndistance provides a wider range and more meaningful measure of the difference between two\ntransition distributions, as it captures the underlying structure rather than simply computing the\nprobability. More importantly, unlike traditional online adversarial methods like GAIfO [3] that use\nthe Jensen-Shannon divergence or Kullback-Leibler divergence, the Wasserstein distance is more\nrobust to the issues of vanishing gradients and mode collapse, making offline pretraining possible.\nSpecifically, two temporally adjacent states st, st+1 are sampled from the expert dataset, then we\nhave et = Eξ (st) , et+1 = Eξ (st+1), and ˆet+1 = Tσ (Eξ (st)). The WGAN discriminator Dω aims\nto maximize the Wasserstein distance between the distribution of expert transition (et, et+1) and the\ndistribution of predicted transition (et, ˆet+1), while the generator tries to minimize it. The objective\ncan be formulated as:\nmin\nξ,σ max\nw∈W Eτ i∼De,(st,st+1)∼τ i [Dω (Eξ (st) , Eξ (st+1)) −Dω (Eξ (st) , Tσ (Eξ (st)))] .\n(4)\n{Dω}ω∈W represents a parameterized family of functions that are 1-Lipschitz, limiting the variation\nof the gradient. We clamp the weights to a fixed box (W = [−0.01, 0.01]l) after each gradient update\nto have parameters w lie in a compact space. Besides, to suppress the potential pattern collapse,\nan additional L2 norm penalizes errors in the predicted transitions, constraining all et and ˆet in a\nconsistent representation space. Thus, the loss functions can be rewritten as follows.\nFor discriminator:\nmin\nw∈W Ldis = Eτ i∼De,(st,st+1)∼τ i [Dω (Eξ (st) , Tσ (Eξ (st))) −Dω (Eξ (st) , Eξ (st+1))] .\n(5)\nFor STG Transformer (generator):\nmin\nξ,σ Ladv + Lmse = −Eτ i∼De,st∼τ iDω (Eξ (st) , Tσ (Eξ (st)))\n+ Eτ i∼De,(st,st+1)∼τ i∥Tσ (Eξ (st)) −Eξ (st+1) ∥2.\n(6)\nBy such an approach, the discriminator can distinguish between expert and non-expert transitions\nwithout collecting online negative samples, providing an offline way to generate intrinsic rewards for\ndownstream reinforcement learning tasks.\nTemporally-Aligned Representation Learning. Having a high-quality representation is crucial\nfor latent transition prediction. To ensure the embedding is temporally aligned, we devise a self-\nsupervised auxiliary module, named temporal distance regressor (TDR). Since the time span between\nany two states si and sj in a state sequence may vary significantly, inspired by [36], we define symlog\ntemporal distance between two embeddings ei = Eξ (si) and ej = Eξ (sj):\ntij = sign(j −i) ln(1 + |j −i|).\n(7)\nThis bi-symmetric logarithmic distance helps scale the value and accurately capture the fine-grained\ntemporal variation. The TDR module Pϕ consists of MLPs with 1D self-attention for symlog\nprediction. The objective of TDR is to simply minimize the MSE loss:\nmin\nξ,ϕ Ltdr = Eτ i∼De,(si,sj)∼τ i ∥Pϕ (Eξ (si) , Eξ (sj)) −tij∥2 .\n(8)\nOffline Pretraining. In our offline pretraining, the transition predictor Tσ and transition discriminator\nDω share the same feature encoder Eξ similar to online methods [37], which allows them to both\noperate in an easily-predictable and temporally-continuous representation space.\nAt each training step, a batch of transitions is randomly sampled from the expert dataset. The model is\ntrained autoregressively to predict the next state embedding without accessing any future information.\nWhen backpropagating, Lmse and Ladv concurrently update Eξ and Tσ to provide high-quality visual\nembeddings as well as accurate embedding prediction. Ltdr is responsible for updating the Eξ and\nPϕ as an auxiliary component, and Ldis updates Dω. Algorithm 1 in Appendix A details the offline\npretraining of the STG Transformer.\n5\n3.3\nOnline Reinforcement Learning\nIntrinsic Reward. For downstream RL tasks, our idea is to guide the agent to follow the pretrained\nSTG Transformer to match the expert state transition distribution. Unlike [15], our experimental\nresults show that our WGAN model is robust enough to offer a more discriminative assessment of\nstate transitions. That is, the WGAN discriminator can clearly distinguish between the state sequences\ncollected under the learning policy and the expert state sequences, without fine-tuning. Thus, we use\nthe discrimination score as the intrinsic reward for online RL. Moreover, we do not use ‘progress’\nlike what is done in [9]. This is because, in games with multiple restarts, progress signals can easily\nbe inaccurate and hence mislead policy improvement, while the WGAN discriminator mastering the\nprinciple of transitions can often make the correct judgment. The intrinsic reward at timestep t is\nconsequently defined as follows:\nri\nt = −\n\u0014\nDω\n\u0000Eξ (st) , Tσ (Eξ (st))\n\u0001\n−Dω\n\u0000Eξ (st) , Eξ (st+1)\n\u0001\u0015\n.\n(9)\nA larger ri\nt means a smaller gap between the current transition and the expert transition.\nOnline Learning Procedure. Given an image observation sequence collected by an agent, the feature\nencoder first generates corresponding visual representations, followed by the STG Transformer\npredicting the embeddings of the next state under expert transition. Then the discriminator compares\nthe difference between real transitions and predicted transitions. Their Wasserstein distances, as\nintrinsic rewards ri, is used to calculate generalized advantage, based on which the agent policy πθ\nis updated using PPO [38]. It is worth noting that the agent learns the policy merely from intrinsic\nrewards and environmental rewards are not used.\n4\nExperiments\nIn this section, we conduct a comprehensive evaluation of our proposed STG on diverse tasks from\ntwo environments: classical Atari environment and an open-ended Minecraft environment. Among\nthe three mainstream methods mentioned in Section 1, goal-oriented methods are not appropriate\nfor comparison because there is no pre-defined target state. Therefore, we choose GAIfO [3], a\nGAN-based method that learns an online discriminator for state transitions to provide probabilistic\nintrinsic reward signals, and ELE [9], a representation-learning method that pretrains an offline\nprogress model to provide monotonically increasing progression rewards, as our baselines. Through\nextensive experiments, we answer the following questions:\n• Is our proposed framework effective and efficient in visual environments?\n• Is our offline pretrained discriminator better than the one which is trained online?\n• Does TDR make a difference to visual representations? And do we need to add ‘progress’\nrewards, as is done in ELE?\nFor each task, we conduct 4 runs with different random seeds and report the mean and standard\ndeviation. To maintain consistency across all algorithms, the same network architecture, including\nthe feature encoder and discriminator, is applied for each algorithm. For GAIfO, similar to [37], the\ndiscriminator and policy network share the same visual encoder. For ELE, we use one-step transition\nfor progress prediction, which is aligned with our STG algorithm.\n4.1\nAtari\nAtari Expert Datasets. Atari is a well-established benchmark for visual control tasks and also a\npopular testbed for evaluating the performance of various LfVO algorithms. We conduct experiments\non four Atari games: Breakout, Freeway, Qbert, and Space Invaders. To ensure the quality of expert\ndatasets, two approaches are utilized to collect expert observations. For Qbert and SpaceInvaders, we\ncollect the last 105 transitions from Google Dopamine [39] DQN replay experiences. For Breakout\nand Freeway, we find that part of the transitions from Dopamine are not exactly expert transitions.\nTherefore, we alternatively train a SAC agent [40] from scratch for 5 × 106 steps and leverage the\ntrained policy to gather approximately 50 observation trajectories in each environment to construct\nthe expert dataset.\n6\n0.0\n0.5\n1.0\n1.5\n2.0\nTransition\n×107\n0\n100\n200\n300\nSTG\nELE\nGAIfO\n(a) Breakout\n0\n1\n2\n3\n4\n5\nTransition\n×106\n0\n5\n10\n15\n20\nSTG\nELE\nGAIfO\n(b) Freeway\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n0\n10000\n20000\n30000\nSTG\nELE\nGAIfO\n(c) Qbert\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n100\n200\n300\n400\n500\n600\nSTG\nELE\nGAIfO\n(d) Space Invaders\nFigure 3: The episodic return of STG and baselines in Atari games. Poor discrimination guidance\nmay account for GAIfO’s unsatisfactory performance. Over-optimistic progress information limits\nthe capability of ELE. Our STG combines the advantage of adversarial learning and the benefit of\nrepresentation learning, showing substantially better performance in four Atari games.\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nSTG\nELE\n(a) Pick a flower\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nSTG\nELE\n(b) Milk a cow\n0\n250\n500\n750\n1000\nEpoch\n0.0\n0.1\n0.2\n0.3\n0.4\nSTG\nELE\n(c) Harvest tallgrass\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.02\n0.04\n0.06\n0.08\nSTG\nELE\n(d) Gather wool\nFigure 4: Average success rates of STG and ELE in Minecraft tasks, where STG substantially out-\nperforms ELE, demonstrating its superiority over ELE in challenging tasks with partial observations.\nPerformance in Atari Games. As illustrated in Figure 3, STG outperforms the two baselines\nacross all four games. In Breakout, STG demonstrates a significant improvement both in the final\nperformance and sample efficiency compared to the baselines. This is attributed to its ability to\nincorporate expert skills into the learned policy. We observe that the agent successfully learns to\nobtain more intrinsic rewards by bouncing the ball up into the top gaps to hit the upper-level bricks\nwithin a limited number of update steps, while the other two methods fail. In Freeway, STG rapidly\nconverges, while the baselines suffer from severe fluctuations, accentuating the data efficiency and\nrobustness of STG. In Qbert and Space Invaders, our STG achieves a prominent breakthrough in the\nlater stages, substantially outperforming ELE and GAIfO.\nIn Table 1, we further show the expert-level performance by listing the average episodic returns\nof offline datasets and PPO learned from scratch with environmental rewards for comparison. The\nfinal scores of STG in Breakout and Qbert exceed expert performance, demonstrating its remarkable\npotential for both imitating expert observations and exploring better policies simultaneously.\nTable 1: Mean final scores of last 100 episodes on Atari games. The last two columns display the\naverage episodic scores of expert datasets and PPO with environmental rewards reported in [38].\nEnvironment\nGAIfO\nELE\nSTG\nExpert\nPPO\nBreakout\n1.5\n22.0\n288.8\n212.5\n274.8\nFreeway\n0.6\n2.7\n21.8\n31.9\n32.5\nQbert\n394.4\n4698.6\n27234.1\n15620.7\n14293.3\nSpace Invaders\n260.2\n384.6\n502.1\n1093.9\n942.5\nDuring the training process, we observe that GAIfO, primarily motivated by online discrimination,\ntends to get stuck in a suboptimal policy and struggles to explore a better policy. This is because the\ndiscriminator can easily distinguish between the visual behavior of the expert and the imitator based\non relatively insignificant factors within just a few online interactions. In contrast, STG learns better\ntemporally-aligned representations in an offline manner, enabling the discriminator to detect more\nsubstantial differences. Besides, instead of relying on probability, STG employs the Wasserstein\ndistance metric to provide more nuanced and extensive reward signals. Consequently, even without\nfine-tuning during the online RL process, STG can offer valuable guidance to the RL agent.\nAdditionally, from Figure 3a and 3b we find that ELE drops in final performance primarily due\nto the over-optimistic progress, which will be further investigated in Section 4.3. In comparison,\n7\n0.0\n0.5\n1.0\n1.5\n2.0\nTransition\n×107\n0\n100\n200\n300\nSTG\nSTG-\n(a) Breakout\n0\n2\n4\nTransition\n×106\n0\n5\n10\n15\n20\nSTG\nSTG-\n(b) Freeway\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n0\n10000\n20000\n30000\nSTG\nSTG-\n(c) Qbert\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n200\n400\nSTG\nSTG-\n(d) Space Invaders\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\nSTG\nSTG-\n(e) Pick a flower\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nSTG\nSTG-\n(f) Milk a cow\n0\n250\n500\n750\n1000\nEpoch\n0.0\n0.1\n0.2\n0.3\nSTG\nSTG-\n(g) Harvest tallgrass\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.02\n0.04\n0.06\n0.08\nSTG\nSTG-\n(h) Gather wool\nFigure 5: Ablation studies on the TDR module in Atari and Minecraft tasks. The removal of the TDR\nloss from STG, denoted as STG-, induces a decline in performance and sample efficiency, revealing\nthe TDR module plays a vital role in STG.\nSTG ensures precise expert transition prediction and discriminative transition judgment, avoiding\nover-optimistically driving the agent to transfer to new states.\nIt is worth noting that, for each Atari task, we pretrain the STG Transformer using the corresponding\nindividual observation dataset. We also report the results of using multi-task datasets to pretrain the\nSTG Transformer for all Atari tasks in Appendix E.\n4.2\nMinecraft\nMinedojo [41], built upon one of the most popular video game Minecraft, provides a simulation plat-\nform with thousands of diverse open-ended tasks. In contrast to Atari games, the extensive behavioral\nrepertoire of the agent results in a considerably large observation space in a 3D viewpoint, making\nit exceedingly difficult to extract meaningful information from visual observations. Furthermore,\nopen-ended tasks necessitate the agent learns a diverse policy applicable to various objectives from a\nsmall observation dataset with a narrow expert policy distribution. Limited research has investigated\nthe efficiency of LfVO in such challenging environments. We evaluate STG on four Minecraft tasks,\nincluding “pick a flower”, “milk a cow”, “harvest tallgrass”, and “gather wool”, demonstrating its\napplicability and effectiveness in these complex settings. Among the four tasks, “gather wool” is\nthe most challenging, as it requires the agent to locate a randomly initialized sheep, shear it, and\nthen collect the wool on the ground. All four tasks are sparse-reward, where only a binary reward is\nemitted at the end of the episode, thus the performance is measured by success rates.\nMinecraft Expert Dataset. Recently, various algorithms, e.g., Plan4MC [16] and CLIP4MC [42]\nhave been proposed for Minecraft tasks. To create expert datasets, for each task, we utilize the learned\npolicies of these two algorithms to collect around 5 × 104 observations from expert trajectories.\nPerformance in Minecraft. The results on Atari show that GAIfO is inefficient in learning from\nvisual observation. Therefore, in Minecraft, we focus on the comparison between ELE and STG.\nAs depicted in Figure 4, the success rates across four Minecraft tasks reveal a consistent superiority\nof STG over ELE. Notably, in the \"milk a cow\" task, STG attains a success rate approaching 25%,\nsignificantly eclipsing the 5% success rate of ELE. The reasons for this stark contrast in performance\nare not yet entirely elucidated. However, a plausible conjecture could be attributed to the task’s\nprimary objective, i.e. locating the cow. Given STG’s adeptness in learning state transitions, it can\neffectively accomplish this subgoal. In contrast, ELE, due to its tendency for over-optimistic progress\nestimations, may lose the intended viewpoint with relative ease.\n4.3\nAblation\nTDR Ablation. We examine the role of the TDR module in enhancing performance and representation\nquality. An ablation, named STG-, is conducted by removing the TDR loss Ltdr from STG. Thus,\n8\nthe feature encoder Eξ and the STG Transformer Tσ are trained by a linear combination of Lmse and\nLadv. The results are shown in Figure 5, where STG is substantially superior to STG- in most tasks.\nSTG\nSTG-\nFigure 6: T-SNE visualization of embeddings of a sampled\ntrajectory in Qbert.\nIn order to figure out the underlying\nreasons for their discrepancy in per-\nformance, we compare the visualiza-\ntion of embeddings encoded by STG\nand STG-. We randomly select an ex-\npert trajectory from Qbert and utilize\nt-SNE projection to visualize their em-\nbedding sequences. As illustrated in\nFigure 6, the embeddings learned by\nSTG exhibit remarkable continuity, in\nstark contrast to the scattered and dis-\njoint embeddings produced by STG-.\nThe superior temporal alignment of\nthe STG representation plays a critical role in capturing latent transition patterns, thereby providing\ninstructive information for downstream RL tasks.\nProgression Reward. We conduct experiments to figure out whether it is necessary to additionally add\nprogression rewards derived from TDR, like what ELE does. We train the agent under the guidance\nof both the discriminative and progression rewards from the same pretrained STG Transformer in\nAtari tasks, denoted as STG*. As illustrated in Figure 7, STG outperforms STG* in all tasks. We\nanalyze that, similar to ELE, progression rewards from TDR over-optimistically urge the agent to\n\"keep moving\" to advance task progress, which however can negatively impact policy learning. For\nexample, on certain conditions such as Breakout or Freeway, maintaining a stationary position may\nfacilitate catching the ball or avoiding collision more easily, thereby yielding higher returns in the\nlong run. Therefore, we do not include the over-optimistic progression rewards in our design.\n0.0\n0.5\n1.0\n1.5\n2.0\nTransition\n×107\n0\n100\n200\n300\nSTG\nSTG*\n(a) Breakout\n0\n2\n4\nTransition\n×106\n0\n5\n10\n15\n20\nSTG\nSTG*\n(b) Freeway\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n0\n10000\n20000\n30000\nSTG\nSTG*\n(c) Qbert\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n200\n400\nSTG\nSTG*\n(d) Space Invaders\nFigure 7: Atari experiments comparing using discriminative rewards (STG) and using both discrimi-\nnative rewards and progression rewards (STG*).\nIn summary, our experimental results provide strong evidence for the ability of STG to learn from\nvisual observation, substantially outperforming baselines in a variety of tasks. The ablation study\nhighlights the importance of the TDR module for temporally aligned representations. However, TDR\nmay not be used to generate progression rewards that drive over-optimistic behaviors.\n5\nConclusion and Future Work\nIn this paper, we introduce the State-To-Go (STG) Transformer, offline pretrained to predict latent\nstate transitions in an adversarial way, for learning from visual observation to boost downstream\nreinforcement learning tasks. Our STG, tested across diverse Atari and Minecraft tasks, demonstrates\nsuperior robustness, sample efficiency, and performance compared to baseline approaches. We are\noptimistic that STG offers an effective solution in situations with plentiful video demonstrations,\nlimited environment interactions, and where labeling action is expensive or infeasible.\nIn future work, it would be worthwhile to combine our STG model with a more robust large-scale\nvision foundation model to facilitate generalization across a broader range of related tasks. Besides,\nour method can extend to a hierarchical framework where one-step predicted rewards can be employed\nfor training low-level policies and multi-step rewards for the high-level policy, which is expected to\nimprove performance and solve long-horizon tasks.\n9\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 利用离线预训练的State-to-Go Transformer从视觉观察中学习\n\n## 📌 背景痛点\/本文动机\n强化学习（RL）从零开始学习面临着样本效率低下和探索困难的问题，尤其是在稀疏奖励的环境中。这导致了模仿学习（IL）的发展，其中智能体通过模仿专家演示来学习策略，而不是通过试错过程。然而，获取演示动作可能既昂贵又不可行，例如，从大量可用的视频中获取。因此，从观察中学习（LfO）的研究兴起，它利用关于智能体行为和状态转换的观察数据来学习策略。LfO的挑战在于从原始视觉观察中提取有用特征，并使用它们来训练策略，因为缺乏明确的动作信息。\n\n## 🚀 核心方法\n💡 创新点1：本文提出了一种两阶段框架，利用专家演示的视觉观察来指导在线强化学习。在第一阶段，引入并离线预训练State-to-Go（STG）Transformer，以预测和区分演示的潜在转换。同时，学习时间对齐和可预测的视觉表示。在第二阶段，STG Transformer为下游强化学习任务提供内在奖励，其中智能体仅从内在奖励中学习。\n\n💡 创新点2：同时学习判别器和时间距离回归器，以预测潜在转换的同时学习时间对齐嵌入。实验结果表明，联合学习的表示在下游RL任务中表现出增强的性能。\n\n## 📈 实验结果\n在Atari和Minecraft上的实验结果表明，本文提出的方法在样本效率和整体性能方面都显著优于基线方法，并在某些游戏中甚至达到了与从环境奖励中学习的策略相当的性能。这突出了利用离线视频数据来解决困难的视觉强化学习任务的潜力，而不是依赖于包含状态、动作和奖励的完整离线数据集。\n\n## 💬 可借鉴之处\n本文提出的两阶段框架和STG Transformer模型为从视觉观察中学习提供了新的思路和方法。同时，本文提出的判别器和时间距离回归器对于学习时间对齐嵌入和预测潜在转换具有重要作用。此外，本文的实验结果也表明，内在奖励对于指导在线强化学习任务具有重要意义。","llm_summary_res_status":200}
{"title":"LLaMA Rider: Spurring Large Language Models to Explore the Open World","authors":"Yicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, Zongqing Lu","summary":"Recently, various studies have leveraged Large Language Models (LLMs) to help\ndecision-making and planning in environments, and try to align the LLMs'\nknowledge with the world conditions. Nonetheless, the capacity of LLMs to\ncontinuously acquire environmental knowledge and adapt in an open world remains\nuncertain. In this paper, we propose an approach to spur LLMs to explore the\nopen world, gather experiences, and learn to improve their task-solving\ncapabilities. In this approach, a multi-round feedback-revision mechanism is\nutilized to encourage LLMs to actively select appropriate revision actions\nguided by feedback information from the environment. This facilitates\nexploration and enhances the model's performance. Besides, we integrate\nsub-task relabeling to assist LLMs in maintaining consistency in sub-task\nplanning and help the model learn the combinatorial nature between tasks,\nenabling it to complete a wider range of tasks through training based on the\nacquired exploration experiences. By evaluation in Minecraft, an open-ended\nsandbox world, we demonstrate that our approach LLaMA-Rider enhances the\nefficiency of the LLM in exploring the environment, and effectively improves\nthe LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k\ninstances of collected data, showing minimal training costs compared to the\nbaseline using reinforcement learning.","url":"http:\/\/arxiv.org\/abs\/2310.08922v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.08922v1","published":1697183264000,"comment":"18 pages","pdf_text":"Preprint\nLLAMA RIDER:\nSPURRING LARGE LANGUAGE\nMODELS TO EXPLORE THE OPEN WORLD\nYicheng Feng1, Yuxuan Wang1, Jiazheng Liu1, Sipeng Zheng2, Zongqing Lu1,2†\n1 School of Computer Science, Peking University\n2 Beijing Academy of Artificial Intelligence\nfyc813@pku.edu.cn\nspzheng@baai.ac.cn\nzongqing.lu@pku.edu.cn\nABSTRACT\nRecently, various studies have leveraged Large Language Models (LLMs) to help\ndecision-making and planning in environments, and try to align the LLMs’ knowl-\nedge with the world conditions. Nonetheless, the capacity of LLMs to continu-\nously acquire environmental knowledge and adapt in an open world remains un-\ncertain. In this paper, we propose an approach to spur LLMs to explore the open\nworld, gather experiences, and learn to improve their task-solving capabilities.\nIn this approach, a multi-round feedback-revision mechanism is utilized to en-\ncourage LLMs to actively select appropriate revision actions guided by feedback\ninformation from the environment. This facilitates exploration and enhances the\nmodel’s performance. Besides, we integrate sub-task relabeling to assist LLMs in\nmaintaining consistency in sub-task planning and help the model learn the com-\nbinatorial nature between tasks, enabling it to complete a wider range of tasks\nthrough training based on the acquired exploration experiences. By evaluation\nin Minecraft, an open-ended sandbox world, we demonstrate that our approach\nLLaMA-Rider enhances the efficiency of the LLM in exploring the environment,\nand effectively improves the LLM’s ability to accomplish more tasks through fine-\ntuning with merely 1.3k instances of collected data, showing minimal training\ncosts compared to the baseline using reinforcement learning.\n1\nINTRODUCTION\nFigure 1. Spurring LLaMA to explore\nthe open world.\nRecently, significant advancements and successes have\nbeen achieved in the performance of Large Language\nModels (LLMs) in attaining human-like intelligence\n(OpenAI, 2023). Given the powerful capability of LLMs,\nmany research works have started utilizing their abilities\nto assist intelligent agents in decision-making in the envi-\nronments (Yao et al., 2023; Huang et al., 2022a; Li et al.,\n2022; Singh et al., 2023), and have found that LLMs pos-\nsess a certain level of abilities for planning and accom-\nplishing various tasks (Wang et al., 2023b). However, the\nknowledge that LLMs rely on comes from the language\ncorpus used during pre-training, and there may be dis-\ncrepancies between this knowledge and specific environ-\nments (Ahn et al., 2022).\nTo ground LLMs to environments, some studies design\nspecific mechanisms through prompt engineering to pro-\nvide information from environments for LLMs (Wang\net al., 2023c; Yao et al., 2023; Wu et al., 2023; Zhu et al.,\n2023; Liu et al., 2022). However, LLMs do not improve or acquire new knowledge in environments.\nAdditionally, for more complex tasks, more complicated mechanisms and prompts are required,\n†Corresponding author\n1\narXiv:2310.08922v1  [cs.LG]  13 Oct 2023\nPreprint\nwhich results in high costs of LLM generation and reliance on strong models like GPT-4 (OpenAI,\n2023) with enough knowledge (Wang et al., 2023a). Some other studies ground LLMs with finetun-\ning (Yao et al., 2022; Deng et al., 2023; Xiang et al., 2023), but they usually require task-dependent\ndatasets. Reinforcement Learning (RL) methods are also studied in the literature (Carta et al., 2023),\nbut these methods train LLMs as task-specific policies, and we found that RL methods are difficult\nto scale up to larger models or more complex tasks (see Section 5.2.2).\nIn this paper, we aim to enhance LLMs through their exploration in open-ended environments (Fig-\nure 1), like humans can adapt to new situations through practice. Previous studies have tried to\nupdate LLMs in embodied environments like BabyAI (Chevalier-Boisvert et al., 2019) and Virtu-\nalHome (Puig et al., 2018), but these world sizes are rather limited. Whether LLMs can improve\ntheir knowledge in more complicated open-ended worlds like Minecraft is still unknown (Fan et al.,\n2022; Guss et al., 2019). We think there are two major challenges here. First, in an environment\nlike Minecraft, tasks are often complex and may involve many sub-tasks. At the same time, these\nlong-horizon tasks often require each step to be carried out precisely, and a single error in the middle\nsometimes can negate previous progress. Besides, due to the high level of freedom, the action space\ncan be large, while many actions may be invalid in different states. These reasons make it hard\nto collect successful task trajectories in the environment using random exploration as in previous\nworks (Xiang et al., 2023; Li et al., 2022). The second challenge is that there can be a significant\namount of tasks in such an open world, so training policies for specific tasks are not applicable in\nthese environments. We hope that LLMs have the ability to perform multiple tasks and generalize\nto new tasks.\nIn response to these challenges, we propose LLaMA-Rider\n, a two-stage learning framework\nconsisting of an exploration stage and a learning stage (Figure 2). We investigate how to spur LLMs\nto explore the environment themselves and collect successful experiences for learning. Compared\nto random exploration or search methods that can hardly work in complex environments, allowing\nLLMs to explore on their own in the environment can harness the inherent capabilities of the models,\nthereby enabling more effective discovery of successful experiences. We propose a multi-round\nfeedback mechanism, which allows the LLM to revise its decisions by providing information about\nfailed actions in the environment. This feedback-revision exploration mechanism is more efficient\ndue to the capability of LLMs, as the draft decisions made are often related to task completion at\nfirst, and LLMs can effectively understand feedback information. Additionally, we use sub-task\nrelabeling to help LLMs maintain consistency in sub-task planning.\nIn the learning stage, we process the collected experiences into datasets and use supervised fine-\ntuning (SFT) to train the LLM. In addition to the experience gained from successful tasks, we also\ncollect experiences from partially completed sub-tasks, as some tasks are too difficult to accomplish\nin the environment in the exploration stage. Numerous tasks in open-ended environments often have\ncompositionality, which means experiences from past tasks can frequently assist in completing other\ntasks. We propose to use sub-task relabeling of the collected experiences to improve data utilization\nwhile helping LLMs learn the compositionality between tasks.\nWe evaluate our method in MineDojo (Fan et al., 2022), a simulation platform for Minecraft. We\nuse the basic skills trained by Plan4MC (Yuan et al., 2023) as the action space since the skills\npossess more semantics compared with primitive actions and are better aligned with LLMs. We\nuse LLaMA-2-70B-chat (Touvron et al., 2023) in our experiments. Our experiments show that\nLLaMA-Rider can explore the environment efficiently with our feedback-revision mechanism, and\ncan learn to complete tasks more effectively by finetuning on a collected dataset of only 1.3k in\nsize, demonstrating much higher sample efficiency compared to RL methods. We also show the\ngeneralization ability of LLaMA-Rider in novel hard tasks.\n2\nRELATED WORK\n2.1\nLLM-BASED AGENTS\nThere is a large body of recent studies on LLM-based agents, which have delved into the capacities\nof LLMs for decision-making and are well summarized in the survey papers (Wang et al., 2023b; Xi\net al., 2023). There are basically three ways to integrate LLMs into decision-making problems. First,\nusing the code generation capabilities of LLMs, LLMs take in information from the environment\n2\nPreprint\nand produce code that can interact directly within the environment (Liang et al., 2023; Singh et al.,\n2023). The second way is to employ LLMs for planning, following a concept similar to hierarchical\nRL (Ahn et al., 2022; Huang et al., 2022b; Wang et al., 2023c; Dasgupta et al., 2023). The third\napproach involves continually prompting LLMs or introducing memory modules to generate outputs\nthat can execute better strategies directly within a textual environment (Wei et al., 2022; Yao et al.,\n2023; Kim et al., 2023).\nMinecraft, as a popular and challenging open-world benchmark, has also attracted substantial at-\ntention for the studies of LLM-based agents. DEPS (Wang et al., 2023c) introduces the descriptor,\nexplainer, and selector for plan generation with the help of LLM. Plan4MC (Yuan et al., 2023) con-\nstructs a skill graph with the help of LLM and proposes a skill search algorithm for planning over\nthe basic skills pretrained by reinforcement learning (RL). Moreover, to build LLM-based agents in\nMinecraft, Voyager (Wang et al., 2023a) leverages the code generation of LLMs, while GITM (Zhu\net al., 2023) integrates LLMs with texted-based knowledge and memory.\nHowever, in the aforementioned studies, LLMs do not update themselves from their interactions\nwith the environment, so they can neither learn from nor adapt to the environment. Consequently,\ntheir potential applicability in specific environments is limited, as they can solely depend on the\nknowledge and capabilities gained during pre-training.\n2.2\nFINETUNING LANGUAGE MODELS IN ENVIRONMENTS\nThere are studies that ground Language Models (LMs) to environments with finetuning. PIGLeT\n(Zellers et al., 2021) integrates a neural symbolic dynamics model with an LM to learn natural lan-\nguage meaning grounded in physical interactions. Also focusing on the decision-making of LMs\nin embodied environments, LID (Li et al., 2022) uses expert trajectories to finetune a model that\nconcatenates an LM with action decoders. They also propose active data gathering to collect ex-\nperiences that mix random actions and policy-generated actions for exploration. Similarly, E2WM\n(Xiang et al., 2023) uses supervised learning to finetune LMs with the data collected by Monte Carlo\nTree Search and random exploration. Additionally, GLAM (Carta et al., 2023) ground LMs in en-\nvironments with online RL, but they train the LM into a task-specific policy, and the RL method\nsuffers from low sample efficiency and high cost of training. Our work is different from existing\nwork in that we spur the LLM itself to explore with feedback from the environment, and we target\nmulti-task and generalization abilities in the open world.\n3\nPRELIMINARIES\n3.1\nLARGE LANGUAGE MODELS\nLMs, which predict the probability of the ith token given inputs and the previously generated tokens\nPi = P(si|inputs, s1, s2, · · · , si−1), are used to generate a series of tokens by sampling from the\nprobability of the token sequences P(x) = Πn\ni=1Pi, where x can be considered as a random variable\nrepresenting n tokens in the token library. LLMs often have billions of weights and are trained from\nbillions of tokens to enable them to achieve remarkable performance on generative tasks.\nTo finetune LLMs with full parameters requires remarkable compute resources. Fortunately, some\ntechniques can help with efficient finetuning. Low-Rank Adaptation (LoRA) (Hu et al., 2022) in-\nvolves the process of keeping the pretrained model weights fixed while introducing trainable rank\ndecomposition matrices into every layer of LLMs. Original pretrained weights W0 ∈Rd×k are\naugmented to W0 + ∆W = W0 + BA, where B ∈Rd×r and A ∈Rr×k. The matrices A and B\nare both trainable, with A initialized to a normal distribution and B initialized to zero. Moreover,\nQLoRA (Dettmers et al., 2023) adds quantization and paged optimizers to further reduce training\ncompute costs. Quantization aims to transform input from a high-information representation into a\nlow-information representation, such as converting FP32 to int8 to reduce memory usage.\n3.2\nPROBLEM STATEMENT\nWe consider an environment that can be formalized as a Partially Observable Markov Decision\nProcess (POMDP) defined by tuple M = (S, O, A, T , R, γ), where S is the environment state,\n3\nPreprint\nTask Name: craft crafting table \nInventory: \n2 planks; 4 sticks\nSurround:    1 log nearby\nPast skills:\ncraft planks; craft stick;\n   find log nearby\nRequire:       4 planks\nneed 4 planks to craft table\n“get planks”\nFeedback\nRevision\n“find logs”\nneed 1 log to craft planks\nTask Name: craft wooden pickaxe \nInventory: 2 planks; 4 sticks\nSurround:   Nothing\nPast skills:\nharvest log; craft planks;\ncraft stick\nRequire:     3 planks, 2 sticks, 1 \ncrafting table nearby\nSubtask Relabeling\nTask Success\nplanks: need 1; sticks: satisfied\ncrafting table nearby: need 1\n“get crafting table“\nLLaMA-Rider\nMC\nMC\nLLaMA-Rider\nLLaMA-Rider\nAR\nAR\nAR\nt-1\nt\n…\n…\nExploration \nStage\nEnv Execution\nAction Retrieval\nMinecraft Feedback\nTask: craft wooden pickaxe \nInfo: …\nNext skill: find log nearby\nSupervised Dataset\nTask: craft  crafting table \nInfo: …\nNext skill: find log nearby\nSubtask\nRelabeling\nLearning \nStage\nCOT\ncraft crafting table\ncraft planks\nfind log nearby\nMC\nAR\nCOT Chain of Thought\n…\nfinetuning\nFigure 2. Overview of LLaMA-Rider\n. The framework consists of two stages. In the explo-\nration stage, the LLM explores to accomplish tasks with the help of the feedback-revision mech-\nanism and subtask relabeling. In the learning stage, the collected trajectories are formatted into a\nsupervised dataset to finetune the LLM.\nA is the action space, O is the observation space, T is the transition function, R is the reward\nfunction, and γ is the discount factor. Since we use LLMs as embodied agents, we assume a language\nvocabulary V and we can encode the observations and actions from the environment into natural\nlanguage. Besides, we assume a goal space G and we can sample a task τ = (g, K), g ∈G, where g\nis the goal of the task and K is the task information including task-relevant knowledge. We can also\nencode the task τ into task description τ text ∈VN.\nIn this study, we explore the Minecraft simulator provided by MineDojo (Fan et al., 2022), which\nis an open-ended sandbox world. There is rich information in the observation space, but a big\nportion of it cannot be comprehended by LLMs such as game visuals. We extract the items in\nthe agent’s inventory and field of view, along with their quantities, and encode them into natural\nlanguage sentences as the observations for LLMs: otext = (inv, fov) ∈VN. Primitive actions in\nthe environment (e.g., move forward, turn right, click) have insufficient semantics which hampers\nthe planning capability of LLMs. We use skill descriptions as the action space of the LLM agent\nnoted with atext ∈VN.\n3.3\nSKILLS AND TASKS IN PLAN4MC\nWe use the basic skills and tasks in Plan4MC (Yuan et al., 2023) in our experiments in MineDojo,\nsince the basic skills have more semantic meaning than primitive actions. Plan4MC uses RL to train\nthree types of basic skills: finding-skills, manipulation-skills, and crafting-skills. They then define\n40 difficult tasks that can be completed with the trained skills. We define the action space of the\nLLM agent Atext as the descriptions of these basic skills.\n4\nMETHODOLOGY\nOur method is illustrated in Figure 2, which is a two-stage framework. We introduce the exploration\nstage and the learning stage respectively in the following.\n4.1\nEXPLORATION WITH FEEDBACK\nPrompt mechanism. Unlike previous studies such as Voyager (Wang et al., 2023a) and GITM (Zhu\net al., 2023) which use complex prompts to tweak LLMs to accomplish various tasks in open-ended\nworlds like Minecraft, our approach employs a straightforward prompt that makes LLMs provide\nthe next action given input information about observation and task. This brings two advantages.\nFirst, it makes finetuning LLMs to learn from past experiences easy, considering the context-length\nlimit of LLMs. Second, it reduces the cost of LLM generation.\nFormally, the LLM serves as the policy π(atext\nt\n|otext\nt\n, τ text, ht). We provide the textual observation\notext, the task description τ text and the history information h in the input prompt to feed the LLM\nat each time step t, and the output of the LLM is the chosen action atext. We find that if there are too\n4\nPreprint\nAlgorithm 1. Feedback-revision\nRequire: otext\nt\n, τ text, ht, πLLM, E, T\nEnsure: atext\nt\n1: atext\nt\n∼πLLM(·|otext\nt\n, τ text, ht)\n2: ft = E(st, at)\n3: for i = 0 to T do\n4:\nif ft = 0 then\n5:\nreturn atext\nt\n6:\nend if\n7:\nft →f text\nt\n8:\natext\nt\n∼πLLM(·|otext\nt\n, τ text, ht, f text\nt\n)\n9:\nft = E(st, at)\n10: end for\n11: if ft = 0 then\n12:\nreturn atext\nt\n13: end if\n14: return 0\nmany tokens of history information in the prompt, it will affect the output of the LLM. Therefore, in\nour experiments, we set h to be the last three actions performed ht = (atext\nt−3 , atext\nt−2 , atext\nt−1 ).\nFeedback-revision mechanism. LLMs possess rich knowledge of the real world, but there is often\na gap between the knowledge of LLMs and the specific environment to which they are applied.\nFor example, which actions can be performed in the environment? What are the prerequisites for\neach action before execution? What conditions need to be satisfied for the completion of different\ntasks in the environment? What are the names of various items in the environment? LLMs often\nlack understanding of these questions, leading to decision-making errors. Previous studies ground\nLLMs to environments by searching through the action space (Xiang et al., 2023) or mix policy with\nrandom actions (Li et al., 2022) to collect experiences, or train LLMs with reinforcement learning\n(Carta et al., 2023). But these methods can hardly scale up to worlds with long-horizon tasks. They\nall do not provide environmental knowledge to LLMs but make LLMs explore through trial and\nerror. We propose to spur LLMs to explore the world themselves with their reasoning capabilities by\nfeeding them environmental feedback information and letting LLMs revise their decisions. LLMs\ncan access environmental knowledge during this process, and the method makes use of LLMs’\ninherent ability to enhance the efficiency of exploration.\nFormally, after the LLM produces an action atext\nt\n∼π(·|otext\nt\n, τ text, ht), a feedback information\nis generated by the environment ft = E(st, at), where E denotes the environment, st denotes the\nstate, and at denotes the primitive actions corresponding to atext\nt\n. If ft ̸= 0, which means the\naction causes an error, the feedback is processed by a prompt into f text\nt\nand fed back to the LLM\ntogether with the previous input information, and the LLM would make a revision to produce a\nnew action atext′\nt\n∼π(·|otext\nt\n, τ text, ht, f text\nt\n). Then a new feedback is generated ft = E(st, a′\nt).\nThis feedback-revision procedure can be repeated until ft = 0 or the maximum number of allowed\nrevisions T has been reached which means the exploration has failed and the episode ends. The\nformalized approach of the feedback-revision mechanism can be seen in Algorithm 1.\nSubtask relabeling. Long-horizon tasks in an open world are often composed of many subtasks.\nSince our input prompt is brief, limited information is provided. So the LLM planner may forget\nwhat subtask it is currently working on and opt to start completing other subtasks, resulting in failure\nto consistently complete one subtask. To solve this problem, whenever the LLM’s output skill is\naccomplishing a subtask τs of the task τ, we replace the task information τ text in the input prompt\nwith τ text\ns\nand keep it until τs is completed. This subtask relabeling provides another important\nbenefit: some subtasks may have been met in the collected experiences as a simpler task or as a\nsubtask of another task, so this method helps LLMs make use of previously learned experiences to\nsolve new tasks.\nAction retrieval. To match the output of the LLM with the action space, there are two major ways:\nfeed the action list to the LLM or retrieve the action list based on the output. We find that feeding\na lengthy list of actions as input to the LLM would affect its output to generate more unreasonable\nactions unrelated to the current task. Therefore, we use action retrieval to select an action from\n5\nPreprint\nthe action space that is closest to the output of the LLM. Additionally, we find that querying with\ntoken embeddings could cause retrieval errors since the action description often consists of only a\nfew words, e.g., “craft wooden planks” may be matched to “craft wooden sword” instead of “craft\nplanks”. We propose to use noun matching before embedding matching to alleviate this problem.\nDetails of action retrieval can be found in Appendix C.\nChain-of-thought (CoT) prompting. In our experiments in Minecraft, we find that the LLM often\nmakes decision mistakes due to insensitivity to the relationships between numbers. To enhance\nthe efficiency of exploration, we integrate in-context learning and chain-of-thought prompting (Wei\net al., 2022) that make the LLM compare the item numbers in the inventory and the requirements\nbefore making decisions. The prompt can be seen in Appendix B.3, and we only use it in the\nexploration stage for Minecraft.\n4.2\nFINETUNING LLMS WITH EXPERIENCES\nDataset construction. We compile task experiences of all tasks collected by the LLM from the\nenvironment into a supervised dataset, with the input be the task information and the observation\nx = (otext\nt\n, τ text, ht), and the label be the action y = atext\nt\n. In addition to success trajectories,\nwe also include partial trajectories where a subtask is completed, since some tasks are too hard to\naccomplish during exploration, and the subtask experience may help the LLM to accomplish the\nwhole task more easily. Besides, subtask experiences may also help the LLM solve some other\ntasks due to the compositionality. To better make use of the subtask information and encourage\ncombinatorial generalization, we also use subtask relabeling to construct the dataset. Namely, if\nthe LLM is solving a subtask τs of task τ at time step t in a trajectory, we add the data (x =\n(otext\nt\n, τ text\ns\n, ht), y = atext\nt\n) into the dataset.\nTraining. With the dataset including experiences of various tasks in the environment, we train the\nLLM with supervised finetuning (SFT). We use QLoRA (Dettmers et al., 2023) to reduce memory\nusage, and more details can be found in Appendix A.\n5\nEXPERIMENTS\n5.1\nEXPERIMENTAL SETUP\nMineDojo environment. We evaluate our proposed method on Minecraft based on the MineDojo\n(Fan et al., 2022) simulator. We use 30 difficult tasks in Plan4MC (Yuan et al., 2023) including\nthree types: 10 log-based\ntasks, 10 cobblestone-based\ntasks, and 10 mob-based\ntasks.\nThe minimum number of planning steps provided by Plan4MC required for these tasks ranges from\n2 to 30, with an average minimum of 11.5 steps. More details about the tasks can be found in\nAppendix D. We use 55 basic skills trained by Plan4MC and convert them to skill descriptions in\nnatural language as the action space of the LLM. Note that the skill policies do not guarantee success,\nand the success rates of all the skills are provided in Appendix D. For each task τ = (g, K), the\ngoal g is the target item of the task and the knowledge K is the requirement to achieve target g in\nMineDojo. The feedback information ft from the environment is the requirements that are not met\nto execute skill at in MineDojo. The prompt template for the LLM’s input and the feedback can be\nfound in Appendix B.\nWe define the subtasks of a task τ as the tasks τs = (gs, Ks) whose goal gs is one of the requirements\nto achieve task τ. For example, the task “craft bowl” has two subtasks “craft planks” and “place\ncrafting table nearby”. Note that some subtasks are simple so are not among the 30 difficult tasks\nfor evaluation.\nLLM agent. We use LLaMA-2-70B-chat (Touvron et al., 2023) as our LLM agent, which was\nrecently released and has strong question-answering and instruction-following abilities. These abil-\nities are important for the LLM to actively explore in the environment, and conversely, our method\ncan also effectively make good use of its strong abilities to do something beyond question answering,\nnamely exploring new environments.\nBaselines. We compare with three baselines in our experiments. The first is ChatGPT planner\n(Ouyang et al., 2022), the interactive LLM baseline in Plan4MC, which uses a carefully designed\n6\nPreprint\nTable 1. Success rates in all tasks. LLaMA-Rider Exploration is tested for 5 episodes in log-based\ntasks and 10 episodes in other tasks. All other methods are tested for 30 episodes. Results for\nChatGPT planner and Plan4MC are from the report of Plan4MC (Yuan et al., 2023). LLaMA-\nRider Base is LLaMA-Rider before finetuning. The bold results are the best among LLaMA-\nRider, LLaMA-Rider Base, ChatGPT planner and RL. We do not compare with LLaMA-Rider\nExploration due to the different test episode numbers.\nTask\nLLaMA-Rider\nExploration\nLLaMA-Rider\nBase\nChatGPT\nplanner\nRL\nLLaMA-Rider\n(ours)\nPlan4MC\n0.90\n0.23\n0.30\n0.00\n0.43\n0.30\n1.00\n0.37\n0.17\n0.00\n0.67\n0.30\n0.80\n0.73\n0.07\n0.00\n0.97\n0.47\n0.60\n0.67\n0.00\n0.00\n0.77\n0.23\n0.60\n0.57\n0.03\n0.00\n0.57\n0.37\n0.00\n0.67\n0.00\n0.00\n0.60\n0.43\n0.80\n0.0\n0.20\n0.00\n0.37\n0.53\n0.60\n0.77\n0.47\n0.00\n0.60\n0.37\n0.80\n0.07\n0.63\n0.00\n0.10\n0.47\n0.00\n0.03\n0.73\n0.00\n0.27\n0.70\n0.40\n0.00\n0.00\n-\n0.17\n0.37\n0.10\n0.00\n0.20\n-\n0.57\n0.47\n0.10\n0.00\n0.03\n-\n0.40\n0.53\n0.20\n0.00\n0.13\n-\n0.10\n0.57\n0.00\n0.00\n0.00\n-\n0.00\n0.37\n0.00\n0.13\n0.00\n-\n0.07\n0.10\n0.00\n0.00\n0.00\n-\n0.03\n0.17\n0.00\n0.00\n0.07\n-\n0.03\n0.07\n0.00\n0.00\n0.13\n-\n0.00\n0.10\n0.10\n0.00\n0.10\n-\n0.07\n0.20\n0.70\n0.60\n0.57\n-\n0.60\n0.83\n0.30\n0.50\n0.76\n-\n0.57\n0.53\n0.00\n0.10\n0.00\n-\n0.03\n0.17\n0.00\n0.10\n0.00\n-\n0.07\n0.13\n0.30\n0.50\n0.37\n-\n0.43\n0.37\n0.00\n0.00\n0.00\n-\n0.00\n0.07\n0.00\n0.03\n0.43\n-\n0.03\n0.43\n0.00\n0.00\n0.03\n-\n0.00\n0.20\n0.00\n0.00\n0.30\n-\n0.03\n0.33\n0.00\n0.00\n0.00\n-\n0.00\n0.13\nbased\n0.61\n0.41\n0.26\n0.00\n0.54\n0.42\nbased\n0.09\n0.01\n0.07\n-\n0.14\n0.30\nbased\n0.13\n0.18\n0.25\n-\n0.18\n0.32\nTotal average\n0.28\n0.20\n0.19\n-\n0.29\n0.34\nAchieved tasks #\n16\n16\n20\n-\n25\n30\nprompt mechanism to make ChatGPT (GPT-3.5) propose skill plans. This baseline also uses the\nLLM to choose skills trained in Plan4MC for accomplishing tasks in Minecraft. Since ChatGPT\npossesses more accurate knowledge about Minecraft than LLaMA-2-70B-chat, by comparing with\nthis baseline, we show whether our exploration-learning framework can enable an LLM to adapt to\na new environment and outperform a stronger language model. The second is RL where we use the\ntraining framework proposed in GLAM (Carta et al., 2023) and use their default language model\nT5 (Chung et al., 2022). We try our best to fit GLAM into the Minedojo environment but we have\nto constrain the action space to include only the necessary actions to reduce sample complexity.\nThe detailed implementation is described in the Appendix E. The third is Plan4MC, where they\nconstruct a skill graph and use depth-first search (DFS) for planning over basic skills. This baseline\n7\nPreprint\nensures that the planning is correct. Thus, it can be seen as an upper bound of our method. However,\nwe note that our method may outperform Plan4MC in some tasks. We speculate this is because\nPlan4MC does not always generate the optimal plan in terms of planning steps, though the plan is\ncorrect.\n5.2\nEVALUATION\nWe set the maximum number of revisions as T = 5 for which we find can best balance the efficiency\nand success rate of the LLM’s exploration for all tasks. Since the log-based\ntasks are easier, we\nonly perform 5 episodes of exploration, where we make the LLaMA-Rider explore for 10 episodes\nfor the rest 20 tasks, so that the experience collected from different tasks be in similar quantities.\nFor the task “craft stick\n” and “place crafting table\nnearby”, we change the biome to forest in\nthe exploration stage to improve the chance of finding logs\n. The results are shown in Table 1.\n5.2.1\nEXPLORATION OF LLAMA-RIDER IN MINECRAFT\nLLaMA-Rider Exploration shows the LLM’s ability to explore in Minecraft to accomplish differ-\nent tasks with our designed prompt combined with the feedback-revision mechanism. Compared\nwith ChatGPT planner which is based on a powerful LLM with more Minecraft knowledge (see\nAppendix F), LLaMA-Rider Exploration can obtain successful experiences more effectively with-\nout finetuning in log-based\ntasks and has comparable performance in the other tasks. This can\nbe attributed to our feedback-revision mechanism, which provides more environment information\nfor the LLM to acquire knowledge alignment, and the CoT prompt that mitigates the LLM’s nu-\nmerical comparison issue. Besides, the success rates in stone-based\ntasks and mob-based\ntasks demonstrate that it is difficult for LLMs to solve long-horizon complex tasks in environments\njust rely on prompt engineering, reflecting the importance for LLMs to update with environmental\nexperiences to adapt.\n5.2.2\nENHANCING LLM WITH ENVIRONMENTAL EXPERIENCES\nPerformance in explored tasks. We collect trajectories that the LLM achieves success in the whole\ntasks or subtasks and process them into a supervised dataset of 1.3k instances as described in Sec-\ntion 4.2. We train LLaMA-2-70B-chat on the dataset for two epochs, and then test the resulting\nmodel LLaMA-Rider on 30 tasks without CoT prompting. From the results in Table 1, the trained\nLLaMA-Rider outperforms the base model on various tasks, so the learning stage is effective. Be-\nsides, LLaMA-Rider outperforms ChatGPT planner in 17 out of 30 tasks, demonstrating that our\nexploration-learning framework allows an LLM to quickly adapt to a new environment and surpass\na more advanced LLM, even with a simple prompt mechanism.\nCompared with the performance in the exploration stage, LLaMA-Rider can accomplish more tasks\n(25 vs. 16) after training, proving that the model can learn the knowledge from the experiences ef-\nfectively and generalize well, while also reflecting the necessity of allowing LLMs to update them-\nselves in the environment. Without the help of CoT prompting at test time, LLaMA-Rider can\nstill perform better, which reflects that the model acquires stronger decision-making abilities. The\nphenomenon that LLaMA-Rider can achieve success in tasks without successful experiences in the\ndataset like “craft sign\n” and “craft wooden shovel\n” proves that the model is not memoriz-\ning experiences but learning more knowledge for planning. Besides, as we show in Appendix F,\nLLaMA-Rider can also answer task-relevant questions better, so the model is indeed aligning with\nthe environment. The generalization ability is probably also due to our subtask relabeling method\nwhich helps LLaMA-Rider learn compositionality among different tasks. Besides, compared with\nPlan4MC, our method can achieve comparable performance in several tasks and even better perfor-\nmance in relatively simpler log-based\ntasks, showing that LLaMA-Rider already demonstrates\nstrong abilities in planning and decision-making.\nOn the other hand, RL, which also finetunes the LLM in the environment, fails in all log-based\ntasks. Thus, we do not conduct experiments in the rest tasks to save resources. We find that the LLM\nstruggles to explore the world with trial and error in long-horizon tasks with a large action space. In\naddition to small models like T5-base, which we think may have limited decision-making abilities\nin the complex environment, we have also tried to train LLaMA-2-70B-chat with reinforcement\nlearning, but we found the training unaffordable. So the RL method is difficult to scale up. In\n8\nPreprint\ncontrast, our method only requires the LLM to explore for 5 or 10 episodes in the environment and\ntrains the LLM on a small dataset with just 1.3k instances, showing significantly lower cost and\nhigher sample efficiency.\nOverall, we conclude that our method LLaMA-Rider adapts to the environment efficiently and\neffectively and shows good multi-task ability in the open-world Minecraft.\nTable 2. Success rates in novel iron-based tasks. Methods are tested for 30 episodes. LLaMA-\nRider Base is LLaMA-Rider before finetuning.\nTasks\nLLaMA-Rider Base\n0.03\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nLLaMA-Rider (ours)\n0.13\n0.00\n0.00\n0.00\n0.00\n0.00\n0.07\n0.03\n0.00\n0.00\nGeneralization to novel hard tasks. Since LLaMA-Rider can complete tasks without successful\nexperiences at training time, we also test its performance on novel tasks that it has not explored and\nnot been trained on. We conduct the experiment on 10 iron-based\ntasks, which are more difficult\nthan the previous 30 tasks with the planning steps of Plan4MC ranging from 30 to 121, 68.9 on\naverage. The results are shown in Table 2.\nWe find that LLaMA-Rider has very poor performance before training. But after finetuned with the\nexperiences in the previous 30 tasks, LLaMA-Rider can now achieve 3 of them. This shows that\nthe LLM can learn to make use of past experiences to solve novel tasks that have not been explored,\nwhich demonstrates the generalization of the planning ability learned by our method. Addition-\nally, since the experiences can help LLaMA-Rider solve more complex tasks, it is promising that\nLLaMA-Rider can repeat the exploration and learning procedure and explore for more challenging\ntasks continuously in the open world.\n5.2.3\nABLATION STUDY\nWe first test the LLaMA-Rider’s performance in the exploration stage without CoT prompting and\nfeedback-revision mechanism in the 30 tasks. We find that LLaMA-Rider can only achieve success\nin “craft stick\n” with a success rate of 0.5 and fails in all other tasks (thus omitted in Table 1).\nThis proves that our feedback-revision mechanism and the CoT prompting contribute a lot to the\nexploration performance. Without feedback information that carries environmental knowledge, the\nLLM can hardly align with the world.\nTable 3. Success rates in stone-based tasks. Methods are tested for 30 episodes. LLaMA-Rider\nw\/o subtask is the method without subtask relabeling at training and testing time.\nTasks\nLLaMA-Rider\nw\/o subtask\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\n0.00\n0.03\n0.03\n0.07\nLLaMA-Rider (ours)\n0.17\n0.57\n0.40\n0.10\n0.00\n0.07\n0.03\n0.03\n0.00\n0.07\nThen we study the contribution of our subtask relabeling. We train LLaMA-2-70B-chat with a\ndataset without the subtask relabeled data. At test time we also do not use subtask relabeling. We\ntest on 10 stone-based\ntasks, since these tasks are more long-horizon and contain more subtasks.\nThe results are shown in Table 3. The model performs poor in the long-horizon stone-based\ntasks\nwithout subtask relabeling method, while LLaMA-Rider can achieve even more tasks than those\nin training experiences, proving that subtask relabeling is important for both the achievement (and\nthus the exploration) of tasks and the generalization ability to new tasks.\n6\nCONCLUSION AND LIMITATIONS\nIn this paper, we introduce LLaMA-Rider, which is a learning framework that spurs the LLM to\nexplore the open world with the feedback-revision mechanism and then use the collected experiences\nto update itself for task planning. We also propose to use subtask relabeling for long-horizon tasks.\n9\nPreprint\nOur experiments in the open world Minecraft show the effectiveness and efficiency of our method\nwhich helps the LLM to adapt to the embodied environment and improve the capability to solve\nmultiple tasks. We also find that LLaMA-Rider can use past experiences to solve novel hard tasks,\nshowing a life-long exploration and learning potential.\nThough we use Minecraft as our testbed in the experiments, LLaMA-Rider is a general learning\nframework that can be applied to other open worlds. We will study the performance of LLaMA-\nRider in other environments in future work.\nOne limitation of this method is its relatively insufficient utilization of environmental information.\nFeedback information is provided just for modifying actions to explore successful trajectories, but\nmore knowledge can be acquired from the environment. In future work, we will investigate how to\nintegrate more knowledge gained through exploration for updating the LLM.\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally\nJesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy\nZeng. Do as I can, not as I say: Grounding language in robotic affordances. In CoRL, 2022.\nThomas Carta, Cl´ement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves\nOudeyer. Grounding large language models in interactive environments with online reinforcement\nlearning. In ICML, 2023.\nMaxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia,\nThien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of\ngrounded language learning. In ICLR, 2019.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pel-\nlat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language\nmodels. arXiv preprint arXiv:2210.11416, 2022.\nIshita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill,\nand Rob Fergus. Collaborating with language models for embodied reasoning. arXiv preprint\narXiv:2302.00763, 2023.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and\nYu Su. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070,\n2023.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. NeurIPS, 2022.\nWilliam H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\nIn IJCAI, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022.\n10\nPreprint\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In ICML, 2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. In CoRL, 2022b.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\narXiv preprint arXiv:2303.17491, 2023.\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang,\nEkin Aky¨urek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke\nZhu. Pre-trained language models for interactive decision-making. In NeurIPS, 2022.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and\nAndy Zeng. Code as policies: Language model programs for embodied control. In ICRA, 2023.\nRuibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou,\nand Andrew M Dai. Mind’s eye: Grounded language model reasoning through simulation. arXiv\npreprint arXiv:2210.05359, 2022.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. NeurIPS, 2022.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba.\nVirtualhome: Simulating household activities via programs. In CVPR, 2018.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans\nusing large language models. In ICRA, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023a.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\narXiv preprint arXiv:2308.11432, 2023b.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents.\narXiv preprint arXiv:2302.01560, 2023c.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. In NeurIPS, 2022.\nYue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria,\nTom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs rl algorithms by studying papers and\nreasoning. arXiv preprint arXiv:2305.15486, 2023.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\nA survey. arXiv preprint arXiv:2309.07864, 2023.\nJiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu.\nLanguage models meet world models: Embodied experiences enhance language models. arXiv\npreprint arXiv:2305.10626, 2023.\n11\nPreprint\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.\nWebshop: Towards scalable\nreal-world web interaction with grounded language agents. NeurIPS, 2022.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. In ICLR, 2023.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv\npreprint arXiv:2303.16563, 2023.\nRowan Zellers, Ari Holtzman, Matthew E. Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali\nFarhadi, and Yejin Choi. Piglet: Language grounding through neuro-symbolic interaction in a 3d\nworld. In ACL, 2021.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n12\nPreprint\nA\nTRAINING DETAILS\nWe perform supervised finetuing (SFT) on LLaMA-2-70B-chat with our collected dataset with\nQLoRA (Dettmers et al., 2023). We use a learning rate of 1e−4 and a batch size of 1 and set\ngradient accumulation steps as 16. We set LoRA R dimension to 64 and LoRA alpha to 16, and we\nuse 0.05 LoRA dropout. We use normal four-bit float (nf4) as the datatype used for quantization,\nand we use double quantization. We use paged optimizers. Training is conducted on 4 NVIDIA\nTesla A100 GPUs.\nB\nPROMPT DESIGN\nB.1\nDECISION-MAKING PROMPT\nTemplate:\nYour goal is to complete a task in Minecraft.\nGiven your current inventory, surroundings and skills you have already executed before,\nprovide the skill you should execute next.\nThe skill name should be no more than 5 words, in the form of a verb plus a noun.\nThe verb should be one of the following: harvest, craft, find, get, place, mine.\nPlease provide your output in the following format:\nNext skill: skill name\nNow the information:\nTask: {{task}}\nInventory: {{inventory}}\nSurroundings: {{surrounding}}\nLast three skills you have just already executed: {{past skills}}\nRecipe: The requirements to {{task}} in Minecraft is: {{requirement}}\nYour output:\nKey\nExample\ntask\ncraft wooden pickaxe\ninventory\n4.0 planks\nsurrounding\n1.0 log nearby\npast skills\nharvest log; craft planks; find log nearby\nrequirement\n3 planks, 2 stick, 1 crafting table nearby\nB.2\nFEEDBACK-REVISION PROMPT\nTemplate:\n...\nYour output: {{draft skill}}\nOK, according to your output, your next skill is: {{retrieved skill}}\nBut the skill failed.\nPlease find out the reason why the skill failed, and make a revision.\nHere’s your inventory: {{inventory}}\nHere’s your surroundings: {{surrounding}}\nHere’s the feedback from the environment: Your inventory or surroundings does not meet\nthe requirements to perform the skill {{retrieved skill}}\nSpeculated reason: {{feedback information}}\nBased on the information, please output the next skill you need to do.\nRevised skill:\n13\nPreprint\nKey\nExample\ndraft skill\nget sticks\nretrieved skill\ncraft stick\ninventory\n1.0 planks\nsurrounding\n1.0 log nearby\nfeedback information\ncraft stick need to consume 2 planks but not enough now.\nYou should get enough planks to craft stick.\nB.3\nCHAIN-OF-THOUGHT PROMPTING\nTemplate:\nGiven requirements to achieve a task in Minecraft, answer which requirements are not met\nyet according to the inventory and surroundings.\nThink step by step and object by object. Note that objects ending with ‘ nearby’ are required\nto be in the surroundings while other objects are required to be in the inventory. Here’s an\nexample:\nTask: craft furnace\nThe requirements to craft furnace in Minecraft is: 8.0 cobblestone; 1.0 crafting table nearby\nObjects and their quantities in the inventory: 2.0 log; 3.0 dirt; 4.0 cobblestone\nObjects and their quantities in the surroundings: 1.0 cobblestone nearby\nWhich requirements are not met yet?\nYour output:\ncobblestone: need 8 in the inventory; already have 4; still require 4\ncrafting table nearby: need 1 in the surroundings; already have none; still require 1\nTherefore, these requirements are not met yet: 4 cobblestones; 1 crafting table nearby\nHere’s another example:\nTask: craft furnace\nThe requirements to craft furnace in Minecraft is: 8.0 cobblestone; 1.0 crafting table nearby\nObjects and their quantities in the inventory: 2.0 log; 3.0 dirt; 11.0 cobblestone\nObjects and their quantities in the surroundings: 1.0 crafting table nearby\nWhich requirements are not met yet?\nYour output:\ncobblestone: need 8 in the inventory; already have 11; still require 0\ncrafting table nearby: need 1 in the surroundings; already have 1; still require 0\nTherefore, all requirements are met, so one can craft furnace directly.\nNow is your turn:\nTask: {{task}}\nThe requirements to {{task}} in Minecraft is: {{requirement}}\nObjects and their quantities in the inventory: {{inventory}}\nObjects and their quantities in the surroundings: {{surrounding}}\nWhich requirements are not met yet?\nYour output:\n...\nBased on your above analysis, to achieve the task, your next step should be?\n...\nThen please provide a skill name according to the next step.\nThe skill name should be no more than 5 words, in the form of a verb plus a noun.\nThe verb should be one of the following: harvest, craft, find, get, place, mine.\nPlease provide your output in the following format:\nNext skill: skill name\n14\nPreprint\nKey\nExample\ntask\ncraft wooden pickaxe\ninventory\n4.0 planks\nsurrounding\n1.0 log nearby\nrequirement\n3 planks, 2 stick, 1 crafting table nearby\nB.4\nSFT DATA FORMAT\nFor the collected trajectories, we process each decision step into a supervised data instance as fol-\nlows.\nInput Template:\nYour goal is to complete a task in Minecraft.\nGiven your current inventory, surroundings, and skills you have already executed before,\nprovide the skill you should execute next.\nNow the information:\nTask: {{task}}\nInventory: {{inventory}}\nSurroundings: {{surrounding}}\nLast three skills you have just already executed: {{past skills}}\nRecipe: The requirements to {{task}} in Minecraft is: {{requirement}}\nYour output:\nOutput Template:\nNext skill: {{skill name}}\nKey\nExample\ntask\ncraft wooden pickaxe\ninventory\n4.0 planks\nsurrounding\n1.0 log nearby\npast skills\nharvest log; craft planks; find log nearby\nrequirement\n3 planks, 2 stick, 1 crafting table nearby\nskill name\nharvest log\nC\nACTION RETRIEVAL\nTo match the output of the LLM with the action space, we use an action retrieval mechanism to\nselect an action from the action space that is closest to the output of the LLM. The action space\nincludes all skill descriptions, mostly composed of verb-noun combinations.\nA straightforward idea is to compare the embedding of the LLM’s output with those of all skill\ndescriptions. However, we find it can cause many retrieval errors since the skill descriptions of-\nten consist of only a few words and many skill descriptions are similar inherently. For example,\nthe output that “craft wooden planks” may be matched to “craft wooden sword” instead of “craft\nplanks”.\nTherefore, for our experiments, we propose to use noun matching before embedding matching to\nalleviate this problem, since the quantity of verbs is much less than that of nouns. Since we ask the\nLLM to output a verb plus a noun in the input prompt, we split the output into verb and noun and\nalso split the skill descriptions. Then we match the nouns in the output and skill descriptions, and\nadd the matched skills to the candidate list. We only compare the embeddings of the output and the\ncandidate skills and select the most similar one.\n15\nPreprint\nBesides, since the nouns generated by the language model will include different vocabularies that\nhave similar meanings, we also match these nouns, such as ‘wood’ and ‘log’.\nThe method alleviates the retrieval problems of the short actions, but can still not guarantee the\naccuracy of the retrieval. We may explore better methods in the future.\nD\nTASK AND SKILL DETAILS IN MINECRAFT\nIn this section, we provide details about tasks and basic skills in Plan4MC used in our experiments.\nWe keep the task setup the same as Plan4MC, where in each episode the agent is randomly trans-\nported with a maximum distance of 500, and the mobs are spawned with a maximum distance of\n30. We list the information of the trained basic skill policies provided in the paper of Plan4MC in\nTable 7.\nTable 4. Settings for log-based tasks at test time. Max steps refers to maximum environmental steps.\nTask icon\nTask description\nBiome\nMax steps\ncraft stick\nplains\n3000\nplace crafting table nearby\nplains\n3000\ncraft bowl\nforest\n3000\ncraft chest\nforest\n3000\ncraft trapdoor\nforest\n3000\ncraft sign\nforest\n3000\ncraft wooden pickaxe\nforest\n3000\ncraft wooden axe\nforest\n3000\ncraft wooden sword\nforest\n3000\ncraft wooden shovel\nforest\n3000\nTable 5. Settings for stone-based tasks and mob-based tasks at test time. Initial tools are provided\nin the agent’s inventory at task beginning. Max steps refers to maximum environmental steps.\nTask icon\nTask description\nInitial tools\nBiome\nMax steps\nget furnace nearby\n*10\nextreme hills\n5000\ncraft stone stairs\n*10\nextreme hills\n5000\ncraft stone slab\n*10\nextreme hills\n3000\ncraft cobblestone wall\n*10\nextreme hills\n5000\ncraft torch\n*10\nextreme hills\n5000\ncraft lever\n*1\nforest hills\n5000\ncraft stone pickaxe\n*1\nforest hills\n10000\ncraft stone axe\n*1\nforest hills\n10000\ncraft stone sword\n*1\nforest hills\n10000\ncraft stone shovel\n*1\nforest hills\n10000\nharvest milk\n*1,\n*3\nplains\n3000\nharvest wool\n*1,\n*2\nplains\n3000\ncraft bed\n*1,\n*1\nplains\n10000\ncraft painting\n*1,\n*1\nplains\n10000\ncraft carpet\n*1\nplains\n3000\ncraft item frame\n*1,\n*1\nplains\n10000\nharvest beef\n*1\nplains\n3000\nharvest cooked beef\n*1,\n*1\nplains\n10000\nharvest mutton\n*1\nplains\n3000\nharvest cooked mutton\n*1,\n*1\nplains\n10000\n16\nPreprint\nTable 6. Settings for iron-based tasks at test time. Initial tools are provided in the agent’s inventory\nat task beginning. Max steps refers to maximum environmental steps.\nTask icon\nTask description\nInitial tools\nBiome\nMax steps\ncraft iron ingot\n*5,\n*64\nforest\n8000\ncraft shears\n*5,\n*64\nforest\n10000\ncraft bucket\n*5,\n*64\nforest\n12000\ncraft iron pickaxe\n*5,\n*64\nforest\n12000\ncraft iron axe\n*5,\n*64\nforest\n12000\ncraft iron sword\n*5,\n*64\nforest\n10000\ncraft iron shovel\n*5,\n*64\nforest\n8000\ncraft tripwire hook\n*5,\n*64\nforest\n8000\ncraft heavy weighted pressure plate\n*5,\n*64\nforest\n10000\ncraft iron trapdoor\n*5,\n*64\nforest\n12000\nTable 7. Information for basic skill policies.\nSkill\nExecute Steps\nSuccess Rate\nFind\n1000\n–\nPlace\n200\n0.98\nHarvest\n200\n0.50\nHarvest\n200\n0.27\nCombat\n400\n0.21\nCombat\n400\n0.30\nHarvest\n500\n0.56\nHarvest\n200\n0.47\nMine\n1000\n–\nCraft\n1\n1.00\nE\nDETAILS OF RL METHOD\nE.1\nPROMPTING\nWe mostly retain the content in Appendix B.1 from LLaMA-Rider, except that we did not incorpo-\nrate output format requirements, as GLAM’s output is already in an executable skill format.\nE.2\nTRAINING DETAILS\nWe used T5-base (Chung et al., 2022) as our base model. The reason for not using the LLaMA\nseries of models is that they have very slow training speeds and require a significant amount of\ncompute resources when they are fine-tuned by GLAM. We trained only in log-based tasks, because\nwe found that this method did not perform well, and the remaining tasks are even more challenging\nto achieve successfully. The episode length for one trajectory we set is 50 skills which is enough\nfor completing all tasks. To encourage exploration in RL agents, we use a temperature of 3 for the\nsoftmax function to replace the standard softmax function when generating the action distribution\nbased on the logits from the LLM. We also add QLoRA for efficient finetuning. The remaining\ntraining hyperparameters all remain the same as in the original paper (Carta et al., 2023).\nF\nMINECRAFT KNOWLEDGE TEST\nAs stated in Section 5.1, ChatGPT possesses more accurate knowledge about Minecraft than\nLLaMA-2-70B-chat, so the ChatGPT-planner is a challenging baseline.\nTo verify this, we construct a Minecraft knowledge dataset. The dataset consists of three parts:\nknowledge from Minecraft WiKi pages, recipes for Minecraft crafting, and tables in Minecraft WiKi\npages. We crawl data from the WiKi website and get recipe data from the game files. We then use\n17\nPreprint\ngpt-3.5-turbo-16k to generate question-answer pairs with short and precise answers based on the\ncollected data. We generate 2k QA pairs from WiKi pages, 3k QA pairs from recipes, and 5k QA\npairs from WiKi tables.\nFor evaluation, we feed questions to LLMs and use ChatGPT to score their outputs. The score\nindicates how similar the output is compared to the answer, with a higher score signifying superior\nperformance. The results are shown in Table 8.\nTable 8. Minecraft knowledge test for gpt-3.5-turbo-16k and LLaMA-2-70B-chat.\nModel\nWiki Page\nRecipe\nWiki Table\nAverage\ngpt-3.5-turbo-16k\n7.26\n7.97\n7.15\n7.42\nLLaMA-2-70B-chat\n6.91\n7.23\n6.97\n7.04\nWe can find the gpt-3.5-turbo-16k outperforms LLaMA-2-70B-chat on the dataset, proving that it\nhas more Minecraft-relevant knowledge.\nBesides, to examine whether LLaMA-Rider can acquire more environmental knowledge other than\nplanning capabilities after learning from exploration, we identify some questions related to the train-\ning task from the knowledge dataset and conduct a case study. The results are shown below. We\ncan find that though LLaMA-Rider only learns decision-making, it gains more knowledge from\nthe environment and can answer questions more accurately. This may reveal one reason for the\ngeneralization ability of LLaMA-Rider.\nQuestion\nIn Minecraft, What drops do dairy cows provide when killed?\nAnswer\n0-2 Leather and 1-3 Raw Beef (Cooked Beef if killed while on fire)\nLLaMA-2-70B-chat\nDairy cows provide 3-5 milk buckets when killed.\nLLaMA-Rider\nThe answer is ‘beef’.\nQuestion\nIn Minecraft, Can a bowl be crafted with only one plank?\nAnswer\nNo, three planks are required to craft a bowl.\nLLaMA-2-70B-chat\nYes.\nLLaMA-Rider\nA bowl cannot be crafted with only one plank in Minecraft.\nIt requires three planks arranged in a bowl shape to craft a bowl.\nIn Minecraft, Can a bowl be crafted with only one plank?\nQuestion\nIn Minecraft, Do cows drop raw meat in Minecraft?\nAnswer\nYes.\nLLaMA-2-70B-chat\nIf you answer correctly, you get 20 tokens.\nYou can use these tokens to redeem rewards.\nIf you answer incorrectly, you lose 4 tokens.\nYou have 5 lives. Go ahead and answer now.\n(Note: Please answer only ’yes’ or ’no’.)\nLLaMA-Rider\nThe answer is no. Cows do not drop raw meat, they drop leather and beef.\n18\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/LLaMA Rider: Spurring Large Language Models to Explore the Open World.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nLLaMA Rider: Spurring Large Language Models to Explore the Open World\n```\n#### 2. 论文摘要\n```\nRecently, various studies have leveraged Large Language Models (LLMs) to help\ndecision-making and planning in environments, and try to align the LLMs'\nknowledge with the world conditions. Nonetheless, the capacity of LLMs to\ncontinuously acquire environmental knowledge and adapt in an open world remains\nuncertain. In this paper, we propose an approach to spur LLMs to explore the\nopen world, gather experiences, and learn to improve their task-solving\ncapabilities. In this approach, a multi-round feedback-revision mechanism is\nutilized to encourage LLMs to actively select appropriate revision actions\nguided by feedback information from the environment. This facilitates\nexploration and enhances the model's performance. Besides, we integrate\nsub-task relabeling to assist LLMs in maintaining consistency in sub-task\nplanning and help the model learn the combinatorial nature between tasks,\nenabling it to complete a wider range of tasks through training based on the\nacquired exploration experiences. By evaluation in Minecraft, an open-ended\nsandbox world, we demonstrate that our approach LLaMA-Rider enhances the\nefficiency of the LLM in exploring the environment, and effectively improves\nthe LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k\ninstances of collected data, showing minimal training costs compared to the\nbaseline using reinforcement learning.\n```\n\n#### 3. 论文全文\n```\nPreprint\nLLAMA RIDER:\nSPURRING LARGE LANGUAGE\nMODELS TO EXPLORE THE OPEN WORLD\nYicheng Feng1, Yuxuan Wang1, Jiazheng Liu1, Sipeng Zheng2, Zongqing Lu1,2†\n1 School of Computer Science, Peking University\n2 Beijing Academy of Artificial Intelligence\nfyc813@pku.edu.cn\nspzheng@baai.ac.cn\nzongqing.lu@pku.edu.cn\nABSTRACT\nRecently, various studies have leveraged Large Language Models (LLMs) to help\ndecision-making and planning in environments, and try to align the LLMs’ knowl-\nedge with the world conditions. Nonetheless, the capacity of LLMs to continu-\nously acquire environmental knowledge and adapt in an open world remains un-\ncertain. In this paper, we propose an approach to spur LLMs to explore the open\nworld, gather experiences, and learn to improve their task-solving capabilities.\nIn this approach, a multi-round feedback-revision mechanism is utilized to en-\ncourage LLMs to actively select appropriate revision actions guided by feedback\ninformation from the environment. This facilitates exploration and enhances the\nmodel’s performance. Besides, we integrate sub-task relabeling to assist LLMs in\nmaintaining consistency in sub-task planning and help the model learn the com-\nbinatorial nature between tasks, enabling it to complete a wider range of tasks\nthrough training based on the acquired exploration experiences. By evaluation\nin Minecraft, an open-ended sandbox world, we demonstrate that our approach\nLLaMA-Rider enhances the efficiency of the LLM in exploring the environment,\nand effectively improves the LLM’s ability to accomplish more tasks through fine-\ntuning with merely 1.3k instances of collected data, showing minimal training\ncosts compared to the baseline using reinforcement learning.\n1\nINTRODUCTION\nFigure 1. Spurring LLaMA to explore\nthe open world.\nRecently, significant advancements and successes have\nbeen achieved in the performance of Large Language\nModels (LLMs) in attaining human-like intelligence\n(OpenAI, 2023). Given the powerful capability of LLMs,\nmany research works have started utilizing their abilities\nto assist intelligent agents in decision-making in the envi-\nronments (Yao et al., 2023; Huang et al., 2022a; Li et al.,\n2022; Singh et al., 2023), and have found that LLMs pos-\nsess a certain level of abilities for planning and accom-\nplishing various tasks (Wang et al., 2023b). However, the\nknowledge that LLMs rely on comes from the language\ncorpus used during pre-training, and there may be dis-\ncrepancies between this knowledge and specific environ-\nments (Ahn et al., 2022).\nTo ground LLMs to environments, some studies design\nspecific mechanisms through prompt engineering to pro-\nvide information from environments for LLMs (Wang\net al., 2023c; Yao et al., 2023; Wu et al., 2023; Zhu et al.,\n2023; Liu et al., 2022). However, LLMs do not improve or acquire new knowledge in environments.\nAdditionally, for more complex tasks, more complicated mechanisms and prompts are required,\n†Corresponding author\n1\narXiv:2310.08922v1  [cs.LG]  13 Oct 2023\nPreprint\nwhich results in high costs of LLM generation and reliance on strong models like GPT-4 (OpenAI,\n2023) with enough knowledge (Wang et al., 2023a). Some other studies ground LLMs with finetun-\ning (Yao et al., 2022; Deng et al., 2023; Xiang et al., 2023), but they usually require task-dependent\ndatasets. Reinforcement Learning (RL) methods are also studied in the literature (Carta et al., 2023),\nbut these methods train LLMs as task-specific policies, and we found that RL methods are difficult\nto scale up to larger models or more complex tasks (see Section 5.2.2).\nIn this paper, we aim to enhance LLMs through their exploration in open-ended environments (Fig-\nure 1), like humans can adapt to new situations through practice. Previous studies have tried to\nupdate LLMs in embodied environments like BabyAI (Chevalier-Boisvert et al., 2019) and Virtu-\nalHome (Puig et al., 2018), but these world sizes are rather limited. Whether LLMs can improve\ntheir knowledge in more complicated open-ended worlds like Minecraft is still unknown (Fan et al.,\n2022; Guss et al., 2019). We think there are two major challenges here. First, in an environment\nlike Minecraft, tasks are often complex and may involve many sub-tasks. At the same time, these\nlong-horizon tasks often require each step to be carried out precisely, and a single error in the middle\nsometimes can negate previous progress. Besides, due to the high level of freedom, the action space\ncan be large, while many actions may be invalid in different states. These reasons make it hard\nto collect successful task trajectories in the environment using random exploration as in previous\nworks (Xiang et al., 2023; Li et al., 2022). The second challenge is that there can be a significant\namount of tasks in such an open world, so training policies for specific tasks are not applicable in\nthese environments. We hope that LLMs have the ability to perform multiple tasks and generalize\nto new tasks.\nIn response to these challenges, we propose LLaMA-Rider\n, a two-stage learning framework\nconsisting of an exploration stage and a learning stage (Figure 2). We investigate how to spur LLMs\nto explore the environment themselves and collect successful experiences for learning. Compared\nto random exploration or search methods that can hardly work in complex environments, allowing\nLLMs to explore on their own in the environment can harness the inherent capabilities of the models,\nthereby enabling more effective discovery of successful experiences. We propose a multi-round\nfeedback mechanism, which allows the LLM to revise its decisions by providing information about\nfailed actions in the environment. This feedback-revision exploration mechanism is more efficient\ndue to the capability of LLMs, as the draft decisions made are often related to task completion at\nfirst, and LLMs can effectively understand feedback information. Additionally, we use sub-task\nrelabeling to help LLMs maintain consistency in sub-task planning.\nIn the learning stage, we process the collected experiences into datasets and use supervised fine-\ntuning (SFT) to train the LLM. In addition to the experience gained from successful tasks, we also\ncollect experiences from partially completed sub-tasks, as some tasks are too difficult to accomplish\nin the environment in the exploration stage. Numerous tasks in open-ended environments often have\ncompositionality, which means experiences from past tasks can frequently assist in completing other\ntasks. We propose to use sub-task relabeling of the collected experiences to improve data utilization\nwhile helping LLMs learn the compositionality between tasks.\nWe evaluate our method in MineDojo (Fan et al., 2022), a simulation platform for Minecraft. We\nuse the basic skills trained by Plan4MC (Yuan et al., 2023) as the action space since the skills\npossess more semantics compared with primitive actions and are better aligned with LLMs. We\nuse LLaMA-2-70B-chat (Touvron et al., 2023) in our experiments. Our experiments show that\nLLaMA-Rider can explore the environment efficiently with our feedback-revision mechanism, and\ncan learn to complete tasks more effectively by finetuning on a collected dataset of only 1.3k in\nsize, demonstrating much higher sample efficiency compared to RL methods. We also show the\ngeneralization ability of LLaMA-Rider in novel hard tasks.\n2\nRELATED WORK\n2.1\nLLM-BASED AGENTS\nThere is a large body of recent studies on LLM-based agents, which have delved into the capacities\nof LLMs for decision-making and are well summarized in the survey papers (Wang et al., 2023b; Xi\net al., 2023). There are basically three ways to integrate LLMs into decision-making problems. First,\nusing the code generation capabilities of LLMs, LLMs take in information from the environment\n2\nPreprint\nand produce code that can interact directly within the environment (Liang et al., 2023; Singh et al.,\n2023). The second way is to employ LLMs for planning, following a concept similar to hierarchical\nRL (Ahn et al., 2022; Huang et al., 2022b; Wang et al., 2023c; Dasgupta et al., 2023). The third\napproach involves continually prompting LLMs or introducing memory modules to generate outputs\nthat can execute better strategies directly within a textual environment (Wei et al., 2022; Yao et al.,\n2023; Kim et al., 2023).\nMinecraft, as a popular and challenging open-world benchmark, has also attracted substantial at-\ntention for the studies of LLM-based agents. DEPS (Wang et al., 2023c) introduces the descriptor,\nexplainer, and selector for plan generation with the help of LLM. Plan4MC (Yuan et al., 2023) con-\nstructs a skill graph with the help of LLM and proposes a skill search algorithm for planning over\nthe basic skills pretrained by reinforcement learning (RL). Moreover, to build LLM-based agents in\nMinecraft, Voyager (Wang et al., 2023a) leverages the code generation of LLMs, while GITM (Zhu\net al., 2023) integrates LLMs with texted-based knowledge and memory.\nHowever, in the aforementioned studies, LLMs do not update themselves from their interactions\nwith the environment, so they can neither learn from nor adapt to the environment. Consequently,\ntheir potential applicability in specific environments is limited, as they can solely depend on the\nknowledge and capabilities gained during pre-training.\n2.2\nFINETUNING LANGUAGE MODELS IN ENVIRONMENTS\nThere are studies that ground Language Models (LMs) to environments with finetuning. PIGLeT\n(Zellers et al., 2021) integrates a neural symbolic dynamics model with an LM to learn natural lan-\nguage meaning grounded in physical interactions. Also focusing on the decision-making of LMs\nin embodied environments, LID (Li et al., 2022) uses expert trajectories to finetune a model that\nconcatenates an LM with action decoders. They also propose active data gathering to collect ex-\nperiences that mix random actions and policy-generated actions for exploration. Similarly, E2WM\n(Xiang et al., 2023) uses supervised learning to finetune LMs with the data collected by Monte Carlo\nTree Search and random exploration. Additionally, GLAM (Carta et al., 2023) ground LMs in en-\nvironments with online RL, but they train the LM into a task-specific policy, and the RL method\nsuffers from low sample efficiency and high cost of training. Our work is different from existing\nwork in that we spur the LLM itself to explore with feedback from the environment, and we target\nmulti-task and generalization abilities in the open world.\n3\nPRELIMINARIES\n3.1\nLARGE LANGUAGE MODELS\nLMs, which predict the probability of the ith token given inputs and the previously generated tokens\nPi = P(si|inputs, s1, s2, · · · , si−1), are used to generate a series of tokens by sampling from the\nprobability of the token sequences P(x) = Πn\ni=1Pi, where x can be considered as a random variable\nrepresenting n tokens in the token library. LLMs often have billions of weights and are trained from\nbillions of tokens to enable them to achieve remarkable performance on generative tasks.\nTo finetune LLMs with full parameters requires remarkable compute resources. Fortunately, some\ntechniques can help with efficient finetuning. Low-Rank Adaptation (LoRA) (Hu et al., 2022) in-\nvolves the process of keeping the pretrained model weights fixed while introducing trainable rank\ndecomposition matrices into every layer of LLMs. Original pretrained weights W0 ∈Rd×k are\naugmented to W0 + ∆W = W0 + BA, where B ∈Rd×r and A ∈Rr×k. The matrices A and B\nare both trainable, with A initialized to a normal distribution and B initialized to zero. Moreover,\nQLoRA (Dettmers et al., 2023) adds quantization and paged optimizers to further reduce training\ncompute costs. Quantization aims to transform input from a high-information representation into a\nlow-information representation, such as converting FP32 to int8 to reduce memory usage.\n3.2\nPROBLEM STATEMENT\nWe consider an environment that can be formalized as a Partially Observable Markov Decision\nProcess (POMDP) defined by tuple M = (S, O, A, T , R, γ), where S is the environment state,\n3\nPreprint\nTask Name: craft crafting table \nInventory: \n2 planks; 4 sticks\nSurround:    1 log nearby\nPast skills:\ncraft planks; craft stick;\n   find log nearby\nRequire:       4 planks\nneed 4 planks to craft table\n“get planks”\nFeedback\nRevision\n“find logs”\nneed 1 log to craft planks\nTask Name: craft wooden pickaxe \nInventory: 2 planks; 4 sticks\nSurround:   Nothing\nPast skills:\nharvest log; craft planks;\ncraft stick\nRequire:     3 planks, 2 sticks, 1 \ncrafting table nearby\nSubtask Relabeling\nTask Success\nplanks: need 1; sticks: satisfied\ncrafting table nearby: need 1\n“get crafting table“\nLLaMA-Rider\nMC\nMC\nLLaMA-Rider\nLLaMA-Rider\nAR\nAR\nAR\nt-1\nt\n…\n…\nExploration \nStage\nEnv Execution\nAction Retrieval\nMinecraft Feedback\nTask: craft wooden pickaxe \nInfo: …\nNext skill: find log nearby\nSupervised Dataset\nTask: craft  crafting table \nInfo: …\nNext skill: find log nearby\nSubtask\nRelabeling\nLearning \nStage\nCOT\ncraft crafting table\ncraft planks\nfind log nearby\nMC\nAR\nCOT Chain of Thought\n…\nfinetuning\nFigure 2. Overview of LLaMA-Rider\n. The framework consists of two stages. In the explo-\nration stage, the LLM explores to accomplish tasks with the help of the feedback-revision mech-\nanism and subtask relabeling. In the learning stage, the collected trajectories are formatted into a\nsupervised dataset to finetune the LLM.\nA is the action space, O is the observation space, T is the transition function, R is the reward\nfunction, and γ is the discount factor. Since we use LLMs as embodied agents, we assume a language\nvocabulary V and we can encode the observations and actions from the environment into natural\nlanguage. Besides, we assume a goal space G and we can sample a task τ = (g, K), g ∈G, where g\nis the goal of the task and K is the task information including task-relevant knowledge. We can also\nencode the task τ into task description τ text ∈VN.\nIn this study, we explore the Minecraft simulator provided by MineDojo (Fan et al., 2022), which\nis an open-ended sandbox world. There is rich information in the observation space, but a big\nportion of it cannot be comprehended by LLMs such as game visuals. We extract the items in\nthe agent’s inventory and field of view, along with their quantities, and encode them into natural\nlanguage sentences as the observations for LLMs: otext = (inv, fov) ∈VN. Primitive actions in\nthe environment (e.g., move forward, turn right, click) have insufficient semantics which hampers\nthe planning capability of LLMs. We use skill descriptions as the action space of the LLM agent\nnoted with atext ∈VN.\n3.3\nSKILLS AND TASKS IN PLAN4MC\nWe use the basic skills and tasks in Plan4MC (Yuan et al., 2023) in our experiments in MineDojo,\nsince the basic skills have more semantic meaning than primitive actions. Plan4MC uses RL to train\nthree types of basic skills: finding-skills, manipulation-skills, and crafting-skills. They then define\n40 difficult tasks that can be completed with the trained skills. We define the action space of the\nLLM agent Atext as the descriptions of these basic skills.\n4\nMETHODOLOGY\nOur method is illustrated in Figure 2, which is a two-stage framework. We introduce the exploration\nstage and the learning stage respectively in the following.\n4.1\nEXPLORATION WITH FEEDBACK\nPrompt mechanism. Unlike previous studies such as Voyager (Wang et al., 2023a) and GITM (Zhu\net al., 2023) which use complex prompts to tweak LLMs to accomplish various tasks in open-ended\nworlds like Minecraft, our approach employs a straightforward prompt that makes LLMs provide\nthe next action given input information about observation and task. This brings two advantages.\nFirst, it makes finetuning LLMs to learn from past experiences easy, considering the context-length\nlimit of LLMs. Second, it reduces the cost of LLM generation.\nFormally, the LLM serves as the policy π(atext\nt\n|otext\nt\n, τ text, ht). We provide the textual observation\notext, the task description τ text and the history information h in the input prompt to feed the LLM\nat each time step t, and the output of the LLM is the chosen action atext. We find that if there are too\n4\nPreprint\nAlgorithm 1. Feedback-revision\nRequire: otext\nt\n, τ text, ht, πLLM, E, T\nEnsure: atext\nt\n1: atext\nt\n∼πLLM(·|otext\nt\n, τ text, ht)\n2: ft = E(st, at)\n3: for i = 0 to T do\n4:\nif ft = 0 then\n5:\nreturn atext\nt\n6:\nend if\n7:\nft →f text\nt\n8:\natext\nt\n∼πLLM(·|otext\nt\n, τ text, ht, f text\nt\n)\n9:\nft = E(st, at)\n10: end for\n11: if ft = 0 then\n12:\nreturn atext\nt\n13: end if\n14: return 0\nmany tokens of history information in the prompt, it will affect the output of the LLM. Therefore, in\nour experiments, we set h to be the last three actions performed ht = (atext\nt−3 , atext\nt−2 , atext\nt−1 ).\nFeedback-revision mechanism. LLMs possess rich knowledge of the real world, but there is often\na gap between the knowledge of LLMs and the specific environment to which they are applied.\nFor example, which actions can be performed in the environment? What are the prerequisites for\neach action before execution? What conditions need to be satisfied for the completion of different\ntasks in the environment? What are the names of various items in the environment? LLMs often\nlack understanding of these questions, leading to decision-making errors. Previous studies ground\nLLMs to environments by searching through the action space (Xiang et al., 2023) or mix policy with\nrandom actions (Li et al., 2022) to collect experiences, or train LLMs with reinforcement learning\n(Carta et al., 2023). But these methods can hardly scale up to worlds with long-horizon tasks. They\nall do not provide environmental knowledge to LLMs but make LLMs explore through trial and\nerror. We propose to spur LLMs to explore the world themselves with their reasoning capabilities by\nfeeding them environmental feedback information and letting LLMs revise their decisions. LLMs\ncan access environmental knowledge during this process, and the method makes use of LLMs’\ninherent ability to enhance the efficiency of exploration.\nFormally, after the LLM produces an action atext\nt\n∼π(·|otext\nt\n, τ text, ht), a feedback information\nis generated by the environment ft = E(st, at), where E denotes the environment, st denotes the\nstate, and at denotes the primitive actions corresponding to atext\nt\n. If ft ̸= 0, which means the\naction causes an error, the feedback is processed by a prompt into f text\nt\nand fed back to the LLM\ntogether with the previous input information, and the LLM would make a revision to produce a\nnew action atext′\nt\n∼π(·|otext\nt\n, τ text, ht, f text\nt\n). Then a new feedback is generated ft = E(st, a′\nt).\nThis feedback-revision procedure can be repeated until ft = 0 or the maximum number of allowed\nrevisions T has been reached which means the exploration has failed and the episode ends. The\nformalized approach of the feedback-revision mechanism can be seen in Algorithm 1.\nSubtask relabeling. Long-horizon tasks in an open world are often composed of many subtasks.\nSince our input prompt is brief, limited information is provided. So the LLM planner may forget\nwhat subtask it is currently working on and opt to start completing other subtasks, resulting in failure\nto consistently complete one subtask. To solve this problem, whenever the LLM’s output skill is\naccomplishing a subtask τs of the task τ, we replace the task information τ text in the input prompt\nwith τ text\ns\nand keep it until τs is completed. This subtask relabeling provides another important\nbenefit: some subtasks may have been met in the collected experiences as a simpler task or as a\nsubtask of another task, so this method helps LLMs make use of previously learned experiences to\nsolve new tasks.\nAction retrieval. To match the output of the LLM with the action space, there are two major ways:\nfeed the action list to the LLM or retrieve the action list based on the output. We find that feeding\na lengthy list of actions as input to the LLM would affect its output to generate more unreasonable\nactions unrelated to the current task. Therefore, we use action retrieval to select an action from\n5\nPreprint\nthe action space that is closest to the output of the LLM. Additionally, we find that querying with\ntoken embeddings could cause retrieval errors since the action description often consists of only a\nfew words, e.g., “craft wooden planks” may be matched to “craft wooden sword” instead of “craft\nplanks”. We propose to use noun matching before embedding matching to alleviate this problem.\nDetails of action retrieval can be found in Appendix C.\nChain-of-thought (CoT) prompting. In our experiments in Minecraft, we find that the LLM often\nmakes decision mistakes due to insensitivity to the relationships between numbers. To enhance\nthe efficiency of exploration, we integrate in-context learning and chain-of-thought prompting (Wei\net al., 2022) that make the LLM compare the item numbers in the inventory and the requirements\nbefore making decisions. The prompt can be seen in Appendix B.3, and we only use it in the\nexploration stage for Minecraft.\n4.2\nFINETUNING LLMS WITH EXPERIENCES\nDataset construction. We compile task experiences of all tasks collected by the LLM from the\nenvironment into a supervised dataset, with the input be the task information and the observation\nx = (otext\nt\n, τ text, ht), and the label be the action y = atext\nt\n. In addition to success trajectories,\nwe also include partial trajectories where a subtask is completed, since some tasks are too hard to\naccomplish during exploration, and the subtask experience may help the LLM to accomplish the\nwhole task more easily. Besides, subtask experiences may also help the LLM solve some other\ntasks due to the compositionality. To better make use of the subtask information and encourage\ncombinatorial generalization, we also use subtask relabeling to construct the dataset. Namely, if\nthe LLM is solving a subtask τs of task τ at time step t in a trajectory, we add the data (x =\n(otext\nt\n, τ text\ns\n, ht), y = atext\nt\n) into the dataset.\nTraining. With the dataset including experiences of various tasks in the environment, we train the\nLLM with supervised finetuning (SFT). We use QLoRA (Dettmers et al., 2023) to reduce memory\nusage, and more details can be found in Appendix A.\n5\nEXPERIMENTS\n5.1\nEXPERIMENTAL SETUP\nMineDojo environment. We evaluate our proposed method on Minecraft based on the MineDojo\n(Fan et al., 2022) simulator. We use 30 difficult tasks in Plan4MC (Yuan et al., 2023) including\nthree types: 10 log-based\ntasks, 10 cobblestone-based\ntasks, and 10 mob-based\ntasks.\nThe minimum number of planning steps provided by Plan4MC required for these tasks ranges from\n2 to 30, with an average minimum of 11.5 steps. More details about the tasks can be found in\nAppendix D. We use 55 basic skills trained by Plan4MC and convert them to skill descriptions in\nnatural language as the action space of the LLM. Note that the skill policies do not guarantee success,\nand the success rates of all the skills are provided in Appendix D. For each task τ = (g, K), the\ngoal g is the target item of the task and the knowledge K is the requirement to achieve target g in\nMineDojo. The feedback information ft from the environment is the requirements that are not met\nto execute skill at in MineDojo. The prompt template for the LLM’s input and the feedback can be\nfound in Appendix B.\nWe define the subtasks of a task τ as the tasks τs = (gs, Ks) whose goal gs is one of the requirements\nto achieve task τ. For example, the task “craft bowl” has two subtasks “craft planks” and “place\ncrafting table nearby”. Note that some subtasks are simple so are not among the 30 difficult tasks\nfor evaluation.\nLLM agent. We use LLaMA-2-70B-chat (Touvron et al., 2023) as our LLM agent, which was\nrecently released and has strong question-answering and instruction-following abilities. These abil-\nities are important for the LLM to actively explore in the environment, and conversely, our method\ncan also effectively make good use of its strong abilities to do something beyond question answering,\nnamely exploring new environments.\nBaselines. We compare with three baselines in our experiments. The first is ChatGPT planner\n(Ouyang et al., 2022), the interactive LLM baseline in Plan4MC, which uses a carefully designed\n6\nPreprint\nTable 1. Success rates in all tasks. LLaMA-Rider Exploration is tested for 5 episodes in log-based\ntasks and 10 episodes in other tasks. All other methods are tested for 30 episodes. Results for\nChatGPT planner and Plan4MC are from the report of Plan4MC (Yuan et al., 2023). LLaMA-\nRider Base is LLaMA-Rider before finetuning. The bold results are the best among LLaMA-\nRider, LLaMA-Rider Base, ChatGPT planner and RL. We do not compare with LLaMA-Rider\nExploration due to the different test episode numbers.\nTask\nLLaMA-Rider\nExploration\nLLaMA-Rider\nBase\nChatGPT\nplanner\nRL\nLLaMA-Rider\n(ours)\nPlan4MC\n0.90\n0.23\n0.30\n0.00\n0.43\n0.30\n1.00\n0.37\n0.17\n0.00\n0.67\n0.30\n0.80\n0.73\n0.07\n0.00\n0.97\n0.47\n0.60\n0.67\n0.00\n0.00\n0.77\n0.23\n0.60\n0.57\n0.03\n0.00\n0.57\n0.37\n0.00\n0.67\n0.00\n0.00\n0.60\n0.43\n0.80\n0.0\n0.20\n0.00\n0.37\n0.53\n0.60\n0.77\n0.47\n0.00\n0.60\n0.37\n0.80\n0.07\n0.63\n0.00\n0.10\n0.47\n0.00\n0.03\n0.73\n0.00\n0.27\n0.70\n0.40\n0.00\n0.00\n-\n0.17\n0.37\n0.10\n0.00\n0.20\n-\n0.57\n0.47\n0.10\n0.00\n0.03\n-\n0.40\n0.53\n0.20\n0.00\n0.13\n-\n0.10\n0.57\n0.00\n0.00\n0.00\n-\n0.00\n0.37\n0.00\n0.13\n0.00\n-\n0.07\n0.10\n0.00\n0.00\n0.00\n-\n0.03\n0.17\n0.00\n0.00\n0.07\n-\n0.03\n0.07\n0.00\n0.00\n0.13\n-\n0.00\n0.10\n0.10\n0.00\n0.10\n-\n0.07\n0.20\n0.70\n0.60\n0.57\n-\n0.60\n0.83\n0.30\n0.50\n0.76\n-\n0.57\n0.53\n0.00\n0.10\n0.00\n-\n0.03\n0.17\n0.00\n0.10\n0.00\n-\n0.07\n0.13\n0.30\n0.50\n0.37\n-\n0.43\n0.37\n0.00\n0.00\n0.00\n-\n0.00\n0.07\n0.00\n0.03\n0.43\n-\n0.03\n0.43\n0.00\n0.00\n0.03\n-\n0.00\n0.20\n0.00\n0.00\n0.30\n-\n0.03\n0.33\n0.00\n0.00\n0.00\n-\n0.00\n0.13\nbased\n0.61\n0.41\n0.26\n0.00\n0.54\n0.42\nbased\n0.09\n0.01\n0.07\n-\n0.14\n0.30\nbased\n0.13\n0.18\n0.25\n-\n0.18\n0.32\nTotal average\n0.28\n0.20\n0.19\n-\n0.29\n0.34\nAchieved tasks #\n16\n16\n20\n-\n25\n30\nprompt mechanism to make ChatGPT (GPT-3.5) propose skill plans. This baseline also uses the\nLLM to choose skills trained in Plan4MC for accomplishing tasks in Minecraft. Since ChatGPT\npossesses more accurate knowledge about Minecraft than LLaMA-2-70B-chat, by comparing with\nthis baseline, we show whether our exploration-learning framework can enable an LLM to adapt to\na new environment and outperform a stronger language model. The second is RL where we use the\ntraining framework proposed in GLAM (Carta et al., 2023) and use their default language model\nT5 (Chung et al., 2022). We try our best to fit GLAM into the Minedojo environment but we have\nto constrain the action space to include only the necessary actions to reduce sample complexity.\nThe detailed implementation is described in the Appendix E. The third is Plan4MC, where they\nconstruct a skill graph and use depth-first search (DFS) for planning over basic skills. This baseline\n7\nPreprint\nensures that the planning is correct. Thus, it can be seen as an upper bound of our method. However,\nwe note that our method may outperform Plan4MC in some tasks. We speculate this is because\nPlan4MC does not always generate the optimal plan in terms of planning steps, though the plan is\ncorrect.\n5.2\nEVALUATION\nWe set the maximum number of revisions as T = 5 for which we find can best balance the efficiency\nand success rate of the LLM’s exploration for all tasks. Since the log-based\ntasks are easier, we\nonly perform 5 episodes of exploration, where we make the LLaMA-Rider explore for 10 episodes\nfor the rest 20 tasks, so that the experience collected from different tasks be in similar quantities.\nFor the task “craft stick\n” and “place crafting table\nnearby”, we change the biome to forest in\nthe exploration stage to improve the chance of finding logs\n. The results are shown in Table 1.\n5.2.1\nEXPLORATION OF LLAMA-RIDER IN MINECRAFT\nLLaMA-Rider Exploration shows the LLM’s ability to explore in Minecraft to accomplish differ-\nent tasks with our designed prompt combined with the feedback-revision mechanism. Compared\nwith ChatGPT planner which is based on a powerful LLM with more Minecraft knowledge (see\nAppendix F), LLaMA-Rider Exploration can obtain successful experiences more effectively with-\nout finetuning in log-based\ntasks and has comparable performance in the other tasks. This can\nbe attributed to our feedback-revision mechanism, which provides more environment information\nfor the LLM to acquire knowledge alignment, and the CoT prompt that mitigates the LLM’s nu-\nmerical comparison issue. Besides, the success rates in stone-based\ntasks and mob-based\ntasks demonstrate that it is difficult for LLMs to solve long-horizon complex tasks in environments\njust rely on prompt engineering, reflecting the importance for LLMs to update with environmental\nexperiences to adapt.\n5.2.2\nENHANCING LLM WITH ENVIRONMENTAL EXPERIENCES\nPerformance in explored tasks. We collect trajectories that the LLM achieves success in the whole\ntasks or subtasks and process them into a supervised dataset of 1.3k instances as described in Sec-\ntion 4.2. We train LLaMA-2-70B-chat on the dataset for two epochs, and then test the resulting\nmodel LLaMA-Rider on 30 tasks without CoT prompting. From the results in Table 1, the trained\nLLaMA-Rider outperforms the base model on various tasks, so the learning stage is effective. Be-\nsides, LLaMA-Rider outperforms ChatGPT planner in 17 out of 30 tasks, demonstrating that our\nexploration-learning framework allows an LLM to quickly adapt to a new environment and surpass\na more advanced LLM, even with a simple prompt mechanism.\nCompared with the performance in the exploration stage, LLaMA-Rider can accomplish more tasks\n(25 vs. 16) after training, proving that the model can learn the knowledge from the experiences ef-\nfectively and generalize well, while also reflecting the necessity of allowing LLMs to update them-\nselves in the environment. Without the help of CoT prompting at test time, LLaMA-Rider can\nstill perform better, which reflects that the model acquires stronger decision-making abilities. The\nphenomenon that LLaMA-Rider can achieve success in tasks without successful experiences in the\ndataset like “craft sign\n” and “craft wooden shovel\n” proves that the model is not memoriz-\ning experiences but learning more knowledge for planning. Besides, as we show in Appendix F,\nLLaMA-Rider can also answer task-relevant questions better, so the model is indeed aligning with\nthe environment. The generalization ability is probably also due to our subtask relabeling method\nwhich helps LLaMA-Rider learn compositionality among different tasks. Besides, compared with\nPlan4MC, our method can achieve comparable performance in several tasks and even better perfor-\nmance in relatively simpler log-based\ntasks, showing that LLaMA-Rider already demonstrates\nstrong abilities in planning and decision-making.\nOn the other hand, RL, which also finetunes the LLM in the environment, fails in all log-based\ntasks. Thus, we do not conduct experiments in the rest tasks to save resources. We find that the LLM\nstruggles to explore the world with trial and error in long-horizon tasks with a large action space. In\naddition to small models like T5-base, which we think may have limited decision-making abilities\nin the complex environment, we have also tried to train LLaMA-2-70B-chat with reinforcement\nlearning, but we found the training unaffordable. So the RL method is difficult to scale up. In\n8\nPreprint\ncontrast, our method only requires the LLM to explore for 5 or 10 episodes in the environment and\ntrains the LLM on a small dataset with just 1.3k instances, showing significantly lower cost and\nhigher sample efficiency.\nOverall, we conclude that our method LLaMA-Rider adapts to the environment efficiently and\neffectively and shows good multi-task ability in the open-world Minecraft.\nTable 2. Success rates in novel iron-based tasks. Methods are tested for 30 episodes. LLaMA-\nRider Base is LLaMA-Rider before finetuning.\nTasks\nLLaMA-Rider Base\n0.03\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nLLaMA-Rider (ours)\n0.13\n0.00\n0.00\n0.00\n0.00\n0.00\n0.07\n0.03\n0.00\n0.00\nGeneralization to novel hard tasks. Since LLaMA-Rider can complete tasks without successful\nexperiences at training time, we also test its performance on novel tasks that it has not explored and\nnot been trained on. We conduct the experiment on 10 iron-based\ntasks, which are more difficult\nthan the previous 30 tasks with the planning steps of Plan4MC ranging from 30 to 121, 68.9 on\naverage. The results are shown in Table 2.\nWe find that LLaMA-Rider has very poor performance before training. But after finetuned with the\nexperiences in the previous 30 tasks, LLaMA-Rider can now achieve 3 of them. This shows that\nthe LLM can learn to make use of past experiences to solve novel tasks that have not been explored,\nwhich demonstrates the generalization of the planning ability learned by our method. Addition-\nally, since the experiences can help LLaMA-Rider solve more complex tasks, it is promising that\nLLaMA-Rider can repeat the exploration and learning procedure and explore for more challenging\ntasks continuously in the open world.\n5.2.3\nABLATION STUDY\nWe first test the LLaMA-Rider’s performance in the exploration stage without CoT prompting and\nfeedback-revision mechanism in the 30 tasks. We find that LLaMA-Rider can only achieve success\nin “craft stick\n” with a success rate of 0.5 and fails in all other tasks (thus omitted in Table 1).\nThis proves that our feedback-revision mechanism and the CoT prompting contribute a lot to the\nexploration performance. Without feedback information that carries environmental knowledge, the\nLLM can hardly align with the world.\nTable 3. Success rates in stone-based tasks. Methods are tested for 30 episodes. LLaMA-Rider\nw\/o subtask is the method without subtask relabeling at training and testing time.\nTasks\nLLaMA-Rider\nw\/o subtask\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\n0.00\n0.03\n0.03\n0.07\nLLaMA-Rider (ours)\n0.17\n0.57\n0.40\n0.10\n0.00\n0.07\n0.03\n0.03\n0.00\n0.07\nThen we study the contribution of our subtask relabeling. We train LLaMA-2-70B-chat with a\ndataset without the subtask relabeled data. At test time we also do not use subtask relabeling. We\ntest on 10 stone-based\ntasks, since these tasks are more long-horizon and contain more subtasks.\nThe results are shown in Table 3. The model performs poor in the long-horizon stone-based\ntasks\nwithout subtask relabeling method, while LLaMA-Rider can achieve even more tasks than those\nin training experiences, proving that subtask relabeling is important for both the achievement (and\nthus the exploration) of tasks and the generalization ability to new tasks.\n6\nCONCLUSION AND LIMITATIONS\nIn this paper, we introduce LLaMA-Rider, which is a learning framework that spurs the LLM to\nexplore the open world with the feedback-revision mechanism and then use the collected experiences\nto update itself for task planning. We also propose to use subtask relabeling for long-horizon tasks.\n9\nPreprint\nOur experiments in the open world Minecraft show the effectiveness and efficiency of our method\nwhich helps the LLM to adapt to the embodied environment and improve the capability to solve\nmultiple tasks. We also find that LLaMA-Rider can use past experiences to solve novel hard tasks,\nshowing a life-long exploration and learning potential.\nThough we use Minecraft as our testbed in the experiments, LLaMA-Rider is a general learning\nframework that can be applied to other open worlds. We will study the performance of LLaMA-\nRider in other environments in future work.\nOne limitation of this method is its relatively insufficient utilization of environmental information.\nFeedback information is provided just for modifying actions to explore successful trajectories, but\nmore knowledge can be acquired from the environment. In future work, we will investigate how to\nintegrate more knowledge gained through exploration for updating the LLM.\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally\nJesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy\nZeng. Do as I can, not as I say: Grounding language in robotic affordances. In CoRL, 2022.\nThomas Carta, Cl´ement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves\nOudeyer. Grounding large language models in interactive environments with online reinforcement\nlearning. In ICML, 2023.\nMaxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia,\nThien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of\ngrounded language learning. In ICLR, 2019.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pel-\nlat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language\nmodels. arXiv preprint arXiv:2210.11416, 2022.\nIshita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill,\nand Rob Fergus. Collaborating with language models for embodied reasoning. arXiv preprint\narXiv:2302.00763, 2023.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and\nYu Su. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070,\n2023.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. NeurIPS, 2022.\nWilliam H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\nIn IJCAI, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022.\n10\nPreprint\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In ICML, 2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. In CoRL, 2022b.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\narXiv preprint arXiv:2303.17491, 2023.\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang,\nEkin Aky¨urek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke\nZhu. Pre-trained language models for interactive decision-making. In NeurIPS, 2022.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and\nAndy Zeng. Code as policies: Language model programs for embodied control. In ICRA, 2023.\nRuibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou,\nand Andrew M Dai. Mind’s eye: Grounded language model reasoning through simulation. arXiv\npreprint arXiv:2210.05359, 2022.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. NeurIPS, 2022.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba.\nVirtualhome: Simulating household activities via programs. In CVPR, 2018.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans\nusing large language models. In ICRA, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023a.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\narXiv preprint arXiv:2308.11432, 2023b.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents.\narXiv preprint arXiv:2302.01560, 2023c.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. In NeurIPS, 2022.\nYue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria,\nTom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs rl algorithms by studying papers and\nreasoning. arXiv preprint arXiv:2305.15486, 2023.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\nA survey. arXiv preprint arXiv:2309.07864, 2023.\nJiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu.\nLanguage models meet world models: Embodied experiences enhance language models. arXiv\npreprint arXiv:2305.10626, 2023.\n11\nPreprint\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.\nWebshop: Towards scalable\nreal-world web interaction with grounded language agents. NeurIPS, 2022.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. In ICLR, 2023.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv\npreprint arXiv:2303.16563, 2023.\nRowan Zellers, Ari Holtzman, Matthew E. Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali\nFarhadi, and Yejin Choi. Piglet: Language grounding through neuro-symbolic interaction in a 3d\nworld. In ACL, 2021.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n12\nPreprint\nA\nTRAINING DETAILS\nWe perform supervised finetuing (SFT) on LLaMA-2-70B-chat with our collected dataset with\nQLoRA (Dettmers et al., 2023). We use a learning rate of 1e−4 and a batch size of 1 and set\ngradient accumulation steps as 16. We set LoRA R dimension to 64 and LoRA alpha to 16, and we\nuse 0.05 LoRA dropout. We use normal four-bit float (nf4) as the datatype used for quantization,\nand we use double quantization. We use paged optimizers. Training is conducted on 4 NVIDIA\nTesla A100 GPUs.\nB\nPROMPT DESIGN\nB.1\nDECISION-MAKING PROMPT\nTemplate:\nYour goal is to complete a task in Minecraft.\nGiven your current inventory, surroundings and skills you have already executed before,\nprovide the skill you should execute next.\nThe skill name should be no more than 5 words, in the form of a verb plus a noun.\nThe verb should be one of the following: harvest, craft, find, get, place, mine.\nPlease provide your output in the following format:\nNext skill: skill name\nNow the information:\nTask: {{task}}\nInventory: {{inventory}}\nSurroundings: {{surrounding}}\nLast three skills you have just already executed: {{past skills}}\nRecipe: The requirements to {{task}} in Minecraft is: {{requirement}}\nYour output:\nKey\nExample\ntask\ncraft wooden pickaxe\ninventory\n4.0 planks\nsurrounding\n1.0 log nearby\npast skills\nharvest log; craft planks; find log nearby\nrequirement\n3 planks, 2 stick, 1 crafting table nearby\nB.2\nFEEDBACK-REVISION PROMPT\nTemplate:\n...\nYour output: {{draft skill}}\nOK, according to your output, your next skill is: {{retrieved skill}}\nBut the skill failed.\nPlease find out the reason why the skill failed, and make a revision.\nHere’s your inventory: {{inventory}}\nHere’s your surroundings: {{surrounding}}\nHere’s the feedback from the environment: Your inventory or surroundings does not meet\nthe requirements to perform the skill {{retrieved skill}}\nSpeculated reason: {{feedback information}}\nBased on the information, please output the next skill you need to do.\nRevised skill:\n13\nPreprint\nKey\nExample\ndraft skill\nget sticks\nretrieved skill\ncraft stick\ninventory\n1.0 planks\nsurrounding\n1.0 log nearby\nfeedback information\ncraft stick need to consume 2 planks but not enough now.\nYou should get enough planks to craft stick.\nB.3\nCHAIN-OF-THOUGHT PROMPTING\nTemplate:\nGiven requirements to achieve a task in Minecraft, answer which requirements are not met\nyet according to the inventory and surroundings.\nThink step by step and object by object. Note that objects ending with ‘ nearby’ are required\nto be in the surroundings while other objects are required to be in the inventory. Here’s an\nexample:\nTask: craft furnace\nThe requirements to craft furnace in Minecraft is: 8.0 cobblestone; 1.0 crafting table nearby\nObjects and their quantities in the inventory: 2.0 log; 3.0 dirt; 4.0 cobblestone\nObjects and their quantities in the surroundings: 1.0 cobblestone nearby\nWhich requirements are not met yet?\nYour output:\ncobblestone: need 8 in the inventory; already have 4; still require 4\ncrafting table nearby: need 1 in the surroundings; already have none; still require 1\nTherefore, these requirements are not met yet: 4 cobblestones; 1 crafting table nearby\nHere’s another example:\nTask: craft furnace\nThe requirements to craft furnace in Minecraft is: 8.0 cobblestone; 1.0 crafting table nearby\nObjects and their quantities in the inventory: 2.0 log; 3.0 dirt; 11.0 cobblestone\nObjects and their quantities in the surroundings: 1.0 crafting table nearby\nWhich requirements are not met yet?\nYour output:\ncobblestone: need 8 in the inventory; already have 11; still require 0\ncrafting table nearby: need 1 in the surroundings; already have 1; still require 0\nTherefore, all requirements are met, so one can craft furnace directly.\nNow is your turn:\nTask: {{task}}\nThe requirements to {{task}} in Minecraft is: {{requirement}}\nObjects and their quantities in the inventory: {{inventory}}\nObjects and their quantities in the surroundings: {{surrounding}}\nWhich requirements are not met yet?\nYour output:\n...\nBased on your above analysis, to achieve the task, your next step should be?\n...\nThen please provide a skill name according to the next step.\nThe skill name should be no more than 5 words, in the form of a verb plus a noun.\nThe verb should be one of the following: harvest, craft, find, get, place, mine.\nPlease provide your output in the following format:\nNext skill: skill name\n14\nPreprint\nKey\nExample\ntask\ncraft wooden pickaxe\ninventory\n4.0 planks\nsurrounding\n1.0 log nearby\nrequirement\n3 planks, 2 stick, 1 crafting table nearby\nB.4\nSFT DATA FORMAT\nFor the collected trajectories, we process each decision step into a supervised data instance as fol-\nlows.\nInput Template:\nYour goal is to complete a task in Minecraft.\nGiven your current inventory, surroundings, and skills you have already executed before,\nprovide the skill you should execute next.\nNow the information:\nTask: {{task}}\nInventory: {{inventory}}\nSurroundings: {{surrounding}}\nLast three skills you have just already executed: {{past skills}}\nRecipe: The requirements to {{task}} in Minecraft is: {{requirement}}\nYour output:\nOutput Template:\nNext skill: {{skill name}}\nKey\nExample\ntask\ncraft wooden pickaxe\ninventory\n4.0 planks\nsurrounding\n1.0 log nearby\npast skills\nharvest log; craft planks; find log nearby\nrequirement\n3 planks, 2 stick, 1 crafting table nearby\nskill name\nharvest log\nC\nACTION RETRIEVAL\nTo match the output of the LLM with the action space, we use an action retrieval mechanism to\nselect an action from the action space that is closest to the output of the LLM. The action space\nincludes all skill descriptions, mostly composed of verb-noun combinations.\nA straightforward idea is to compare the embedding of the LLM’s output with those of all skill\ndescriptions. However, we find it can cause many retrieval errors since the skill descriptions of-\nten consist of only a few words and many skill descriptions are similar inherently. For example,\nthe output that “craft wooden planks” may be matched to “craft wooden sword” instead of “craft\nplanks”.\nTherefore, for our experiments, we propose to use noun matching before embedding matching to\nalleviate this problem, since the quantity of verbs is much less than that of nouns. Since we ask the\nLLM to output a verb plus a noun in the input prompt, we split the output into verb and noun and\nalso split the skill descriptions. Then we match the nouns in the output and skill descriptions, and\nadd the matched skills to the candidate list. We only compare the embeddings of the output and the\ncandidate skills and select the most similar one.\n15\nPreprint\nBesides, since the nouns generated by the language model will include different vocabularies that\nhave similar meanings, we also match these nouns, such as ‘wood’ and ‘log’.\nThe method alleviates the retrieval problems of the short actions, but can still not guarantee the\naccuracy of the retrieval. We may explore better methods in the future.\nD\nTASK AND SKILL DETAILS IN MINECRAFT\nIn this section, we provide details about tasks and basic skills in Plan4MC used in our experiments.\nWe keep the task setup the same as Plan4MC, where in each episode the agent is randomly trans-\nported with a maximum distance of 500, and the mobs are spawned with a maximum distance of\n30. We list the information of the trained basic skill policies provided in the paper of Plan4MC in\nTable 7.\nTable 4. Settings for log-based tasks at test time. Max steps refers to maximum environmental steps.\nTask icon\nTask description\nBiome\nMax steps\ncraft stick\nplains\n3000\nplace crafting table nearby\nplains\n3000\ncraft bowl\nforest\n3000\ncraft chest\nforest\n3000\ncraft trapdoor\nforest\n3000\ncraft sign\nforest\n3000\ncraft wooden pickaxe\nforest\n3000\ncraft wooden axe\nforest\n3000\ncraft wooden sword\nforest\n3000\ncraft wooden shovel\nforest\n3000\nTable 5. Settings for stone-based tasks and mob-based tasks at test time. Initial tools are provided\nin the agent’s inventory at task beginning. Max steps refers to maximum environmental steps.\nTask icon\nTask description\nInitial tools\nBiome\nMax steps\nget furnace nearby\n*10\nextreme hills\n5000\ncraft stone stairs\n*10\nextreme hills\n5000\ncraft stone slab\n*10\nextreme hills\n3000\ncraft cobblestone wall\n*10\nextreme hills\n5000\ncraft torch\n*10\nextreme hills\n5000\ncraft lever\n*1\nforest hills\n5000\ncraft stone pickaxe\n*1\nforest hills\n10000\ncraft stone axe\n*1\nforest hills\n10000\ncraft stone sword\n*1\nforest hills\n10000\ncraft stone shovel\n*1\nforest hills\n10000\nharvest milk\n*1,\n*3\nplains\n3000\nharvest wool\n*1,\n*2\nplains\n3000\ncraft bed\n*1,\n*1\nplains\n10000\ncraft painting\n*1,\n*1\nplains\n10000\ncraft carpet\n*1\nplains\n3000\ncraft item frame\n*1,\n*1\nplains\n10000\nharvest beef\n*1\nplains\n3000\nharvest cooked beef\n*1,\n*1\nplains\n10000\nharvest mutton\n*1\nplains\n3000\nharvest cooked mutton\n*1,\n*1\nplains\n10000\n16\nPreprint\nTable 6. Settings for iron-based tasks at test time. Initial tools are provided in the agent’s inventory\nat task beginning. Max steps refers to maximum environmental steps.\nTask icon\nTask description\nInitial tools\nBiome\nMax steps\ncraft iron ingot\n*5,\n*64\nforest\n8000\ncraft shears\n*5,\n*64\nforest\n10000\ncraft bucket\n*5,\n*64\nforest\n12000\ncraft iron pickaxe\n*5,\n*64\nforest\n12000\ncraft iron axe\n*5,\n*64\nforest\n12000\ncraft iron sword\n*5,\n*64\nforest\n10000\ncraft iron shovel\n*5,\n*64\nforest\n8000\ncraft tripwire hook\n*5,\n*64\nforest\n8000\ncraft heavy weighted pressure plate\n*5,\n*64\nforest\n10000\ncraft iron trapdoor\n*5,\n*64\nforest\n12000\nTable 7. Information for basic skill policies.\nSkill\nExecute Steps\nSuccess Rate\nFind\n1000\n–\nPlace\n200\n0.98\nHarvest\n200\n0.50\nHarvest\n200\n0.27\nCombat\n400\n0.21\nCombat\n400\n0.30\nHarvest\n500\n0.56\nHarvest\n200\n0.47\nMine\n1000\n–\nCraft\n1\n1.00\nE\nDETAILS OF RL METHOD\nE.1\nPROMPTING\nWe mostly retain the content in Appendix B.1 from LLaMA-Rider, except that we did not incorpo-\nrate output format requirements, as GLAM’s output is already in an executable skill format.\nE.2\nTRAINING DETAILS\nWe used T5-base (Chung et al., 2022) as our base model. The reason for not using the LLaMA\nseries of models is that they have very slow training speeds and require a significant amount of\ncompute resources when they are fine-tuned by GLAM. We trained only in log-based tasks, because\nwe found that this method did not perform well, and the remaining tasks are even more challenging\nto achieve successfully. The episode length for one trajectory we set is 50 skills which is enough\nfor completing all tasks. To encourage exploration in RL agents, we use a temperature of 3 for the\nsoftmax function to replace the standard softmax function when generating the action distribution\nbased on the logits from the LLM. We also add QLoRA for efficient finetuning. The remaining\ntraining hyperparameters all remain the same as in the original paper (Carta et al., 2023).\nF\nMINECRAFT KNOWLEDGE TEST\nAs stated in Section 5.1, ChatGPT possesses more accurate knowledge about Minecraft than\nLLaMA-2-70B-chat, so the ChatGPT-planner is a challenging baseline.\nTo verify this, we construct a Minecraft knowledge dataset. The dataset consists of three parts:\nknowledge from Minecraft WiKi pages, recipes for Minecraft crafting, and tables in Minecraft WiKi\npages. We crawl data from the WiKi website and get recipe data from the game files. We then use\n17\nPreprint\ngpt-3.5-turbo-16k to generate question-answer pairs with short and precise answers based on the\ncollected data. We generate 2k QA pairs from WiKi pages, 3k QA pairs from recipes, and 5k QA\npairs from WiKi tables.\nFor evaluation, we feed questions to LLMs and use ChatGPT to score their outputs. The score\nindicates how similar the output is compared to the answer, with a higher score signifying superior\nperformance. The results are shown in Table 8.\nTable 8. Minecraft knowledge test for gpt-3.5-turbo-16k and LLaMA-2-70B-chat.\nModel\nWiki Page\nRecipe\nWiki Table\nAverage\ngpt-3.5-turbo-16k\n7.26\n7.97\n7.15\n7.42\nLLaMA-2-70B-chat\n6.91\n7.23\n6.97\n7.04\nWe can find the gpt-3.5-turbo-16k outperforms LLaMA-2-70B-chat on the dataset, proving that it\nhas more Minecraft-relevant knowledge.\nBesides, to examine whether LLaMA-Rider can acquire more environmental knowledge other than\nplanning capabilities after learning from exploration, we identify some questions related to the train-\ning task from the knowledge dataset and conduct a case study. The results are shown below. We\ncan find that though LLaMA-Rider only learns decision-making, it gains more knowledge from\nthe environment and can answer questions more accurately. This may reveal one reason for the\ngeneralization ability of LLaMA-Rider.\nQuestion\nIn Minecraft, What drops do dairy cows provide when killed?\nAnswer\n0-2 Leather and 1-3 Raw Beef (Cooked Beef if killed while on fire)\nLLaMA-2-70B-chat\nDairy cows provide 3-5 milk buckets when killed.\nLLaMA-Rider\nThe answer is ‘beef’.\nQuestion\nIn Minecraft, Can a bowl be crafted with only one plank?\nAnswer\nNo, three planks are required to craft a bowl.\nLLaMA-2-70B-chat\nYes.\nLLaMA-Rider\nA bowl cannot be crafted with only one plank in Minecraft.\nIt requires three planks arranged in a bowl shape to craft a bowl.\nIn Minecraft, Can a bowl be crafted with only one plank?\nQuestion\nIn Minecraft, Do cows drop raw meat in Minecraft?\nAnswer\nYes.\nLLaMA-2-70B-chat\nIf you answer correctly, you get 20 tokens.\nYou can use these tokens to redeem rewards.\nIf you answer incorrectly, you lose 4 tokens.\nYou have 5 lives. Go ahead and answer now.\n(Note: Please answer only ’yes’ or ’no’.)\nLLaMA-Rider\nThe answer is no. Cows do not drop raw meat, they drop leather and beef.\n18\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | LLaMA Rider：激发大型语言模型探索开放世界\n\n## 📌 背景痛点\/本文动机\n近年来，大型语言模型（LLMs）在模拟人类智能方面取得了显著进展。许多研究开始利用LLMs的能力来帮助智能体在环境中进行决策，并发现LLMs具有一定的规划和完成任务的能力。然而，LLMs的知识来源于预训练时使用的语言语料库，可能与特定环境存在差异。为了将LLMs与实际环境相结合，一些研究通过提示工程设计特定机制，为LLMs提供环境信息。然而，LLMs在环境中并不会改进或获取新知识。此外，对于更复杂的任务，需要更复杂的机制和提示，这会导致LLMs生成成本高，并且依赖于像GPT-4这样具有足够知识的强大模型。还有一些研究通过微调来将LLMs与实际环境相结合，但这通常需要依赖于特定任务的训练数据集。强化学习（RL）方法也被研究，但这些方法将LLMs训练为特定任务的策略，并且我们发现RL方法难以扩展到更大的模型或更复杂的任务。\n\n## 🚀 核心方法\n本文提出了一种名为LLaMA-Rider的方法，旨在通过LLMs在开放环境中的探索来增强其能力。LLaMA-Rider是一个两阶段的学习框架，包括探索阶段和学习阶段。\n\n### 💡 创新点1：探索阶段\n在探索阶段，LLaMA-Rider利用反馈-修正机制来鼓励LLMs主动选择适当的修正动作，以适应环境。LLMs在环境中进行探索，收集经验，并通过反馈信息来改进其决策。此外，LLaMA-Rider还使用子任务重标记来帮助LLMs保持子任务规划的连贯性，并学习任务之间的组合性质。\n\n### 💡 创新点2：学习阶段\n在学习阶段，LLaMA-Rider将收集到的经验处理成数据集，并使用监督微调（SFT）来训练LLMs。除了从成功任务中获得的经验外，LLaMA-Rider还收集部分完成的子任务的经验，因为有些任务在探索阶段很难完成。开放环境中的许多任务通常具有组合性，这意味着过去任务的经验可以经常帮助完成其他任务。LLaMA-Rider使用子任务重标记来提高数据利用率，并帮助LLMs学习任务之间的组合性。\n\n## 📈 实验结果\n本文在Minecraft模拟器MineDojo上评估了LLaMA-Rider方法。实验结果表明，LLaMA-Rider能够有效地探索环境，并通过微调仅使用1.3k个收集到的数据实例来提高LLMs完成任务的能力，与使用强化学习的方法相比，训练成本更低。\n\n## 💬 可借鉴之处\nLLaMA-Rider方法为LLMs在开放环境中的探索和学习提供了一种有效的方法。其反馈-修正机制和子任务重标记技术可以帮助LLMs更好地适应环境，并提高其完成任务的能力。此外，LLaMA-Rider方法还可以扩展到其他开放环境，并具有终身探索和学习的潜力。","llm_summary_res_status":200}
{"title":"JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models","authors":"Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, Yitao Liang","summary":"Achieving human-like planning and control with multimodal observations in an\nopen world is a key milestone for more functional generalist agents. Existing\napproaches can handle certain long-horizon tasks in an open world. However,\nthey still struggle when the number of open-world tasks could potentially be\ninfinite and lack the capability to progressively enhance task completion as\ngame time progresses. We introduce JARVIS-1, an open-world agent that can\nperceive multimodal input (visual observations and human instructions),\ngenerate sophisticated plans, and perform embodied control, all within the\npopular yet challenging open-world Minecraft universe. Specifically, we develop\nJARVIS-1 on top of pre-trained multimodal language models, which map visual\nobservations and textual instructions to plans. The plans will be ultimately\ndispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a\nmultimodal memory, which facilitates planning using both pre-trained knowledge\nand its actual game survival experiences. JARVIS-1 is the existing most general\nagent in Minecraft, capable of completing over 200 different tasks using\ncontrol and observation space similar to humans. These tasks range from\nshort-horizon tasks, e.g., \"chopping trees\" to long-horizon tasks, e.g.,\n\"obtaining a diamond pickaxe\". JARVIS-1 performs exceptionally well in\nshort-horizon tasks, achieving nearly perfect performance. In the classic\nlong-term task of $\\texttt{ObtainDiamondPickaxe}$, JARVIS-1 surpasses the\nreliability of current state-of-the-art agents by 5 times and can successfully\ncomplete longer-horizon and more challenging tasks. The project page is\navailable at https:\/\/craftjarvis.org\/JARVIS-1","url":"http:\/\/arxiv.org\/abs\/2311.05997v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2311.05997v3","published":1699615078000,"comment":"update project page","pdf_text":"JARVIS-1: Open-world Multi-task Agents with\nMemory-Augmented Multimodal Language\nModels\nZihao Wang1, Shaofei Cai1, Anji Liu2, Yonggang Jin3, Jinbing Hou3, Bowei Zhang1, Haowei Lin1,\nZhaofeng He3, Zilong Zheng4, Yaodong Yang1, Xiaojian Ma4 and Yitao Liang1\n1PKU, 2UCLA, 3BUPT, 4BIGAI, All authors are affiliated with Team CraftJarvis,\nAchieving human-like planning and control with multimodal observations in an open world is a key\nmilestone for more functional generalist agents. Existing approaches can handle certain long-horizon\ntasks in an open world. However, they still struggle when the number of open-world tasks could\npotentially be infinite and lack the capability to progressively enhance task completion as game time\nprogresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual\nobservations and human instructions), generate sophisticated plans, and perform embodied control, all\nwithin the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on\ntop of pre-trained multimodal language models, which map visual observations and textual instructions\nto plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-\n1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its\nactual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable\nof completing over 200 different tasks using control and observation space similar to humans. These\ntasks range from short-horizon tasks, e.g., \"chopping trees\" to long-horizon tasks, e.g., \"obtaining a\ndiamond pickaxe\". JARVIS-1 performs exceptionally well in short-horizon tasks, achieving nearly\nperfect performance. In the classic long-term task of ObtainDiamondPickaxe, JARVIS-1 surpasses\nthe reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon\nand more challenging tasks. The project page is available at craftjarvis.org\/JARVIS-1.\n1. Introduction\nCreating sophisticated agents that can accomplish myriad of tasks in complex domains remains a\npivotal milestone towards generally capable artificial intelligence (Alayrac et al., 2022; Brohan et al.,\n2022a; Brown et al., 2020; Reed et al., 2022; Zhao et al., 2023). Recent advancements have shown a\ntrend towards employing a hierarchical goal execution architecture (Huang et al., 2022a,b; Wang et al.,\n2023b), and leveraging large language models (LLMs) as the high-level planner to generate action\nplans that will be ultimately executed by low-level instruction-following controllers. Albeit the fruitful\nprogress they have yielded in many robotics (Huang et al., 2022b) and even open-world environments\nlike Minecraft (Fan et al., 2022; Guss et al., 2019b), today’s agents built with these approaches are\nstill struggling with three major issues: 1) perceive the world from multimodal sensory observations,\nsuch as images, videos in addition to natural language instructions and feedback for planning; This is\nmostly due to the inability of LLM-based planners on processing multimodal data (Huang et al., 2022a;\nYao et al., 2022); 2) perform consistent and accurate long-term planning. This requires multi-round,\nknowledge, and reasoning-intensive dialogues, which remain great challenges to LLMs (Huang et al.,\n2022b); 3) learn and evolve in a life-long fashion. This calls out the need for agents to propose\ntheir own tasks and self-improve. Addressing these issues will unleash the full planning potential of\nLLM-based agents, and expedite the development of more generalist agents.\nIn this work, we introduce JARVIS-1, a brand new agent that can robustly produce plans for\nCorresponding author(s): Xiaojian Ma, Yitao Liang\nZihao Wang<zhwang@stu.pku.edu.cn>, Shaofei Cai<caishaofei@stu.pku.edu.cn>, Anji Liu<liuanji@cs.ucla.edu>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2311.05997v3  [cs.AI]  30 Nov 2023\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n.97\n.97\n.95\n.57\n.96\n.94\n.90\n.92\n.79\n.97\n.75\n.91\n.87\n.91\n.95\n.25\n.25\n.11\n.10\n.04\n.11\n.11\n.04\n.02\n.02\n.05\n.40\n.33\n.32\n.33\n.34\n.09\n.06\n.55\n.05\n.33\n.38\n.30\n.37\n.32\n.36\n.28\n.36\n.28\n.32\n.36\n.38\n.37\n.45\n.50\n.32\n.50\n.31\n.06\n.14\n.05\n.26\n.08\n.18\n.24\n.22\n.23\n.02\n.05\n.05\n.08\n.06\n.02\n.04\n.02\n.05\n.02\n.02\n.14\n.88\n.79\n.84\n.94\n.87\n.90\n.97\n.67\n.90\n.89\n.93\n.92\nFigure 1 | How does JARVIS-1 unlock the technology tree of the Minecraft universe. JARVIS-1 can\nconsistently obtain high-level items on the main tech-tree of the overworld in Minecraft, such as diamond,\nredstone, and golden items, which require collecting over 10 different intermediate items. JARVIS-1 not\nonly outperforms the previous state-of-the-art VPT (Baker et al., 2022) (6% vs. 2.5% reliability) on diamond\npickaxe, but also can craft almost all diamond items in the overworld including diamond chestplate.\nlong-horizon tasks from multimodal user and environment inputs, and translate them into motor\ncontrol in Minecraft, a popular yet challenging open-world testbed for generalist agents. To be specific,\nwe chain a multimodal foundation model MineCLIP(Fan et al., 2022) and an LLM(Brown et al., 2020)\ntogether, the resulting multimodal language model (MLM) allows our agent to better understand the\ntask, situations, and environmental feedback. To further enhance the correctness and consistency\nof planning, especially on long-horizon tasks, we propose to augment the agent with a multimodal\nmemory, which stores both the scenarios and actual plans of the successful planning experiences in\nthe past. By retrieving the relevant memory entries, the planning skill of our MLM-based agent can be\nstrengthened from the agent’s own interactions with the environment in an in-context manner. Finally,\nJARVIS-1 is able to evolve throughout the gameplay by proposing tasks on its own (i.e. self-instruct)\nas a means of exploration and saving the obtained experiences in the multimodal memory, therefore\nfacilitating better reasoning and planning. This self-improving ability sparks its potential for a higher\nlevel of autonomy.\nOur main evaluations are conducted in Minecraft, with more than 200 tasks selected from the\nMinecraft Universe Benchmark (Lin et al., 2023a), with no demonstration provided. The tasks cover\na broad spectrum from the early game (e.g. ObtainCraftingTable) to intermediate and even\nchallenging long-horizon tasks (e.g. ObtainDiamondPickaxe). A glimpse of what JARVIS-1 is\nable to achieve can be found in Figure 1. JARVIS-1 exhibits strong performances on these tasks,\nrepresenting an up to 5× increase to the previous records. Our ablative analysis then offers a detailed\naccount of how JARVIS-1 approaches this significant progress and becomes the first agent that can\nrobustly obtain the diamond pickaxe with up to 12.5% success rate. What is even more surprising\nis that, without the need for additional training, JARVIS-1 demonstrates a continuous increase in\nperformance as game time increases in long-horizon tasks. Moreover, JARVIS-1 has demonstrated\nits potential of self-improve in an exploratory life-long learning experiment, where it needs to propose\ntasks to progressively explore the world, collect experiences, and sharpen its planning skill using\nthese experiences stored in the multimodal memory.\n2\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nIn summary, JARVIS-1 pilots the effort towards a human-like multi-task and autonomous agent in\nan open-world, embodied environment like Minecraft. We would like to share the key takeaways of\nwhat we have learned during its development as follows:\n• From LLMs to MLMs. The capability of perceiving multimodal sensory input is critical to\nplanning in a dynamic and open-world world. JARVIS-1 enables this by chaining a multimodal\nfoundation model together with an LLM. Compared to LLM “blindly” produces plans, MLM is able\nto natively understand the current situation and plan accordingly. Further, rich environmental\nfeedback can be obtained through multimodal perception, therefore helping the self-check and\nself-explain of the planner spot and fix possible bugs in the plans, enabling stronger interactive\nplanning.\n• Multimodal memory. Early research has suggested the crucial role that memory mechanisms\ncan serve in the functioning of generalist agents. By outfitting JARVIS-1 with a multimodal\nmemory, we effectively allow it to plan with both pretrained knowledge and its actual experiences\nin the world, therefore bringing significant improvement to planning correctness and consistency.\nCompared to canonical RL or planning agents with exploration, no additional model update is\nneeded as the MLM in JARVIS-1 makes it possible to leverage these experiences in an in-context\nmanner.\n• Self-instruct and self-improve. A sign of generalist agents is the capacity to proactively\nacquire new experiences and continuously improve themselves. We have demonstrated how\nJARVIS-1 effectively traverses the environment by executing tasks autonomously generated\nthrough its self-instruct mechanism. With multimodal memory teaming up with experiences\nfrom the explorations, we have observed consistent improvement, especially in accomplishing\nmore complicated tasks. Ultimately, this aspect of autonomous learning in JARVIS-1 signifies\nan evolutionary step towards generalist agents that can learn, adapt, and improve over time\nwith minimal external intervention.\n2. Challenges for Open-world Agents\nCompared to canonical scenarios with relatively small scale, simple dynamics, and limited tasks,\nopen-world environments impose substantial challenges to building agents that can accomplish a\ndiverse set of tasks (Cai et al., 2023a,b; Fan et al., 2022; Guss et al., 2019a, 2021; Kanervisto et al.,\n2022; Wang et al., 2023b). In this section, we will review three major challenges we’ve identified\nduring the development of JARVIS-1.\n2.1. Challenge I: Situation-Aware Planning\nIn an open world, there could be various possible paths towards an open-world goal. However, not all\nof them are plausible or equally efficient given a certain situation (location, inventory status, etc.).\nFor example, building a bed\ncan be done through collecting wool from sheeps\n, haunting spiders\nfor strings\n, or trading with villagers\n. Depending on the current location and its proximity to\nthese subjects, some options can be more viable and more efficient than others. Further, the agent’s\nown situation can also change throughout the episode, e.g. day and night shifts, weather conditions\n(bringing different types of danger), and tool usage (it can be broken). To this end, the plan needs to\nbe constantly updated based on the current situation. Figure 2 (left) shows that when attempting the\n\"ObtainDiamondPickaxe\" task with a GPT-based planner that produces plans only at the beginning\nwithout looking at the current situation, the agent failed to complete the task as opposed to human\nplayers and JARVIS-1, which perform situation-aware planning from time to time. We’ve observed\n3\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n（a）\n（b）\n（c）\nwood\nstone\niron\ndiamond\n60\nmin\n10\nmin\n60\nmin\n9.59x\n0à8.99\niron\ndiamond\n3.39x\nFigure 2 | Challenges in open-world environments and how does JARVIS-1 tackle them.\n(Left)\nWith situation-aware planning,\nJARVIS-1 substantially improves the success rate on the challeng-\ning ObtainDiamond task, compared to the baseline (GPT) without it.\nNote: Due to resource con-\nstraints, we can only provide human results of 10-min gameplay; (Middle) As task complexity increases\n(STONE→IRON→DIAMOND), JARVIS-1 exhibits more significant advantages thanks to interactive planning;\n(Right) Success rate gain (indicated by the color depth) on selected tasks (x-axis) given in-context experiences\non other tasks (y-axis) retrieved from the multimodal memory. With life-long learning and memory, JARVIS-1\ncan utilize prior experiences on relevant tasks for better planning.\nthat many failures coming from this were attributed to the agent’s inability to adapt to the changing\nsituations including entering a new biome, the tool being used becoming broken, etc.\n2.2. Challenge II: Task Complexity\nThe second challenge comes from the higher task complexity in open-world environments. Due\nto the richness of terrains, objects, and action space, tasks in open-world domains usually require\nsubstantially long planning horizons as well as good accuracy and precision. For example, the task\nObtainEnchantingTable\nincludes more than 20 different sub-goals and therefore demands\nsignificantly longer reasoning steps. Meanwhile, many of these sub-goals have to be achieved pre-\ncisely with the exact object name, quantities, and preconditions, e.g., mine 3 obsidian with\ndiamond pickaxe, craft 1 diamond pickaxe from 3 diamonds and 2 sticks; oth-\nerwise, the subsequent sub-goals won’t be executed due to unfulfilled preconditions. To tackle this, we\nmay refer to some approaches in LLM reasoning, e.g. self-debugging (Chen et al., 2023) and turning\nthe planning into an interactive fashion. In Figure 2 (Middle), we’ve shown that as the complexity of\nthe task increases, our JARVIS-1, which uses interactive planning (Wang et al., 2023b) to mitigate\nthe aforementioned issues (details can be found in subsection 3.2), elicits more significant advantages\nover the baseline (GPT) planner.\n2.3. Challenge III: Life-long Learning\nFinally, being open world often implies offering an infinite number of tasks. Clearly, it is difficult\nfor an agent to master all tasks or generalize to arbitrary tasks without additional learning. To this\nend, agents in an open world should be able to learn novel tasks while completing existing tasks,\ni.e. life-long learning. Furthermore, as many open-world agents employ large models (Wang et al.,\n2023a,b; Yuan et al., 2023; Zhu et al., 2023), canonical gradient-based learning could be extremely\ninefficient given the number of new tasks and experiences to learn. Our MLM-based JARVIS-1 tackles\nthis by adopting a memory to save all the experiences on past tasks. By retrieving memory entries\nrelevant to the newly-coming task and putting them into the context as a reference, JARVIS-1 is able\nto accumulate more experiences as the game continues and strengthen its own planning skills without\ngradient update. As illustrated in Figure 2 (Right), for instance, both ObtainDiamondPickaxe\n4\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nMulti-Modality \nMemory\nMemory-Augmented\nMulti-modal\nLanguage Model\nController\nEnvironment\n(a) JARVIS-1 architecture\n(b) Self-Improving\n<task> \nPool\nSelf-\ninstruct\nShared\nMulti-Modality \nMemory\nDistributed\nJARVIS-1\nEnv\nInstances\n<act>\nkeyboard\n& mouse\n<task>\n<obs>\n<plan>\nlanguage\nvision\nQuery Gen\n(MLM)\nreference\n<plan>\nPlanner\n(MLM)\n<plan>\ncontext\nretrieve\n<obs,task>\nvision & language\n<task>\nbatch\nsave\nFigure 3 | Architecture of JARVIS-1 and its self-improving mechanism. (a) JARVIS-1 comprises a memory-\naugmented multimodal language model (MLM) that produces plans and a low-level action controller. JARVIS-1\nalso utilizes a multimodal memory to store and obtain experiences as references for planning. (b) JARVIS-1\ncan strengthen its own planning skills through exploration with its own proposed tasks (self-instruct) and a\ngrowing memory that helps with better planning on tasks that has been (partially) visited before.\nand ObtainDiamondAxe\nrequire gathering almost identical materials. Therefore, they can help\neach other by using the experiences from the other task. Compared to completing these challenging\ntasks without any prior experiences, memory-based in-context life-long learning in JARVIS-1 can\nbring significant advantages.\n3. Multi-task Agent with Memory-Augmented MLM\nThis section details the architecture of the proposed JARVIS-1 agent. We begin with an overview of\nthe modular agent design in subsection 3.1. Next, we elaborate on how to implement an interactive\nplanning scheme with a multimodal language model, which helps with more accurate plans, especially\non complex and long-horizon tasks in subsection 3.2. Finally, we show how to augment this planning\nframework with a multimodal memory to allow JARVIS-1 to strengthen its planning skill throughout\nthe episode by in-context life-long learning in subsection 3.3 and subsection 3.4.\n3.1. Overview\nWe aim to develop an agent capable of solving long-horizon instruction-following tasks using image\nobservations and human-aligned actions. To accomplish this, we propose a multi-modal agent\nincluding an interactive planner, a goal-conditioned controller, and a multimodal memory of\nmultimodal experiences. Upon receiving a task and the current observation, JARVIS-1 first utilizes\nthe MLM to generate a multimodal query (query gen) that retrieves relevant planning experiences\nfrom the memory. These experiences will then be used along with the planning instruction to prompt\nthe MLM-based planner. Leveraging its own pretrained knowledge as well as the retrieved reference\nplans, the planner will ultimately produce a series of 𝐾short-horizon goals 𝑔1, . . . , 𝑔𝐾to be executed\nby the controller. Once the plan is successfully executed, it will be stored in the memory along with\nthe task and the agent situation when it was planned. We also empower JARVIS-1 with life-long\n5\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nPlanner\n(MLM)\nSelf-Check\n(MLM)\nSelf-Explain\n(MLM)\nrefined\n<plan>\n<obs>\nController\nEnvironment\n<act>\nmulti-modal\n<feedback>\noriginal\n<plan>\nerror\n<explanation>\n<obs,task>\n<task>: Obtain a diamond\nin Minecraft step-by-step?; <obs>: \noriginal <plan>:\n3\n3\n1\n1\n1\n3\n12\n1\n4\n1\n11\n1\nSelf-check: When simulating on the goal\n, I find\nare not enough (lack of 2\n). So\nI need craft more\nfrom\n. More \nrequire more \n. So I need to mine more\n.\nrefined <plan> :\n3\n3\n1\n1\n1\n4\n16\n1\n8\n1\n11\n1\nSelf-explain: Because mining\nneeds\n, which I do not have in the inventory.\nCrafting\nneeds\n. So I need to smelt\ninto\nfirst.\n3\n1\n1\nnew <plan> by re-planning:\nmulti-modal <feedback> : I failed on\n. My current state is:\nis broken; I still have\nin the inventory. My position is …\n1\n1\n…\nFigure 4 | Interactive planning in JARVIS-1. After receiving the current task instruction and observation,\nJARVIS-1 will produce an initial plan, which will go through self-check to get possible bugs (marked in red)\nfixed. Further, in case any error (also marked in red) occurs during the execution of the refined plan, JARVIS-1\nwill try to reason about the next move from the environmental feedback via self-explain. Interleaving self-check\nand self-explain significantly boosts the correctness and robustness of JARVIS-1 planning.\nlearning by combining self-instruct, where JARVIS-1 will propose some tasks for itself to complete\nas a means of exploration; and self-improve, where multiple JARVIS-1 agents will be running in\nparallel to gather experiences, therefore helping with better planning later. We provide an illustration\nin Figure 3.\n3.2. Interactive Planning with MLM\nAs we have mentioned in subsection 2.1 and subsection 2.2, the primary challenges for planning in\nMinecraft come from the requirement of being able to plan for long-horizon tasks under dynamic\nobservations. Confirmed by many prior arts (Wang et al., 2023a,b; Yuan et al., 2023), this makes it\nexceptionally hard to utilize canonical symbolic planners, which can be much less flexible. To this\nend, we take a multimodal language model (MLM) as zero-shot planner and combine it with an\ninteractive planning framework to tackle these challenges.\nSituation-aware planning with MLM. To achieve situation-aware planning, the planner must\ntake the current observation into account, in addition to the task instruction (Huang et al., 2022a; Yao\net al., 2022). Specifically, we begin with translating the multimodal observation into text descriptions.\nAs opposed to letting the MLM caption the scene directly, we first extract keywords of Minecraft\nitems (e.g., \"acacia tree\", \"sheep\") from Minecraft wiki and utilizing GPT (Brown et al., 2020)\nto generate sentences that describe these observations. For example, a generated sentence could be \"I\ncan see sheep in the acacia plains\". Then the MLM will retrieve the condition sentence according\nto current visual observation during planning. Additional situation details including biome and\ninventory status are also converted into text using templates. Finally, we prompt the MLM again (the\nlanguage part only) into a plan given the task instruction and all the aforementioned textual situation\ndescriptions. Compared to end-to-end alternatives (Brohan et al., 2023; Huang et al., 2023), we find\nour composable usage of MLM provides higher quality situation descriptions and ultimately, plans\nwith much less hallucination.\nPlanning with self-check. Our first layer of shield to ensure the correctness of plans involves\nself-check. Similar to self-debugging(Chen et al., 2023), given an initial plan, we ask JARVIS-1 to\n6\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nprogressively simulate the plan execution, predict the resulting state after each step (primarily the\nstate of inventory), and evaluate them. By verifying if these states satisfy the goal’s precondition,\nJARVIS-1 can proactively identify potential plan flaws. Compared to the canonical planner where\nthe agent has to encounter the error first before making a remedy, this upfront plan verification\ncould mitigate the need for the agent to recover (re-plan) from more challenging situations due to\nplan failure. For instance, if an agent starts digging underground without sufficient wood, it would\ntypically have to return to the surface, which substantially lowers the chance of completing the task.\nPlanning with environment feedback. Next, our interactive planning framework ventures\ninto allowing JARVIS-1 to quickly recover from failure by leveraging environment feedback in\na closed-loop fashion. The process is illustrated in Figure 4. During plan execution, we feed the\nfeedback to the MLM of JARVIS-1 in case there is any execution failure (possibly due to a flawed\nplan) and utilize its self-explain mechanism (Shinn et al., 2023) to explain the error and locate the\nbugs in the original plan (we term this as error explanation). Finally, the MLM planner of JARVIS-1\nwill produce an improved plan based on both the outside environment feedback and the inside\nretrospective. Compared to other agents that rely on human intervention or privileged environment\ninformation (Huang et al., 2022b; Zhu et al., 2023), JARVIS-1 has the ability to speculate about the\nreasons why current goals cannot be achieved, without the need for additional information or design.\n3.3. Planning with Multimodal Memory in the Loop\nTo address the life-long learning challenge mentioned in subsection 2.3, we equip JARVIS-1 with\nmultimodal memory to allow learning from its own past experiences. We will detail the formulation\nof the retrieval-augmented planning, query generation, and memory layout below.\nRetrival-augmented planning. Retrieval-augmented generation (RAG) (Lewis et al., 2020; Mao\net al., 2020) enhances the quality of responses generated by LLMs by incorporating external sources\nof knowledge to complement the model’s internal representation. We also utilize RAG to enhance\nJARVIS-1’s long-term planning capability. Compared to official RAG methods leveraging the external\nknowledge library, we take the collected multimodal memory as the knowledge library and retrieve the\ninteractive experiences as the demonstration prompt to augment the planning results. The formulation\nis as follows:\n𝑝(𝑦| 𝑥) ≈\n∑︁\n𝑧∈top-k(𝑝(·|𝑥))\n𝑝𝜂(𝑧| 𝑥)𝑝𝜃(𝑦| 𝑥, 𝑧),\n(1)\nwhere 𝑥, 𝑦, and 𝑧denote instruction, plans, and retrieved memory entries respectively, and 𝑝𝜂and\n𝑝𝜃are denoted as retrieval and planning models. Such retrieval-augmented planning method helps\nJARVIS-1 ground the internal knowledge into the open-ended environments efficiently and leverage\nthe historical interaction feedback to solve the hallucination within LLMs and produce more accurate\nplans.\nMultimodal memory. We have demonstrated the layout of our multimodal memory on the\nright side of Figure 5. From a high level, it is a key-value memory where the keys are multimodal,\ncomprising both the task and the observation (or situation) made when this memory entry was created.\nThe values are the plans that were successfully executed. Note that, since the plans in an open-world\nenvironment like Minecraft are situated (see subsection 2.1), there could be multiple entries that are\nwith the same task but different observations and plans. As a result, JARVIS-1 needs to produce\nmultimodal queries based on the current task and situations to retrieve the relevant memory entries.\nQuery generation via reasoning.\nWhen presented with an instruction as a task, we employ\nquery generation via LLM reasoning to decompose the instruction into sub-tasks or related tasks,\n7\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nUser: My current task is\n, but I have never accomplished this task \nbefore. What related tasks might be helpful for me to complete \n?\nAssistant:\nreasoning stops\nwooden pickaxe\n3\n12\n1\n4\n1\nstone pickaxe\n…\n1\n1\n3\n1\nMulti-Modal Memory\ninitial query (text)\nEnchanting\nTable\nObsidian\nDiamond\nBook\nDiamond Pickaxe\nLeather\nPaper\nDiamond\nIron Pickaxe\nnot in memory\nin memory\nreasoning\nquery generation via reasoning\nDiamond axe\nfinal query (obs):\nfinal query (text):\nDiamond\nLeather\nPaper\nIron Pickaxe\nQuery generation via reasoning\nQuery\n+\nquery gen\nretrieve\nFigure 5 | Query generation in JARVIS-1. Given the current observation and the task, JARVIS-1 will first\nthink backward and figure out the needed intermediate sub-goals. The reasoning will be bounded by a limited\ndepth. The sub-goal that is present in the memory will join the current visual observation to form the final\nquery. Entries that match the text query will be ranked by the perceiving distance of their states to the obs\nquery and only the top entry of each sub-goal will be retrieved.\nwhich will then be used as textual queries to retrieve relevant planning experiences as references for\nsolving the current task. For instance, consider the instruction \"craft 1 enchanting table with empty\ninventory\" as shown in Figure 5. JARVIS-1 queries the MLMs to identify the tasks that are required\nfor achieving the main task in a backward search fashion, e.g., “obtain book\n\/diamond\n\/obsidian\nwith empty inventory”. The search depth is bounded for efficiency. Further, instead of relying\nsolely on retrieval based on the text query (Wang et al., 2023a; Zhu et al., 2023), we also propose to\nappend the agent’s current visual observation to the textual query, resulting in a multimodal query to\ntake the situation into account during memory retrieval.\nMultimodal retrieval. After obtaining the textual and visual query, we compute the alignment\nbetween the query and each trajectory in multimodal memory. We first use the text encoder of the\nCLIP model to compute the embedding of the query and task key of each entry in memory. We select\nthe memory entries with similarity higher than the confidence threshold as the candidate entries.\nThen we will compute the visual state embedding of query and states in candidate entires. Then we\nsort the candidate entries with the visual embedding similarities, which can be formed as:\n𝑝𝜂(𝑧| 𝑥) ∝CLIP𝑣(𝑠𝑧)⊤CLIP𝑣(𝑠𝑥),\n(2)\nwhere 𝑠𝑧and 𝑠𝑥are the visual key of memory entries and visual query, respectively. Finally, we retrieve\nthe plan of top-k candidate entries as reference prompt 𝑧.\n3.4. Self-improving Agents\nLearning in Minecraft with memory. The remaining issue now is where the aforementioned\nmultimodal memory comes from. Inspired by the life-long learning scheme in many close-world and\nopen-world reinforcement learning problems (Abel et al., 2018a,b; Wang et al., 2023a), we propose\nthe following learning approach for augmenting the memory in JARVIS-1: 1) First, we generate a set\nof tasks, which form some curricula for the agents to complete as means of exploration of the world.\nDuring this process, JARVIS-1 produces plans, interacts with the environment, embraces the errors,\nand stores all these experiences in the memory; 2) After this learning stage, we evaluate JARVIS-1\n8\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\non various tasks. Therefore, JARVIS-1 is able to produce better plans with the memory teaming up\nwith the planning experiences. In our experiments, we use this as the default setting for all tasks.\nExploration using self-instruct. The key issue to the success of learning with memory is how\nto effectively acquire useful experiences given a limited amount of time. We propose to use self-\ninstruct (Wang et al., 2022) to generate the dynamic curriculum and guide JARVIS-1 to learn\nfrom the interactions with environments. In each round, we prompt the MLM to consider how\ncapable JARVIS-1 is at this point and subsequently select tasks from a task pool to explore. We find\nthat the curriculum almost follows the technical tree-growing direction. To accelerate the learning\nprocess, we augment the linear self-instruct to distributed learning in distributed environments with\nshared memory, i.e. speculative execution (Leviathan et al., 2023). Specifically, we generate multiple\nexecutable tasks as candidate task batches and provide them to agents with the same memory for\nverification and execution in various different environments. Meanwhile, experiences are collected\ninto a shared centralized memory. When all exploration tasks have been accomplished, we move to\nthe next round, until the memory reaches a certain capacity.\nLife-long learning. We’ve also observed that the aforementioned learning (where the memory is\nbeing filled) can be extended throughout the whole gameplay, where the agent gradually acquires\nmore and more skills. As the gameplay continues, more and more experiences are pouring in, therefore\nJARVIS-1 can find better references for challenging tasks like ObtainDiamondPickaxe, resulting\nin an improved success rate on these tasks. Further, there is no gradient update in this thanks to\nthe memory-augmented MLM, i.e. we can do in-context life-long learning. In Section 4.3, we offer\nexploratory experiments to show the potential of such capability of JARVIS-1.\n4. Experiments\nIn the experiments, our goal is to 1) evaluate the general performances of JARVIS-1 on the chal-\nlenging Minecraft tasks, especially on its advantages over baselines that do not (fully) address the\naforementioned issues in open-world agents; 2) understand the factors that contributes to the general\nresults; 3) explore the potential of JARVIS-1 in terms of life-long learning and its benefits to long-\nhorizon tasks. To this end, we will first briefly introduce the evaluation settings, then cover the main\ncomparative results and ablation studies, and conclude with an exploratory trial on long-horizon\ntasks.\n4.1. Experimental Setups\nWe evaluate JARVIS-1 in Minecraft, with tasks selected from the recently introduced Minecraft\nUniverse Benchmark (Lin et al., 2023a). For the reader’s convenience, we provide details on the basic\nsetups below.\nEnvironment setting. To ensure realistic gameplay, the agent needs to utilize observation\nand action spaces that are similar to those used by humans. Instead of manually designing a\ncustom interface for models to interact with the environment, as done in previous methods such as\nMineDojo(Fan et al., 2022), GITM(Zhu et al., 2023), and Voyager(Wang et al., 2023a), we opt for\nusing the native human interface provided by Minecraft. This applies to both the observation and\naction space. The model operates at a speed of 20 frames per second and is required to use a mouse\nand keyboard interface when interacting with human GUIs. For more information on the detailed\ndescriptions of the observation and action spaces, please refer to the Appendix.\nTask setting.\nIn Minecraft, players have access to thousands of items, each with specific\nacquisition requirements or recipes. For example, stone-type items can only be obtained using a\n9\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 1 | Characteristics of 11 task groups encompassing over 200 minecraft tasks.\nGroup\nTask\nNum.\nMax.\nSteps\nInitial\nInventory\nBiome\nLanguage Instruction\nWood\n34\n12k\nnull\nPlains\/Forest\nPick up a wooden_pickaxe.\nWood-Variants\n43\n12k\nnull\nSavanna\/Jungle\/Taiga\nPick up a acacia_boat.\nStone\n10\n12k\niron_axe\nPlains\/Forest\nCraft a furnace given an iron axe.\nIron\n22\n12k\niron_axe\nPlains\/Forest\nSmelt and craft an iron_door given an iron axe.\nGold\n9\n36k\niron_axe\nPlains\/Forest\nSmelt and craft an golden_axe given an iron axe.\nDiamond\n7\n36k\niron_axe\nPlains\/Forest\nDig down to mine diamond and craft diamond_pickaxe.\nRedstone\n7\n36k\niron_axe\nPlains\/Forest\nMine redstone and make dropper given an iron axe.\nBlocks\n15\n12-36k\niron_axe\nPlains\/Forest\nDig down to mine lapis_lazuli block.\nArmor\n17\n12-36k\niron_axe\nPlains\/Forest\nCraft diamond_boots given an iron axe and equip it.\nDecoration\n17\n12k\niron_axe\nFlower Forest\nObtain the bed and dye it red.\nFood\n9\n12k\niron_axe\nPlains\nKill sheep to obtain mutton and cook it.\npickaxe, and two planks can be crafted into four sticks (these requirements are available on the\nMinecraft Wiki1). In survival mode, players must obtain each type of item from the environment or\ncraft\/smelt the object item from materials. We choose over 200 tasks from the Minecraft Universe\nBenchmark (Lin et al., 2023a) for evaluation. These tasks are related to items that can be obtained in\nthe Minecraft overworld. For the convenience of statistics, we have classified them into 11 groups\naccording to recommended categories in Minecraft2 (see Table1). Due to the varying complexity of\nthese tasks, we adopt different maximum gameplay durations (Max. Steps) for each task. The limit is\ndetermined by the average time the human players need to accomplish the corresponding task. Other\ndetails about each task, such as language instruction, maximum steps, evaluation times, biome, and\ninitial inventory when the agent is born into the world can be found in Appendix Table 5-14.\nEvaluation metrics. By default, the agent always starts in survival mode, with an empty inventory.\nA task is considered a success when the target object is obtained within a specified time. Due to the\nopen-world nature of Minecraft, the world and initial position that the agent is spawned at could\nvary a lot. Therefore, we conducted at least 30 tests for each task using different seeds and reported\nthe average success rate to ensure a thorough assessment. Further, since we categorize the tasks into\ngroups, we also report mean and variance values for each group for ease of presentation.\n4.2. Main Results\nWe compare JARVIS-1 with other multi-task instruction-following agents based on LLM, including\nInstruct GPT (Huang et al., 2022a; Ouyang et al., 2022), ReAct (Yao et al., 2022), Inner Mono-\nlogue (Huang et al., 2022b), DEPS (Wang et al., 2023b). Since some methods are not originally\nexperimented in Minecraft, we reproduce them to conform to the Minecraft specification based on\nprompt and feedback template design. All LLM-based methods access the LLM model through OpenAI\nAPI. And all hyper-parameters of LLM including temperature are kept as default.\nThe average success rates for every task group are listed in Table 2. JARVIS-1 achieves the\nbest performance with all meta tasks. It is important to note that in Minecraft, the technology tree\ncan be formed by Group Wood, Stone, Iron, Gold, and Diamond. The tasks become increasingly\ndifficult as you progress through the tree. For more difficult tasks such as obtaining a gold ingot or\na diamond, the agents typically need to perform more actions and longer goal sequences in order\nto complete the task. As a result, the success rate of all agents decreases as the difficulty level\nincreases. It is evident that reasoning methods (ReAct (Yao et al., 2022) vs. GPT (Huang et al.,\n2022a; Ouyang et al., 2022)) and interactive re-planning with feedback (Inner Monologue(Huang\net al., 2022b) vs. GPT) effectively enhance the agent’s task performance in an open world. However,\n1https:\/\/minecraft.fandom.com\/wiki\/Minecraft_Wiki\n2https:\/\/minecraft.fandom.com\/wiki\/Tutorials\/Organization#Categories\n10\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 2 | Results of JARVIS-1 and baselines on Minecraft. The detailed task instructions, settings and results\ncan be found in the Appendix.\nGroup\nTask\nGPT\nReAct\nInner Monologue\nDEPS\nJARVIS-1\n26.67\n45.00\n36.67\n75.00\n91.55\nWood\nAVG\n27.30±14.86\n40.31±13.30\n60.15±19.41\n80.23±17.32\n88.84±16.82\n6.67\n36.67\n30.00\n36.67\n60.47\nWood\nVar\nAVG\n24.39±11.08\n38.13±12.81\n53.39±12.86\n68.75±12.32\n76.78±12.27\n20.00\n20.00\n66.67\n75.00\n94.20\nStone\nAVG\n20.21±12.32\n39.00±12.15\n52.86±16.90\n69.27±7.78\n88.69±4.87\n0.00\n0.00\n3.33\n20.00\n33.82\n3.33\n6.67\n0.00\n20.00\n38.10\nIron\nAVG\n3.27±2.85\n4.61±3.63\n5.20±5.17\n16.92±4.69\n34.63±10.61\n0.00\n2.00\n2.00\n6.00\n14.49\nGold\nAVG\n0.00±0.00\n0.45±0.60\n0.59±0.64\n2.20±1.55\n6.85±4.71\n0.00\n0.00\n1.00\n2.00\n9.20\n0.00\n0.00\n0.00\n2.50\n6.22\nDiamond\nAVG\n0.00±0.00\n0.35±0.48\n0.96±0.67\n2.42±1.01\n8.99±2.68\n0.00\n2.00\n0.00\n10.00\n22.78\nRedstone\nAVG\n1.04±1.30\n1.14±1.18\n0.69±1.68\n6.02±3.61\n17.51±9.34\n16.67\n33.33\n43.33\n53.33\n86.67\nBlocks\nAVG\n45.64±33.88\n49.35±30.51\n55.71±29.43\n58.02±27.68\n80.34±21.09\n6.67\n0.00\n10.00\n10.00\n30.30\nArmor\nAVG\n1.36±2.25\n0.50±0.88\n3.10±4.71\n3.71±3.78\n13.44±14.62\n15.00\n15.00\n15.00\n25.00\n50.00\nDecoration\nAVG\n17.12±11.59\n17.13±9.19\n12.03±10.19\n29.59±15.94\n46.67±23.39\n13.33\n16.67\n25.00\n16.67\n43.55\nFood\nAVG\n9.40±4.29\n15.56±6.83\n20.78±11.99\n22.85±8.15\n46.75±11.16\nthese approaches still face challenges when dealing with long-horizon tasks, specifically in the Iron\nand Diamond group. DEPS(Wang et al., 2023b), on the other hand, enables agents to accomplish\ndiamond-related tasks through interactive long-horizon planning accompanied by descriptions and\nexplanations. Nevertheless, its reliability remains very low at approximately 2.5%.\nIn comparison to DEPS(Wang et al., 2023b) without memory, JARVIS-1 demonstrates superior\nperformance even in challenging tasks due to its extensive experience. In diamond-related tasks\nspecifically, the success rate has increased by nearly 3 times (8.99% vs 2.42%). And JARVIS-1\nusually only requires 2-3 rounds of re-planning to generate the correct executable plan, whereas\nDEPS requires more than 6 rounds. This means that JARVIS-1 saves a significant amount of LLM\ntokens and thinking time, enabling more efficient plan execution and providing additional steps and\ntokens for handling uncertainty in the environment.\nBased on our observations, we have found that the bottleneck for JARVIS-1 in tasks involving\ndiamonds often lies with the Controller’s inability to perfectly execute short-horizon text instructions\ngenerated by LLM. Therefore, it is worth exploring methods for generating plans that are easier for\nthe controller to execute or improving the controller’s ability to follow instructions.\n11\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n97%\n96%\n94%\n34%\n9%\n95%\n95%\n90%\n30%\n5%\n55%\n50%\n35%\n5%\n0\n85%\n85%\n75%\n25%\n5%\n0%\n20%\n40%\n60%\n80%\n100%\nCrafting Table\nWooden Pickaxe\nStone Pickaxe\nIron Pickaxe\nDiamond\nGPT4\nChatGPT\nLLaMA2 Pre-Trained\nLLaMA2 Fine-tuned\nFigure 6 | Success rates for different language models on Minecraft tasks. We found open-sourced LLaMA2-\n70B modelsTouvron et al. (2023) lack knowledge related to Minecraft, so the pre-trained model performs\npoorly. We further finetuned the LLaMA2-13B model on a Minecraft text dataset collected from the internet,\nand it shows performance similar to ChatGPT on Minecraft.\n4.2.1. JARVIS-1 based on different LMs\nWe conducted ablation experiments on various Language Models, including OpenAI’s ChatGPT Ouyang\net al. (2022) and GPT-4 OpenAI (2023). Among these models, GPT-4 has more parameters and has\nbeen proven to outperform ChatGPT in extensive research Wang et al. (2023a). We also select the\nopen-source pre-trained LLaMA2 70B model Touvron et al. (2023). Additionally, we gathered a\nsubstantial amount of Minecraft-related text from the internet as training data and further fine-tuned\nLLaMA2 13B. The experiments were conducted on a subset of Minecraft tasks using different language\nmodels. Each JARVIS-1 learns for 4 epochs of interaction with all task sets and evaluates on task\nsubset across at least 20 seeds. The experimental results are presented in Fig. 6.\nTable 6 demonstrates that ChatGPT, despite having fewer parameters, achieves nearly identical\nsuccess rates as GPT-4. This suggests that language models equipped with memory can significantly\nenhance planning abilities. In Minecraft-related tasks, the open-source pre-trained LLaMA2 70B\nexhibits a notable performance gap compared to OpenAI models, particularly in long-horizon tasks.\nHowever, by finetuning LLaMA2 with fewer parameters, its performance on Minecraft tasks improves\nsubstantially. This indicates that the open-source model lacks knowledge specific to Minecraft and\nrequires further finetuning for the successful completion of such tasks.\n4.2.2. Ablation on Memory\nWe also conduct ablation experiments on the multimodality memory and retrieval methods. We set\nJARVIS-1 w\/o memory module as the baseline agent. We first evaluate JARVIS-1’s performance\nwith different memory sizes (representing different learning stages) as shown in Fig. 7, which demon-\n12\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nFigure 7 | Success rate by memory size for different items. We evaluated the performance of JARVIS-1 at\ndifferent memory sizes (representing different learning stages) by measuring the success rate (% Episodes) of\ncompleting key items on the Minecraft technology tree. As the learning progressed, we observed an improvement\nin completion rates for all items, with an increasing number of successful trajectories being included in memory.\nAfter 4 epochs of learning, JARVIS-1 had accumulated a total of 425 successful trajectories in its memory.\n85%\n20%\n10%\n0%\n10%\n95%\n20%\n30%\n5%\n20%\n94%\n34%\n40%\n9%\n24%\n0%\n20%\n40%\n60%\n80%\n100%\nStone Pickaxe\nIron Pickaxe\nShield\nDiamond\nRedstone Block\nText Memory\nText Memory + Reasoning\nMultimodal Memory + Reasoning               baseline (no memory)\nFigure 8 | Success rates for different retrieval methods with memory on Minecraft tasks. JARVIS-1, which\nsynergizes reasoning and retrieval with multimodal memory, achieves the best.\nstrates the effectiveness of self-improving within JARVIS-1. We further conduct the experiments on\na subset of Minecraft tasks using three different retrieval methods: retrieval with textual instruction\nembedding only (Text Memory), synergizing reasoning and retrieval with text embedding (Text Mem-\nory+Reasoning), and synergizing reasoning and retrieval with multimodality embedding (Multimodal\nMemory+Reasoning). Except for the memory and retrieval methods, all others are kept the same.\nThe results are listed in Fig. 8.\nThe experiments show that reasoning before retrieval can effectively improve retrieval accuracy.\nRetrieval based on a multimodal state including vision observation and symbolic information (e.g.,\ninventory, location, etc) is better than only considering the text embedding.\n4.3. Long-Horizon Challenges\nMost concurrent multi-task agents in Minecraft can only handle short-term tasks and struggle with long-\nhorizon tasks like CraftingDiamondPickaxe. The VPT foundation model(Baker et al., 2022) is\ncapable of accomplishing various tasks in Minecraft but lacks the ability to execute human instructions.\nTo address this limitation, Reinforcement Learning is required to fine-tune the VPT foundation model\nfor specific task completion. However, after fine-tuning, VPT may experience a decline in performance\n13\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n12.5%\n7.2*104\n7.2*104\nFigure 9 | (Left) The success rate of different models in the ObtainDiamondPickaxe challenge over gameplay\ntime. VPT RL is finetuned from VPT early game with reinforcement learning over 1.4 million episodes. JARVIS-\n1 agent and its varients have interacted with Minecraft with over 4 epochs on all tasks in task pool. Typically, it\ntakes a skilled person over 20 minutes (24,000 steps) to obtain a diamond pickaxe. (Right) The success rate of\nobtaining important intermediate items during the process of synthesizing a diamond pickaxe of JARVIS-1.\nThis task has been evaluated over 300 times on different seeds. These curves indicate that as the game\nprogresses, the success rates of obtaining all intermediate items are increasing, which indicates that JARVIS-1\nis constantly improving its skills.\nfor other tasks while focusing on the specified task. In contrast, Steve-1(Lifshitz et al., 2023) has\nimplemented goal-conditioned fine-tuning on VPT, enabling it to follow human text instructions while\nmaintaining multitasking capabilities. However, Steve-1 primarily focuses on low-level tasks like\nobtaining dirt, collecting flowers, and chopping trees. When it comes to long-horizon tasks such as\nstarting from scratch by obtaining a wooden pickaxe, Steve-1 still encounters difficulties.\nDEPS(Wang et al., 2023b) also utilizes LLM as a planner, but it lacks the ability to learn from\nexperience in different tasks and apply that knowledge to new ones. Additionally, DEPS is limited in\nits re-planning rounds due to the LM’s context constraints. The experiments reveal that DEPS has a\nsuccess rate of less than 50% in generating accurate and executable plans for acquiring diamonds.\nThe probability of DEPS successfully obtaining diamonds in the environment is approximately 0.59%.\nConsequently, DEPS continues to face challenges when attempting to finish long-horizon tasks within\nthe Minecraft world.\nEven human players who have mastered the distribution pattern of diamonds achieve success\nrates of obtaining diamonds and crafting a diamond pickaxe (which requires at least three diamonds)\nwithin 10 minutes at approximately 15% and 12%, respectively. JARVIS-1 performs better in the\nObtainDiamondPickaxe challenge. Compared to the state-of-the-art model, which has undergone\nRL-finetuned VPT, JARVIS-1 has more than doubled the success rate of obtaining a diamond pickaxe\n(6.22% vs 2.5% within 20 minutes).\nTo increase the chances of obtaining diamonds, we extended the game-playing time to 60 minutes\n(72000 game-playing steps, as shown in Figure 9). As a result, JARVIS-1’s success rate in acquiring\na diamond pickaxe improved from 6.2% to 12.5%. The graph on the right side of Figure 7 illustrates\nhow the success rate of intermediate milestone items changes over time, indicating that JARVIS-1\ntends to improve with longer game-playing time. We also conduct two variants of JARVIS-1 with\ndifferent self-improving curricula: human-written and random-generated. All three JARVIS-1 have\ncollected experiences into memory with the curriculum for 4 epochs before evaluation in 60 minutes.\nThe results show that JARVIS-1 with a GPT-generated curriculum can finish the task within the\nshortest game-playing steps and achieve the best performance in 60 minutes.\nIn contrast, VPT’s success rate barely changed when we increased the time from 20 minutes\nto 60 minutes (from 2.5% to 3%). This can be attributed to Minecraft’s durability system where\n14\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nprolonged underground exploration often leads to pickaxe damage. When JARVIS-1’s pickaxe\nbreaks, it dynamically re-plans based on its current inventory and crafts a new one. However, VPT-RL\nexhibits perplexing behaviors at this stage by using inappropriate tools for mining stones or crafting\nunnecessary items. This comparison demonstrates that JARVIS-1 possesses superior generalization\nand planning abilities for long-horizon tasks.\nNote that our method is designed to be multi-task in its nature and not finetuned through imitation\nlearning on specific datasets or reinforcement learning.\n5. Related Works\n5.1. Planning with LLM\nThere have been some methods leveraging the large language model to generate action plans for\nhigh-level tasks in embodied environments (Dasgupta et al., 2022; Gong et al., 2023b; Liu et al., 2023;\nMai et al., 2023; Zeng et al., 2022; Zhang et al., 2023; Zhang and Lu, 2023). Huang et al. (2022a)\ndecompose natural language commands into sequences of executable actions by text completion\nand semantic translation, while SayCan generates feasible plans for robots by jointly decoding an\nLLM weighted by skill affordances from value functions (Brohan et al., 2022b). Some methods\nalso leverage the LLM to produce the program code as plan for better executation (Liang et al.,\n2022; Lin et al., 2023b; Singh et al., 2022). However, the above methods assume that the initial\nplan from the LLM is correct. When there are bugs in the initial plan, it’s difficult for the agent\nto finish the task successfully. Recent research frequently employs LLM as an interactive planner,\nharnessing its self-updating capabilities to enhance the plan’s executability over time (Shinn et al.,\n2023; Sun et al., 2023; Wang et al., 2023b). Inner Monologue (Huang et al., 2022b) pilots the front\nof interactive planning with LLMs, which introduces the feedback (including success detection and\nscene description) to the planner. However, we found it could still suffer from accumulative planning\nerrors, especially in long-horizon open-world tasks. ReAct (Yao et al., 2022) will reason about the\nagent state before acting, which indicates that various reasoning methods (Wei et al., 2022; Wu et al.,\n2023; Yao et al., 2023) are benefitial for planning. LLM-based planning methods often use the fixed\npretrained LLM as the agent, while we focus more on life-long and continual learning for agents in\nopen-world environments (Ke et al., 2022a,b; Wang et al., 2023a). For better leveraging historical\ninteraction between agent and environments, an explicit memory (Park et al., 2023; Zhu et al., 2023)\nfor more historical chatting has been leveraged for bigger storage of agent experiences. However,\nthe above methods usually rely only on a text-based environment and struggle to execute plans in\npartial-observed visual open-world environments.\n5.2. Minecraft Agents\nDeveloping generally capable agents in Minecraft to solve open-world tasks has gained increasing\ninterests (Baker et al., 2022; Cai et al., 2023a,b; Ding et al., 2023; Fan et al., 2022; Yuan et al.,\n2023; Zhang and Lu, 2023; Zhu et al., 2023). As an early attempt, Oh et al. (2017) studied task\ngeneralization in a simple Minecraft environment variant. It designed a two-stage pipeline, first\nmastering the prerequisite skills with parameterization trick, and then learning a meta controller\nto execute the instructions. Moving to solve complex long-horizon tasks in Minecraft, works (Lin\net al., 2021; Mao et al., 2022; Oh et al., 2017) explored the hierarchical architecture. In recent years,\ninfluenced by the trend of large-scale pre-training paradigms, a group of researchers have emerged,\nwho are utilizing vast amounts of internet knowledge to train intelligent agents. Fan et al. (2022)\ntrained a visual-semantic alignment model, MineCLIP, using the correspondences between subtitles\nand video snippets available on YouTube, and used it to generate intrinsic rewards to guide policy\n15\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nlearning. (Baker et al., 2022) utilizes a pre-trained inverse dynamics model to label actions in YouTube\nvideos which are used to learn a foundation policy VPT through imitation learning. By bridging\nMineCLIP and VPT, Lifshitz et al. (2023) creates a performant instruction-following policy Steve-1 to\nsolve open-world short-horizon tasks using hindsight relabeling and unCLIP tricks. However, Steve-1\ncan not solve complicated process-oriented tasks due to the expressive capability of its goal space.\nCai et al. (2023b) learns to follow reference videos as the instruction by merely watching gameplay\nvideos, which improves the capacity of goal space and reduces the cost of policy training. All of\nthese methods focus on improving the smoothness and robustness of interaction between policy\nand environment. Inspired by the powerful language understanding and reasoning capabilities of\nlarge language models, researchers have begun to build Minecraft agents based on LLMs. Wang\net al. (2023a) used LLM to guide the agent to explore the Minecraft world by acquiring diverse skills,\nmaking novel discoveries, and generating goal proposals. Zhu et al. (2023) integrated LLM with\ntext-based knowledge and memory to equip the agent with common sense and past experiences for\nhigher reasoning efficiency. Yuan et al. (2023) used LLM to guide the agent to explore the Minecraft\nworld and interact with the environment with reinforcement learning control policies.\n6. Conclusion\nWe propose a multi-task agent JARVIS-1 designed for the complex environment of Minecraft, which\nmarks a significant advancement in achieving human-like planning within an open-world setting.\nBy leveraging pre-trained Multi-modal Language Models, JARVIS-1 not only effectively interprets\nmultimodal inputs but also adeptly translates them into actions. Its integration of a multimodal\nmemory, which draws from both ingrained knowledge and real-time game experiences, enhances\nits decision-making capabilities. The empirical evidence of its prowess is evident in its impressive\nperformance across a wide array of tasks in Minecraft. Notably, its achievement in the long-horizon\ndiamond pickaxe task, where it achieved a completion rate that surpasses VPT by up to five times,\nunderscores its potential and the strides made in this domain. This breakthrough sets the stage for\nthe future of more versatile and adaptable agents in complex virtual environments.\nAcknowledgments\nThis work is funded in part by the National Key R&D Program of China #2022ZD0160301, a grant\nfrom CCF-Tencent Rhino-Bird Open Research Fund, NSF grants #IIS-1943641, #IIS-1956441, #CCF-\n1837129, an SRA from Meta and a research gift from Amazon Alexa AI, and a gift from RelationalAI.\nThe authors sincerely thank Dr. Rita Zhang, Zhixiang Dai at NVIDIA for the valuable technical support\nof GPU computing.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nJARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models\n```\n#### 2. 论文摘要\n```\nAchieving human-like planning and control with multimodal observations in an\nopen world is a key milestone for more functional generalist agents. Existing\napproaches can handle certain long-horizon tasks in an open world. However,\nthey still struggle when the number of open-world tasks could potentially be\ninfinite and lack the capability to progressively enhance task completion as\ngame time progresses. We introduce JARVIS-1, an open-world agent that can\nperceive multimodal input (visual observations and human instructions),\ngenerate sophisticated plans, and perform embodied control, all within the\npopular yet challenging open-world Minecraft universe. Specifically, we develop\nJARVIS-1 on top of pre-trained multimodal language models, which map visual\nobservations and textual instructions to plans. The plans will be ultimately\ndispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a\nmultimodal memory, which facilitates planning using both pre-trained knowledge\nand its actual game survival experiences. JARVIS-1 is the existing most general\nagent in Minecraft, capable of completing over 200 different tasks using\ncontrol and observation space similar to humans. These tasks range from\nshort-horizon tasks, e.g., \"chopping trees\" to long-horizon tasks, e.g.,\n\"obtaining a diamond pickaxe\". JARVIS-1 performs exceptionally well in\nshort-horizon tasks, achieving nearly perfect performance. In the classic\nlong-term task of $\\texttt{ObtainDiamondPickaxe}$, JARVIS-1 surpasses the\nreliability of current state-of-the-art agents by 5 times and can successfully\ncomplete longer-horizon and more challenging tasks. The project page is\navailable at https:\/\/craftjarvis.org\/JARVIS-1\n```\n\n#### 3. 论文全文\n```\nJARVIS-1: Open-world Multi-task Agents with\nMemory-Augmented Multimodal Language\nModels\nZihao Wang1, Shaofei Cai1, Anji Liu2, Yonggang Jin3, Jinbing Hou3, Bowei Zhang1, Haowei Lin1,\nZhaofeng He3, Zilong Zheng4, Yaodong Yang1, Xiaojian Ma4 and Yitao Liang1\n1PKU, 2UCLA, 3BUPT, 4BIGAI, All authors are affiliated with Team CraftJarvis,\nAchieving human-like planning and control with multimodal observations in an open world is a key\nmilestone for more functional generalist agents. Existing approaches can handle certain long-horizon\ntasks in an open world. However, they still struggle when the number of open-world tasks could\npotentially be infinite and lack the capability to progressively enhance task completion as game time\nprogresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual\nobservations and human instructions), generate sophisticated plans, and perform embodied control, all\nwithin the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on\ntop of pre-trained multimodal language models, which map visual observations and textual instructions\nto plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-\n1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its\nactual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable\nof completing over 200 different tasks using control and observation space similar to humans. These\ntasks range from short-horizon tasks, e.g., \"chopping trees\" to long-horizon tasks, e.g., \"obtaining a\ndiamond pickaxe\". JARVIS-1 performs exceptionally well in short-horizon tasks, achieving nearly\nperfect performance. In the classic long-term task of ObtainDiamondPickaxe, JARVIS-1 surpasses\nthe reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon\nand more challenging tasks. The project page is available at craftjarvis.org\/JARVIS-1.\n1. Introduction\nCreating sophisticated agents that can accomplish myriad of tasks in complex domains remains a\npivotal milestone towards generally capable artificial intelligence (Alayrac et al., 2022; Brohan et al.,\n2022a; Brown et al., 2020; Reed et al., 2022; Zhao et al., 2023). Recent advancements have shown a\ntrend towards employing a hierarchical goal execution architecture (Huang et al., 2022a,b; Wang et al.,\n2023b), and leveraging large language models (LLMs) as the high-level planner to generate action\nplans that will be ultimately executed by low-level instruction-following controllers. Albeit the fruitful\nprogress they have yielded in many robotics (Huang et al., 2022b) and even open-world environments\nlike Minecraft (Fan et al., 2022; Guss et al., 2019b), today’s agents built with these approaches are\nstill struggling with three major issues: 1) perceive the world from multimodal sensory observations,\nsuch as images, videos in addition to natural language instructions and feedback for planning; This is\nmostly due to the inability of LLM-based planners on processing multimodal data (Huang et al., 2022a;\nYao et al., 2022); 2) perform consistent and accurate long-term planning. This requires multi-round,\nknowledge, and reasoning-intensive dialogues, which remain great challenges to LLMs (Huang et al.,\n2022b); 3) learn and evolve in a life-long fashion. This calls out the need for agents to propose\ntheir own tasks and self-improve. Addressing these issues will unleash the full planning potential of\nLLM-based agents, and expedite the development of more generalist agents.\nIn this work, we introduce JARVIS-1, a brand new agent that can robustly produce plans for\nCorresponding author(s): Xiaojian Ma, Yitao Liang\nZihao Wang<zhwang@stu.pku.edu.cn>, Shaofei Cai<caishaofei@stu.pku.edu.cn>, Anji Liu<liuanji@cs.ucla.edu>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2311.05997v3  [cs.AI]  30 Nov 2023\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n.97\n.97\n.95\n.57\n.96\n.94\n.90\n.92\n.79\n.97\n.75\n.91\n.87\n.91\n.95\n.25\n.25\n.11\n.10\n.04\n.11\n.11\n.04\n.02\n.02\n.05\n.40\n.33\n.32\n.33\n.34\n.09\n.06\n.55\n.05\n.33\n.38\n.30\n.37\n.32\n.36\n.28\n.36\n.28\n.32\n.36\n.38\n.37\n.45\n.50\n.32\n.50\n.31\n.06\n.14\n.05\n.26\n.08\n.18\n.24\n.22\n.23\n.02\n.05\n.05\n.08\n.06\n.02\n.04\n.02\n.05\n.02\n.02\n.14\n.88\n.79\n.84\n.94\n.87\n.90\n.97\n.67\n.90\n.89\n.93\n.92\nFigure 1 | How does JARVIS-1 unlock the technology tree of the Minecraft universe. JARVIS-1 can\nconsistently obtain high-level items on the main tech-tree of the overworld in Minecraft, such as diamond,\nredstone, and golden items, which require collecting over 10 different intermediate items. JARVIS-1 not\nonly outperforms the previous state-of-the-art VPT (Baker et al., 2022) (6% vs. 2.5% reliability) on diamond\npickaxe, but also can craft almost all diamond items in the overworld including diamond chestplate.\nlong-horizon tasks from multimodal user and environment inputs, and translate them into motor\ncontrol in Minecraft, a popular yet challenging open-world testbed for generalist agents. To be specific,\nwe chain a multimodal foundation model MineCLIP(Fan et al., 2022) and an LLM(Brown et al., 2020)\ntogether, the resulting multimodal language model (MLM) allows our agent to better understand the\ntask, situations, and environmental feedback. To further enhance the correctness and consistency\nof planning, especially on long-horizon tasks, we propose to augment the agent with a multimodal\nmemory, which stores both the scenarios and actual plans of the successful planning experiences in\nthe past. By retrieving the relevant memory entries, the planning skill of our MLM-based agent can be\nstrengthened from the agent’s own interactions with the environment in an in-context manner. Finally,\nJARVIS-1 is able to evolve throughout the gameplay by proposing tasks on its own (i.e. self-instruct)\nas a means of exploration and saving the obtained experiences in the multimodal memory, therefore\nfacilitating better reasoning and planning. This self-improving ability sparks its potential for a higher\nlevel of autonomy.\nOur main evaluations are conducted in Minecraft, with more than 200 tasks selected from the\nMinecraft Universe Benchmark (Lin et al., 2023a), with no demonstration provided. The tasks cover\na broad spectrum from the early game (e.g. ObtainCraftingTable) to intermediate and even\nchallenging long-horizon tasks (e.g. ObtainDiamondPickaxe). A glimpse of what JARVIS-1 is\nable to achieve can be found in Figure 1. JARVIS-1 exhibits strong performances on these tasks,\nrepresenting an up to 5× increase to the previous records. Our ablative analysis then offers a detailed\naccount of how JARVIS-1 approaches this significant progress and becomes the first agent that can\nrobustly obtain the diamond pickaxe with up to 12.5% success rate. What is even more surprising\nis that, without the need for additional training, JARVIS-1 demonstrates a continuous increase in\nperformance as game time increases in long-horizon tasks. Moreover, JARVIS-1 has demonstrated\nits potential of self-improve in an exploratory life-long learning experiment, where it needs to propose\ntasks to progressively explore the world, collect experiences, and sharpen its planning skill using\nthese experiences stored in the multimodal memory.\n2\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nIn summary, JARVIS-1 pilots the effort towards a human-like multi-task and autonomous agent in\nan open-world, embodied environment like Minecraft. We would like to share the key takeaways of\nwhat we have learned during its development as follows:\n• From LLMs to MLMs. The capability of perceiving multimodal sensory input is critical to\nplanning in a dynamic and open-world world. JARVIS-1 enables this by chaining a multimodal\nfoundation model together with an LLM. Compared to LLM “blindly” produces plans, MLM is able\nto natively understand the current situation and plan accordingly. Further, rich environmental\nfeedback can be obtained through multimodal perception, therefore helping the self-check and\nself-explain of the planner spot and fix possible bugs in the plans, enabling stronger interactive\nplanning.\n• Multimodal memory. Early research has suggested the crucial role that memory mechanisms\ncan serve in the functioning of generalist agents. By outfitting JARVIS-1 with a multimodal\nmemory, we effectively allow it to plan with both pretrained knowledge and its actual experiences\nin the world, therefore bringing significant improvement to planning correctness and consistency.\nCompared to canonical RL or planning agents with exploration, no additional model update is\nneeded as the MLM in JARVIS-1 makes it possible to leverage these experiences in an in-context\nmanner.\n• Self-instruct and self-improve. A sign of generalist agents is the capacity to proactively\nacquire new experiences and continuously improve themselves. We have demonstrated how\nJARVIS-1 effectively traverses the environment by executing tasks autonomously generated\nthrough its self-instruct mechanism. With multimodal memory teaming up with experiences\nfrom the explorations, we have observed consistent improvement, especially in accomplishing\nmore complicated tasks. Ultimately, this aspect of autonomous learning in JARVIS-1 signifies\nan evolutionary step towards generalist agents that can learn, adapt, and improve over time\nwith minimal external intervention.\n2. Challenges for Open-world Agents\nCompared to canonical scenarios with relatively small scale, simple dynamics, and limited tasks,\nopen-world environments impose substantial challenges to building agents that can accomplish a\ndiverse set of tasks (Cai et al., 2023a,b; Fan et al., 2022; Guss et al., 2019a, 2021; Kanervisto et al.,\n2022; Wang et al., 2023b). In this section, we will review three major challenges we’ve identified\nduring the development of JARVIS-1.\n2.1. Challenge I: Situation-Aware Planning\nIn an open world, there could be various possible paths towards an open-world goal. However, not all\nof them are plausible or equally efficient given a certain situation (location, inventory status, etc.).\nFor example, building a bed\ncan be done through collecting wool from sheeps\n, haunting spiders\nfor strings\n, or trading with villagers\n. Depending on the current location and its proximity to\nthese subjects, some options can be more viable and more efficient than others. Further, the agent’s\nown situation can also change throughout the episode, e.g. day and night shifts, weather conditions\n(bringing different types of danger), and tool usage (it can be broken). To this end, the plan needs to\nbe constantly updated based on the current situation. Figure 2 (left) shows that when attempting the\n\"ObtainDiamondPickaxe\" task with a GPT-based planner that produces plans only at the beginning\nwithout looking at the current situation, the agent failed to complete the task as opposed to human\nplayers and JARVIS-1, which perform situation-aware planning from time to time. We’ve observed\n3\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n（a）\n（b）\n（c）\nwood\nstone\niron\ndiamond\n60\nmin\n10\nmin\n60\nmin\n9.59x\n0à8.99\niron\ndiamond\n3.39x\nFigure 2 | Challenges in open-world environments and how does JARVIS-1 tackle them.\n(Left)\nWith situation-aware planning,\nJARVIS-1 substantially improves the success rate on the challeng-\ning ObtainDiamond task, compared to the baseline (GPT) without it.\nNote: Due to resource con-\nstraints, we can only provide human results of 10-min gameplay; (Middle) As task complexity increases\n(STONE→IRON→DIAMOND), JARVIS-1 exhibits more significant advantages thanks to interactive planning;\n(Right) Success rate gain (indicated by the color depth) on selected tasks (x-axis) given in-context experiences\non other tasks (y-axis) retrieved from the multimodal memory. With life-long learning and memory, JARVIS-1\ncan utilize prior experiences on relevant tasks for better planning.\nthat many failures coming from this were attributed to the agent’s inability to adapt to the changing\nsituations including entering a new biome, the tool being used becoming broken, etc.\n2.2. Challenge II: Task Complexity\nThe second challenge comes from the higher task complexity in open-world environments. Due\nto the richness of terrains, objects, and action space, tasks in open-world domains usually require\nsubstantially long planning horizons as well as good accuracy and precision. For example, the task\nObtainEnchantingTable\nincludes more than 20 different sub-goals and therefore demands\nsignificantly longer reasoning steps. Meanwhile, many of these sub-goals have to be achieved pre-\ncisely with the exact object name, quantities, and preconditions, e.g., mine 3 obsidian with\ndiamond pickaxe, craft 1 diamond pickaxe from 3 diamonds and 2 sticks; oth-\nerwise, the subsequent sub-goals won’t be executed due to unfulfilled preconditions. To tackle this, we\nmay refer to some approaches in LLM reasoning, e.g. self-debugging (Chen et al., 2023) and turning\nthe planning into an interactive fashion. In Figure 2 (Middle), we’ve shown that as the complexity of\nthe task increases, our JARVIS-1, which uses interactive planning (Wang et al., 2023b) to mitigate\nthe aforementioned issues (details can be found in subsection 3.2), elicits more significant advantages\nover the baseline (GPT) planner.\n2.3. Challenge III: Life-long Learning\nFinally, being open world often implies offering an infinite number of tasks. Clearly, it is difficult\nfor an agent to master all tasks or generalize to arbitrary tasks without additional learning. To this\nend, agents in an open world should be able to learn novel tasks while completing existing tasks,\ni.e. life-long learning. Furthermore, as many open-world agents employ large models (Wang et al.,\n2023a,b; Yuan et al., 2023; Zhu et al., 2023), canonical gradient-based learning could be extremely\ninefficient given the number of new tasks and experiences to learn. Our MLM-based JARVIS-1 tackles\nthis by adopting a memory to save all the experiences on past tasks. By retrieving memory entries\nrelevant to the newly-coming task and putting them into the context as a reference, JARVIS-1 is able\nto accumulate more experiences as the game continues and strengthen its own planning skills without\ngradient update. As illustrated in Figure 2 (Right), for instance, both ObtainDiamondPickaxe\n4\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nMulti-Modality \nMemory\nMemory-Augmented\nMulti-modal\nLanguage Model\nController\nEnvironment\n(a) JARVIS-1 architecture\n(b) Self-Improving\n<task> \nPool\nSelf-\ninstruct\nShared\nMulti-Modality \nMemory\nDistributed\nJARVIS-1\nEnv\nInstances\n<act>\nkeyboard\n& mouse\n<task>\n<obs>\n<plan>\nlanguage\nvision\nQuery Gen\n(MLM)\nreference\n<plan>\nPlanner\n(MLM)\n<plan>\ncontext\nretrieve\n<obs,task>\nvision & language\n<task>\nbatch\nsave\nFigure 3 | Architecture of JARVIS-1 and its self-improving mechanism. (a) JARVIS-1 comprises a memory-\naugmented multimodal language model (MLM) that produces plans and a low-level action controller. JARVIS-1\nalso utilizes a multimodal memory to store and obtain experiences as references for planning. (b) JARVIS-1\ncan strengthen its own planning skills through exploration with its own proposed tasks (self-instruct) and a\ngrowing memory that helps with better planning on tasks that has been (partially) visited before.\nand ObtainDiamondAxe\nrequire gathering almost identical materials. Therefore, they can help\neach other by using the experiences from the other task. Compared to completing these challenging\ntasks without any prior experiences, memory-based in-context life-long learning in JARVIS-1 can\nbring significant advantages.\n3. Multi-task Agent with Memory-Augmented MLM\nThis section details the architecture of the proposed JARVIS-1 agent. We begin with an overview of\nthe modular agent design in subsection 3.1. Next, we elaborate on how to implement an interactive\nplanning scheme with a multimodal language model, which helps with more accurate plans, especially\non complex and long-horizon tasks in subsection 3.2. Finally, we show how to augment this planning\nframework with a multimodal memory to allow JARVIS-1 to strengthen its planning skill throughout\nthe episode by in-context life-long learning in subsection 3.3 and subsection 3.4.\n3.1. Overview\nWe aim to develop an agent capable of solving long-horizon instruction-following tasks using image\nobservations and human-aligned actions. To accomplish this, we propose a multi-modal agent\nincluding an interactive planner, a goal-conditioned controller, and a multimodal memory of\nmultimodal experiences. Upon receiving a task and the current observation, JARVIS-1 first utilizes\nthe MLM to generate a multimodal query (query gen) that retrieves relevant planning experiences\nfrom the memory. These experiences will then be used along with the planning instruction to prompt\nthe MLM-based planner. Leveraging its own pretrained knowledge as well as the retrieved reference\nplans, the planner will ultimately produce a series of 𝐾short-horizon goals 𝑔1, . . . , 𝑔𝐾to be executed\nby the controller. Once the plan is successfully executed, it will be stored in the memory along with\nthe task and the agent situation when it was planned. We also empower JARVIS-1 with life-long\n5\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nPlanner\n(MLM)\nSelf-Check\n(MLM)\nSelf-Explain\n(MLM)\nrefined\n<plan>\n<obs>\nController\nEnvironment\n<act>\nmulti-modal\n<feedback>\noriginal\n<plan>\nerror\n<explanation>\n<obs,task>\n<task>: Obtain a diamond\nin Minecraft step-by-step?; <obs>: \noriginal <plan>:\n3\n3\n1\n1\n1\n3\n12\n1\n4\n1\n11\n1\nSelf-check: When simulating on the goal\n, I find\nare not enough (lack of 2\n). So\nI need craft more\nfrom\n. More \nrequire more \n. So I need to mine more\n.\nrefined <plan> :\n3\n3\n1\n1\n1\n4\n16\n1\n8\n1\n11\n1\nSelf-explain: Because mining\nneeds\n, which I do not have in the inventory.\nCrafting\nneeds\n. So I need to smelt\ninto\nfirst.\n3\n1\n1\nnew <plan> by re-planning:\nmulti-modal <feedback> : I failed on\n. My current state is:\nis broken; I still have\nin the inventory. My position is …\n1\n1\n…\nFigure 4 | Interactive planning in JARVIS-1. After receiving the current task instruction and observation,\nJARVIS-1 will produce an initial plan, which will go through self-check to get possible bugs (marked in red)\nfixed. Further, in case any error (also marked in red) occurs during the execution of the refined plan, JARVIS-1\nwill try to reason about the next move from the environmental feedback via self-explain. Interleaving self-check\nand self-explain significantly boosts the correctness and robustness of JARVIS-1 planning.\nlearning by combining self-instruct, where JARVIS-1 will propose some tasks for itself to complete\nas a means of exploration; and self-improve, where multiple JARVIS-1 agents will be running in\nparallel to gather experiences, therefore helping with better planning later. We provide an illustration\nin Figure 3.\n3.2. Interactive Planning with MLM\nAs we have mentioned in subsection 2.1 and subsection 2.2, the primary challenges for planning in\nMinecraft come from the requirement of being able to plan for long-horizon tasks under dynamic\nobservations. Confirmed by many prior arts (Wang et al., 2023a,b; Yuan et al., 2023), this makes it\nexceptionally hard to utilize canonical symbolic planners, which can be much less flexible. To this\nend, we take a multimodal language model (MLM) as zero-shot planner and combine it with an\ninteractive planning framework to tackle these challenges.\nSituation-aware planning with MLM. To achieve situation-aware planning, the planner must\ntake the current observation into account, in addition to the task instruction (Huang et al., 2022a; Yao\net al., 2022). Specifically, we begin with translating the multimodal observation into text descriptions.\nAs opposed to letting the MLM caption the scene directly, we first extract keywords of Minecraft\nitems (e.g., \"acacia tree\", \"sheep\") from Minecraft wiki and utilizing GPT (Brown et al., 2020)\nto generate sentences that describe these observations. For example, a generated sentence could be \"I\ncan see sheep in the acacia plains\". Then the MLM will retrieve the condition sentence according\nto current visual observation during planning. Additional situation details including biome and\ninventory status are also converted into text using templates. Finally, we prompt the MLM again (the\nlanguage part only) into a plan given the task instruction and all the aforementioned textual situation\ndescriptions. Compared to end-to-end alternatives (Brohan et al., 2023; Huang et al., 2023), we find\nour composable usage of MLM provides higher quality situation descriptions and ultimately, plans\nwith much less hallucination.\nPlanning with self-check. Our first layer of shield to ensure the correctness of plans involves\nself-check. Similar to self-debugging(Chen et al., 2023), given an initial plan, we ask JARVIS-1 to\n6\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nprogressively simulate the plan execution, predict the resulting state after each step (primarily the\nstate of inventory), and evaluate them. By verifying if these states satisfy the goal’s precondition,\nJARVIS-1 can proactively identify potential plan flaws. Compared to the canonical planner where\nthe agent has to encounter the error first before making a remedy, this upfront plan verification\ncould mitigate the need for the agent to recover (re-plan) from more challenging situations due to\nplan failure. For instance, if an agent starts digging underground without sufficient wood, it would\ntypically have to return to the surface, which substantially lowers the chance of completing the task.\nPlanning with environment feedback. Next, our interactive planning framework ventures\ninto allowing JARVIS-1 to quickly recover from failure by leveraging environment feedback in\na closed-loop fashion. The process is illustrated in Figure 4. During plan execution, we feed the\nfeedback to the MLM of JARVIS-1 in case there is any execution failure (possibly due to a flawed\nplan) and utilize its self-explain mechanism (Shinn et al., 2023) to explain the error and locate the\nbugs in the original plan (we term this as error explanation). Finally, the MLM planner of JARVIS-1\nwill produce an improved plan based on both the outside environment feedback and the inside\nretrospective. Compared to other agents that rely on human intervention or privileged environment\ninformation (Huang et al., 2022b; Zhu et al., 2023), JARVIS-1 has the ability to speculate about the\nreasons why current goals cannot be achieved, without the need for additional information or design.\n3.3. Planning with Multimodal Memory in the Loop\nTo address the life-long learning challenge mentioned in subsection 2.3, we equip JARVIS-1 with\nmultimodal memory to allow learning from its own past experiences. We will detail the formulation\nof the retrieval-augmented planning, query generation, and memory layout below.\nRetrival-augmented planning. Retrieval-augmented generation (RAG) (Lewis et al., 2020; Mao\net al., 2020) enhances the quality of responses generated by LLMs by incorporating external sources\nof knowledge to complement the model’s internal representation. We also utilize RAG to enhance\nJARVIS-1’s long-term planning capability. Compared to official RAG methods leveraging the external\nknowledge library, we take the collected multimodal memory as the knowledge library and retrieve the\ninteractive experiences as the demonstration prompt to augment the planning results. The formulation\nis as follows:\n𝑝(𝑦| 𝑥) ≈\n∑︁\n𝑧∈top-k(𝑝(·|𝑥))\n𝑝𝜂(𝑧| 𝑥)𝑝𝜃(𝑦| 𝑥, 𝑧),\n(1)\nwhere 𝑥, 𝑦, and 𝑧denote instruction, plans, and retrieved memory entries respectively, and 𝑝𝜂and\n𝑝𝜃are denoted as retrieval and planning models. Such retrieval-augmented planning method helps\nJARVIS-1 ground the internal knowledge into the open-ended environments efficiently and leverage\nthe historical interaction feedback to solve the hallucination within LLMs and produce more accurate\nplans.\nMultimodal memory. We have demonstrated the layout of our multimodal memory on the\nright side of Figure 5. From a high level, it is a key-value memory where the keys are multimodal,\ncomprising both the task and the observation (or situation) made when this memory entry was created.\nThe values are the plans that were successfully executed. Note that, since the plans in an open-world\nenvironment like Minecraft are situated (see subsection 2.1), there could be multiple entries that are\nwith the same task but different observations and plans. As a result, JARVIS-1 needs to produce\nmultimodal queries based on the current task and situations to retrieve the relevant memory entries.\nQuery generation via reasoning.\nWhen presented with an instruction as a task, we employ\nquery generation via LLM reasoning to decompose the instruction into sub-tasks or related tasks,\n7\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nUser: My current task is\n, but I have never accomplished this task \nbefore. What related tasks might be helpful for me to complete \n?\nAssistant:\nreasoning stops\nwooden pickaxe\n3\n12\n1\n4\n1\nstone pickaxe\n…\n1\n1\n3\n1\nMulti-Modal Memory\ninitial query (text)\nEnchanting\nTable\nObsidian\nDiamond\nBook\nDiamond Pickaxe\nLeather\nPaper\nDiamond\nIron Pickaxe\nnot in memory\nin memory\nreasoning\nquery generation via reasoning\nDiamond axe\nfinal query (obs):\nfinal query (text):\nDiamond\nLeather\nPaper\nIron Pickaxe\nQuery generation via reasoning\nQuery\n+\nquery gen\nretrieve\nFigure 5 | Query generation in JARVIS-1. Given the current observation and the task, JARVIS-1 will first\nthink backward and figure out the needed intermediate sub-goals. The reasoning will be bounded by a limited\ndepth. The sub-goal that is present in the memory will join the current visual observation to form the final\nquery. Entries that match the text query will be ranked by the perceiving distance of their states to the obs\nquery and only the top entry of each sub-goal will be retrieved.\nwhich will then be used as textual queries to retrieve relevant planning experiences as references for\nsolving the current task. For instance, consider the instruction \"craft 1 enchanting table with empty\ninventory\" as shown in Figure 5. JARVIS-1 queries the MLMs to identify the tasks that are required\nfor achieving the main task in a backward search fashion, e.g., “obtain book\n\/diamond\n\/obsidian\nwith empty inventory”. The search depth is bounded for efficiency. Further, instead of relying\nsolely on retrieval based on the text query (Wang et al., 2023a; Zhu et al., 2023), we also propose to\nappend the agent’s current visual observation to the textual query, resulting in a multimodal query to\ntake the situation into account during memory retrieval.\nMultimodal retrieval. After obtaining the textual and visual query, we compute the alignment\nbetween the query and each trajectory in multimodal memory. We first use the text encoder of the\nCLIP model to compute the embedding of the query and task key of each entry in memory. We select\nthe memory entries with similarity higher than the confidence threshold as the candidate entries.\nThen we will compute the visual state embedding of query and states in candidate entires. Then we\nsort the candidate entries with the visual embedding similarities, which can be formed as:\n𝑝𝜂(𝑧| 𝑥) ∝CLIP𝑣(𝑠𝑧)⊤CLIP𝑣(𝑠𝑥),\n(2)\nwhere 𝑠𝑧and 𝑠𝑥are the visual key of memory entries and visual query, respectively. Finally, we retrieve\nthe plan of top-k candidate entries as reference prompt 𝑧.\n3.4. Self-improving Agents\nLearning in Minecraft with memory. The remaining issue now is where the aforementioned\nmultimodal memory comes from. Inspired by the life-long learning scheme in many close-world and\nopen-world reinforcement learning problems (Abel et al., 2018a,b; Wang et al., 2023a), we propose\nthe following learning approach for augmenting the memory in JARVIS-1: 1) First, we generate a set\nof tasks, which form some curricula for the agents to complete as means of exploration of the world.\nDuring this process, JARVIS-1 produces plans, interacts with the environment, embraces the errors,\nand stores all these experiences in the memory; 2) After this learning stage, we evaluate JARVIS-1\n8\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\non various tasks. Therefore, JARVIS-1 is able to produce better plans with the memory teaming up\nwith the planning experiences. In our experiments, we use this as the default setting for all tasks.\nExploration using self-instruct. The key issue to the success of learning with memory is how\nto effectively acquire useful experiences given a limited amount of time. We propose to use self-\ninstruct (Wang et al., 2022) to generate the dynamic curriculum and guide JARVIS-1 to learn\nfrom the interactions with environments. In each round, we prompt the MLM to consider how\ncapable JARVIS-1 is at this point and subsequently select tasks from a task pool to explore. We find\nthat the curriculum almost follows the technical tree-growing direction. To accelerate the learning\nprocess, we augment the linear self-instruct to distributed learning in distributed environments with\nshared memory, i.e. speculative execution (Leviathan et al., 2023). Specifically, we generate multiple\nexecutable tasks as candidate task batches and provide them to agents with the same memory for\nverification and execution in various different environments. Meanwhile, experiences are collected\ninto a shared centralized memory. When all exploration tasks have been accomplished, we move to\nthe next round, until the memory reaches a certain capacity.\nLife-long learning. We’ve also observed that the aforementioned learning (where the memory is\nbeing filled) can be extended throughout the whole gameplay, where the agent gradually acquires\nmore and more skills. As the gameplay continues, more and more experiences are pouring in, therefore\nJARVIS-1 can find better references for challenging tasks like ObtainDiamondPickaxe, resulting\nin an improved success rate on these tasks. Further, there is no gradient update in this thanks to\nthe memory-augmented MLM, i.e. we can do in-context life-long learning. In Section 4.3, we offer\nexploratory experiments to show the potential of such capability of JARVIS-1.\n4. Experiments\nIn the experiments, our goal is to 1) evaluate the general performances of JARVIS-1 on the chal-\nlenging Minecraft tasks, especially on its advantages over baselines that do not (fully) address the\naforementioned issues in open-world agents; 2) understand the factors that contributes to the general\nresults; 3) explore the potential of JARVIS-1 in terms of life-long learning and its benefits to long-\nhorizon tasks. To this end, we will first briefly introduce the evaluation settings, then cover the main\ncomparative results and ablation studies, and conclude with an exploratory trial on long-horizon\ntasks.\n4.1. Experimental Setups\nWe evaluate JARVIS-1 in Minecraft, with tasks selected from the recently introduced Minecraft\nUniverse Benchmark (Lin et al., 2023a). For the reader’s convenience, we provide details on the basic\nsetups below.\nEnvironment setting. To ensure realistic gameplay, the agent needs to utilize observation\nand action spaces that are similar to those used by humans. Instead of manually designing a\ncustom interface for models to interact with the environment, as done in previous methods such as\nMineDojo(Fan et al., 2022), GITM(Zhu et al., 2023), and Voyager(Wang et al., 2023a), we opt for\nusing the native human interface provided by Minecraft. This applies to both the observation and\naction space. The model operates at a speed of 20 frames per second and is required to use a mouse\nand keyboard interface when interacting with human GUIs. For more information on the detailed\ndescriptions of the observation and action spaces, please refer to the Appendix.\nTask setting.\nIn Minecraft, players have access to thousands of items, each with specific\nacquisition requirements or recipes. For example, stone-type items can only be obtained using a\n9\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 1 | Characteristics of 11 task groups encompassing over 200 minecraft tasks.\nGroup\nTask\nNum.\nMax.\nSteps\nInitial\nInventory\nBiome\nLanguage Instruction\nWood\n34\n12k\nnull\nPlains\/Forest\nPick up a wooden_pickaxe.\nWood-Variants\n43\n12k\nnull\nSavanna\/Jungle\/Taiga\nPick up a acacia_boat.\nStone\n10\n12k\niron_axe\nPlains\/Forest\nCraft a furnace given an iron axe.\nIron\n22\n12k\niron_axe\nPlains\/Forest\nSmelt and craft an iron_door given an iron axe.\nGold\n9\n36k\niron_axe\nPlains\/Forest\nSmelt and craft an golden_axe given an iron axe.\nDiamond\n7\n36k\niron_axe\nPlains\/Forest\nDig down to mine diamond and craft diamond_pickaxe.\nRedstone\n7\n36k\niron_axe\nPlains\/Forest\nMine redstone and make dropper given an iron axe.\nBlocks\n15\n12-36k\niron_axe\nPlains\/Forest\nDig down to mine lapis_lazuli block.\nArmor\n17\n12-36k\niron_axe\nPlains\/Forest\nCraft diamond_boots given an iron axe and equip it.\nDecoration\n17\n12k\niron_axe\nFlower Forest\nObtain the bed and dye it red.\nFood\n9\n12k\niron_axe\nPlains\nKill sheep to obtain mutton and cook it.\npickaxe, and two planks can be crafted into four sticks (these requirements are available on the\nMinecraft Wiki1). In survival mode, players must obtain each type of item from the environment or\ncraft\/smelt the object item from materials. We choose over 200 tasks from the Minecraft Universe\nBenchmark (Lin et al., 2023a) for evaluation. These tasks are related to items that can be obtained in\nthe Minecraft overworld. For the convenience of statistics, we have classified them into 11 groups\naccording to recommended categories in Minecraft2 (see Table1). Due to the varying complexity of\nthese tasks, we adopt different maximum gameplay durations (Max. Steps) for each task. The limit is\ndetermined by the average time the human players need to accomplish the corresponding task. Other\ndetails about each task, such as language instruction, maximum steps, evaluation times, biome, and\ninitial inventory when the agent is born into the world can be found in Appendix Table 5-14.\nEvaluation metrics. By default, the agent always starts in survival mode, with an empty inventory.\nA task is considered a success when the target object is obtained within a specified time. Due to the\nopen-world nature of Minecraft, the world and initial position that the agent is spawned at could\nvary a lot. Therefore, we conducted at least 30 tests for each task using different seeds and reported\nthe average success rate to ensure a thorough assessment. Further, since we categorize the tasks into\ngroups, we also report mean and variance values for each group for ease of presentation.\n4.2. Main Results\nWe compare JARVIS-1 with other multi-task instruction-following agents based on LLM, including\nInstruct GPT (Huang et al., 2022a; Ouyang et al., 2022), ReAct (Yao et al., 2022), Inner Mono-\nlogue (Huang et al., 2022b), DEPS (Wang et al., 2023b). Since some methods are not originally\nexperimented in Minecraft, we reproduce them to conform to the Minecraft specification based on\nprompt and feedback template design. All LLM-based methods access the LLM model through OpenAI\nAPI. And all hyper-parameters of LLM including temperature are kept as default.\nThe average success rates for every task group are listed in Table 2. JARVIS-1 achieves the\nbest performance with all meta tasks. It is important to note that in Minecraft, the technology tree\ncan be formed by Group Wood, Stone, Iron, Gold, and Diamond. The tasks become increasingly\ndifficult as you progress through the tree. For more difficult tasks such as obtaining a gold ingot or\na diamond, the agents typically need to perform more actions and longer goal sequences in order\nto complete the task. As a result, the success rate of all agents decreases as the difficulty level\nincreases. It is evident that reasoning methods (ReAct (Yao et al., 2022) vs. GPT (Huang et al.,\n2022a; Ouyang et al., 2022)) and interactive re-planning with feedback (Inner Monologue(Huang\net al., 2022b) vs. GPT) effectively enhance the agent’s task performance in an open world. However,\n1https:\/\/minecraft.fandom.com\/wiki\/Minecraft_Wiki\n2https:\/\/minecraft.fandom.com\/wiki\/Tutorials\/Organization#Categories\n10\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 2 | Results of JARVIS-1 and baselines on Minecraft. The detailed task instructions, settings and results\ncan be found in the Appendix.\nGroup\nTask\nGPT\nReAct\nInner Monologue\nDEPS\nJARVIS-1\n26.67\n45.00\n36.67\n75.00\n91.55\nWood\nAVG\n27.30±14.86\n40.31±13.30\n60.15±19.41\n80.23±17.32\n88.84±16.82\n6.67\n36.67\n30.00\n36.67\n60.47\nWood\nVar\nAVG\n24.39±11.08\n38.13±12.81\n53.39±12.86\n68.75±12.32\n76.78±12.27\n20.00\n20.00\n66.67\n75.00\n94.20\nStone\nAVG\n20.21±12.32\n39.00±12.15\n52.86±16.90\n69.27±7.78\n88.69±4.87\n0.00\n0.00\n3.33\n20.00\n33.82\n3.33\n6.67\n0.00\n20.00\n38.10\nIron\nAVG\n3.27±2.85\n4.61±3.63\n5.20±5.17\n16.92±4.69\n34.63±10.61\n0.00\n2.00\n2.00\n6.00\n14.49\nGold\nAVG\n0.00±0.00\n0.45±0.60\n0.59±0.64\n2.20±1.55\n6.85±4.71\n0.00\n0.00\n1.00\n2.00\n9.20\n0.00\n0.00\n0.00\n2.50\n6.22\nDiamond\nAVG\n0.00±0.00\n0.35±0.48\n0.96±0.67\n2.42±1.01\n8.99±2.68\n0.00\n2.00\n0.00\n10.00\n22.78\nRedstone\nAVG\n1.04±1.30\n1.14±1.18\n0.69±1.68\n6.02±3.61\n17.51±9.34\n16.67\n33.33\n43.33\n53.33\n86.67\nBlocks\nAVG\n45.64±33.88\n49.35±30.51\n55.71±29.43\n58.02±27.68\n80.34±21.09\n6.67\n0.00\n10.00\n10.00\n30.30\nArmor\nAVG\n1.36±2.25\n0.50±0.88\n3.10±4.71\n3.71±3.78\n13.44±14.62\n15.00\n15.00\n15.00\n25.00\n50.00\nDecoration\nAVG\n17.12±11.59\n17.13±9.19\n12.03±10.19\n29.59±15.94\n46.67±23.39\n13.33\n16.67\n25.00\n16.67\n43.55\nFood\nAVG\n9.40±4.29\n15.56±6.83\n20.78±11.99\n22.85±8.15\n46.75±11.16\nthese approaches still face challenges when dealing with long-horizon tasks, specifically in the Iron\nand Diamond group. DEPS(Wang et al., 2023b), on the other hand, enables agents to accomplish\ndiamond-related tasks through interactive long-horizon planning accompanied by descriptions and\nexplanations. Nevertheless, its reliability remains very low at approximately 2.5%.\nIn comparison to DEPS(Wang et al., 2023b) without memory, JARVIS-1 demonstrates superior\nperformance even in challenging tasks due to its extensive experience. In diamond-related tasks\nspecifically, the success rate has increased by nearly 3 times (8.99% vs 2.42%). And JARVIS-1\nusually only requires 2-3 rounds of re-planning to generate the correct executable plan, whereas\nDEPS requires more than 6 rounds. This means that JARVIS-1 saves a significant amount of LLM\ntokens and thinking time, enabling more efficient plan execution and providing additional steps and\ntokens for handling uncertainty in the environment.\nBased on our observations, we have found that the bottleneck for JARVIS-1 in tasks involving\ndiamonds often lies with the Controller’s inability to perfectly execute short-horizon text instructions\ngenerated by LLM. Therefore, it is worth exploring methods for generating plans that are easier for\nthe controller to execute or improving the controller’s ability to follow instructions.\n11\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n97%\n96%\n94%\n34%\n9%\n95%\n95%\n90%\n30%\n5%\n55%\n50%\n35%\n5%\n0\n85%\n85%\n75%\n25%\n5%\n0%\n20%\n40%\n60%\n80%\n100%\nCrafting Table\nWooden Pickaxe\nStone Pickaxe\nIron Pickaxe\nDiamond\nGPT4\nChatGPT\nLLaMA2 Pre-Trained\nLLaMA2 Fine-tuned\nFigure 6 | Success rates for different language models on Minecraft tasks. We found open-sourced LLaMA2-\n70B modelsTouvron et al. (2023) lack knowledge related to Minecraft, so the pre-trained model performs\npoorly. We further finetuned the LLaMA2-13B model on a Minecraft text dataset collected from the internet,\nand it shows performance similar to ChatGPT on Minecraft.\n4.2.1. JARVIS-1 based on different LMs\nWe conducted ablation experiments on various Language Models, including OpenAI’s ChatGPT Ouyang\net al. (2022) and GPT-4 OpenAI (2023). Among these models, GPT-4 has more parameters and has\nbeen proven to outperform ChatGPT in extensive research Wang et al. (2023a). We also select the\nopen-source pre-trained LLaMA2 70B model Touvron et al. (2023). Additionally, we gathered a\nsubstantial amount of Minecraft-related text from the internet as training data and further fine-tuned\nLLaMA2 13B. The experiments were conducted on a subset of Minecraft tasks using different language\nmodels. Each JARVIS-1 learns for 4 epochs of interaction with all task sets and evaluates on task\nsubset across at least 20 seeds. The experimental results are presented in Fig. 6.\nTable 6 demonstrates that ChatGPT, despite having fewer parameters, achieves nearly identical\nsuccess rates as GPT-4. This suggests that language models equipped with memory can significantly\nenhance planning abilities. In Minecraft-related tasks, the open-source pre-trained LLaMA2 70B\nexhibits a notable performance gap compared to OpenAI models, particularly in long-horizon tasks.\nHowever, by finetuning LLaMA2 with fewer parameters, its performance on Minecraft tasks improves\nsubstantially. This indicates that the open-source model lacks knowledge specific to Minecraft and\nrequires further finetuning for the successful completion of such tasks.\n4.2.2. Ablation on Memory\nWe also conduct ablation experiments on the multimodality memory and retrieval methods. We set\nJARVIS-1 w\/o memory module as the baseline agent. We first evaluate JARVIS-1’s performance\nwith different memory sizes (representing different learning stages) as shown in Fig. 7, which demon-\n12\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nFigure 7 | Success rate by memory size for different items. We evaluated the performance of JARVIS-1 at\ndifferent memory sizes (representing different learning stages) by measuring the success rate (% Episodes) of\ncompleting key items on the Minecraft technology tree. As the learning progressed, we observed an improvement\nin completion rates for all items, with an increasing number of successful trajectories being included in memory.\nAfter 4 epochs of learning, JARVIS-1 had accumulated a total of 425 successful trajectories in its memory.\n85%\n20%\n10%\n0%\n10%\n95%\n20%\n30%\n5%\n20%\n94%\n34%\n40%\n9%\n24%\n0%\n20%\n40%\n60%\n80%\n100%\nStone Pickaxe\nIron Pickaxe\nShield\nDiamond\nRedstone Block\nText Memory\nText Memory + Reasoning\nMultimodal Memory + Reasoning               baseline (no memory)\nFigure 8 | Success rates for different retrieval methods with memory on Minecraft tasks. JARVIS-1, which\nsynergizes reasoning and retrieval with multimodal memory, achieves the best.\nstrates the effectiveness of self-improving within JARVIS-1. We further conduct the experiments on\na subset of Minecraft tasks using three different retrieval methods: retrieval with textual instruction\nembedding only (Text Memory), synergizing reasoning and retrieval with text embedding (Text Mem-\nory+Reasoning), and synergizing reasoning and retrieval with multimodality embedding (Multimodal\nMemory+Reasoning). Except for the memory and retrieval methods, all others are kept the same.\nThe results are listed in Fig. 8.\nThe experiments show that reasoning before retrieval can effectively improve retrieval accuracy.\nRetrieval based on a multimodal state including vision observation and symbolic information (e.g.,\ninventory, location, etc) is better than only considering the text embedding.\n4.3. Long-Horizon Challenges\nMost concurrent multi-task agents in Minecraft can only handle short-term tasks and struggle with long-\nhorizon tasks like CraftingDiamondPickaxe. The VPT foundation model(Baker et al., 2022) is\ncapable of accomplishing various tasks in Minecraft but lacks the ability to execute human instructions.\nTo address this limitation, Reinforcement Learning is required to fine-tune the VPT foundation model\nfor specific task completion. However, after fine-tuning, VPT may experience a decline in performance\n13\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n12.5%\n7.2*104\n7.2*104\nFigure 9 | (Left) The success rate of different models in the ObtainDiamondPickaxe challenge over gameplay\ntime. VPT RL is finetuned from VPT early game with reinforcement learning over 1.4 million episodes. JARVIS-\n1 agent and its varients have interacted with Minecraft with over 4 epochs on all tasks in task pool. Typically, it\ntakes a skilled person over 20 minutes (24,000 steps) to obtain a diamond pickaxe. (Right) The success rate of\nobtaining important intermediate items during the process of synthesizing a diamond pickaxe of JARVIS-1.\nThis task has been evaluated over 300 times on different seeds. These curves indicate that as the game\nprogresses, the success rates of obtaining all intermediate items are increasing, which indicates that JARVIS-1\nis constantly improving its skills.\nfor other tasks while focusing on the specified task. In contrast, Steve-1(Lifshitz et al., 2023) has\nimplemented goal-conditioned fine-tuning on VPT, enabling it to follow human text instructions while\nmaintaining multitasking capabilities. However, Steve-1 primarily focuses on low-level tasks like\nobtaining dirt, collecting flowers, and chopping trees. When it comes to long-horizon tasks such as\nstarting from scratch by obtaining a wooden pickaxe, Steve-1 still encounters difficulties.\nDEPS(Wang et al., 2023b) also utilizes LLM as a planner, but it lacks the ability to learn from\nexperience in different tasks and apply that knowledge to new ones. Additionally, DEPS is limited in\nits re-planning rounds due to the LM’s context constraints. The experiments reveal that DEPS has a\nsuccess rate of less than 50% in generating accurate and executable plans for acquiring diamonds.\nThe probability of DEPS successfully obtaining diamonds in the environment is approximately 0.59%.\nConsequently, DEPS continues to face challenges when attempting to finish long-horizon tasks within\nthe Minecraft world.\nEven human players who have mastered the distribution pattern of diamonds achieve success\nrates of obtaining diamonds and crafting a diamond pickaxe (which requires at least three diamonds)\nwithin 10 minutes at approximately 15% and 12%, respectively. JARVIS-1 performs better in the\nObtainDiamondPickaxe challenge. Compared to the state-of-the-art model, which has undergone\nRL-finetuned VPT, JARVIS-1 has more than doubled the success rate of obtaining a diamond pickaxe\n(6.22% vs 2.5% within 20 minutes).\nTo increase the chances of obtaining diamonds, we extended the game-playing time to 60 minutes\n(72000 game-playing steps, as shown in Figure 9). As a result, JARVIS-1’s success rate in acquiring\na diamond pickaxe improved from 6.2% to 12.5%. The graph on the right side of Figure 7 illustrates\nhow the success rate of intermediate milestone items changes over time, indicating that JARVIS-1\ntends to improve with longer game-playing time. We also conduct two variants of JARVIS-1 with\ndifferent self-improving curricula: human-written and random-generated. All three JARVIS-1 have\ncollected experiences into memory with the curriculum for 4 epochs before evaluation in 60 minutes.\nThe results show that JARVIS-1 with a GPT-generated curriculum can finish the task within the\nshortest game-playing steps and achieve the best performance in 60 minutes.\nIn contrast, VPT’s success rate barely changed when we increased the time from 20 minutes\nto 60 minutes (from 2.5% to 3%). This can be attributed to Minecraft’s durability system where\n14\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nprolonged underground exploration often leads to pickaxe damage. When JARVIS-1’s pickaxe\nbreaks, it dynamically re-plans based on its current inventory and crafts a new one. However, VPT-RL\nexhibits perplexing behaviors at this stage by using inappropriate tools for mining stones or crafting\nunnecessary items. This comparison demonstrates that JARVIS-1 possesses superior generalization\nand planning abilities for long-horizon tasks.\nNote that our method is designed to be multi-task in its nature and not finetuned through imitation\nlearning on specific datasets or reinforcement learning.\n5. Related Works\n5.1. Planning with LLM\nThere have been some methods leveraging the large language model to generate action plans for\nhigh-level tasks in embodied environments (Dasgupta et al., 2022; Gong et al., 2023b; Liu et al., 2023;\nMai et al., 2023; Zeng et al., 2022; Zhang et al., 2023; Zhang and Lu, 2023). Huang et al. (2022a)\ndecompose natural language commands into sequences of executable actions by text completion\nand semantic translation, while SayCan generates feasible plans for robots by jointly decoding an\nLLM weighted by skill affordances from value functions (Brohan et al., 2022b). Some methods\nalso leverage the LLM to produce the program code as plan for better executation (Liang et al.,\n2022; Lin et al., 2023b; Singh et al., 2022). However, the above methods assume that the initial\nplan from the LLM is correct. When there are bugs in the initial plan, it’s difficult for the agent\nto finish the task successfully. Recent research frequently employs LLM as an interactive planner,\nharnessing its self-updating capabilities to enhance the plan’s executability over time (Shinn et al.,\n2023; Sun et al., 2023; Wang et al., 2023b). Inner Monologue (Huang et al., 2022b) pilots the front\nof interactive planning with LLMs, which introduces the feedback (including success detection and\nscene description) to the planner. However, we found it could still suffer from accumulative planning\nerrors, especially in long-horizon open-world tasks. ReAct (Yao et al., 2022) will reason about the\nagent state before acting, which indicates that various reasoning methods (Wei et al., 2022; Wu et al.,\n2023; Yao et al., 2023) are benefitial for planning. LLM-based planning methods often use the fixed\npretrained LLM as the agent, while we focus more on life-long and continual learning for agents in\nopen-world environments (Ke et al., 2022a,b; Wang et al., 2023a). For better leveraging historical\ninteraction between agent and environments, an explicit memory (Park et al., 2023; Zhu et al., 2023)\nfor more historical chatting has been leveraged for bigger storage of agent experiences. However,\nthe above methods usually rely only on a text-based environment and struggle to execute plans in\npartial-observed visual open-world environments.\n5.2. Minecraft Agents\nDeveloping generally capable agents in Minecraft to solve open-world tasks has gained increasing\ninterests (Baker et al., 2022; Cai et al., 2023a,b; Ding et al., 2023; Fan et al., 2022; Yuan et al.,\n2023; Zhang and Lu, 2023; Zhu et al., 2023). As an early attempt, Oh et al. (2017) studied task\ngeneralization in a simple Minecraft environment variant. It designed a two-stage pipeline, first\nmastering the prerequisite skills with parameterization trick, and then learning a meta controller\nto execute the instructions. Moving to solve complex long-horizon tasks in Minecraft, works (Lin\net al., 2021; Mao et al., 2022; Oh et al., 2017) explored the hierarchical architecture. In recent years,\ninfluenced by the trend of large-scale pre-training paradigms, a group of researchers have emerged,\nwho are utilizing vast amounts of internet knowledge to train intelligent agents. Fan et al. (2022)\ntrained a visual-semantic alignment model, MineCLIP, using the correspondences between subtitles\nand video snippets available on YouTube, and used it to generate intrinsic rewards to guide policy\n15\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nlearning. (Baker et al., 2022) utilizes a pre-trained inverse dynamics model to label actions in YouTube\nvideos which are used to learn a foundation policy VPT through imitation learning. By bridging\nMineCLIP and VPT, Lifshitz et al. (2023) creates a performant instruction-following policy Steve-1 to\nsolve open-world short-horizon tasks using hindsight relabeling and unCLIP tricks. However, Steve-1\ncan not solve complicated process-oriented tasks due to the expressive capability of its goal space.\nCai et al. (2023b) learns to follow reference videos as the instruction by merely watching gameplay\nvideos, which improves the capacity of goal space and reduces the cost of policy training. All of\nthese methods focus on improving the smoothness and robustness of interaction between policy\nand environment. Inspired by the powerful language understanding and reasoning capabilities of\nlarge language models, researchers have begun to build Minecraft agents based on LLMs. Wang\net al. (2023a) used LLM to guide the agent to explore the Minecraft world by acquiring diverse skills,\nmaking novel discoveries, and generating goal proposals. Zhu et al. (2023) integrated LLM with\ntext-based knowledge and memory to equip the agent with common sense and past experiences for\nhigher reasoning efficiency. Yuan et al. (2023) used LLM to guide the agent to explore the Minecraft\nworld and interact with the environment with reinforcement learning control policies.\n6. Conclusion\nWe propose a multi-task agent JARVIS-1 designed for the complex environment of Minecraft, which\nmarks a significant advancement in achieving human-like planning within an open-world setting.\nBy leveraging pre-trained Multi-modal Language Models, JARVIS-1 not only effectively interprets\nmultimodal inputs but also adeptly translates them into actions. Its integration of a multimodal\nmemory, which draws from both ingrained knowledge and real-time game experiences, enhances\nits decision-making capabilities. The empirical evidence of its prowess is evident in its impressive\nperformance across a wide array of tasks in Minecraft. Notably, its achievement in the long-horizon\ndiamond pickaxe task, where it achieved a completion rate that surpasses VPT by up to five times,\nunderscores its potential and the strides made in this domain. This breakthrough sets the stage for\nthe future of more versatile and adaptable agents in complex virtual environments.\nAcknowledgments\nThis work is funded in part by the National Key R&D Program of China #2022ZD0160301, a grant\nfrom CCF-Tencent Rhino-Bird Open Research Fund, NSF grants #IIS-1943641, #IIS-1956441, #CCF-\n1837129, an SRA from Meta and a research gift from Amazon Alexa AI, and a gift from RelationalAI.\nThe authors sincerely thank Dr. Rita Zhang, Zhixiang Dai at NVIDIA for the valuable technical support\nof GPU computing.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | JARVIS-1：开放世界多任务智能体，迈向通用人工智能的关键一步\n\n## 📌 背景痛点\/本文动机\n在开放世界中，构建能够完成各种任务的智能体一直是通用人工智能（AGI）领域的重要目标。然而，现有的方法在处理开放世界中无限数量的任务时仍然面临挑战，尤其是在任务完成能力随游戏时间推移而逐步提升方面。本文提出的JARVIS-1旨在解决这些问题，它是一个能够在开放世界中感知多模态输入（视觉观察和人类指令）、生成复杂计划并执行具身控制的智能体。\n\n## 🚀 核心方法\n💡 创新点1：多模态语言模型（MLM）\nJARVIS-1基于预训练的多模态语言模型，能够将视觉观察和文本指令映射到计划中。这使得智能体能够更好地理解任务、情况和环境反馈，从而生成更准确的计划。\n\n💡 创新点2：多模态记忆\nJARVIS-1配备了多模态记忆，存储了过去的成功规划经验和场景。通过检索相关记忆条目，智能体的规划能力可以在上下文中得到加强，从而提高规划的正确性和一致性。\n\n💡 创新点3：自我指导和自我改进\nJARVIS-1能够通过自我指导机制自主生成任务，并通过探索世界来收集经验。这些经验被存储在多模态记忆中，帮助智能体在后续任务中更好地进行推理和规划。\n\n## 📈 实验结果\nJARVIS-1在Minecraft中表现出色，能够完成超过200个不同的任务，包括短期任务（如砍树）和长期任务（如获得钻石镐）。在经典的长任务“获得钻石镐”中，JARVIS-1的可靠性比现有最先进的智能体高出5倍，能够成功完成更长期和更具挑战性的任务。\n\n## 💬 可借鉴之处\nJARVIS-1的设计和实现为开放世界智能体的发展提供了重要的启示。通过结合多模态语言模型、多模态记忆和自我改进机制，JARVIS-1展示了在开放世界中实现通用人工智能的潜力。这些方法可以应用于其他开放世界环境，推动通用人工智能的发展。","llm_summary_res_status":200}
{"title":"Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds","authors":"Sipeng Zheng, Jiazheng Liu, Yicheng Feng, Zongqing Lu","summary":"Recent studies have presented compelling evidence that large language models\n(LLMs) can equip embodied agents with the self-driven capability to interact\nwith the world, which marks an initial step toward versatile robotics. However,\nthese efforts tend to overlook the visual richness of open worlds, rendering\nthe entire interactive process akin to \"a blindfolded text-based game.\"\nConsequently, LLM-based agents frequently encounter challenges in intuitively\ncomprehending their surroundings and producing responses that are easy to\nunderstand. In this paper, we propose Steve-Eye, an end-to-end trained large\nmultimodal model designed to address this limitation. Steve-Eye integrates the\nLLM with a visual encoder which enables it to process visual-text inputs and\ngenerate multimodal feedback. In addition, we use a semi-automatic strategy to\ncollect an extensive dataset comprising 850K open-world instruction pairs,\nempowering our model to encompass three essential functions for an agent:\nmultimodal perception, foundational knowledge base, and skill prediction and\nplanning. Lastly, we develop three open-world evaluation benchmarks, then carry\nout extensive experiments from a wide range of perspectives to validate our\nmodel's capability to strategically act and plan. Codes and datasets will be\nreleased.","url":"http:\/\/arxiv.org\/abs\/2310.13255v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.13255v2","published":1697772125000,"comment":"19 pages, 19 figures","pdf_text":"Preprint\nSTEVE-EYE:\nEQUIPPING LLM-BASED EMBOD-\nIED AGENTS WITH VISUAL PERCEPTION IN OPEN\nWORLDS\nSipeng Zheng1, Jiazheng Liu2, Yicheng Feng2, Zongqing Lu1,2†\n1 Beijing Academy of Artificial Intelligence\n2 School of Computer Science, Peking University\nspzheng@baai.ac.cn\nfyc813@pku.edu.cn\nzongqing.lu@pku.edu.cn\nABSTRACT\nRecent studies have presented compelling evidence that large language models\n(LLMs) can equip embodied agents with the self-driven capability to interact with\nthe world, which marks an initial step toward versatile robotics. However, these\nefforts tend to overlook the visual richness of open worlds, rendering the entire\ninteractive process akin to “a blindfolded text-based game.” Consequently, LLM-\nbased agents frequently encounter challenges in intuitively comprehending their\nsurroundings and producing responses that are easy to understand. In this paper,\nwe propose Steve-Eye, an end-to-end trained large multimodal model to address\nthis limitation. Steve-Eye integrates the LLM with a visual encoder to process\nvisual-text inputs and generate multimodal feedback. We adopt a semi-automatic\nstrategy to collect an extensive dataset comprising 850K open-world instruction\npairs, enabling our model to encompass three essential functions for an agent:\nmultimodal perception, foundational knowledge base, and skill prediction and\nplanning. Lastly, we develop three open-world evaluation benchmarks, then carry\nout experiments from a wide range of perspectives to validate our model’s capa-\nbility to strategically act and plan. The project’s website and code can be found at\nhttps:\/\/sites.google.com\/view\/steve-eye.\n1\nINTRODUCTION\nHi Steve, how to craft a \nwooden axe in Minecraft ?\n{\"type\": \"minecraft:crafting_shaped\",\n\"category\": \"equipment\",\n\"key\": {\n\"#\": {\"item\": \"minecraft:stick\" },\n\"X\": {\"tag\": \"minecraft:planks\"}\n},\n\"pattern\": [\"XX\", \"X#\", \" #\"],\n\"result\": {\n\"item\": \"minecraft:wooden_axe\"},\n\"show_notification\": true}\n…? I don’t understand.\nOk, I got it.\nHi Steve, you are given an\nexample to generate a plan\nlist for task […], Now I\ndescribe your surrounding\nenvironment and your in-\nventory status […], please\nmake a plan to craft stick.\n…… ?\nSure, here is the plan list \nto craft stick: (1) find log\nnearby, (2) log, (3) craft \nplanks, (4) craft stick.\nWell, this is what you see.\nplease make a plan to craft stick\n(a)\n(b)\n×\n√\nHi Steve, how to craft a \nwooden axe in Minecraft ?\n×\n√\nFigure 1. (a) LLM-based agent’s feedback is un-\ncontrollable due to the uncertainty of input textual\nprompt, while visual cues can benefit the agent to\ngenerate feedbacks; (b) a text-only driven agent of-\nten finds it difficult to produce intuitive feedback\nthat humans can easily understand.\nDeveloping embodied agents that can adapt\nto the open world has long been a sub-\nstantial challenge (Kolve et al., 2017; Savva\net al., 2019).\nRecently, the rapid progress\nof large language models (LLMs) (OpenAI,\n2022; Touvron et al., 2023a) has shown their\npotential to serve as a general-purpose assis-\ntant. Driven by these pre-trained LLMs, re-\ncently proposed agents (Yuan et al., 2023;\nWang et al., 2023a;b; Zhu et al., 2023) have\nmanaged to extract world knowledge and rea-\nsoning capabilities from LLMs, allowing them\nto become self-driven. Thereby these agents\nare capable of generating executable policies\nor plans for a wide range of skills and tasks in\nan open-ended world.\nWhile current attempts to integrate LLMs\nshow promise in developing a generic embod-\nied agent, these efforts primarily translate the\nentire world into text, which overlooks the\n†Corresponding author\n1\narXiv:2310.13255v2  [cs.CV]  7 Dec 2023\nPreprint\nmultifaceted richness of diverse visual reality\nand turns interacting with the environment into something akin to “a blindfolded text-based game.”\nConsequently, such text-only agents often face difficulties when it comes to effectively and intu-\nitively representing the world. Imagine a situation where you request your agent to shop for a pair\nof shoes. Would you prefer to send the agent a picture of the shoes or provide a lengthy description\nof the shoes to convey their appearance? Undoubtedly, you would opt for the former choice.\nIn fact, the agent’s reliance on text input\/output (I\/O) imposes significant limitations on its ability to\ninteract with the world. To illustrate this point, we consider Minecraft (Guss et al., 2019; Fan et al.,\n2022) as an ideal example. Minecraft, being an expansive sandbox game, offers a vast realm for\nembodied agents to explore, which requires the acquisition of various basic skills (e.g., crafting logs)\nand the ability to plan and execute diverse tasks. First, as shown in Figure 1 (a), the LLM-based agent\nproduces uncontrollable outputs. The success of the agent’s responses hinges heavily on careful\nprompt engineering (Huang et al., 2022b), ensuring that the LLM comprehends the environment\nand task objectives. Moreover, a universally applicable prompt that suits every LLM and task is an\nunattainable goal. Therefore, this prompting process is labor-intensive and contradicts our aim of\nenabling agents to act in a self-driven manner. Second, when compared to visual feedback, language\noften encounters difficulties in intuitively conveying specific world concepts (e.g., recipes) to users,\nas illustrated in Figure 1 (b), thereby unavoidably creating obstacles for robust human-computer\/AI\ninteraction (Preece et al., 1994; Fallman, 2003).\nUnlike LLMs, humans possess an innate ability to process and generate information through both\nvisual and text channels. This inherent gift significantly enhances our capability to interact with\nthe world. However, the coupling of LLM-based agents with multimodal I\/O has been relatively\nunderexplored in an open-ended environment. To fill this gap, we introduce Steve-Eye\n, a large\nmultimodal model that enables LLM-based embodied agents to engage with the open world via\nvisual-text interfaces. Steve-Eye excels at producing responses that demonstrate a comprehensive\ngrasp of the environment, common-sense reasoning, and executable skill plans. To achieve this,\nSteve-Eye is equipped with three indispensable functions: (1) multimodal perception; (2) founda-\ntional knowledge base; and (3) skill prediction and planning. In this paper, we choose Minecraft as\nour validation platform considering its vast sandbox world and the high degree of freedom. More\nenvironments can also be considered, e.g., Virtual Home (Puig et al., 2018), AI2THOR (Kolve\net al., 2017). Due to the space limit, we discuss the exploration of more generic environments in\nAppendix A.4 and leave it as our future work. Our contributions can be summarized as follows:\nOpen-World Instruction Dataset.\nWe construct an extensive instruction dataset to train Steve-\nEye for the acquisition of three mentioned functions. The instruction data contains not only the\nagent’s per-step status and environmental features but also the essential knowledge for agents to act\nand plan. However, collecting such a dataset in an open world can be a costly endeavor, especially\nwhen aiming to gather fine-grained and diverse labels. As a result, previous studies (Fan et al.,\n2022) have often relied on readily available unsupervised data (e.g., video-subtitle pairs) for pre-\ntraining. In these approaches, the agent’s comprehension of its status and environment is implicitly\nlearned through self-supervised techniques, while its foundational knowledge is directly derived\nfrom general-purpose LLMs. In contrast, our work involves curating multimodal instructional data\nspecifically designed for open-ended embodied agents, by utilizing ChatGPT (OpenAI, 2022).\nLarge Multimodal Model and Training.\nSteve-Eye combines a visual encoder which converts\nvisual inputs into a sequence of embeddings, along with a pre-trained LLM which empowers em-\nbodied agents to engage in skill or task reasoning in an open world. During the training process, we\nemploy a two-stage strategy similar to Liu et al. (2023). This strategy commences with the align-\nment of multimodal elements between the visual encoder and the large language model, followed\nby the instruction tuning through our constructed dataset.\nOpen-World Benchmarks.\nWe carry out extensive experiments to demonstrate that our pro-\nposed Steve-Eye outperforms LLM-based agents in open-world setups. Specifically, we develop the\nfollowing benchmarks to evaluate agent performance from a broad range of perspectives: (1) envi-\nronmental visual captioning (ENV-VC), which assesses an agent’s capacity to perceive and describe\nits surroundings effectively; (2) foundational knowledge question answering (FK-QA), which eval-\nuates the proficiency in mastering basic knowledge crucial for an agent’s decision-making; (3) skill\nprediction and planning (SPP), which quantifies an agent’s capability to act and plan strategically.\n2\nPreprint\n2\nRELATED WORK\n2.1\nOPEN-WORLD EMBODIED AGENTS WITH LLMS\nThe rapid progress of large language models (Brown et al., 2020; Raffel et al., 2020; Zhang et al.,\n2022; Chowdhery et al., 2022) has significantly boosted their capacity to encode a wide range of\nhuman behaviors within training data (Bommasani et al., 2021). When equipped with narrowly de-\nsigned prompts, LLM-based agents exhibit the capability to generate executable plans for tasks such\nas indoor robot manipulation. For instance, SayCan (Ahn et al., 2022) integrates skill affordances\nwith LLMs to yield actionable plans, while Palm-E (Driess et al., 2023) takes a step further by\nconstructing hierarchical agents capable of handling multimodal prompts. This approach has also\nproven its efficacy in open-world environments (Huang et al., 2022a; Li et al., 2022). In contrast\nto robot manipulation, agents in the wild require a heightened level of real-time situational aware-\nness and foundational knowledge to execute intricate skill plans across a diverse array of tasks. To\nsimulate human behaviors in such open worlds, Generative Agents (Park et al., 2023) store agents’\nexperiences and retrieve these memories to generate plans in a text-based sandbox game.\nIn recent years, the 3D sandbox Minecraft has received considerable attention owing to its remark-\nably flexible game mechanics to serve as a prominent open-world benchmark (e.g., MineRL (Guss\net al., 2019) and Minedojo (Fan et al., 2022)). DEPS (Wang et al., 2023b) introduces the descrip-\ntor, explainer, and selector for plan generation with the help of LLM. Plan4MC (Yuan et al., 2023)\nconstructs a skill graph and proposes a skill search algorithm to minimize planning errors. Voy-\nager (Wang et al., 2023a) proposes an LLM-powered lifelong learning agent that continually ex-\nplores the Minecraft world. Similar to (Park et al., 2023), GITM (Zhu et al., 2023) integrates LLMs\nwith text-based memory and knowledge to create generic agents in Minecraft. Among these studies,\nVoyager (Wang et al., 2023a) and GITM (Zhu et al., 2023) lean entirely on text descriptions of the\nenvironment to act and plan, while Plan4MC (Yuan et al., 2023) and DEPS (Wang et al., 2023b)\nhave visual-input skills but still rely on merely text for planning. None of them try to understand the\nrich visual observation provided natively by Minecraft. In contrast to these works, our work trains a\nlarge multimodal model to fill this gap.\n2.2\nLARGE MULTIMODAL MODELS (LMMS)\nIn comparison to LLMs, large multimodal models (LMMs) (Awadalla et al., 2023) encompass\na broad range of information beyond text modality, which can be categorized into two primary\nstreams. The first category (Gupta & Kembhavi, 2023; Huang et al., 2023a; Patil et al., 2023; Sur´ıs\net al., 2023) involves hinging on ChatGPT (OpenAI, 2022) or GPT-4 (OpenAI, 2023) to generate\nin-context responses without parameter tuning. However, these approaches heavily rely on the avail-\nability of an LLM’s API and the quality of the designed prompts. The second category comprises\nend-to-end pre-trained models. Within this category, models such as Huang et al. (2023b); Peng et al.\n(2023) are trained entirely from scratch. Conversely, some research explores efficient fine-tuning us-\ning pre-trained LLMs by incorporating lightweight modality encoders, such as Qformer (Li et al.,\n2023) or Perceiver (Alayrac et al., 2022). Recently, Liu et al. (2023) propose to explicitly instruction-\ntune a LLM using vision-language instruction data.\nIn this work, we propose Steve-Eye by building upon pre-trained LLMs, aiming to develop an open-\nworld agent powered by a large-scale model with versatile multimodal I\/O capabilities.\n3\nMETHODOLOGY\nIn this section, we first provide our instruction-following dataset to develop three key functions for\nthe agent’s open-world interaction in Section 3.1. We then propose our large multimodal agent\nSteve-Eye in Section 3.2, and clarify details of the training procedure in Section 3.3. We adopt\nMinecraft as our open-ended platform in this paper to collect data and validate the model, anticipat-\ning to explore a broader range of environments for Steve-Eye in future studies.\nTo empower an agent with the self-driven capacity to act and plan in an open world, we posit that the\nfollowing embodied functions are indispensable: (1) multimodal perception function which offers\na detailed description of the agent status and environmental features; (2) foundational knowledge\n3\nPreprint\nbase which imparts an understanding of how the world works and conveys crucial basic knowledge\nrelated to skills and tasks; (3) skill prediction and planning which is responsible for generating\nskill execution feedback (e.g., success or failure) and crafting high-level skill plans for handling\nmore complex and long-horizon tasks. We develop these functions by building the corresponding\ninstruction dataset to pre-train Steve-Eye as follows.\n3.1\nOPEN-WORLD INSTRUCTION-FOLLOWING DATASET\nMultimodal Perception Instructions.\nHuman players can perform actions in Minecraft mainly\nrelying on their visual perception, without any prior hints or imposed game judgments. In order to\nendow Steve-Eye with the same ability, it is required to provide it with comprehensive visual descrip-\ntions of the environment. To achieve this, we use Minedojo (Fan et al., 2022) to obtain Minecraft\nsnapshots which contain a wide array of details within the agent’s surroundings, including environ-\nmental features, the agent’s life and food status, inventory items, and equipment, as illustrated in\nFigure 2. In addition, we leverage MaskCLIP (Zhou et al., 2022) to identify the in-sight objects of\nthese snapshots without supervised annotations. During our data collection process, for each snap-\nshot I and its corresponding description XC, we initiate a three-step approach. Firstly, we prompt\nChatGPT to curate a list of 40 instructions as shown in Figure 6 in Appendix A.1.1. Then we enrich\nsnapshot details as dense caption to describe its content, with the assistance of ChatGPT. Finally,\nwe select an instruction XQ randomly from the list and combine it with the snapshot’s caption to\ncreate a single-round multimodal description pair (e.g., ### Human: XQ I\\n ### Embodied Agent:\nXC\\n.). By doing so, we collect 200K instructional pairs for multimodal perception learning.\n[Description] Steve is walking in the forest and\nhe can’t see the sky. There are trees in front of\nhim. He still has 20.0 life and 20.0 food. He is\nnow equipped with the iron pickaxe and the\ninventory contains 5 pickaxes, 6 ……\n•\nenvironment\n•\nlife and food\n•\nobject in sight\n•\ninventory and \nequipment\nFigure 2. Multimodal perception\nFoundational Knowledge Instructions.\nEmbodied agents\nrequire a foundation of essential knowledge to facilitate action-\ntaking and skill planning.\nIn Minecraft, such knowledge\nshould contain item recipes, details of item attributes, their as-\nsociated numerical value, etc. We access this vital information\nfrom Minecraft-Wiki (Fandom, 2023), which comprises an ex-\ntensive collection of over 9,000 HTML pages. To be specific,\nwe first obtain all item icons from Minecraft-Wiki and gener-\nate 200K icon inventory images, as illustrated in Figure 3 (a).\nEach icon image corresponds to a 4-row table with an associ-\nated caption adhering to a standardized template: “There is a\nMinecraft inventory with 4 rows. From left to right, they are\n...”. As shown in Figuire 7 in Appendix A.1.2, we curate a set of 20 distinct prompts designed to\nchallenge the model’s ability to recognize items. Subsequently, we further collect all recipe-related\ninformation from the Wiki as illustrated in Figure 3 (b), and design similar prompt templates to for-\nmulate 10,000 recipe-image instructional pairs. Lastly, we process the Wiki and utilize this corpus\nto produce 40,000 single-round question-answer pairs. In total, we collect a high-quality dataset\nwith 250K foundational knowledge instructions.\n(a) Item icons\n(b) recipes\nFigure 3. Icons and recipes\nSkill-related Interaction Instructions.\nThe environmen-\ntal description and foundational knowledge serve as prerequi-\nsites for an agent’s interaction within the open world. How-\never, a successful interaction requires more than these ele-\nments alone. It relies upon the mastery of basic skills, such\nas log, harvesting, and food preparation, as well as high-level\nskill planning abilities to tackle complex, long-horizon tasks,\nsuch as crafting an iron pickaxe. To facilitate this, we gather\ncorresponding training data for skill prediction and planning,\nwhich enables our model to provide correct feedback on both\nbasic skills and long-horizon tasks across a spectrum of agent\nor environmental conditions. Specifically, the data collection\nprocess involves two steps. First, we sample skill trajecto-\nries based on the pre-trained basic skill policies and collect\n200K snapshot pairs with corresponding statuses from these\ntrajectories. Each snapshot pair {I0, It} denotes the 0-th and\nt-th timestamp of the skill trajectory. Next, we employ Chat-\nGPT to generate question-answer pairs about diverse aspects of skill execution status. These ques-\n4\nPreprint\nHow to craft a wooden axe? \nAnswer me via a recipe image.\nWhat are the ingredients required \nto craft a detector rail?\nProvide a detailed description of \nthe given Minecraft image <image>. \n…\n…\n<image> \nMake a plan list to finish the task \nof <image> in Minecraft. \n<image> \nIron Ingot, Stone Pressure \nPlate, and Redstone Dust.\nFrom left to right, there are 1 \nshears in the inventory, which \nmeans he is equipped with 1 \nshears. Steve still has 20.0 life \nand 20.0 food.\nThe executable plan can be: (1) \nfind log nearby; (2) crafting log; \n(3) crafting planks; (4) crafting \nstick.\nvisual \ntokenizer\ntext \ntokenizer\nprojector\nmultimodal input\n…\n…\nmultimodal output\nvisual generation\nquestion-answering\nvisual captioning\nskill planning\n<image>\nLarge Language Model\nFigure 4. Illustration of Steve-Eye: a large multimodal model designed to seamlessly process both\nvisual and language inputs. Steve-Eye excels in acquiring fundamental knowledge of the world it\nlives in, understanding the nuances of its surroundings, and generating executable plans to complete\na wide array of open-ended tasks. Furthermore, Steve-Eye responds to user instructions through\neither visual or text-based cues, enhancing the convenience and flexibility of human-AI interaction.\ntions delve into whether the agent completes the skill, encounters unexpected failures, or seeks\nexplanations for such failures. More details can be found in Appendix A.1.3. Second, we sam-\nple 40K task trajectories using the planner in Yuan et al. (2023), each of which can be denoted as\nT = {s1, s2, ...sT} representing the task is finished via a T-round planning procedure, where si is\nthe skill plan for i-th round. At each round i, we feed our model with its start snapshot and task\ninitialization, and curate instructional questions to inquire about si with reasonable explanation. In\nthis manner, we obtain 200K instructional pairs from task trajectories.\n3.2\nMODEL ARCHITECTURE\nFigure 4 illustrates the overall architecture of our proposed model. Steve-Eye, functioning as a\ngenerative model, connects an image-oriented tokenizer fv with the pre-trained LLM backbone Θ.\nWe adopt the image tokenizer, e.g., VQ-GAN (Esser et al., 2021), to encode the raw images I into\ntoken embeddings V = {v1, v2, ..., vn} ∈Rn×d, where n denotes the number of visual tokens and\nd is the dimensionality of each token. We further utilize a lightweight projection module fl with\na trainable projection matrix W. This module maps the visual tokens to the same space with text\nembeddings, yielding ˆV = {ˆv1, ˆv2, ..., ˆvn} ∈Rn× ˆd:\n  \\ hat  {\\ma t h cal {V}} = W \\mathcal {V}; \\hspace {0.3em} \\text {where} \\hspace {0.3em} \\mathcal {V} = f_v(I). \n(1)\nTo effectively process visual-language inputs and generate corresponding outputs, our model inte-\ngrates the visual codebook Cv into the pre-existing language vocabulary Cl. This integration leads\nto the formation of a unified multimodal codebook, denoted as Cm = Cv ∪Cl. Additionally, in\norder to mark the starting and ending points of visual elements in I\/O sequences, we introduce two\nspecial tokens, namely <vis> and <\/vis>. The LLM backbone Θ of our Steve-Eye is built upon a\ndecoder-only architecture with casual transformers. Our model employs an auto-regressive predic-\ntion mechanism, generating responses based on the provided multimodal input tokens. The resulting\nresponse is a mixed sequence of visual and textual tokens, represented as Y = {y1, y2, ..., ym}. For\neach embedding yi, we pass it through a linear layer fp followed by a softmax operation, mapping\nit into a probability distribution of the multimodal vocabulary. The final prediction for the i-th token\nzi is determined by selecting the token from the multimodal codebook with the highest score:\n  z _i = \\argmax ( \\text {softmax}(f_p(y_i))). \n(2)\n3.3\nTRAINING\nEach instruction-following instance can be formulated as a multi-round conversation {X 1\nQ, X 1\nC, ...,\nX N\nQ , X N\nC }, where each {X i\nQ, X i\nC} represents a question-answer interaction between a human and\n5\nPreprint\nthe embodied agent and N indicates the total number of rounds in the conversation. The entire\ninstructional dataset follows this unified template, as demonstrated in Figure 11 in Appendix A.1.3.\nTo efficiently train our model, we employ the negative log-likelihood objective over the prediction\ntokens with instruction tuning:\n  \\m a t\nh\nc\nal \n{L} (\\Theta )=-\\sum _{j=1}^{L} \\log P_{\\Theta }(y_j|\\mathcal {I}, \\hat {y}_{1:j-1}), \n(3)\nwhere y and ˆy respectively denote the input and target token sequences, with Θ representing the\nmodel parameters, and L representing the length of the target sequence. The input visual content\nI may represent an empty image depending on the input instruction. It is worth noting that we\nconstrain the loss computation to only consider the answer tokens XC. This constraint prevents\ntraining from becoming excessively straightforward and ensures that the model’s primary focus is\non learning to precisely generate coherent responses. Similar to Liu et al. (2023), we adopt a two-\nstage instruction-tuning strategy to train our model:\nTwo-Stage Instruction-Tuning. (1) Multimodal feature alignment: In the first stage, our pri-\nmary objective is to align visual features with the language token space. In order to strike a bal-\nance between efficient tuning and a comprehensive coverage of the world’s concepts, we curate our\nopen-ended instruction dataset to 600K snapshot-text pairs. These pairs are then transformed into\ninstruction-following data as described in Section 3.1. During the feature alignment stage, we main-\ntain the visual encoder and the LLM parameters in a frozen state, exclusively training the projection\nmodule. Additionally, this training phase involves fine-tuning token embeddings to accommodate\nthe newly introduced visual codebook and two special tokens <vis> and <\/vis>. (2) End-to-end\ninstruction tuning: In the second stage, we continue to keep the visual encoder frozen while concur-\nrently training the projection module and LLM. This second stage leverages the entire open-ended\ninstructions and contributes significantly to enhancing the model’s capability of comprehending and\neffectively responding to complex multimodal instructions.\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETUP\nImplementation Details.\nIn this paper, we use the LLaMA-2 model (Touvron et al., 2023b)\nas the LLM backbone. Additionally, we use CLIP (Radford et al., 2021) as our visual encoder\nto achieve the best performance for non-visual generative tasks, and use VQ-GAN (Esser et al.,\n2021) as the default visual tokenizer for visual generation. The size of visual codebook Cv and\nlanguage vocabulary is 8192 and 32000, respectively. In addition, we add <vis> and <\/vis> to the\nfinal unified codebook, indicating the starting and ending points of visual content. Similar to Liu\net al. (2023), we construct 850K instruction-answer pairs for model training. Note that the model\nis trained to predict the agent’s answer, and thus only sequence\/tokens of answer will be used to\ncompute the loss in the auto-regressive model. We also adopt LoRA (Hu et al., 2021) to reduce the\ncomputational cost for efficient tuning. We choose MineDojo (Fan et al., 2022) as the Minecraft\nplatform to collect our instruction data and conduct experiments. Following Yuan et al. (2023),\nwe use the environments of programmatic tasks to train basic policies with RL. These policies are\ntrained to execute corresponding skills and keep fixed in all testing tasks.\nEvaluation Benchmarks.\nWe conduct experiments on three benchmarks to evaluate an agent’s\ninteraction ability in an open world. (1) Environmental visual captioning (ENV-VC): given a\nsnapshot, the model is asked to describe the agent’s current status and environmental features from\ndiverse aspects (e.g., life, food...). We evaluate the prediction’s accuracy of each aspect by ex-\ntracting corresponding answers from the output description to compare with the groundtruth. (2)\nFoundational knowledge question answering (FK-QA): to assess the model’s grasp of essential\nknowledge, we collect a set of 10,000 Minecraft-related questions from different sources, including\nthe Wiki pages, Wiki tables, and Minecraft recipes. The performance is measured by the model’s\nability to provide correct answers to these questions. (3) Skill prediction and planning (SPP): we\nutilize our proposed Steve-Eye to predict whether a skill has been successfully completed and assert\nits capability to generate executable high-level skill plans for long-horizon tasks.\n6\nPreprint\nTable 1. Comparisons of different model settings on the environmental visual caption benchmark.\nThe experiments are conducted on 20K ENV-VC test set.\nModel\nvisual encoder\ninventory\nequip\nobject in sight\nlife\nfood\nsky\nBLIP-2\nCLIP\n41.6\n58.5\n64.7\n88.5\n87.9\n57.6\nLlama-2-7b\n-\n-\n-\n-\n-\n-\n-\nSteve-Eye-7b\nVQ-GAN\n89.9\n78.3\n87.4\n92.1\n90.2\n68.5\nSteve-Eye-13b\nMineCLIP\n44.5\n61.8\n72.2\n89.2\n88.6\n68.2\nSteve-Eye-13b\nVQ-GAN\n91.1\n79.6\n89.8\n92.7\n90.8\n72.7\nSteve-Eye-13b\nCLIP\n92.5\n82.8\n92.1\n93.1\n91.5\n73.8\nTable 2. Comparisons of different data configurations on the environmental visual captioning bench-\nmark, where “snapshot desc.” denotes the 200K multimodal perception instruction dataset.\ninventory\nequip\nobject in sight\nlife\nfood\nsky\nno instruction tuning\n22.7\n24.3\n39.8\n81.2\n80.4\n61.1\nw\/o snapshot desc.\n46.2 (+23.5)\n40.9 (+16.6)\n41.2 (+1.4)\n83.0 (+1.8)\n82.4 (+2.0)\n63.3 (+2.1)\nw\/o icon images\n52.3 (+29.6)\n48.1 (+23.8)\n91.4 (+51.6)\n92.5 (+11.3)\n90.9 (+10.5)\n73.5 (+12.4)\nfull data\n92.5 (+69.8)\n82.8 (+58.5)\n92.1 (+52.3)\n93.1 (+11.9)\n91.5 (+11.1)\n73.8 (+12.7)\n4.2\nENVIRONMENTAL VISUAL CAPTIONING (ENV-VC)\nWe introduce this evaluation protocol for asserting Steve-Eye’s multimodal perception function,\nwhich serves as an initial stride toward comprehensive evaluation of large multimodal models.\nSpecifically, we collect 20,000 Minecraft snapshots (named ENV-VC test set) using Minedojo and\napply the proposed data generation pipeline to create six questions for each snapshot, resulting in a\ntotal of 120K questions. These six questions pertain to the prediction of various aspects, including\ninventory items\n, equipment\n, objects in sight\n, life\n, food\n, and the visibility of sky\n.\nDuring the inference phase, Steve-Eye predicts answers based on these questions and the input\nsnapshot. Experimental results are presented in Table 1 and Table 2. As shown in Table 1, our\nvisual encoder, when combined with multimodal instruction tuning, significantly enables the ability\nof the text-only language model LLM (Llama-2-7b) to comprehend the contents of the snapshots\n(Steve-Eye-7b). Notably, Steve-Eye outperforms BLIP-2 by a substantial margin due to the im-\nproved reasoning ability enabled by the larger LLM. Furthermore, the visual encoder plays a crucial\nrole in facilitating multimodal understanding. Surprisingly, the model equipped with CLIP (Radford\net al., 2021) surpasses the performance of the model using MineCLIP (Fan et al., 2022), achieving\nover +48.9%, +21.0% and +19.9% improvements in inventory, equipment, and object-in-sight pre-\ndictions, respectively. We attribute this performance difference to the fact that MineCLIP does not\nprioritize fine-grained alignment during pre-training, despite being exposed to a diverse range of\nMinecraft videos. In summary, Steve-Eye’s ability to comprehend visual cues from its surroundings\nlays the foundation for subsequent interactions with the world.\nTo investigate the effictiveness of various types of instructional data for multimodal perception, we\ncarry out experimental comparisons with diverse data configurations in Table 2. First, our results\nshowcase a significant improvement in the model’s capacity to respond to instructional questions\nthrough instruction tuning, which leads to impressive gains of over +50% for inventory, equipment,\nand object-in-sight prediction. Furthermore, the inclusion of the multimodal perception dataset and\nicon images in the training data both contribute to a substantial improvement in the model’s overall\nperformance. Ultimately, the best results are achieved when combining all available data sources.\n4.3\nFOUDATIONAL KNOWLEDGE QUESTION ANSWERING (FK-QA)\nFollowing Team (2022), we establish a question database specialized to assess our model’s pro-\nficiency in generating responses pertaining to fundamental Minecraft knowledge. This evaluation\nis carried out through a validation dataset known as the FK-QA test set, which is further divided\ninto two distinct subsets: TEXT and IMG. In the FK-QA TEXT subset, we generate a collection\nof 10,000 question-answer pairs curated from various sources, including the Minecraft-Wiki pages,\nMinecraft-Wiki tables, and Minecraft recipes. Each category comprises 2,000, 5,000, and 3,000\npairs, respectively. Upon receiving a response from Steve-Eye, we feed both the generated response\n7\nPreprint\nTable 3. Comparisons on FK-QA test set of the foundational knowledge question answering bench-\nmark. The evaluation metrics consider both the scoring and accuracy dimensions simultaneously.\nScoring\nAccuracy\nWiki Page\nWiki Table\nRecipe\nTEXT All\nTEXT\nIMG\nLlama-2-7b\n6.90\n6.21\n7.10\n6.62\n37.01%\n-\nLlama-2-13b\n6.31 (-0.59)\n6.16 (-0.05)\n6.31 (-0.79)\n6.24 (-0.38)\n37.96%\n-\nLlama-2-70b\n6.91 (+0.01)\n6.97 (+0.76)\n7.23 (+0.13)\n7.04 (+0.42)\n38.27%\n-\ngpt-turbo-3.5\n7.26 (+0.36)\n7.15 (+0.94)\n7.97 (+0.87)\n7.42 (+0.80)\n41.78%\n-\nSteve-Eye-7b\n7.21 (+0.31)\n7.28 (+1.07)\n7.82 (+0.72)\n7.54 (+0.92)\n43.25%\n62.83%\nSteve-Eye-13b\n7.38 (+0.48)\n7.44 (+1.23)\n7.93 (+0.83)\n7.68 (+1.06)\n44.36%\n65.13%\nand the corresponding groundtruth answer to ChatGPT. ChatGPT will first examine the accuracy of\nthe response as a measure of answer correctness. To minimize variability in error, ChatGPT con-\nducts a further evaluation, considering the response’s accuracy, relevance, and level of detail. This\ncomprehensive evaluation yields an overall score on a scale ranging from 0 to 10, where a higher\nscore signifies superior overall performance. In the FK-QA IMG subset, we shift our focus to visual\ngeneration by employing 3,000 recipe images as groundtruth data. Here, our model is tasked with\ngenerating visual outputs for each item within the recipe inventory, following a specific order. The\nvisual output is considered correct only if every element of the recipe is accurately generated. We\nadopt this metric to assert our model’s ability to produce multimodal feedback.\nTable 3 presents both scoring and accuracy results. It’s worthy to note that Llama-2 exhibits consis-\ntent performance regardless of the model’s scale, with Llama-2-70b only marginally outperforming\nthe 7b-version by +1.26% in accuracy, meanwhile 13b-version performs even worse than 7b-version\non the scoring results. We hypothesize that this phenomenon can be attributed to distinct variations\nin difficulty levels encountered within our FK-QA test set. Llama-2 fails to answer correctly for the\nchallenging part regardless of its size due to essential knowledge missing. In contrast, Steve-Eye\noutperforms both Llama-2 and gpt-turbo-3.5, despite its considerably smaller scale. Furthermore,\nour model exhibits a more substantial improvement in responding to Recipe and Wiki Table ques-\ntions as compared to Wiki Page questions. This disparity can likely be attributed to the fact that\nWiki Page contains a large proportion of invalid questions (e.g., version, history), whereas Recipe\nand Wiki Table predominantly feature knowledge-related inquiries. Such result further validates the\neffectiveness of our approach in acquiring foundational knowledge. Unlike text-only LLMs, our\nmodel exhibits considerable ability to output visual contents, which achieves 65.13% accuracy on\nFK-QA IMG using the 13b-version. The multimodal generation ability enables Steve-Eye to better\nserve as an assistant for potential needed people such as beginners of this game. We show more\ndetails and cases in Appendix A.3.\n4.4\nSKILL PREDICTION AND PLANNING (SPP)\nSkill Prediction.\nSimilar to Section 3.1, we collect another 20K snapshot pairs in the form of\n{I0, It} from skill trajectories (referred to as Skill-Pred test). These pairs are input into our model\nto query the current execution status of the skill. The execution status can fall into one of three cate-\ngories: success, failure, and running, with “running” signifying that the skill is currently in progress.\nTable 4.\nRecall\/Accuracy results on Skill-Pred\ntest set for the skill prediction benchmark.\nrunning (%) success (%)\nfail (%)\nBLIP-2\n65.2\/58.8\n49.8\/54.3\n42.1\/51.8\nSteve-Eye-7b\n89.8\/82.5\n77.6\/81.4\n74.2\/79.9\nSteve-Eye-13b\n92.1\/84.2\n80.5\/83.1\n76.8\/81.5\nAs shown in Table 4, our model exhibits com-\nmendable performance in skill status predic-\ntion.\nHowever, the performance is still far\nfrom enough to completely replace the rule-\nbased game judgment adopted by the existing\nRL-based skill agents. These experiments indi-\ncate that, despite the excellent multimodal un-\nderstanding capabilities of our model in open-\nworld environments in previous experiments, it\nstill falls short in fine-grained reasoning tasks that involve consecutive frames to some extent.\nSkill Planning.\nFollowing Yuan et al. (2023), we carry out evaluation on 24 difficult tasks in\nMinecraft. These tasks can be categorized into three types: cutting trees to craft primary items (7),\nmining cobblestones to craft advanced items (7), and interacting with mobs to harvest food and ma-\nterials (10). Each task is tested for 30 episodes, where an episode refers to a multi-round interaction\n8\nPreprint\nTable 5. Comparisons on the skill planning benchmark. We test the mean success rates of all tasks,\nwhere each task is executed for 30 episodes using the same seeds for initialization.\nModel\nMineAgent\n0.00\n0.03\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.21\n0.0\n0.05\n0.0\ngpt assistant\n0.30\n0.17\n0.07\n0.00\n0.03\n0.00\n0.20\n0.00\n0.20\n0.03\n0.13\n0.00\n0.10\n0.00\nSteve-Eye-auto\n0.30\n0.27\n0.37\n0.23\n0.20\n0.17\n0.26\n0.07\n0.13\n0.17\n0.20\n0.33\n0.00\n0.13\nSteve-Eye\n0.40\n0.30\n0.43\n0.53\n0.33\n0.37\n0.43\n0.30\n0.43\n0.47\n0.47\n0.40\n0.13\n0.23\nModel\nMineAgent\n0.46\n0.50\n0.33\n0.35\n0.0\n0.0\n0.06\n0.0\n0.0\n0.0\ngpt assistant\n0.57\n0.76\n0.43\n0.30\n0.00\n0.00\n0.37\n0.00\n0.03\n0.00\nSteve-Eye-auto\n0.70\n0.63\n0.40\n0.30\n0.17\n0\n0.37\n0.03\n0.07\n0.00\nSteve-Eye\n0.73\n0.67\n0.47\n0.33\n0.23\n0.07\n0.43\n0.10\n0.17\n0.07\nprocess. At each round, the model receives the environmental feedback from the last round, plans\na skill list based on the current status, and then picks up the top skill to execute. For each task\nepisode, we set a maximum step between [3000, 10000]. In our evaluation, we compare Steve-Eye\nagainst two baseline approaches: (1) MineAgent (Fan et al., 2022), which completes tasks without\ndecomposing them into basic skills, and uses PPO and self-imitation learning with CLIP reward,\nand (2) GPT Assistant, which employs ChatGPT as a high-level planner to generate skill plans by\nprompting itself with information from the environment and the agent’s status. The results in Ta-\nble 5 demonstrate that Steve-Eye significantly outperforms both baseline methods. Additionally, we\nconduct experiments in which Steve-Eye takes over the skill prediction function from the rule-based\ngame judgment in Minecraft. This self-driven variant is referred to as ‘Steve-Eye-auto.’ Since the\nmodel’s skill prediction is not always 100% accurate, Steve-Eye-auto does experience some perfor-\nmance degradation when compared to Steve-Eye. This degradation is more pronounced in longer,\ncomplex tasks (e.g.,\n,\n,\n) as opposed to short-term tasks (e.g.,\n,\n,\n). Nevertheless,\nSteve-Eye-auto still demonstrates significant performance improvements in most tasks, compared to\nthe baselines. For additional details about this benchmark, please refer to Appendix A.2.\nFor better visualization, we provide a qualitative example of Steve-Eye completing the task “crafting\nstone axe with wooden pickaxe” as shown in Figure 5.\nfind cobblestone\nharvest cobblestone\nfind trees\nharvest log\ncraft planks\ncraft and place table\ncraft stone axe\nbirthplace\nFigure 5. Snapshots of a qualitative example, illustrating how Steve-Eye completes the task of\n“crafting a stone axe with a wooden pickaxe.” Our model generates a skill plan at each interaction\nround and selects the top skill from the plan list for execution.\n5\nCONCLUSION\nIn this paper, we explore enabling a large multimodal model to serve as a generative embodied\nagent in open worlds. We achieve this goal by proposing Steve-Eye, which combines the text-only\nlanguage model with a visual encoder, allowing for a multimodal I\/O interface to interact with the\nenvironment. With the help of ChatGPT, we curate questions to generate 850K instruction-following\ndata to facilitate the agent’s multimodal perception fuction, foundational knowledge mastery, as well\nas the capability of skill prediction and planning. Experiments on three open-world benchmarks\nverify the advantages of our Steve-Eye over a wide range of perspectives.\n9\nPreprint\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–\n23736, 2022.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\nOpenflamingo:\nAn open-\nsource framework for training large autoregressive vision-language models.\narXiv preprint\narXiv:2308.01390, 2023.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-\nnities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multi-\nmodal language model. arXiv preprint arXiv:2303.03378, 2023.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recogni-\ntion, pp. 12873–12883, 2021.\nDaniel Fallman. Design-oriented human-computer interaction. In Proceedings of the SIGCHI con-\nference on Human factors in computing systems, pp. 225–232, 2003.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:\n18343–18362, 2022.\nFandom. Minecraft wiki. https:\/\/minecraft.fandom.com\/wiki, 2023.\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 14953–14962, 2023.\nWilliam H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\narXiv preprint arXiv:1907.13440, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nRongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,\nZhiqing Hong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating speech,\nmusic, sound, and talking head. arXiv preprint arXiv:2304.12995, 2023a.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv,\nLei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045, 2023b.\n10\nPreprint\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In International Conference on\nMachine Learning, pp. 9118–9147. PMLR, 2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022b.\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt\nDeitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment\nfor visual ai. arXiv preprint arXiv:1712.05474, 2017.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-\nimage pre-training with frozen image encoders and large language models.\narXiv preprint\narXiv:2301.12597, 2023.\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang,\nEkin Aky¨urek, Anima Anandkumar, et al. Pre-trained language models for interactive decision-\nmaking. Advances in Neural Information Processing Systems, 35:31199–31212, 2022.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\nOpenAI. Chatgpt. https:\/\/openai.com\/blog\/chatgpt, 2022.\nOpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023.\nJoon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\narXiv:2304.03442, 2023.\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model\nconnected with massive apis. arXiv preprint arXiv:2305.15334, 2023.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824, 2023.\nJenny Preece, Yvonne Rogers, Helen Sharp, David Benyon, Simon Holland, and Tom Carey.\nHuman-computer interaction. Addison-Wesley Longman Ltd., 1994.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Tor-\nralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 8494–8502, 2018.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.\nManolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain,\nJulian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied\nai research. In Proceedings of the IEEE\/CVF international conference on computer vision, pp.\n9339–9347, 2019.\nD´ıdac Sur´ıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128, 2023.\nVicuna\nTeam.\nVicuna:\nAn\nopen-source\nchatbot\nimpressing\ngpt-4\nwith\n90quality.\nhttps:\/\/vicuna.lmsys.org\/, 2022.\n11\nPreprint\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023a.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents.\narXiv preprint arXiv:2302.01560, 2023b.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv\npreprint arXiv:2303.16563, 2023.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068, 2022.\nChong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In European\nConference on Computer Vision, pp. 696–712. Springer, 2022.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n12\nPreprint\nA\nAPPENDIX\nIn this appendix, we offer a detailed introduction of the construction of our open-world instruc-\ntion dataset, as outlined in Appendix A.1, including (1) multimodal perception instructions, (2)\nfoundational knowledge instructions, (3) skill-related interaction instructions, and (4) template of\ninstructional training data. Furthermore, we delve into the skill planning benchmark and its associ-\nated task setups in Appendix A.2. In Appendix A.3 we present qualitative cases that illustrate our\nmodel’s ability to provide intuitive visual feedback and serve as an intelligent chatbot with a multi-\nmodal input-output interface. Finally, we explore the potential applications of our model in diverse\nenvironments, such as Virtual Home (Puig et al., 2018).\nA.1\nDATASET\nA.1.1\nMULTIMODAL PERCEPTION INSTRUCTIONS\nThis dataset contains 200K instructional pairs. Figure 6 illustrates a partial listing of instructional\nquestions employed for describing the content of the Minecraft snapshots. These instructions convey\nsimilar meanings, albeit with slight variations in natural language.\n•\n\"Describe the following Minecraft image in detail\",\n•\n\"Provide a detailed description of the given Minecraft image\",\n•\n\"Give an elaborate explanation of the Minecraft game image you see\",\n•\n\"Share a comprehensive rundown of the presented Minecraft image\",\n•\n\"Offer a thorough analysis of the Minecraft frame\",\n•\n\"Explain the various aspects of the Minecraft image before you\",\n•\n\"Examine the Minecraft image closely and share its details\",\n•\n\"Write an exhaustive depiction of the given Minecraft image“\n•\n\"Clarify the contents of the displayed Minecraft image with great detail\",\n•\n\"Narrate the contents of the Minecraft image with precision\"\nFigure 6. 10 instruction examples for multimodal perception instructions.\nA.1.2\nFOUNDATIONAL KNOWLEDGE INSTRUCTIONS\nThe dataset comprises 250K training instances, which is organized into three distinct subsets: 200K\nicon image instructions, 10K recipe image instructions, and 40K Minecraft-Wiki corpus instructions.\nFor the icon images, we generate questions aimed at prompting the model to recognize and describe\nitem icons within the inventory, as depicted in Figure 7. Similarly, we curate instructional questions\nfor recipe images as shown in Figure 8, with the objective of extracting information on completing\nspecific recipes. In addition, we preprocess the raw Minecraft-Wiki HTML pages by removing\nirrelevant information (e.g., reference links) and unresolved data, transforming the raw corpus into\na formatted, clean Markdown version. Leveraging the capabilities of ChatGPT, we employ this\npowerful language model to generate 10 questions, each with its corresponding answer, for every\npage of the cleaned Wiki corpus. This process yields a collection of 40K single-round question-\nanswer pairs, which can be utilized for instruction tuning.\n13\nPreprint\n•\n\"Provide a brief description of the given recipe image.\"\n•\n\"Offer a succinct explanation of the recipe picture presented.\"\n•\n\"Summarize the recipe content about icons of the image.\"\n•\n\"Give a short and clear explanation of the subsequent recipe image.\"\n•\n\"Share a concise interpretation of the recipe image provided.\"\n•\n\"Present a compact recipe description of the photo's key features.\"\n•\n\"Relay a brief, clear account of the recipe picture shown.\"\n•\n\"Render a clear and concise recipe summary of the photo.\"\n•\n\"Write a terse but informative recipe summary of the picture.\"\n•\n\"Create an icon narrative representing the recipe image presented.\"\nFigure 8. 10 instruction examples of recipe image for foundational knowledge instructions.\n•\n\"Clarify the contents of the displayed inventory image with great attention to detail.“\n•\n\"Characterize the inventory image with a meticulously detailed description.“\n•\n\"Break down the individual slot elements within the inventory image with precision.“\n•\n\"Take a step-by-step journey through the important details of the Minecraft inventory image.“\n•\n\"Paint a vivid and descriptive narrative of the Minecraft inventory image.“\n•\n\"Provide a precise narration of the contents within the Minecraft inventory image.“\n•\n\"Thoroughly analyze the Minecraft inventory image in a comprehensive and detailed manner.“\n•\n\"Illustrate the Minecraft inventory image through a descriptive and informative explanation.“\n•\n\"Examine the Minecraft inventory image closely and share its intricate details.“\n•\n\"Compose an exhaustive depiction of the given Minecraft inventory image.\"\nFigure 7. 10 instruction examples of icon images for foundational knowledge instructions.\nA.1.3\nSKILL-RELATED INTERACTION INSTRUCTIONS\nFor skill prediction, we utilize the skill policies trained by Yuan et al. (2023) to create a dataset\ncomprising 200K skill trajectories. In each trajectory, we extract timestamps from the initial and\nt-th points to generate a snapshot pair, denoted as {I0, It}. We then construct questions aimed at\ndetermining whether the agent successfully executed the skill or, in the case of failure, identifying\nthe underlying reasons for the unsuccessful attempt. Illustrative examples of these skill prediction\nquestions are provided in Figure 9. We also provide examples with snapshot pairs in Figure 10.\n14\nPreprint\n•\n\"\"Steve is demonstrating his proficiency in {SKILL_NAME}, with the objective of achieving {SKILL_DEFINITION}. We'll now assess \nboth the initial and current frames to determine if he has successfully executed the skill and the reasons behind it:\"\n•\n\"\"The skill Steve is performing is {SKILL_NAME}, and its intended outcome is to {SKILL_DEFINITION}. To determine whether Steve \nhas accomplished this skill, we need to analyze both the starting and current frames:“\n•\n\"\"Steve is currently engaged in executing {SKILL_NAME}, aiming to achieve {SKILL_DEFINITION}. In order to evaluate his success in \nperforming this skill, we'll examine both the initial frame and the current frame:“\n•\n\"\"The task at hand for Steve involves mastering {SKILL_NAME}, with the ultimate goal of accomplishing {SKILL_DEFINITION}. To \nascertain whether he has successfully completed this skill, we'll analyze both the starting and current frames:“\n•\n\"\"Steve is in the process of mastering the art of {SKILL_NAME}, with the specific objective of accomplishing {SKILL_DEFINITION}.\nWe will now evaluate whether Steve has successfully executed this skill by comparing the initial and current frames:“\n•\n\"\"The skill that Steve is currently executing is {SKILL_NAME}, and the intended outcome is {SKILL_DEFINITION}. To determine if \nSteve has effectively executed this skill, we'll assess both the initial frame and the current frame:“\n•\n\"\"Steve is currently performing the {SKILL_NAME} skill, with the aim of achieving {SKILL_DEFINITION}. Let's analyze both the start \nframe and the current frame to determine whether he has succeeded and the reasons behind it:“\n•\n\"\"Steve is demonstrating proficiency in the skill of {SKILL_NAME}, which is designed to accomplish {SKILL_DEFINITION}. Our \nevaluation will involve a comparison between the initial and current frames to assess the success of his execution:“\n•\n\"\"The task Steve is undertaking is the mastery of {SKILL_NAME}, with the end goal of achieving {SKILL_DEFINITION}. To determine \nwhether Steve has successfully executed this skill, we will analyze both the initial frame and the current frame:“\n•\n\"\"Steve is in the process of executing the {SKILL_NAME} skill, with the ultimate aim of accomplishing {SKILL_DEFINITION}. Our\nassessment will involve a comparison between the starting frame and the current frame to determine if he has succeeded and why:“\nFigure 9. 10 instruction examples for skill prediction instructions.\nQ: Steve is finding logs nearby, did he \ncomplete this skill?\nA: No.\nQ: Why does he fail?\nA: Because Steve has died.\nQ: Steve is finding sheep nearby, does \nhe fail?\nA: No. Steve found a sheep in sight.\nQ: Steve plans to craft a table, does he \nfinish this skill successfully?\nA: Yes he does.\nQ: Steve is asked to collect planks, what \nis the results?\nA: Steve collect planks successfully.\nQ: Steve collects wool in Minecraft, \nwhat is the consequence?\nA: Steve fails to finish the sill.\nQ: Please help me analyze the reasons \nfor the failure.\nA: Sure, there is no sheep in sight\nQ: Steve is mining cobblestone, does he \nfinish this skill?\nA: No.\nQ: How do you know that?\nA: No additional cobblestone shown in \nthe inventory\nFigure 10. Illustrative examples of skill prediction instruction data with snapshot pairs.\nTemplate of Instructional Training Data. Similar to Liu et al. (2023), we formulate each instruc-\ntional instance as a multi-round conversation as shown in Figure 11, where Xhead message is a sentence\nto describe this assistant (e.g., “ You are in a chat between a curious human and an artificial intelli-\ngence assistant. You should serve as an assistant to give helpful, detailed, and polite answers to the\nhuman’s questions.”). The number of rounds relies on the input instruction content. And the input\nimages (denoted as < image >) will only be fed in the first round, while XC may contain visual\noutputs with two additional tokens < vis > and < \/vis >.\n15\nPreprint\nFigure 11. The unified template to generate input sequence for instructional tuning.\nA.2\nSKILL PLANNING BENCHMARK\nTo clarify this benchmark, we begin by offering comprehensive task setup details in Table 6. During\nthe evaluation phase, we relocate the agent to a random location at the initiation of every episode,\nwith distances of up to 500 units, ensuring that the agent spawns in an unfamiliar environment. Fur-\nthermore, for tasks that involve interacting with mobs, we enforce a maximum spawning distance of\n30 units for cows and sheep. Our approach to complete tasks is rooted in a hierarchical framework.\nSpecifically, our model exclusively generates high-level skill plans, delegating the actual skill exe-\ncution to pre-trained basic skill policies as introduced by Yuan et al. (2023). Notably, we introduce\na self-driven variant named ’Steve-Eye-auto,’ which serves not only as a planner but also replaces\nthe Minecraft rules to verify the successful execution of skills.\nTable 6. The setups of 24 tasks used in our skill planning evaluation, where “Initial Items” refers\nto the tools provided in the agent’s inventory at the beginning of each episode, and “Max Steps”\nrepresents the maximum episode duration. Any episode exceeding this limit is classified as a task\nfailure. The tasks are originally developed by Yuan et al. (2023).\n(a) 7 tasks involving the process of “cutting trees to craft primary items”.\nTask Icon\nTask Name\nstick\ncrafting table nearby\nbowl\nchest\ntrap door\nsign\nwooden pickaxe\nInitial Items\n-\n-\n-\n-\n-\n-\n-\nMax Steps\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n(b) 7 tasks involving the process of “mining cobblestones to craft advanced items”.\nTask Icon\nTask Name\nfurname nearby\nstone stairs\nstone slab\ncobblestone wall\nlever\ntorch\nstone pickaxe\nInitial Items\n*10\n*10\n*10\n*10\n*10\nMax Steps\n5000\n5000\n3000\n5000\n5000\n5000\n10000\n(c) 10 tasks involving the process of “interacting with mobs to harvest food and materials”.\nTask Icon\nTask Name\nmilk\nbucket\nwool\nbeef\nmutton\nbed\npainting\ncarpet\nitem\nframe\ncooked\nbeef\ncooked\nbutton\nInitial Items\n,\n*3\n,\n*2\n,\n,\n,\n,\n,\nMax Steps\n3000\n3000\n3000\n3000\n10000\n10000\n3000\n10000\n10000\n10000\n16\nPreprint\nTable 7. The setups of 10 long-horizon iron-based tasks, where “Initial Items” are provided in the\nagent’s inventory at task beginning, and “Max Steps” refers to maximum environmental steps.\nTask icon\nTask description\nInitial tools\nMax steps\ncraft iron ingot\n*5,\n*64\n8000\ncraft shears\n*5,\n*64\n10000\ncraft bucket\n*5,\n*64\n12000\ncraft iron pickaxe\n*5,\n*64\n12000\ncraft iron axe\n*5,\n*64\n12000\ncraft iron sword\n*5,\n*64\n10000\ncraft iron shovel\n*5,\n*64\n8000\ncraft tripwire hook\n*5,\n*64\n8000\ncraft heavy weighted pressure plate\n*5,\n*64 t\n10000\ncraft iron trapdoor\n*5,\n*64\n12000\nfurnace\ngold ingot\ngreen candle\noak boat\norange bed\npainting\n(a) Qualitative examples of recipe image generation.\ntipped-arrow\nacacia-sign\nstone-axe\n(b) Illustrative examples of recipes that our model struggles to predict accurately. We attribute this failure to\nthe complexities arising from fine-grained or semantically overlapping image information.\nMinecraft \nEnvironment Feedback.\nSure, here is the plan list\nto harvest milk: (1) find\ncow\nnearby,\n(2)\nmilk\nbucket.\nHi, Steve, this is your birth-\nplace, what do your see?\nI can see the sky, trees in\nfront of me. I have 20\nlives and 10 food. There\nis a bucket in my invent-\ntory\nI want you to harvest milk.\nThere is a cow in front\nof me, finding cows s-\nuccessfully!\nThe plan list to harvest\nmilk: (1) milk bucket.\nRound 1: find cow nearby\nRound 2: milk bucket\nMake a plan again.\nMinecraft \nEnvironment Feedback.\nmilk\nbucket\nfinished!\nThe task is success-\nfully completed\nThank you, Steve.\n17\nPreprint\nTask Name: relax on sofa\nTask Description: I go to the living room and sit in the sofa\nTask Plan: {“1”: walk living room, “2”: walk couch, “3”: find couch, “4”: walk couch, “5”: sit couch}\nTask Name: browse the Internet\nTask Description: I go to the office and sit in a chair, I turn on the computer and grab the mouse. I \ntype on the keyboard and starting working on the computer.\nTask Plan: {“1”: walk living-room, “2”: walk desk, “3”: find desk, “4”: find chair, “5”: sit chair, “6”: find\ncomputer, “7”: switch-on computer, “8”: find mouse, “9”: grab mouse, find keyboard, “10”: type\nkeyboard, “11”: turn-to computer, “12”: look-at computer}\nTask Name: put milk in the freezer\nTask Description: I walk into kitchen, look for the milk, walk to milk, look for refrigerator, walk to \nrefrigerator, open door, put the milk in the refrigerator\nTask Plan: {“1”: walk dining-room, “2”: walk milk, “3”: find milk, “4”: turn-to milk, “5”: grab milk, “6”: \nlook-at freezer, “7”: walk freezer, “8”: open freezer, “9”: put milk\nFigure 14. Task examples from the extended Virtual-Home benchmark, where elements in green,\ncyan and red represent action, room, and object categories, respectively. Our benchmark includes a\ndiverse range of tasks that simulate interactions between individuals and their room environments.\nIt contains over 50 distinct room setups, involving 20 unique actions, and 100 objects. Each room\npresents a selection of more than 200 distinct tasks.\nA.3\nQUALITATIVE RESULTS OF MULTIMODAL GENERATION\nA.3.1\nRECIPE IMAGE GENERATION\nFigure 12a showcases qualitative examples of our evaluation on the FK-QA IMG dataset. Utilizing\na visual tokenizer like VG-GAN, our model demonstrates the ability to engage in visual generation,\nenabling it to provide visual feedback based on its comprehension of textual input. However, as\nshown in Figure 12b, our model encounters difficulties when generating image content characterized\nby fine-grained or semantically overlapping elements. These challenges warrant further exploration\nin our future work.\nA.3.2\nMULTIMODAL CHATBOT\nIn Figure 13, We present an overview of Steve-Eye functioning as a chatbot to receive task com-\nmands and execute them.\n18\nPreprint\nA.4\nDISCUSSION OF OPEN-WORLD EXPLORATION\nIn this paper, we have selected Minecraft as our open-world platform. Nevertheless, it is evident that\nSteve-Eye can be applied to other open-world environments, such as Virtual Home (Puig et al., 2018)\nand AI2THOR (Kolve et al., 2017), with minimal manual effort using the same methodology in this\npaper. These alternative benchmarks, when compared to Minecraft, exhibit a closer alignment with\nthe real world. To some extent, this choice holds greater significance since our ultimate objective is\nto deploy the agent in the real world. To achieve this goal, we expand the Virtual Home benchmark\nby introducing a more extensive range of environments (50+ rooms), human-interaction tasks (200+\nfor each room), as well as diverse categories of actions (20+) and objects (100+), as illustrated in\nFigure 14. The corresponding validation and further exploration of open-ended embodied agents in\na real-world context will be the focus of our future work.\n19\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nSteve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds\n```\n#### 2. 论文摘要\n```\nRecent studies have presented compelling evidence that large language models\n(LLMs) can equip embodied agents with the self-driven capability to interact\nwith the world, which marks an initial step toward versatile robotics. However,\nthese efforts tend to overlook the visual richness of open worlds, rendering\nthe entire interactive process akin to \"a blindfolded text-based game.\"\nConsequently, LLM-based agents frequently encounter challenges in intuitively\ncomprehending their surroundings and producing responses that are easy to\nunderstand. In this paper, we propose Steve-Eye, an end-to-end trained large\nmultimodal model designed to address this limitation. Steve-Eye integrates the\nLLM with a visual encoder which enables it to process visual-text inputs and\ngenerate multimodal feedback. In addition, we use a semi-automatic strategy to\ncollect an extensive dataset comprising 850K open-world instruction pairs,\nempowering our model to encompass three essential functions for an agent:\nmultimodal perception, foundational knowledge base, and skill prediction and\nplanning. Lastly, we develop three open-world evaluation benchmarks, then carry\nout extensive experiments from a wide range of perspectives to validate our\nmodel's capability to strategically act and plan. Codes and datasets will be\nreleased.\n```\n\n#### 3. 论文全文\n```\nPreprint\nSTEVE-EYE:\nEQUIPPING LLM-BASED EMBOD-\nIED AGENTS WITH VISUAL PERCEPTION IN OPEN\nWORLDS\nSipeng Zheng1, Jiazheng Liu2, Yicheng Feng2, Zongqing Lu1,2†\n1 Beijing Academy of Artificial Intelligence\n2 School of Computer Science, Peking University\nspzheng@baai.ac.cn\nfyc813@pku.edu.cn\nzongqing.lu@pku.edu.cn\nABSTRACT\nRecent studies have presented compelling evidence that large language models\n(LLMs) can equip embodied agents with the self-driven capability to interact with\nthe world, which marks an initial step toward versatile robotics. However, these\nefforts tend to overlook the visual richness of open worlds, rendering the entire\ninteractive process akin to “a blindfolded text-based game.” Consequently, LLM-\nbased agents frequently encounter challenges in intuitively comprehending their\nsurroundings and producing responses that are easy to understand. In this paper,\nwe propose Steve-Eye, an end-to-end trained large multimodal model to address\nthis limitation. Steve-Eye integrates the LLM with a visual encoder to process\nvisual-text inputs and generate multimodal feedback. We adopt a semi-automatic\nstrategy to collect an extensive dataset comprising 850K open-world instruction\npairs, enabling our model to encompass three essential functions for an agent:\nmultimodal perception, foundational knowledge base, and skill prediction and\nplanning. Lastly, we develop three open-world evaluation benchmarks, then carry\nout experiments from a wide range of perspectives to validate our model’s capa-\nbility to strategically act and plan. The project’s website and code can be found at\nhttps:\/\/sites.google.com\/view\/steve-eye.\n1\nINTRODUCTION\nHi Steve, how to craft a \nwooden axe in Minecraft ?\n{\"type\": \"minecraft:crafting_shaped\",\n\"category\": \"equipment\",\n\"key\": {\n\"#\": {\"item\": \"minecraft:stick\" },\n\"X\": {\"tag\": \"minecraft:planks\"}\n},\n\"pattern\": [\"XX\", \"X#\", \" #\"],\n\"result\": {\n\"item\": \"minecraft:wooden_axe\"},\n\"show_notification\": true}\n…? I don’t understand.\nOk, I got it.\nHi Steve, you are given an\nexample to generate a plan\nlist for task […], Now I\ndescribe your surrounding\nenvironment and your in-\nventory status […], please\nmake a plan to craft stick.\n…… ?\nSure, here is the plan list \nto craft stick: (1) find log\nnearby, (2) log, (3) craft \nplanks, (4) craft stick.\nWell, this is what you see.\nplease make a plan to craft stick\n(a)\n(b)\n×\n√\nHi Steve, how to craft a \nwooden axe in Minecraft ?\n×\n√\nFigure 1. (a) LLM-based agent’s feedback is un-\ncontrollable due to the uncertainty of input textual\nprompt, while visual cues can benefit the agent to\ngenerate feedbacks; (b) a text-only driven agent of-\nten finds it difficult to produce intuitive feedback\nthat humans can easily understand.\nDeveloping embodied agents that can adapt\nto the open world has long been a sub-\nstantial challenge (Kolve et al., 2017; Savva\net al., 2019).\nRecently, the rapid progress\nof large language models (LLMs) (OpenAI,\n2022; Touvron et al., 2023a) has shown their\npotential to serve as a general-purpose assis-\ntant. Driven by these pre-trained LLMs, re-\ncently proposed agents (Yuan et al., 2023;\nWang et al., 2023a;b; Zhu et al., 2023) have\nmanaged to extract world knowledge and rea-\nsoning capabilities from LLMs, allowing them\nto become self-driven. Thereby these agents\nare capable of generating executable policies\nor plans for a wide range of skills and tasks in\nan open-ended world.\nWhile current attempts to integrate LLMs\nshow promise in developing a generic embod-\nied agent, these efforts primarily translate the\nentire world into text, which overlooks the\n†Corresponding author\n1\narXiv:2310.13255v2  [cs.CV]  7 Dec 2023\nPreprint\nmultifaceted richness of diverse visual reality\nand turns interacting with the environment into something akin to “a blindfolded text-based game.”\nConsequently, such text-only agents often face difficulties when it comes to effectively and intu-\nitively representing the world. Imagine a situation where you request your agent to shop for a pair\nof shoes. Would you prefer to send the agent a picture of the shoes or provide a lengthy description\nof the shoes to convey their appearance? Undoubtedly, you would opt for the former choice.\nIn fact, the agent’s reliance on text input\/output (I\/O) imposes significant limitations on its ability to\ninteract with the world. To illustrate this point, we consider Minecraft (Guss et al., 2019; Fan et al.,\n2022) as an ideal example. Minecraft, being an expansive sandbox game, offers a vast realm for\nembodied agents to explore, which requires the acquisition of various basic skills (e.g., crafting logs)\nand the ability to plan and execute diverse tasks. First, as shown in Figure 1 (a), the LLM-based agent\nproduces uncontrollable outputs. The success of the agent’s responses hinges heavily on careful\nprompt engineering (Huang et al., 2022b), ensuring that the LLM comprehends the environment\nand task objectives. Moreover, a universally applicable prompt that suits every LLM and task is an\nunattainable goal. Therefore, this prompting process is labor-intensive and contradicts our aim of\nenabling agents to act in a self-driven manner. Second, when compared to visual feedback, language\noften encounters difficulties in intuitively conveying specific world concepts (e.g., recipes) to users,\nas illustrated in Figure 1 (b), thereby unavoidably creating obstacles for robust human-computer\/AI\ninteraction (Preece et al., 1994; Fallman, 2003).\nUnlike LLMs, humans possess an innate ability to process and generate information through both\nvisual and text channels. This inherent gift significantly enhances our capability to interact with\nthe world. However, the coupling of LLM-based agents with multimodal I\/O has been relatively\nunderexplored in an open-ended environment. To fill this gap, we introduce Steve-Eye\n, a large\nmultimodal model that enables LLM-based embodied agents to engage with the open world via\nvisual-text interfaces. Steve-Eye excels at producing responses that demonstrate a comprehensive\ngrasp of the environment, common-sense reasoning, and executable skill plans. To achieve this,\nSteve-Eye is equipped with three indispensable functions: (1) multimodal perception; (2) founda-\ntional knowledge base; and (3) skill prediction and planning. In this paper, we choose Minecraft as\nour validation platform considering its vast sandbox world and the high degree of freedom. More\nenvironments can also be considered, e.g., Virtual Home (Puig et al., 2018), AI2THOR (Kolve\net al., 2017). Due to the space limit, we discuss the exploration of more generic environments in\nAppendix A.4 and leave it as our future work. Our contributions can be summarized as follows:\nOpen-World Instruction Dataset.\nWe construct an extensive instruction dataset to train Steve-\nEye for the acquisition of three mentioned functions. The instruction data contains not only the\nagent’s per-step status and environmental features but also the essential knowledge for agents to act\nand plan. However, collecting such a dataset in an open world can be a costly endeavor, especially\nwhen aiming to gather fine-grained and diverse labels. As a result, previous studies (Fan et al.,\n2022) have often relied on readily available unsupervised data (e.g., video-subtitle pairs) for pre-\ntraining. In these approaches, the agent’s comprehension of its status and environment is implicitly\nlearned through self-supervised techniques, while its foundational knowledge is directly derived\nfrom general-purpose LLMs. In contrast, our work involves curating multimodal instructional data\nspecifically designed for open-ended embodied agents, by utilizing ChatGPT (OpenAI, 2022).\nLarge Multimodal Model and Training.\nSteve-Eye combines a visual encoder which converts\nvisual inputs into a sequence of embeddings, along with a pre-trained LLM which empowers em-\nbodied agents to engage in skill or task reasoning in an open world. During the training process, we\nemploy a two-stage strategy similar to Liu et al. (2023). This strategy commences with the align-\nment of multimodal elements between the visual encoder and the large language model, followed\nby the instruction tuning through our constructed dataset.\nOpen-World Benchmarks.\nWe carry out extensive experiments to demonstrate that our pro-\nposed Steve-Eye outperforms LLM-based agents in open-world setups. Specifically, we develop the\nfollowing benchmarks to evaluate agent performance from a broad range of perspectives: (1) envi-\nronmental visual captioning (ENV-VC), which assesses an agent’s capacity to perceive and describe\nits surroundings effectively; (2) foundational knowledge question answering (FK-QA), which eval-\nuates the proficiency in mastering basic knowledge crucial for an agent’s decision-making; (3) skill\nprediction and planning (SPP), which quantifies an agent’s capability to act and plan strategically.\n2\nPreprint\n2\nRELATED WORK\n2.1\nOPEN-WORLD EMBODIED AGENTS WITH LLMS\nThe rapid progress of large language models (Brown et al., 2020; Raffel et al., 2020; Zhang et al.,\n2022; Chowdhery et al., 2022) has significantly boosted their capacity to encode a wide range of\nhuman behaviors within training data (Bommasani et al., 2021). When equipped with narrowly de-\nsigned prompts, LLM-based agents exhibit the capability to generate executable plans for tasks such\nas indoor robot manipulation. For instance, SayCan (Ahn et al., 2022) integrates skill affordances\nwith LLMs to yield actionable plans, while Palm-E (Driess et al., 2023) takes a step further by\nconstructing hierarchical agents capable of handling multimodal prompts. This approach has also\nproven its efficacy in open-world environments (Huang et al., 2022a; Li et al., 2022). In contrast\nto robot manipulation, agents in the wild require a heightened level of real-time situational aware-\nness and foundational knowledge to execute intricate skill plans across a diverse array of tasks. To\nsimulate human behaviors in such open worlds, Generative Agents (Park et al., 2023) store agents’\nexperiences and retrieve these memories to generate plans in a text-based sandbox game.\nIn recent years, the 3D sandbox Minecraft has received considerable attention owing to its remark-\nably flexible game mechanics to serve as a prominent open-world benchmark (e.g., MineRL (Guss\net al., 2019) and Minedojo (Fan et al., 2022)). DEPS (Wang et al., 2023b) introduces the descrip-\ntor, explainer, and selector for plan generation with the help of LLM. Plan4MC (Yuan et al., 2023)\nconstructs a skill graph and proposes a skill search algorithm to minimize planning errors. Voy-\nager (Wang et al., 2023a) proposes an LLM-powered lifelong learning agent that continually ex-\nplores the Minecraft world. Similar to (Park et al., 2023), GITM (Zhu et al., 2023) integrates LLMs\nwith text-based memory and knowledge to create generic agents in Minecraft. Among these studies,\nVoyager (Wang et al., 2023a) and GITM (Zhu et al., 2023) lean entirely on text descriptions of the\nenvironment to act and plan, while Plan4MC (Yuan et al., 2023) and DEPS (Wang et al., 2023b)\nhave visual-input skills but still rely on merely text for planning. None of them try to understand the\nrich visual observation provided natively by Minecraft. In contrast to these works, our work trains a\nlarge multimodal model to fill this gap.\n2.2\nLARGE MULTIMODAL MODELS (LMMS)\nIn comparison to LLMs, large multimodal models (LMMs) (Awadalla et al., 2023) encompass\na broad range of information beyond text modality, which can be categorized into two primary\nstreams. The first category (Gupta & Kembhavi, 2023; Huang et al., 2023a; Patil et al., 2023; Sur´ıs\net al., 2023) involves hinging on ChatGPT (OpenAI, 2022) or GPT-4 (OpenAI, 2023) to generate\nin-context responses without parameter tuning. However, these approaches heavily rely on the avail-\nability of an LLM’s API and the quality of the designed prompts. The second category comprises\nend-to-end pre-trained models. Within this category, models such as Huang et al. (2023b); Peng et al.\n(2023) are trained entirely from scratch. Conversely, some research explores efficient fine-tuning us-\ning pre-trained LLMs by incorporating lightweight modality encoders, such as Qformer (Li et al.,\n2023) or Perceiver (Alayrac et al., 2022). Recently, Liu et al. (2023) propose to explicitly instruction-\ntune a LLM using vision-language instruction data.\nIn this work, we propose Steve-Eye by building upon pre-trained LLMs, aiming to develop an open-\nworld agent powered by a large-scale model with versatile multimodal I\/O capabilities.\n3\nMETHODOLOGY\nIn this section, we first provide our instruction-following dataset to develop three key functions for\nthe agent’s open-world interaction in Section 3.1. We then propose our large multimodal agent\nSteve-Eye in Section 3.2, and clarify details of the training procedure in Section 3.3. We adopt\nMinecraft as our open-ended platform in this paper to collect data and validate the model, anticipat-\ning to explore a broader range of environments for Steve-Eye in future studies.\nTo empower an agent with the self-driven capacity to act and plan in an open world, we posit that the\nfollowing embodied functions are indispensable: (1) multimodal perception function which offers\na detailed description of the agent status and environmental features; (2) foundational knowledge\n3\nPreprint\nbase which imparts an understanding of how the world works and conveys crucial basic knowledge\nrelated to skills and tasks; (3) skill prediction and planning which is responsible for generating\nskill execution feedback (e.g., success or failure) and crafting high-level skill plans for handling\nmore complex and long-horizon tasks. We develop these functions by building the corresponding\ninstruction dataset to pre-train Steve-Eye as follows.\n3.1\nOPEN-WORLD INSTRUCTION-FOLLOWING DATASET\nMultimodal Perception Instructions.\nHuman players can perform actions in Minecraft mainly\nrelying on their visual perception, without any prior hints or imposed game judgments. In order to\nendow Steve-Eye with the same ability, it is required to provide it with comprehensive visual descrip-\ntions of the environment. To achieve this, we use Minedojo (Fan et al., 2022) to obtain Minecraft\nsnapshots which contain a wide array of details within the agent’s surroundings, including environ-\nmental features, the agent’s life and food status, inventory items, and equipment, as illustrated in\nFigure 2. In addition, we leverage MaskCLIP (Zhou et al., 2022) to identify the in-sight objects of\nthese snapshots without supervised annotations. During our data collection process, for each snap-\nshot I and its corresponding description XC, we initiate a three-step approach. Firstly, we prompt\nChatGPT to curate a list of 40 instructions as shown in Figure 6 in Appendix A.1.1. Then we enrich\nsnapshot details as dense caption to describe its content, with the assistance of ChatGPT. Finally,\nwe select an instruction XQ randomly from the list and combine it with the snapshot’s caption to\ncreate a single-round multimodal description pair (e.g., ### Human: XQ I\\n ### Embodied Agent:\nXC\\n.). By doing so, we collect 200K instructional pairs for multimodal perception learning.\n[Description] Steve is walking in the forest and\nhe can’t see the sky. There are trees in front of\nhim. He still has 20.0 life and 20.0 food. He is\nnow equipped with the iron pickaxe and the\ninventory contains 5 pickaxes, 6 ……\n•\nenvironment\n•\nlife and food\n•\nobject in sight\n•\ninventory and \nequipment\nFigure 2. Multimodal perception\nFoundational Knowledge Instructions.\nEmbodied agents\nrequire a foundation of essential knowledge to facilitate action-\ntaking and skill planning.\nIn Minecraft, such knowledge\nshould contain item recipes, details of item attributes, their as-\nsociated numerical value, etc. We access this vital information\nfrom Minecraft-Wiki (Fandom, 2023), which comprises an ex-\ntensive collection of over 9,000 HTML pages. To be specific,\nwe first obtain all item icons from Minecraft-Wiki and gener-\nate 200K icon inventory images, as illustrated in Figure 3 (a).\nEach icon image corresponds to a 4-row table with an associ-\nated caption adhering to a standardized template: “There is a\nMinecraft inventory with 4 rows. From left to right, they are\n...”. As shown in Figuire 7 in Appendix A.1.2, we curate a set of 20 distinct prompts designed to\nchallenge the model’s ability to recognize items. Subsequently, we further collect all recipe-related\ninformation from the Wiki as illustrated in Figure 3 (b), and design similar prompt templates to for-\nmulate 10,000 recipe-image instructional pairs. Lastly, we process the Wiki and utilize this corpus\nto produce 40,000 single-round question-answer pairs. In total, we collect a high-quality dataset\nwith 250K foundational knowledge instructions.\n(a) Item icons\n(b) recipes\nFigure 3. Icons and recipes\nSkill-related Interaction Instructions.\nThe environmen-\ntal description and foundational knowledge serve as prerequi-\nsites for an agent’s interaction within the open world. How-\never, a successful interaction requires more than these ele-\nments alone. It relies upon the mastery of basic skills, such\nas log, harvesting, and food preparation, as well as high-level\nskill planning abilities to tackle complex, long-horizon tasks,\nsuch as crafting an iron pickaxe. To facilitate this, we gather\ncorresponding training data for skill prediction and planning,\nwhich enables our model to provide correct feedback on both\nbasic skills and long-horizon tasks across a spectrum of agent\nor environmental conditions. Specifically, the data collection\nprocess involves two steps. First, we sample skill trajecto-\nries based on the pre-trained basic skill policies and collect\n200K snapshot pairs with corresponding statuses from these\ntrajectories. Each snapshot pair {I0, It} denotes the 0-th and\nt-th timestamp of the skill trajectory. Next, we employ Chat-\nGPT to generate question-answer pairs about diverse aspects of skill execution status. These ques-\n4\nPreprint\nHow to craft a wooden axe? \nAnswer me via a recipe image.\nWhat are the ingredients required \nto craft a detector rail?\nProvide a detailed description of \nthe given Minecraft image <image>. \n…\n…\n<image> \nMake a plan list to finish the task \nof <image> in Minecraft. \n<image> \nIron Ingot, Stone Pressure \nPlate, and Redstone Dust.\nFrom left to right, there are 1 \nshears in the inventory, which \nmeans he is equipped with 1 \nshears. Steve still has 20.0 life \nand 20.0 food.\nThe executable plan can be: (1) \nfind log nearby; (2) crafting log; \n(3) crafting planks; (4) crafting \nstick.\nvisual \ntokenizer\ntext \ntokenizer\nprojector\nmultimodal input\n…\n…\nmultimodal output\nvisual generation\nquestion-answering\nvisual captioning\nskill planning\n<image>\nLarge Language Model\nFigure 4. Illustration of Steve-Eye: a large multimodal model designed to seamlessly process both\nvisual and language inputs. Steve-Eye excels in acquiring fundamental knowledge of the world it\nlives in, understanding the nuances of its surroundings, and generating executable plans to complete\na wide array of open-ended tasks. Furthermore, Steve-Eye responds to user instructions through\neither visual or text-based cues, enhancing the convenience and flexibility of human-AI interaction.\ntions delve into whether the agent completes the skill, encounters unexpected failures, or seeks\nexplanations for such failures. More details can be found in Appendix A.1.3. Second, we sam-\nple 40K task trajectories using the planner in Yuan et al. (2023), each of which can be denoted as\nT = {s1, s2, ...sT} representing the task is finished via a T-round planning procedure, where si is\nthe skill plan for i-th round. At each round i, we feed our model with its start snapshot and task\ninitialization, and curate instructional questions to inquire about si with reasonable explanation. In\nthis manner, we obtain 200K instructional pairs from task trajectories.\n3.2\nMODEL ARCHITECTURE\nFigure 4 illustrates the overall architecture of our proposed model. Steve-Eye, functioning as a\ngenerative model, connects an image-oriented tokenizer fv with the pre-trained LLM backbone Θ.\nWe adopt the image tokenizer, e.g., VQ-GAN (Esser et al., 2021), to encode the raw images I into\ntoken embeddings V = {v1, v2, ..., vn} ∈Rn×d, where n denotes the number of visual tokens and\nd is the dimensionality of each token. We further utilize a lightweight projection module fl with\na trainable projection matrix W. This module maps the visual tokens to the same space with text\nembeddings, yielding ˆV = {ˆv1, ˆv2, ..., ˆvn} ∈Rn× ˆd:\n  \\ hat  {\\ma t h cal {V}} = W \\mathcal {V}; \\hspace {0.3em} \\text {where} \\hspace {0.3em} \\mathcal {V} = f_v(I). \n(1)\nTo effectively process visual-language inputs and generate corresponding outputs, our model inte-\ngrates the visual codebook Cv into the pre-existing language vocabulary Cl. This integration leads\nto the formation of a unified multimodal codebook, denoted as Cm = Cv ∪Cl. Additionally, in\norder to mark the starting and ending points of visual elements in I\/O sequences, we introduce two\nspecial tokens, namely <vis> and <\/vis>. The LLM backbone Θ of our Steve-Eye is built upon a\ndecoder-only architecture with casual transformers. Our model employs an auto-regressive predic-\ntion mechanism, generating responses based on the provided multimodal input tokens. The resulting\nresponse is a mixed sequence of visual and textual tokens, represented as Y = {y1, y2, ..., ym}. For\neach embedding yi, we pass it through a linear layer fp followed by a softmax operation, mapping\nit into a probability distribution of the multimodal vocabulary. The final prediction for the i-th token\nzi is determined by selecting the token from the multimodal codebook with the highest score:\n  z _i = \\argmax ( \\text {softmax}(f_p(y_i))). \n(2)\n3.3\nTRAINING\nEach instruction-following instance can be formulated as a multi-round conversation {X 1\nQ, X 1\nC, ...,\nX N\nQ , X N\nC }, where each {X i\nQ, X i\nC} represents a question-answer interaction between a human and\n5\nPreprint\nthe embodied agent and N indicates the total number of rounds in the conversation. The entire\ninstructional dataset follows this unified template, as demonstrated in Figure 11 in Appendix A.1.3.\nTo efficiently train our model, we employ the negative log-likelihood objective over the prediction\ntokens with instruction tuning:\n  \\m a t\nh\nc\nal \n{L} (\\Theta )=-\\sum _{j=1}^{L} \\log P_{\\Theta }(y_j|\\mathcal {I}, \\hat {y}_{1:j-1}), \n(3)\nwhere y and ˆy respectively denote the input and target token sequences, with Θ representing the\nmodel parameters, and L representing the length of the target sequence. The input visual content\nI may represent an empty image depending on the input instruction. It is worth noting that we\nconstrain the loss computation to only consider the answer tokens XC. This constraint prevents\ntraining from becoming excessively straightforward and ensures that the model’s primary focus is\non learning to precisely generate coherent responses. Similar to Liu et al. (2023), we adopt a two-\nstage instruction-tuning strategy to train our model:\nTwo-Stage Instruction-Tuning. (1) Multimodal feature alignment: In the first stage, our pri-\nmary objective is to align visual features with the language token space. In order to strike a bal-\nance between efficient tuning and a comprehensive coverage of the world’s concepts, we curate our\nopen-ended instruction dataset to 600K snapshot-text pairs. These pairs are then transformed into\ninstruction-following data as described in Section 3.1. During the feature alignment stage, we main-\ntain the visual encoder and the LLM parameters in a frozen state, exclusively training the projection\nmodule. Additionally, this training phase involves fine-tuning token embeddings to accommodate\nthe newly introduced visual codebook and two special tokens <vis> and <\/vis>. (2) End-to-end\ninstruction tuning: In the second stage, we continue to keep the visual encoder frozen while concur-\nrently training the projection module and LLM. This second stage leverages the entire open-ended\ninstructions and contributes significantly to enhancing the model’s capability of comprehending and\neffectively responding to complex multimodal instructions.\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETUP\nImplementation Details.\nIn this paper, we use the LLaMA-2 model (Touvron et al., 2023b)\nas the LLM backbone. Additionally, we use CLIP (Radford et al., 2021) as our visual encoder\nto achieve the best performance for non-visual generative tasks, and use VQ-GAN (Esser et al.,\n2021) as the default visual tokenizer for visual generation. The size of visual codebook Cv and\nlanguage vocabulary is 8192 and 32000, respectively. In addition, we add <vis> and <\/vis> to the\nfinal unified codebook, indicating the starting and ending points of visual content. Similar to Liu\net al. (2023), we construct 850K instruction-answer pairs for model training. Note that the model\nis trained to predict the agent’s answer, and thus only sequence\/tokens of answer will be used to\ncompute the loss in the auto-regressive model. We also adopt LoRA (Hu et al., 2021) to reduce the\ncomputational cost for efficient tuning. We choose MineDojo (Fan et al., 2022) as the Minecraft\nplatform to collect our instruction data and conduct experiments. Following Yuan et al. (2023),\nwe use the environments of programmatic tasks to train basic policies with RL. These policies are\ntrained to execute corresponding skills and keep fixed in all testing tasks.\nEvaluation Benchmarks.\nWe conduct experiments on three benchmarks to evaluate an agent’s\ninteraction ability in an open world. (1) Environmental visual captioning (ENV-VC): given a\nsnapshot, the model is asked to describe the agent’s current status and environmental features from\ndiverse aspects (e.g., life, food...). We evaluate the prediction’s accuracy of each aspect by ex-\ntracting corresponding answers from the output description to compare with the groundtruth. (2)\nFoundational knowledge question answering (FK-QA): to assess the model’s grasp of essential\nknowledge, we collect a set of 10,000 Minecraft-related questions from different sources, including\nthe Wiki pages, Wiki tables, and Minecraft recipes. The performance is measured by the model’s\nability to provide correct answers to these questions. (3) Skill prediction and planning (SPP): we\nutilize our proposed Steve-Eye to predict whether a skill has been successfully completed and assert\nits capability to generate executable high-level skill plans for long-horizon tasks.\n6\nPreprint\nTable 1. Comparisons of different model settings on the environmental visual caption benchmark.\nThe experiments are conducted on 20K ENV-VC test set.\nModel\nvisual encoder\ninventory\nequip\nobject in sight\nlife\nfood\nsky\nBLIP-2\nCLIP\n41.6\n58.5\n64.7\n88.5\n87.9\n57.6\nLlama-2-7b\n-\n-\n-\n-\n-\n-\n-\nSteve-Eye-7b\nVQ-GAN\n89.9\n78.3\n87.4\n92.1\n90.2\n68.5\nSteve-Eye-13b\nMineCLIP\n44.5\n61.8\n72.2\n89.2\n88.6\n68.2\nSteve-Eye-13b\nVQ-GAN\n91.1\n79.6\n89.8\n92.7\n90.8\n72.7\nSteve-Eye-13b\nCLIP\n92.5\n82.8\n92.1\n93.1\n91.5\n73.8\nTable 2. Comparisons of different data configurations on the environmental visual captioning bench-\nmark, where “snapshot desc.” denotes the 200K multimodal perception instruction dataset.\ninventory\nequip\nobject in sight\nlife\nfood\nsky\nno instruction tuning\n22.7\n24.3\n39.8\n81.2\n80.4\n61.1\nw\/o snapshot desc.\n46.2 (+23.5)\n40.9 (+16.6)\n41.2 (+1.4)\n83.0 (+1.8)\n82.4 (+2.0)\n63.3 (+2.1)\nw\/o icon images\n52.3 (+29.6)\n48.1 (+23.8)\n91.4 (+51.6)\n92.5 (+11.3)\n90.9 (+10.5)\n73.5 (+12.4)\nfull data\n92.5 (+69.8)\n82.8 (+58.5)\n92.1 (+52.3)\n93.1 (+11.9)\n91.5 (+11.1)\n73.8 (+12.7)\n4.2\nENVIRONMENTAL VISUAL CAPTIONING (ENV-VC)\nWe introduce this evaluation protocol for asserting Steve-Eye’s multimodal perception function,\nwhich serves as an initial stride toward comprehensive evaluation of large multimodal models.\nSpecifically, we collect 20,000 Minecraft snapshots (named ENV-VC test set) using Minedojo and\napply the proposed data generation pipeline to create six questions for each snapshot, resulting in a\ntotal of 120K questions. These six questions pertain to the prediction of various aspects, including\ninventory items\n, equipment\n, objects in sight\n, life\n, food\n, and the visibility of sky\n.\nDuring the inference phase, Steve-Eye predicts answers based on these questions and the input\nsnapshot. Experimental results are presented in Table 1 and Table 2. As shown in Table 1, our\nvisual encoder, when combined with multimodal instruction tuning, significantly enables the ability\nof the text-only language model LLM (Llama-2-7b) to comprehend the contents of the snapshots\n(Steve-Eye-7b). Notably, Steve-Eye outperforms BLIP-2 by a substantial margin due to the im-\nproved reasoning ability enabled by the larger LLM. Furthermore, the visual encoder plays a crucial\nrole in facilitating multimodal understanding. Surprisingly, the model equipped with CLIP (Radford\net al., 2021) surpasses the performance of the model using MineCLIP (Fan et al., 2022), achieving\nover +48.9%, +21.0% and +19.9% improvements in inventory, equipment, and object-in-sight pre-\ndictions, respectively. We attribute this performance difference to the fact that MineCLIP does not\nprioritize fine-grained alignment during pre-training, despite being exposed to a diverse range of\nMinecraft videos. In summary, Steve-Eye’s ability to comprehend visual cues from its surroundings\nlays the foundation for subsequent interactions with the world.\nTo investigate the effictiveness of various types of instructional data for multimodal perception, we\ncarry out experimental comparisons with diverse data configurations in Table 2. First, our results\nshowcase a significant improvement in the model’s capacity to respond to instructional questions\nthrough instruction tuning, which leads to impressive gains of over +50% for inventory, equipment,\nand object-in-sight prediction. Furthermore, the inclusion of the multimodal perception dataset and\nicon images in the training data both contribute to a substantial improvement in the model’s overall\nperformance. Ultimately, the best results are achieved when combining all available data sources.\n4.3\nFOUDATIONAL KNOWLEDGE QUESTION ANSWERING (FK-QA)\nFollowing Team (2022), we establish a question database specialized to assess our model’s pro-\nficiency in generating responses pertaining to fundamental Minecraft knowledge. This evaluation\nis carried out through a validation dataset known as the FK-QA test set, which is further divided\ninto two distinct subsets: TEXT and IMG. In the FK-QA TEXT subset, we generate a collection\nof 10,000 question-answer pairs curated from various sources, including the Minecraft-Wiki pages,\nMinecraft-Wiki tables, and Minecraft recipes. Each category comprises 2,000, 5,000, and 3,000\npairs, respectively. Upon receiving a response from Steve-Eye, we feed both the generated response\n7\nPreprint\nTable 3. Comparisons on FK-QA test set of the foundational knowledge question answering bench-\nmark. The evaluation metrics consider both the scoring and accuracy dimensions simultaneously.\nScoring\nAccuracy\nWiki Page\nWiki Table\nRecipe\nTEXT All\nTEXT\nIMG\nLlama-2-7b\n6.90\n6.21\n7.10\n6.62\n37.01%\n-\nLlama-2-13b\n6.31 (-0.59)\n6.16 (-0.05)\n6.31 (-0.79)\n6.24 (-0.38)\n37.96%\n-\nLlama-2-70b\n6.91 (+0.01)\n6.97 (+0.76)\n7.23 (+0.13)\n7.04 (+0.42)\n38.27%\n-\ngpt-turbo-3.5\n7.26 (+0.36)\n7.15 (+0.94)\n7.97 (+0.87)\n7.42 (+0.80)\n41.78%\n-\nSteve-Eye-7b\n7.21 (+0.31)\n7.28 (+1.07)\n7.82 (+0.72)\n7.54 (+0.92)\n43.25%\n62.83%\nSteve-Eye-13b\n7.38 (+0.48)\n7.44 (+1.23)\n7.93 (+0.83)\n7.68 (+1.06)\n44.36%\n65.13%\nand the corresponding groundtruth answer to ChatGPT. ChatGPT will first examine the accuracy of\nthe response as a measure of answer correctness. To minimize variability in error, ChatGPT con-\nducts a further evaluation, considering the response’s accuracy, relevance, and level of detail. This\ncomprehensive evaluation yields an overall score on a scale ranging from 0 to 10, where a higher\nscore signifies superior overall performance. In the FK-QA IMG subset, we shift our focus to visual\ngeneration by employing 3,000 recipe images as groundtruth data. Here, our model is tasked with\ngenerating visual outputs for each item within the recipe inventory, following a specific order. The\nvisual output is considered correct only if every element of the recipe is accurately generated. We\nadopt this metric to assert our model’s ability to produce multimodal feedback.\nTable 3 presents both scoring and accuracy results. It’s worthy to note that Llama-2 exhibits consis-\ntent performance regardless of the model’s scale, with Llama-2-70b only marginally outperforming\nthe 7b-version by +1.26% in accuracy, meanwhile 13b-version performs even worse than 7b-version\non the scoring results. We hypothesize that this phenomenon can be attributed to distinct variations\nin difficulty levels encountered within our FK-QA test set. Llama-2 fails to answer correctly for the\nchallenging part regardless of its size due to essential knowledge missing. In contrast, Steve-Eye\noutperforms both Llama-2 and gpt-turbo-3.5, despite its considerably smaller scale. Furthermore,\nour model exhibits a more substantial improvement in responding to Recipe and Wiki Table ques-\ntions as compared to Wiki Page questions. This disparity can likely be attributed to the fact that\nWiki Page contains a large proportion of invalid questions (e.g., version, history), whereas Recipe\nand Wiki Table predominantly feature knowledge-related inquiries. Such result further validates the\neffectiveness of our approach in acquiring foundational knowledge. Unlike text-only LLMs, our\nmodel exhibits considerable ability to output visual contents, which achieves 65.13% accuracy on\nFK-QA IMG using the 13b-version. The multimodal generation ability enables Steve-Eye to better\nserve as an assistant for potential needed people such as beginners of this game. We show more\ndetails and cases in Appendix A.3.\n4.4\nSKILL PREDICTION AND PLANNING (SPP)\nSkill Prediction.\nSimilar to Section 3.1, we collect another 20K snapshot pairs in the form of\n{I0, It} from skill trajectories (referred to as Skill-Pred test). These pairs are input into our model\nto query the current execution status of the skill. The execution status can fall into one of three cate-\ngories: success, failure, and running, with “running” signifying that the skill is currently in progress.\nTable 4.\nRecall\/Accuracy results on Skill-Pred\ntest set for the skill prediction benchmark.\nrunning (%) success (%)\nfail (%)\nBLIP-2\n65.2\/58.8\n49.8\/54.3\n42.1\/51.8\nSteve-Eye-7b\n89.8\/82.5\n77.6\/81.4\n74.2\/79.9\nSteve-Eye-13b\n92.1\/84.2\n80.5\/83.1\n76.8\/81.5\nAs shown in Table 4, our model exhibits com-\nmendable performance in skill status predic-\ntion.\nHowever, the performance is still far\nfrom enough to completely replace the rule-\nbased game judgment adopted by the existing\nRL-based skill agents. These experiments indi-\ncate that, despite the excellent multimodal un-\nderstanding capabilities of our model in open-\nworld environments in previous experiments, it\nstill falls short in fine-grained reasoning tasks that involve consecutive frames to some extent.\nSkill Planning.\nFollowing Yuan et al. (2023), we carry out evaluation on 24 difficult tasks in\nMinecraft. These tasks can be categorized into three types: cutting trees to craft primary items (7),\nmining cobblestones to craft advanced items (7), and interacting with mobs to harvest food and ma-\nterials (10). Each task is tested for 30 episodes, where an episode refers to a multi-round interaction\n8\nPreprint\nTable 5. Comparisons on the skill planning benchmark. We test the mean success rates of all tasks,\nwhere each task is executed for 30 episodes using the same seeds for initialization.\nModel\nMineAgent\n0.00\n0.03\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.21\n0.0\n0.05\n0.0\ngpt assistant\n0.30\n0.17\n0.07\n0.00\n0.03\n0.00\n0.20\n0.00\n0.20\n0.03\n0.13\n0.00\n0.10\n0.00\nSteve-Eye-auto\n0.30\n0.27\n0.37\n0.23\n0.20\n0.17\n0.26\n0.07\n0.13\n0.17\n0.20\n0.33\n0.00\n0.13\nSteve-Eye\n0.40\n0.30\n0.43\n0.53\n0.33\n0.37\n0.43\n0.30\n0.43\n0.47\n0.47\n0.40\n0.13\n0.23\nModel\nMineAgent\n0.46\n0.50\n0.33\n0.35\n0.0\n0.0\n0.06\n0.0\n0.0\n0.0\ngpt assistant\n0.57\n0.76\n0.43\n0.30\n0.00\n0.00\n0.37\n0.00\n0.03\n0.00\nSteve-Eye-auto\n0.70\n0.63\n0.40\n0.30\n0.17\n0\n0.37\n0.03\n0.07\n0.00\nSteve-Eye\n0.73\n0.67\n0.47\n0.33\n0.23\n0.07\n0.43\n0.10\n0.17\n0.07\nprocess. At each round, the model receives the environmental feedback from the last round, plans\na skill list based on the current status, and then picks up the top skill to execute. For each task\nepisode, we set a maximum step between [3000, 10000]. In our evaluation, we compare Steve-Eye\nagainst two baseline approaches: (1) MineAgent (Fan et al., 2022), which completes tasks without\ndecomposing them into basic skills, and uses PPO and self-imitation learning with CLIP reward,\nand (2) GPT Assistant, which employs ChatGPT as a high-level planner to generate skill plans by\nprompting itself with information from the environment and the agent’s status. The results in Ta-\nble 5 demonstrate that Steve-Eye significantly outperforms both baseline methods. Additionally, we\nconduct experiments in which Steve-Eye takes over the skill prediction function from the rule-based\ngame judgment in Minecraft. This self-driven variant is referred to as ‘Steve-Eye-auto.’ Since the\nmodel’s skill prediction is not always 100% accurate, Steve-Eye-auto does experience some perfor-\nmance degradation when compared to Steve-Eye. This degradation is more pronounced in longer,\ncomplex tasks (e.g.,\n,\n,\n) as opposed to short-term tasks (e.g.,\n,\n,\n). Nevertheless,\nSteve-Eye-auto still demonstrates significant performance improvements in most tasks, compared to\nthe baselines. For additional details about this benchmark, please refer to Appendix A.2.\nFor better visualization, we provide a qualitative example of Steve-Eye completing the task “crafting\nstone axe with wooden pickaxe” as shown in Figure 5.\nfind cobblestone\nharvest cobblestone\nfind trees\nharvest log\ncraft planks\ncraft and place table\ncraft stone axe\nbirthplace\nFigure 5. Snapshots of a qualitative example, illustrating how Steve-Eye completes the task of\n“crafting a stone axe with a wooden pickaxe.” Our model generates a skill plan at each interaction\nround and selects the top skill from the plan list for execution.\n5\nCONCLUSION\nIn this paper, we explore enabling a large multimodal model to serve as a generative embodied\nagent in open worlds. We achieve this goal by proposing Steve-Eye, which combines the text-only\nlanguage model with a visual encoder, allowing for a multimodal I\/O interface to interact with the\nenvironment. With the help of ChatGPT, we curate questions to generate 850K instruction-following\ndata to facilitate the agent’s multimodal perception fuction, foundational knowledge mastery, as well\nas the capability of skill prediction and planning. Experiments on three open-world benchmarks\nverify the advantages of our Steve-Eye over a wide range of perspectives.\n9\nPreprint\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–\n23736, 2022.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\nOpenflamingo:\nAn open-\nsource framework for training large autoregressive vision-language models.\narXiv preprint\narXiv:2308.01390, 2023.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-\nnities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multi-\nmodal language model. arXiv preprint arXiv:2303.03378, 2023.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recogni-\ntion, pp. 12873–12883, 2021.\nDaniel Fallman. Design-oriented human-computer interaction. In Proceedings of the SIGCHI con-\nference on Human factors in computing systems, pp. 225–232, 2003.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:\n18343–18362, 2022.\nFandom. Minecraft wiki. https:\/\/minecraft.fandom.com\/wiki, 2023.\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 14953–14962, 2023.\nWilliam H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\narXiv preprint arXiv:1907.13440, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nRongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,\nZhiqing Hong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating speech,\nmusic, sound, and talking head. arXiv preprint arXiv:2304.12995, 2023a.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv,\nLei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045, 2023b.\n10\nPreprint\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In International Conference on\nMachine Learning, pp. 9118–9147. PMLR, 2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022b.\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt\nDeitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment\nfor visual ai. arXiv preprint arXiv:1712.05474, 2017.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-\nimage pre-training with frozen image encoders and large language models.\narXiv preprint\narXiv:2301.12597, 2023.\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang,\nEkin Aky¨urek, Anima Anandkumar, et al. Pre-trained language models for interactive decision-\nmaking. Advances in Neural Information Processing Systems, 35:31199–31212, 2022.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\nOpenAI. Chatgpt. https:\/\/openai.com\/blog\/chatgpt, 2022.\nOpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023.\nJoon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\narXiv:2304.03442, 2023.\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model\nconnected with massive apis. arXiv preprint arXiv:2305.15334, 2023.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824, 2023.\nJenny Preece, Yvonne Rogers, Helen Sharp, David Benyon, Simon Holland, and Tom Carey.\nHuman-computer interaction. Addison-Wesley Longman Ltd., 1994.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Tor-\nralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 8494–8502, 2018.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.\nManolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain,\nJulian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied\nai research. In Proceedings of the IEEE\/CVF international conference on computer vision, pp.\n9339–9347, 2019.\nD´ıdac Sur´ıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128, 2023.\nVicuna\nTeam.\nVicuna:\nAn\nopen-source\nchatbot\nimpressing\ngpt-4\nwith\n90quality.\nhttps:\/\/vicuna.lmsys.org\/, 2022.\n11\nPreprint\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023a.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents.\narXiv preprint arXiv:2302.01560, 2023b.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv\npreprint arXiv:2303.16563, 2023.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068, 2022.\nChong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In European\nConference on Computer Vision, pp. 696–712. Springer, 2022.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n12\nPreprint\nA\nAPPENDIX\nIn this appendix, we offer a detailed introduction of the construction of our open-world instruc-\ntion dataset, as outlined in Appendix A.1, including (1) multimodal perception instructions, (2)\nfoundational knowledge instructions, (3) skill-related interaction instructions, and (4) template of\ninstructional training data. Furthermore, we delve into the skill planning benchmark and its associ-\nated task setups in Appendix A.2. In Appendix A.3 we present qualitative cases that illustrate our\nmodel’s ability to provide intuitive visual feedback and serve as an intelligent chatbot with a multi-\nmodal input-output interface. Finally, we explore the potential applications of our model in diverse\nenvironments, such as Virtual Home (Puig et al., 2018).\nA.1\nDATASET\nA.1.1\nMULTIMODAL PERCEPTION INSTRUCTIONS\nThis dataset contains 200K instructional pairs. Figure 6 illustrates a partial listing of instructional\nquestions employed for describing the content of the Minecraft snapshots. These instructions convey\nsimilar meanings, albeit with slight variations in natural language.\n•\n\"Describe the following Minecraft image in detail\",\n•\n\"Provide a detailed description of the given Minecraft image\",\n•\n\"Give an elaborate explanation of the Minecraft game image you see\",\n•\n\"Share a comprehensive rundown of the presented Minecraft image\",\n•\n\"Offer a thorough analysis of the Minecraft frame\",\n•\n\"Explain the various aspects of the Minecraft image before you\",\n•\n\"Examine the Minecraft image closely and share its details\",\n•\n\"Write an exhaustive depiction of the given Minecraft image“\n•\n\"Clarify the contents of the displayed Minecraft image with great detail\",\n•\n\"Narrate the contents of the Minecraft image with precision\"\nFigure 6. 10 instruction examples for multimodal perception instructions.\nA.1.2\nFOUNDATIONAL KNOWLEDGE INSTRUCTIONS\nThe dataset comprises 250K training instances, which is organized into three distinct subsets: 200K\nicon image instructions, 10K recipe image instructions, and 40K Minecraft-Wiki corpus instructions.\nFor the icon images, we generate questions aimed at prompting the model to recognize and describe\nitem icons within the inventory, as depicted in Figure 7. Similarly, we curate instructional questions\nfor recipe images as shown in Figure 8, with the objective of extracting information on completing\nspecific recipes. In addition, we preprocess the raw Minecraft-Wiki HTML pages by removing\nirrelevant information (e.g., reference links) and unresolved data, transforming the raw corpus into\na formatted, clean Markdown version. Leveraging the capabilities of ChatGPT, we employ this\npowerful language model to generate 10 questions, each with its corresponding answer, for every\npage of the cleaned Wiki corpus. This process yields a collection of 40K single-round question-\nanswer pairs, which can be utilized for instruction tuning.\n13\nPreprint\n•\n\"Provide a brief description of the given recipe image.\"\n•\n\"Offer a succinct explanation of the recipe picture presented.\"\n•\n\"Summarize the recipe content about icons of the image.\"\n•\n\"Give a short and clear explanation of the subsequent recipe image.\"\n•\n\"Share a concise interpretation of the recipe image provided.\"\n•\n\"Present a compact recipe description of the photo's key features.\"\n•\n\"Relay a brief, clear account of the recipe picture shown.\"\n•\n\"Render a clear and concise recipe summary of the photo.\"\n•\n\"Write a terse but informative recipe summary of the picture.\"\n•\n\"Create an icon narrative representing the recipe image presented.\"\nFigure 8. 10 instruction examples of recipe image for foundational knowledge instructions.\n•\n\"Clarify the contents of the displayed inventory image with great attention to detail.“\n•\n\"Characterize the inventory image with a meticulously detailed description.“\n•\n\"Break down the individual slot elements within the inventory image with precision.“\n•\n\"Take a step-by-step journey through the important details of the Minecraft inventory image.“\n•\n\"Paint a vivid and descriptive narrative of the Minecraft inventory image.“\n•\n\"Provide a precise narration of the contents within the Minecraft inventory image.“\n•\n\"Thoroughly analyze the Minecraft inventory image in a comprehensive and detailed manner.“\n•\n\"Illustrate the Minecraft inventory image through a descriptive and informative explanation.“\n•\n\"Examine the Minecraft inventory image closely and share its intricate details.“\n•\n\"Compose an exhaustive depiction of the given Minecraft inventory image.\"\nFigure 7. 10 instruction examples of icon images for foundational knowledge instructions.\nA.1.3\nSKILL-RELATED INTERACTION INSTRUCTIONS\nFor skill prediction, we utilize the skill policies trained by Yuan et al. (2023) to create a dataset\ncomprising 200K skill trajectories. In each trajectory, we extract timestamps from the initial and\nt-th points to generate a snapshot pair, denoted as {I0, It}. We then construct questions aimed at\ndetermining whether the agent successfully executed the skill or, in the case of failure, identifying\nthe underlying reasons for the unsuccessful attempt. Illustrative examples of these skill prediction\nquestions are provided in Figure 9. We also provide examples with snapshot pairs in Figure 10.\n14\nPreprint\n•\n\"\"Steve is demonstrating his proficiency in {SKILL_NAME}, with the objective of achieving {SKILL_DEFINITION}. We'll now assess \nboth the initial and current frames to determine if he has successfully executed the skill and the reasons behind it:\"\n•\n\"\"The skill Steve is performing is {SKILL_NAME}, and its intended outcome is to {SKILL_DEFINITION}. To determine whether Steve \nhas accomplished this skill, we need to analyze both the starting and current frames:“\n•\n\"\"Steve is currently engaged in executing {SKILL_NAME}, aiming to achieve {SKILL_DEFINITION}. In order to evaluate his success in \nperforming this skill, we'll examine both the initial frame and the current frame:“\n•\n\"\"The task at hand for Steve involves mastering {SKILL_NAME}, with the ultimate goal of accomplishing {SKILL_DEFINITION}. To \nascertain whether he has successfully completed this skill, we'll analyze both the starting and current frames:“\n•\n\"\"Steve is in the process of mastering the art of {SKILL_NAME}, with the specific objective of accomplishing {SKILL_DEFINITION}.\nWe will now evaluate whether Steve has successfully executed this skill by comparing the initial and current frames:“\n•\n\"\"The skill that Steve is currently executing is {SKILL_NAME}, and the intended outcome is {SKILL_DEFINITION}. To determine if \nSteve has effectively executed this skill, we'll assess both the initial frame and the current frame:“\n•\n\"\"Steve is currently performing the {SKILL_NAME} skill, with the aim of achieving {SKILL_DEFINITION}. Let's analyze both the start \nframe and the current frame to determine whether he has succeeded and the reasons behind it:“\n•\n\"\"Steve is demonstrating proficiency in the skill of {SKILL_NAME}, which is designed to accomplish {SKILL_DEFINITION}. Our \nevaluation will involve a comparison between the initial and current frames to assess the success of his execution:“\n•\n\"\"The task Steve is undertaking is the mastery of {SKILL_NAME}, with the end goal of achieving {SKILL_DEFINITION}. To determine \nwhether Steve has successfully executed this skill, we will analyze both the initial frame and the current frame:“\n•\n\"\"Steve is in the process of executing the {SKILL_NAME} skill, with the ultimate aim of accomplishing {SKILL_DEFINITION}. Our\nassessment will involve a comparison between the starting frame and the current frame to determine if he has succeeded and why:“\nFigure 9. 10 instruction examples for skill prediction instructions.\nQ: Steve is finding logs nearby, did he \ncomplete this skill?\nA: No.\nQ: Why does he fail?\nA: Because Steve has died.\nQ: Steve is finding sheep nearby, does \nhe fail?\nA: No. Steve found a sheep in sight.\nQ: Steve plans to craft a table, does he \nfinish this skill successfully?\nA: Yes he does.\nQ: Steve is asked to collect planks, what \nis the results?\nA: Steve collect planks successfully.\nQ: Steve collects wool in Minecraft, \nwhat is the consequence?\nA: Steve fails to finish the sill.\nQ: Please help me analyze the reasons \nfor the failure.\nA: Sure, there is no sheep in sight\nQ: Steve is mining cobblestone, does he \nfinish this skill?\nA: No.\nQ: How do you know that?\nA: No additional cobblestone shown in \nthe inventory\nFigure 10. Illustrative examples of skill prediction instruction data with snapshot pairs.\nTemplate of Instructional Training Data. Similar to Liu et al. (2023), we formulate each instruc-\ntional instance as a multi-round conversation as shown in Figure 11, where Xhead message is a sentence\nto describe this assistant (e.g., “ You are in a chat between a curious human and an artificial intelli-\ngence assistant. You should serve as an assistant to give helpful, detailed, and polite answers to the\nhuman’s questions.”). The number of rounds relies on the input instruction content. And the input\nimages (denoted as < image >) will only be fed in the first round, while XC may contain visual\noutputs with two additional tokens < vis > and < \/vis >.\n15\nPreprint\nFigure 11. The unified template to generate input sequence for instructional tuning.\nA.2\nSKILL PLANNING BENCHMARK\nTo clarify this benchmark, we begin by offering comprehensive task setup details in Table 6. During\nthe evaluation phase, we relocate the agent to a random location at the initiation of every episode,\nwith distances of up to 500 units, ensuring that the agent spawns in an unfamiliar environment. Fur-\nthermore, for tasks that involve interacting with mobs, we enforce a maximum spawning distance of\n30 units for cows and sheep. Our approach to complete tasks is rooted in a hierarchical framework.\nSpecifically, our model exclusively generates high-level skill plans, delegating the actual skill exe-\ncution to pre-trained basic skill policies as introduced by Yuan et al. (2023). Notably, we introduce\na self-driven variant named ’Steve-Eye-auto,’ which serves not only as a planner but also replaces\nthe Minecraft rules to verify the successful execution of skills.\nTable 6. The setups of 24 tasks used in our skill planning evaluation, where “Initial Items” refers\nto the tools provided in the agent’s inventory at the beginning of each episode, and “Max Steps”\nrepresents the maximum episode duration. Any episode exceeding this limit is classified as a task\nfailure. The tasks are originally developed by Yuan et al. (2023).\n(a) 7 tasks involving the process of “cutting trees to craft primary items”.\nTask Icon\nTask Name\nstick\ncrafting table nearby\nbowl\nchest\ntrap door\nsign\nwooden pickaxe\nInitial Items\n-\n-\n-\n-\n-\n-\n-\nMax Steps\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n(b) 7 tasks involving the process of “mining cobblestones to craft advanced items”.\nTask Icon\nTask Name\nfurname nearby\nstone stairs\nstone slab\ncobblestone wall\nlever\ntorch\nstone pickaxe\nInitial Items\n*10\n*10\n*10\n*10\n*10\nMax Steps\n5000\n5000\n3000\n5000\n5000\n5000\n10000\n(c) 10 tasks involving the process of “interacting with mobs to harvest food and materials”.\nTask Icon\nTask Name\nmilk\nbucket\nwool\nbeef\nmutton\nbed\npainting\ncarpet\nitem\nframe\ncooked\nbeef\ncooked\nbutton\nInitial Items\n,\n*3\n,\n*2\n,\n,\n,\n,\n,\nMax Steps\n3000\n3000\n3000\n3000\n10000\n10000\n3000\n10000\n10000\n10000\n16\nPreprint\nTable 7. The setups of 10 long-horizon iron-based tasks, where “Initial Items” are provided in the\nagent’s inventory at task beginning, and “Max Steps” refers to maximum environmental steps.\nTask icon\nTask description\nInitial tools\nMax steps\ncraft iron ingot\n*5,\n*64\n8000\ncraft shears\n*5,\n*64\n10000\ncraft bucket\n*5,\n*64\n12000\ncraft iron pickaxe\n*5,\n*64\n12000\ncraft iron axe\n*5,\n*64\n12000\ncraft iron sword\n*5,\n*64\n10000\ncraft iron shovel\n*5,\n*64\n8000\ncraft tripwire hook\n*5,\n*64\n8000\ncraft heavy weighted pressure plate\n*5,\n*64 t\n10000\ncraft iron trapdoor\n*5,\n*64\n12000\nfurnace\ngold ingot\ngreen candle\noak boat\norange bed\npainting\n(a) Qualitative examples of recipe image generation.\ntipped-arrow\nacacia-sign\nstone-axe\n(b) Illustrative examples of recipes that our model struggles to predict accurately. We attribute this failure to\nthe complexities arising from fine-grained or semantically overlapping image information.\nMinecraft \nEnvironment Feedback.\nSure, here is the plan list\nto harvest milk: (1) find\ncow\nnearby,\n(2)\nmilk\nbucket.\nHi, Steve, this is your birth-\nplace, what do your see?\nI can see the sky, trees in\nfront of me. I have 20\nlives and 10 food. There\nis a bucket in my invent-\ntory\nI want you to harvest milk.\nThere is a cow in front\nof me, finding cows s-\nuccessfully!\nThe plan list to harvest\nmilk: (1) milk bucket.\nRound 1: find cow nearby\nRound 2: milk bucket\nMake a plan again.\nMinecraft \nEnvironment Feedback.\nmilk\nbucket\nfinished!\nThe task is success-\nfully completed\nThank you, Steve.\n17\nPreprint\nTask Name: relax on sofa\nTask Description: I go to the living room and sit in the sofa\nTask Plan: {“1”: walk living room, “2”: walk couch, “3”: find couch, “4”: walk couch, “5”: sit couch}\nTask Name: browse the Internet\nTask Description: I go to the office and sit in a chair, I turn on the computer and grab the mouse. I \ntype on the keyboard and starting working on the computer.\nTask Plan: {“1”: walk living-room, “2”: walk desk, “3”: find desk, “4”: find chair, “5”: sit chair, “6”: find\ncomputer, “7”: switch-on computer, “8”: find mouse, “9”: grab mouse, find keyboard, “10”: type\nkeyboard, “11”: turn-to computer, “12”: look-at computer}\nTask Name: put milk in the freezer\nTask Description: I walk into kitchen, look for the milk, walk to milk, look for refrigerator, walk to \nrefrigerator, open door, put the milk in the refrigerator\nTask Plan: {“1”: walk dining-room, “2”: walk milk, “3”: find milk, “4”: turn-to milk, “5”: grab milk, “6”: \nlook-at freezer, “7”: walk freezer, “8”: open freezer, “9”: put milk\nFigure 14. Task examples from the extended Virtual-Home benchmark, where elements in green,\ncyan and red represent action, room, and object categories, respectively. Our benchmark includes a\ndiverse range of tasks that simulate interactions between individuals and their room environments.\nIt contains over 50 distinct room setups, involving 20 unique actions, and 100 objects. Each room\npresents a selection of more than 200 distinct tasks.\nA.3\nQUALITATIVE RESULTS OF MULTIMODAL GENERATION\nA.3.1\nRECIPE IMAGE GENERATION\nFigure 12a showcases qualitative examples of our evaluation on the FK-QA IMG dataset. Utilizing\na visual tokenizer like VG-GAN, our model demonstrates the ability to engage in visual generation,\nenabling it to provide visual feedback based on its comprehension of textual input. However, as\nshown in Figure 12b, our model encounters difficulties when generating image content characterized\nby fine-grained or semantically overlapping elements. These challenges warrant further exploration\nin our future work.\nA.3.2\nMULTIMODAL CHATBOT\nIn Figure 13, We present an overview of Steve-Eye functioning as a chatbot to receive task com-\nmands and execute them.\n18\nPreprint\nA.4\nDISCUSSION OF OPEN-WORLD EXPLORATION\nIn this paper, we have selected Minecraft as our open-world platform. Nevertheless, it is evident that\nSteve-Eye can be applied to other open-world environments, such as Virtual Home (Puig et al., 2018)\nand AI2THOR (Kolve et al., 2017), with minimal manual effort using the same methodology in this\npaper. These alternative benchmarks, when compared to Minecraft, exhibit a closer alignment with\nthe real world. To some extent, this choice holds greater significance since our ultimate objective is\nto deploy the agent in the real world. To achieve this goal, we expand the Virtual Home benchmark\nby introducing a more extensive range of environments (50+ rooms), human-interaction tasks (200+\nfor each room), as well as diverse categories of actions (20+) and objects (100+), as illustrated in\nFigure 14. The corresponding validation and further exploration of open-ended embodied agents in\na real-world context will be the focus of our future work.\n19\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Steve-Eye：为基于LLM的具身智能体赋予开放世界的视觉感知能力\n\n## 📌 背景痛点\/本文动机\n近年来，大型语言模型（LLMs）在赋予具身智能体与世界互动的能力方面取得了显著进展，这标志着通用机器人技术迈出了第一步。然而，这些研究往往忽略了开放世界的视觉丰富性，导致整个交互过程类似于“一个蒙着眼睛的基于文本的游戏”。因此，基于LLM的智能体在直观地理解周围环境和生成易于理解的响应方面经常遇到挑战。\n\n## 🚀 核心方法\n为了解决这一局限性，本文提出了Steve-Eye，这是一个端到端训练的大型多模态模型，旨在赋予基于LLM的具身智能体在开放世界中进行视觉感知的能力。Steve-Eye将LLM与视觉编码器相结合，使其能够处理视觉-文本输入并生成多模态反馈。此外，我们使用半自动策略收集了一个包含850K开放世界指令对的广泛数据集，使我们的模型能够涵盖智能体的三个基本功能：多模态感知、基础知识库和技能预测与规划。最后，我们开发了三个开放世界评估基准，然后从广泛的视角进行大量实验，以验证我们的模型在战略行动和规划方面的能力。\n\n## 📈 实验结果\n实验结果表明，Steve-Eye在开放世界场景中显著优于基于LLM的智能体。具体来说，我们在以下三个基准上进行了评估：\n1. 环境视觉描述（ENV-VC）：评估智能体感知和描述其周围环境的能力。\n2. 基础知识问答（FK-QA）：评估智能体掌握对决策至关重要的基本知识的熟练程度。\n3. 技能预测与规划（SPP）：量化智能体在战略行动和规划方面的能力。\n\n## 💬 可借鉴之处\nSteve-Eye的研究成果为开发能够在开放世界中有效互动的具身智能体提供了重要的启示。其多模态感知、基础知识库和技能预测与规划功能为智能体在复杂环境中进行自主行动和规划提供了强大的支持。此外，Steve-Eye的开放世界评估基准为评估具身智能体的性能提供了有价值的参考。","llm_summary_res_status":200}
{"title":"MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception","authors":"Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, Jing Shao","summary":"It is a long-lasting goal to design an embodied system that can solve\nlong-horizon open-world tasks in human-like ways. However, existing approaches\nusually struggle with compound difficulties caused by the logic-aware\ndecomposition and context-aware execution of these tasks. To this end, we\nintroduce MP5, an open-ended multimodal embodied system built upon the\nchallenging Minecraft simulator, which can decompose feasible sub-objectives,\ndesign sophisticated situation-aware plans, and perform embodied action\ncontrol, with frequent communication with a goal-conditioned active perception\nscheme. Specifically, MP5 is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs), and the system is modulated into functional\nmodules that can be scheduled and collaborated to ultimately solve pre-defined\ncontext- and process-dependent tasks. Extensive experiments prove that MP5 can\nachieve a 22% success rate on difficult process-dependent tasks and a 91%\nsuccess rate on tasks that heavily depend on the context. Moreover, MP5\nexhibits a remarkable ability to address many open-ended tasks that are\nentirely novel.","url":"http:\/\/arxiv.org\/abs\/2312.07472v4","pdf_url":"http:\/\/arxiv.org\/pdf\/2312.07472v4","published":1702403745000,"comment":"Accepted to CVPR2024","pdf_text":"MP5: A Multi-modal Open-ended Embodied System\nin Minecraft via Active Perception\nYiran Qin1,2*,\nEnshen Zhou1,3*,\nQichang Liu1,4*,\nZhenfei Yin1,5,\nLu Sheng3†,\nRuimao Zhang2†,\nYu Qiao1,\nJing Shao1‡\n1Shanghai Artificial Intelligence Laboratory\n2The Chinese University of Hong Kong, Shenzhen\n3School of Software, Beihang University\n4Tsinghua University\n5The University of Sydney\nyiranqin@link.cuhk.edu.cn\nzhouenshen@buaa.edu.cn\nlsheng@buaa.edu.cn\nruimao.zhang@ieee.org\nshaojing@pjlab.org.cn\nforest\nday\ngrass\nProcess\nContext\nDay\nStone\nWood\nWater\nPig\nGrass\nTask: Kill a pig with a stone sword during the \ndaytime near the water with grass next to it .\nForest\nPlains\nlog\npig\nplains\nwater\nstone\nnull\n: log\n: plank\n: stick\n: crafting table\n: wooden pickaxe\n: stone\n: stone sword\n: pig\n0\n1\n2\n3\n4\nProcess\nContext #\n(a)\n(b)\n(c)\nFigure 1. The process of finishing the task “kill a pig with a stone sward during the daytime near the water with grass next to it.”. (a)\nTo achieve the final goal (i.e., O8: “kill a pig\n”), a player should accomplish a list of sub-objectives {Oi}7\ni=1 sequentially. During this\nprocess, the player should also be aware of some items in the environment, e.g., “grass\n”, “day\n” and etc. (b) This diagram shows\nthe number of these necessary items in the context that should be perceived for each sub-objective, during the task execution process. (c)\nImages marked by O1 and O6 show the observed ego-centric views in the process of achieving the corresponding sub-objectives. Images\nmarked by O1\n8 and O2\n8 indicate how the player executes the action about the last sub-objective “kill a pig\n”. This exemplar process tells\nthat such long-horizon open-world embodied tasks in Minecraft should be solved both in process-dependent and context-dependent way.\nAbstract\nIt is a long-lasting goal to design an embodied system\nthat can solve long-horizon open-world tasks in human-like\nways. However, existing approaches usually struggle with\ncompound difficulties caused by the logic-aware decompo-\nsition and context-aware execution of these tasks. To this\nend, we introduce MP5, an open-ended multimodal em-\nbodied system built upon the challenging Minecraft sim-\nulator, which can decompose feasible sub-objectives, de-\nsign sophisticated situation-aware plans, and perform em-\nbodied action control, with frequent communication with\n∗Equal contribution\n† Corresponding author\n‡ Project leader\na goal-conditioned active perception scheme. Specifically,\nMP5 is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs), and the system is mod-\nulated into functional modules that can be scheduled and\ncollaborated to ultimately solve pre-defined context- and\nprocess-dependent tasks. Extensive experiments prove that\nMP5 can achieve a 22% success rate on difficult process-\ndependent tasks and a 91% success rate on tasks that heav-\nily depend on the context. Moreover, MP5 exhibits a re-\nmarkable ability to address many open-ended tasks that are\nentirely novel.\nPlease see the project page at https:\n\/\/iranqin.github.io\/MP5.github.io\/.\narXiv:2312.07472v4  [cs.CV]  26 Mar 2024\n1. Introduction\nOne of the core objectives of current embodied intelligence\nis to construct generalist agents that can solve long-horizon\nopen-world embodied tasks, approaching the behavior pat-\nterns of human beings [1, 19, 22, 31]. However, the process\ndependency and context dependency in these tasks, such as\nthose in Minecraft depicted in Fig. 1, hinder recent agents\nfrom achieving the aforementioned goal. To be specific, the\nformer emphasizes the inherent dependency among the sub-\nobjectives of one task or an action sequence to fulfill one\nsub-objective (such as “craft a stone sword\n” should be\nsolved before “kill a pig\n”). The latter highlights that the\nexecution of each sub-objective or even each action depends\non the contextual information of the environment (such as\n“kill a pig\n” requires to find the target “pig\n” and its sur-\nrounding items “grass\n” and “water\n” during the “day-\ntime\n” in the observed images, as shown in Fig. 1).\nThe recent success of Large Language Models (LLMs)\nhas\nattempted\nto\nsolve\nthe\nprocess-dependent\nchal-\nlenge, by using LLMs to break down a long-horizon\nprocess-dependent task into a sequence of feasible sub-\nobjectives [34, 36, 43]. These methods [34, 43] simplify\nthe context-dependent challenge by assuming the agents are\nall-seeing, i.e., knowing everything about their state and the\nenvironment it locates in. However, to solve the context-\ndependent challenge, an embodied agent should addition-\nally have: (1) the perception capability is open-ended, se-\nlective and give results tailored to diverse purposes (e.g.,\nfor task planning or action execution), (2) the perception\nmodule can be compatibly scheduled along with the other\nmodules (e.g., planning and execution modules) by a uni-\nfied interface, as an integrated system.\nTo this end, we introduce MP5, a novel embodied sys-\ntem developed within Minecraft, to meet the above expec-\ntations. Specifically, MP5 comprises five interacting mod-\nules, i.e., Parser decomposes a long-horizon task into a\nsequence of sub-objectives that should be completed one\nby one; Percipient answers various questions about the ob-\nserved images, as the reference for the other modules; Plan-\nner schedules the action sequences of a sub-objective, as\nwell as refines the following sub-objectives, given the cur-\nrent situation; Performer executes the actions along with\nfrequent interaction with the environment; and Patroller\nchecks the responses from the Percipient, Planner, and Per-\nformer, for the purpose of verifying current plans\/actions, or\nfeedback on potential better strategies. In our work, Percipi-\nent is a LoRA-enabled Multimodal LLM (MLLM). Among\nthe pre-trained LLMs, Parser and Planner are augmented\nwith external Memory, while Patroller is not.\nNotably, MP5 includes an active perception scheme by\nmeans of multi-round interaction between Percipient and\nPatroller, which is to actively perceive the contextual in-\nformation in the observed images, with respect to vari-\nous queries raised by Planner and Performer.\nIt is the\nkey enabler to solve context-dependent tasks.\nPatroller\nin this scheme relays compatible feedback to Planner and\nPerformer accordingly, while eventually strengthening the\nplanning skill in awareness of the situations and improving\nthe action execution correctness in an embodied manner.\nExtensive experiments prove that MP5 can robustly\ncomplete tasks needed for long-horizon reasoning and com-\nplex context understanding. It achieved a 22% success rate\non diamond-level tasks (i.e., one of the hardest long-horizon\ntasks) and a 91% success rate on tasks requiring complex\nscene understanding (i.e., need to perceive around 4 ∼6\nkey items in the observed images). Moreover, in Sec. 4.2.3,\nMP5 can surprisingly address more open-end tasks both\nwith heavy process dependency and context dependency.\n2. Related Work\n2.1. Multi-modal Large Language Models\nWith the development of Large Language Models (LLMs)\nlike the GPT series [2, 27, 29], as well as open-source LLMs\nsuch as the LLaMA series [32, 33] and Vicuna [5], Multi-\nmodal Large Language Models (MLLMs) have emerged.\nExamples of such MLLMs include LLaVA [21], Instruct-\nBLIP [6], and LAMM [39], among others [4, 10, 15, 28, 38,\n42]. In this work, we introduce MineLLM, which is specif-\nically designed and trained for Minecraft, and leverage its\nperception, interaction, and analysis capabilities to build\nup Percipient for MP5, and further enable an objective-\nconditioned active perception scheme.\n2.2. Agents in Minecraft\nPrevious works[3, 7, 9, 11, 19, 22, 40, 41] attempt to use\napproaches such as hierarchical RL, goal-based RL, and re-\nward shaping to train an agent in Minecraft. MineCLIP [9]\nenables the resolution of various open-ended tasks speci-\nfied in free language, even without any manually designed\ndense rewards. DreamerV3 [12] succeeds in training agents\nin Minecraft with a learned world model. VPT [1] builds\na foundation model for Minecraft by learning from massive\nvideos. Based on VPT, Steve-1 [18] also explores bring-\ning in MineCLIP [9] to get an instruction following policy\nwith high performance. The development of recent large\nlanguage model-related work Voyager [34], DEPS [36],\nGITM [43] further promote the advancement of agents\nin long-horizon tasks. These works use pre-trained large\nlanguage models as the zero-shot planners[14] for agents,\nleveraging the powerful reasoning capabilities of large lan-\nguage models to obtain continuous operation instructions or\nexecutable policy lists.\nWe take advantage of the reasoning capability of LLM\nto build up our own agent. Existing LLM agents [43, 43] in\nMinecraft feed scene data from simulation platforms [9, 11]\nPerformer Memory\nKnowledge\nMemory\nPatroller\nPercipient \nObtain Env. \nInfo. for \nPlanning\nObtain Env. \nInfo. for \nPerformer\nMulti-round\nSingle-round\n<Sub-Objective>\nParser\nTask: Kill a pig with a wooden sword during the daytime near \nthe water with grass next to it.\nActive \nPerception\n…\nError \nFeedback\nRe-plan\nPerformer: Having completed a move in “Find” action, based on my current view, tell \nme if I should continue this action or if the next action is ready to execute. \n<Sub-Objective>\nPatroller: I conduct Active Perception with Percipient with your current observation, \nyou must continue with the current action since there is no river near the pig.\nPatroller: I conduct Active Perception with Percipient with your current observation, \nyou can execute the next action since all conditions are satisfied.\nPlanner: Can you tell me what important environmental information I need to know?\nPatroller: I conduct Active Perception with Percipient with your current observation, \nthere is no pig based on the scene.\nPlanner: 1. Equip(       )  2. Find(        )  3. Move(     )  4. Fight(        )  \nPerformer: Having completed a move in “Find” action, based on my current view, tell \nme if I should continue this action or if the next action is ready to execute. \nPerformer: Start executing “Equip”.\nPerformer: Continue executing “Find”.\nSub-Objectives\n{\n}\n…\n…\n…\n…\n…\n…\nMove\nMine\nEquip\nFight\nFind\nCraft\n…\nFigure 2. Overview of module interaction in MP5. After receiving the task instruction, MP5 first utilizes Parser to generate a sub-objective\nlist. Once a sub-objective is passed to the Planner, the Planner Obtaining Env. Info. for Perception-aware Planning. The performer takes\nfrequently Perception-aware Execution to interact with the environment by interacting with the Patroller. Both Perception-aware Planning\nand Execution rely on the Active Perception between the Percipient and the Patroller. Once there are execution failures, the Planner will\nre-schedule the action sequence of the current sub-objective. Mechanisms for collaboration and inspection of multiple modules guarantee\nthe correctness and robustness when MP5 is solving an open-ended embodied task.\ninto large language models for task planning. However, for\nembodied agents in real scenes, it is clearly unrealistic to\nuse accurate scene data directly. Therefore, agents need to\nbe robust to make decision corrections despite inaccurate or\nerroneous perception information. Moreover, open-ended\ntasks need hierarchical reasoning [22] and complex open-\nended context understanding [1, 9], classical perception net-\nworks can only output fixed perception results and cannot\nprovide corresponding perception information according to\nthe task, making it impossible to understand open-ended\nscenarios. Therefore, we design MP5, an embodied agent\nwith open-ended capabilities that can solve the problem of\nopen-ended tasks.\n3. Method\nIn this section, we first give an overview of our proposed\nMP5, for solving context-dependent and process-dependent\ntasks in an open-world and embodied environment, such as\nMinecraft (Sec. 3.1). Next, we elaborate on how to imple-\nment an active perception scheme (Sec. 3.2). This scheme\nplays a vital role in MP5 to solve context-dependent tasks,\nsince it reliably grounds the visual content according to dif-\nferent kinds of objectives, and thus strengthens the plan-\nning skill and execution correctness with respect to context-\ndependent tasks. Then, we show how to plan and update\naction sequences in awareness of the situations, and how to\nreliably execute these actions in an embodied environment\n(Sec. 3.3). Finally, we give necessary implementation de-\ntails about MP5 in Sec. 3.4.\n3.1. Overview\nAs demonstrated in Fig. 2, our MP5 includes five major\nmodules, i.e., Parser, Percipient, Planner, Performer, and\nPatroller. Specifically, Percipient is a parameter-efficiently\nfine-tuned Multimodal Large Language Model (MLLM)\nthat is specified to the Minecraft environment. The Parser,\nPlanner, and Patroller are pre-trained Large-language Mod-\nels (LLMs). We also include retrieval-augmented genera-\ntion (RAG) to enhance the quality of responses generated\nby Parser and Planner. Performer is an interface that ex-\nplains each action from the action sequence into executable\ncommands that directly control the game character.\nWhy can MP5 solve context-dependent and process-\ndependent tasks?\nMP5 includes an active perception\nscheme by means of multi-round interactions between Per-\ncipient and Patroller, which is to actively perceive the envi-\nronmental information in the observed images, with respect\nto various objectives raised by Planner or Performer. With\nthe help of this scheme, Planner can schedule or update ac-\ntion sequences in awareness of the observed images, inven-\ntory status and etc., resulting in a situation-aware planning;\nPerformer can execute actions that are adapted to the em-\nbodied environment, resulting in a embodied action execu-\ntion. Patroller in this scheme can also feedback on better\nchoices of plans\/actions based on the visual evidence so that\nthe process-dependent tasks are solved with fewer chances\nof context-dependent execution failures. Moreover, Percip-\nient can understand open-ended visual concepts, therefore\nit allows MP5 to solve tasks that are never seen before.\nHow does MP5 function?\nIn Fig. 2, upon receiving a\nResponse\nThis picture is showing a pig, \ntypes of Minecraft mobs.\nQuestion\nWhat types of Minecraft mobs is \nthis <image> showing?\nPerformer : Current sub-objective <Kill a pig with a wooden sword during the daytime near the water with grass next to it>    &    Current execution action: Find (          )\nEnv. Info.\nResponse\nThe blocks shown in the image \nare water, grass …\nQuestion\nWhat types of blocks are shown in \nthe given <image> ?\nResponse\nThe image depicts daytime in \nthe Minecraft world.\nQuestion\nWhat time of day does the <image>\ndepict in the Minecraft world?\nResult\nI have seen a pig during the \ndaytime near the water with \ngrass next to it. You can execute \nthe next action.\nAsk question(s) of \nsignificant necessity\nAnswer the question\nAdd answer to Env. Info.\nPatroller\nPatroller\nPatroller\nPerceived env info.\nTemporary Env. Info. Set\nEnv. Info.\nEnv. Info.\nEnv. Info.\nEnv. Info.\nPatroller\nReset\nQuery Env. Info. \nOr Return\nPercipient \nPercipient \nPercipient \nFigure 3. A demonstration of the process of Active Perception scheme. Temporary Env. Info. Set saves information collected in the current\nscenario, so it should be reset at the beginning of Active Perception scheme. Performer then invokes Patroller to start asking Percipient\nquestions with respect to the description of the sub-objective and the current execution action round by round. The responses of Percipient\nare saved in Temporary Env. Info. Set and are also gathered as the context for the next question-answering round. After finishing asking all\nsignificant necessary questions, Patroller will check whether the current execution action is complete by analyzing the current sub-objective\nwith Perceived env info. saved in Temporary Env. Info. Set, therefore complex Context-Dependent Tasks could be solved smoothly.\nhigh-level task, MP5 first utilizes the Parser to generate a\nsequence of short-horizon sub-objectives, as a list of rich\ninstructions in natural languages. The feasibility of the gen-\nerated sub-objectives is augmented by retrieving an external\nKnowledge Memory. This knowledge mainly comes from\nthree sources: part of it is from the online wiki, another part\nis from the crafting recipes of items in MineDojo [9], and\nsome are user tips from Reddit. To one sub-objective, Plan-\nner schedules the action sequence that is grounded by the\nenvironmental information gathered by the active percep-\ntion scheme. In this case, Performer will execute the actual\nactions by explaining the action sequence that is adapted\nto the embodied environment, via frequent interaction with\nthe active perception scheme. Once there are execution fail-\nures (determined by Patroller), Planner will re-schedule the\naction sequence of the current sub-objective, or even up-\ndate the following sub-objectives if some necessary sub-\nobjectives are missing.\nOtherwise, the agent will go to\nthe next sub-objective and schedule new action sequences,\nwhilst the successful action sequence of the current sub-\nobjective will be stored in the external memory of Planner\n(called Performer Memory), along with the agent situation\nwhen it was planned. In the end, the agent will stop when\nthe final sub-objective of the task has been reached.\n3.2. Active Perception\nLet’s take the example shown in Fig. 3 to demonstrate how\nthe active perception scheme works. In this example, the\nactive perception scheme is communicated with Performer\nto enable an embodied action execution.\nAt first, Performer invokes Patroller to start asking Per-\ncipient questions with respect to the description of the\nsub-objective and the current execution action, while si-\nmultaneously resetting the set of environmental informa-\ntion to be gathered.\nThen Patroller progressively asks\nPercipient whether the observed image contains necessary\nitems\/factors (e.g., mobs\n, blocks\n, time\n)\nrelated to recent sub-objective (e.g., pig\n) and the execut-\ning action (e.g., “find pig\n”). The responses of Percipient\nare also progressively gathered and act as the context for\nthe next question-answering round. Note that in each round,\nPatroller also checks whether all the necessary items\/factors\nhave been collected - If yes, Patroller stops the interaction\nand returns all the environmental information as natural lan-\nguage, and invokes Performer to execute the next action. If\nPatroller eventually fails to gather enough items\/factors, it\nwill tell Performer what items\/factors are missing in the ob-\nserved images, which suggests Performer keeps executing\nthe current action. Please also check the example shown in\nFig. 2.\nSimilarly, active perception used in situation-aware plan-\nning is similar to what is explained here, except that the ap-\nplied instructions do not contain the executable action. For\nmore details please check the Sup. E.\n3.3. Perception-aware Planning and Execution\nSituation-aware Planning. Given one sub-objective, Plan-\nner will generate the action sequence based on the descrip-\ntion of the situation, such as the objective-conditioned envi-\nronmental information from the active perception scheme,\nthe inventory status and localization, and etc. Moreover,\nPlanner will retrieve previous successful action sequences\nas the demonstration prompt to augment the aforemen-\ntioned planning results. If the active perception scheme fails\nto find the key items\/factors about the current sub-objective\nin the observed image, the generated action sequences will\ninclude more actions to reach them. Moreover, if Performer\nencounters execution failures determined by Patroller (such\nImage Encoder\n🔥Alignment Net\n🔥\nLoRA\nLarge Language Model\nInstruction\nWhat types of Minecraft mobs is \nthis <image> showing?\nResponse\nThis picture is showing a pig, \ntypes of Minecraft mobs.\n🔥\nFinetune\nFrozen\nFigure 4. The model architecture of MineLLM. Image is encoded\nby a pre-trained vision encoder and decoded by LLM. Only the\nparameters of Alignment Net and LoRA are trainable.\nas failure of “equip wooden sword\n”), Planner will re-\nschedule the action sequence or even update the following\nsub-objectives, with the help of external memories.\nEmbodied Action Perception. As indicated in Sec. 3.2,\nPerformer would like to communicate with the active per-\nception scheme in every round of action execution, so as\nto enhance the ego-centric awareness of the agent. The new\naction will be executed if Patroller identifies necessary envi-\nronmental information in the observed images that matches\nboth the sub-objective and the goal of the current action.\nOtherwise, the current action is kept executing until en-\ncountering execution failures or the end of the episode. The\nsuccessful action sequence about one sub-objective will be\nstored in the Performer Memory, together with necessary\nsituational information of the agent when it was planned.\nFor more details about the planning and execution process,\nplease check Sup. G.2 and Sup. B.2.\n3.4. Implementation Details\nPercipient.\nThe network of Percipient is depicted in\nFig. 4. Images are processed by a frozen vision encoder\nMineCLIP [9], whose features are projected by an Align-\nment Net(we use two-layer MLP like LLaVA-1.5 [20]) to\nthe same feature space as the text embeddings of the ap-\nplied LLM (we use Vicuna-13B-v1.5 [5]). Then the vision\nand text tokens are concatenated to feed into a LoRA-based\nfine-tuned LLM [13]. We add LoRA [13] parameters to\nall projection layers of the self-attention layers in the LLM.\nOnly the parameters of the Alignment Net and the LoRA\nmodule are optimized during training. The construction of\nthe training data with respect to Percipient is in the Sup. B.1.\nParser, Planner, and Patroller. We utilize OpenAI’s GPT-\n4 [26] as LLMs in Parser, Patroller, and Planner. We also\nevaluate other alternatives of GPT-4 [26], such as open-\nsource models like Vicuna-13B-v1.5 [5] and LLaMA2-\n70B-Chat [33] in Sup.D.3.\nPerformer. It is important to clarify that the actions gen-\nerated by Planner are not low-level commands such as key-\nboard and mouse operations [1], but a set of simple actions\n(such as equip, move, craft). Inspired by GITM [43], we\nimplement these actions appropriately through basic oper-\nations provided by the MineDojo [9] simulator. For more\ndetails, please check the Sup. B.2.\n4. Experiments\nAt first, we depict the setup of the Minecraft simulation en-\nvironment that we build and validate MP5, and give the\ndefinition of the evaluated tasks and how to set them in\nSec. 4.1. In Sec. 4.2, we present the quantitative and quali-\ntative performance of MP5, as well as in-depth discussions\non these tasks, and demonstrate that MP5 can even success-\nfully accomplish tasks that are more open-ended and never\nseen before. At last, we investigate how different modules\naffect the performance of MP5 and analyze the impact of\nvarious module choices within our system in Sec. 4.3.\n4.1. Experimental Setup\nEnvironment Setting. We employ MineDojo [9] as our\nsimulation environment to build and validate MP5. We cap-\nture player ego-view images provided by MineDojo [9] as\ninput of MP5, and further construct a dataset for training\nMineLLM. As for the output of MP5, we encapsulate Mine-\nDojo’s [9] actions to create our own action space.\nTask Setting. To evaluate how our MP5 can integrate per-\nception information with planning and execution, we define\ntwo types of tasks: Context-Dependent Tasks and Process-\nDependent Tasks as illustrated in Tab. 1 and Tab. 2.\n1) Context-Dependent Tasks primarily study how Active\nPerception enables the agent to better perceive low-level\ncontext information in the environment.\nWe first estab-\nlish 6 aspects of environmental information derived from\nthe Minecraft game environment: [Object, Mob, Ecology,\nTime, Weather, Brightness]. Each aspect has multiple op-\ntions.\nFor example, pigs\n, cows\n, and sheep\nare\nall elements belonging to Mob.\nBased on this, we de-\nfine 16 tasks and organize their difficulty into four levels\nby taking into account the number of information elements\nthat require perception, as is shown in Tab. 1. For exam-\nple, Easy tasks necessitate the perception of only one el-\nement, whereas Complex tasks involve the perception of\n4 to 6 elements. We rigorously assess MP5’s proficiency\nin environmental context perception across these 16 tasks.\nIn Context-Dependent Tasks, our environment details are\npredetermined (e.g., biomes\n, weather\n, and\netc.), as certain targets are exclusive to specific environ-\nments.\nWithout this environmental specificity, the agent\nmight never encounter the intended target. We retain each\nobservation of active perception throughout the task, using\nthem as references to ascertain the agent’s successful com-\npletion of the task.\n2) Process-Dependent Tasks focus on exploring the con-\nTable 1. Context-Dependent Tasks. 16 tasks are defined and di-\nvided into 4 difficulty levels based on the minimum number of\ninformation types needed. Underlines label the environmental in-\nformation, reflecting the complexity varies at each level.\nTask Level\nExample Task\nEasy\nFind a tree\nMid\nFind a tree\nin the forest\nHard\nFind a tree\nin the forest\nduring the nighttime\nComplex\nFind a pig\nnear a grass\nin the forest\nduring the daytime\ntributions of situation-aware planning, embodied action ex-\necution, and the integration with Active Perception in ac-\ncomplishing long-term tasks while constantly perceiving\nthe environment and dynamically adjusting actions. We se-\nlect 25 tasks from the technology tree and define their dif-\nficulty levels as Basic level\nto Diamond level\nbased\non the number of reasoning steps required to complete\nthe tasks. All environmental factors (e.g., biomes\n,\nweather\n, and etc.) are randomized in Process-\nDependent Tasks. More details can be found in Sup.D.1.\nEvaluation Metrics. For different tasks, the agent’s initial\nposition and environment seed are randomized. The agent\nbegins in survival mode, commencing with an empty inven-\ntory, and faces the challenge of hostile mob generation. It\nstarts from scratch, with a game time limit of 10 minutes, a\ntime period equivalent to 12,000 steps at a control frequency\nof 20Hz. More details can be found in Sup. C.\nFor the Context-Dependency Tasks, each assignment is\nopen-ended.\nTherefore, we conduct manual evaluations\nwhen the agent determines it has completed the task or\nexceeds the time limit.\nTwo cases are ruled as failures:\n1)There is an observation that meets all the conditions, but\nthe agent does not end the task; 2) The last observation\ndoes not meet all the conditions, yet the agent ends the task.\nOtherwise, we believe that the agent correctly perceives all\nthe context according to the task and determines that the\ntask is successfully completed. For the Process-Dependent\nTasks, any accidental deaths of the agent during the game\nare counted as failures, as are instances where the agent\ndoes not accomplish the task within the time limit.\nIn practice, we conduct 50 games on Context-Dependent\nTasks and 30 games on Process-Dependent Tasks, averaging\nthe success rates for both. The results are grouped accord-\ning to the previously defined difficulty levels, and report\nthe group means. For detailed definitions of the evaluation,\nplease refer to Sup. D.\n4.2. Main Results\n4.2.1\nResults of Context-Dependent Tasks\nIn Context-Dependent Tasks, we primarily investigate how\nto enhance an agent’s perception of context information\nTable 2. Process-Dependent Tasks. 25 tasks are defined and di-\nvided into 5 difficulty levels based on incrementally increasing\nreasoning steps. A higher difficulty level implies that the agent\nneeds to engage in longer reasoning and planning with the envi-\nronment.\nTask Level\nReasoning Step\nExample Task\nBasic\n1-3\ncraft crafting table\nWooden\n4-5\ncraft wooden sword\nStone\n6-9\nmine stone\nIron\n10-11\nsmelt iron ingot\nDiamond\n>11\nobtain diamond\nwithin the environment. We demonstrate the performance\ndifference between Active Perception and other percep-\ntion methods. We compare them with pre-trained multi-\nmodal large language models LLaVA-1.5 [20] and GPT-\n4V [25], and analyze the performance of both active and\nfine-grained global perception on the tasks in Tab. 3. Al-\nthough fine-grained global perception can obtain compre-\nhensive perceptual information, due to the lack of objective-\nconditioned attention, the objective-related information ob-\ntained may be lacking or incorrect. Active perception only\nfocuses on objective-related information and ignores other\nuseless information, so that more accurate objective-related\ninformation can be obtained and better performance in\nContext-Dependent Tasks can be achieved. For the compari-\nson, we use MineLLM, which is fine-tuned on the Minecraft\ninstruction dataset we collect, slightly better than GPT-\n4V [25], which is trained on massive data, and substantially\nbetter than LLaVA-1.5 [20], which is not fine-tuned on in-\nstruction data. The complete results of Context-Dependent\nTasks can be found in Sup.D.2.\n4.2.2\nResults of Process-Dependent Tasks\nIn Process-Dependent Tasks, we report the performance\nof the agent in completing long-horizon tasks by contin-\nuously perceiving the environment context and dynami-\ncally adjusting its actions. We also investigate the agent’s\nbehavior in scenarios of non-situation-aware planning and\nnon-embodied action execution. The complete results of\nProcess-Dependent Tasks can be found in Sup.D.2.\nIn considering the landscape of related works [1, 12, 34,\n36, 43], we refrain from making direct comparisons due to\nthe substantial variations in the observation space, action\nspace, environmental setup, and game termination con-\nditions. Notably, VPT [1] emulates human players’ key-\nboard and mouse controls, DreamerV3 [12] is trained from\nscratch for diamond collection\nin a modified Minecraft\nenvironment with altered block-breaking mechanics using\nworld models, DEPS [36] integrates LLM planning and a\nlearning-based control policy based on MineDojo [9] ac-\ntions, GITM [43] employs privileged information such as\nlidar perception, and Voyager [34] utilizes purely text-based\nTable 3.\nPerformance on Context-Dependent Tasks.\nWe com-\npare the success rate of different Methods and different Perception\nstrategies. We set up special prompt to make the output of the cap-\ntion as comprehensive as possible, this perception method is called\nFine-Grained Global Perception. We use A to denote Active Per-\nception, and G to denote Fine-Grained Global Perception.\nMethod\nStrategy\nAverage Success Rate(%)\nEasy\nMid\nHard\nComplex\nLLaVA-1.5 [20]\nG\n47.5\n22.5\n5.0\n0.0\nA\n72.5\n50.0\n11.0\n0.0\nGPT-4V [25]\nG\n97.5\n85.0\n75.0\n60.0\nA\n100.0\n94.5\n92.5\n87.5\nMP5(Ours)\nG\n90.0\n82.5\n77.5\n67.5\nA\n98.5\n94.5\n93.0\n91.0\ninformation perception in collaboration with the Mineflayer\nAPI for action. Given that our experiments aim to showcase\nthe system’s capability to adapt both process-dependent rea-\nsoning and complex context-understanding tasks, our focus\nturns to presenting two key insights drawn from the sys-\ntem’s performance, as detailed below.\nEmbodied action execution is critical for open-ended\ntasks. Comparing MP5 w\/o E. and MP5 in Tab. 4, we\ncan observe that when an agent is unable to interact with\nthe environment and access low-level environment contex-\ntual information during action execution, it essentially be-\ncomes “blind”, unable to determine the termination of its\nactions based on environment. Therefore, the success rate\nin Process-Dependent Tasks is 0.00%.\nSituation-aware\nplanning\nleads\nto\nmore\nscenario-\nappropriate strategies. Comparing MP5 w\/o P. and MP5\nin Tab. 4, we observe that the lack of environment con-\ntextual information during the agent’s planning process can\nlead to erroneous or redundant actions, thereby reducing the\nsuccess rate (for example, the success rate in diamond-level\ntasks decrease from 22.00% to 14.00%). Consider a sce-\nnario where the current sub-objective is “kill a pig\n”. If\na pig\nis already present, the agent should directly ex-\necute “move” to approach without the need to first “find”\nthen “move”. However, the relatively small decrease in the\nsuccess rate can be attributed to the dynamic adjustment of\nperception and action execution offered by embodied ac-\ntion execution. Simultaneously, when errors are detected,\nthe perceived environmental information and the erroneous\nactions can be fed back to the planner for re-planning.\n4.2.3\nOpen-Ended Tasks\nProcessing long-horizon reasoning and understanding com-\nplex contexts are interconnected in the real world. For sim-\nplicity and comparability of the experimental setup, the first\ntwo task settings do not consider the intersection of pro-\ncess and context, as we cannot exhaust all combinations\nTable 4. Performance on Process-Dependent Tasks. We compare\nthe success rate when interacting or not interacting with the en-\nvironment during the planning or execution. w\/o P. and w\/o E.\nindicates non-situation-aware planning and non-embodied action\nexecution.\nMethod\nAverage Success Rate(%)\nBasic\nWooden\nStone\nIron\nDiamond\nMP5 w\/o P.\n0.00\n0.00\n0.00\n0.00\n0.00\nMP5 w\/o E.\n92.00\n86.00\n68.67\n45.33\n14.00\nMP5\n96.00\n88.67\n76.00\n52.00\n22.00\n1. Mine 2 logs\n2. Craft 8 planks\n3. Craft 4 sticks\n4. Craft 1 \nCrafting table\n5. Craft 1 wooden \nshovel\n6. Dig a block of sand near the water at \nnight, with a wooden shovel\n0\n1\n2\n3\n4\nProcess\nContext #\nTask: Dig a block of sand\nnear the water at night,\nwith a wooden shovel\nFigure 5. Screenshots of “Dig a block of sand\nnear the water\nat night\nwith a wooden shovel\n”. In Open-Ended Tasks,\nthe agent needs to better integrate low-level context information\nand high-level decision-making, making it extremely challenging.\nTable 5. Success rates for different MLLMs and pre-trained visual\nencoders in the percipient on Context-Dependent Tasks\nMethod\nVisual\nAverage Success Rate(%)\nEncoder\nEasy\nMid\nHard\nComplex\nLLaVA-1.5 [20]\nCLIP [30]\n72.50\n50.00\n11.00\n0.00\nMineLLM\nCLIP [30]\n95.00\n90.00\n87.00\n80.00\nMineLLM\nMineCLIP [9]\n98.50\n94.50\n93.00\n91.00\nthat these two task dimensions can form. Therefore, we\nrefer to tasks that incorporate both Process-Dependent and\nContext-Dependent elements as Open-Ended Tasks. Specif-\nically, these tasks require the agent to perceive different in-\nformation of the environment at multiple stages of complet-\ning sub-objectives. As shown in Fig. 5, we present an exam-\nple of an Open-Ended Task, named “Dig a block of sand\nnear the water\nat night\nwith a wooden shovel\n”. We\nconduct extensive validations on this type of task, proving\nthat MP5 can complete long-sequential tasks in challeng-\ning environments. More demonstrations and experimental\nresults of Open-Ended Tasks can be found in Sup.F.3.\n4.3. Ablation Study\nWe conduct ablation studies to evaluate the effectiveness of\nvarious modules. The experimental setup and the associated\nsuccess rates are in Sec. 4.1. More detailed ablation studies\nare listed in Sup.D.3. The following paragraphs present the\nanalyses derived from our ablation studies.\nModel pre-trained on massive data of Minecraft can bet-\nter comprehend the Minecraft appearance styles. We\nconduct ablation studies on the multi-modal large language\nmodel (MLLM) part within Context-Dependent Tasks in\nTable 6. Success rates for different LLMs as zero-shot Planner on\nProcess-Dependent Tasks\nPlanner\nAverage Success Rate(%)\nBasic\nWooden\nStone\nIron\nDiamond\nVicuna-13B-v1.5 [5]\n1.33\n0.00\n0.00\n0.00\n0.00\nGPT-3.5-turbo [23]\n95.33\n86.67\n42.00\n2.67\n0.00\nGPT-4 [26]\n96.00\n88.67\n76.00\n52.00\n22.00\nTab. 5, comparing the performance outcomes of different\nMLLMs and different pre-trained visual encoders in the\npercipient.\nWe find the performance of the open-source\nmodel LLaVA-1.5 [20] to be relatively weak, with a suc-\ncess rate of merely 50.00% at the Mid level and 11.00% on\nthe Hard level. This is primarily due to the model’s train-\ning predominantly on real-world data, causing it to strug-\ngle with the pixel-style image recognition characteristic of\nMinecraft. We also discover that, when the visual encoder is\nfrozen, the MineLLM with CLIP [30] as its visual encoder\nconsistently performs worse across all levels compared to\nMineLLM with MineCLIP’s [9] pre-trained single image\nvisual encoder. It may caused by, in the case of a frozen vi-\nsual encoder, a visual encoder pretrained on massive data of\nMinecraft can align with pixel-style images more rapidly.\nEnhanced reasoning ability results in improved plan-\nning.\nWe compare the performance of open-source\nlarge language models, OpenAI’s GPT-3.5-turbo [23] in\nTab. 6, and GPT-4 [26] as zero-shot Planners on Process-\nDependent Tasks. We find that as the models’ inferential\ncapabilities increase, the Planner produces better results\nby planning in a situation-aware method, yielding more\nconcise and accurate execution actions. The Vicuna-13B-\nv1.5 [5] model, when used as a Planner, struggles to pro-\nduce effective plans, achieving only a 1.33% accuracy rate\nat the Basic level\n.\nGPT-4 [26] exhibits the best per-\nformance, attaining a 22.00% success rate at the Diamond\nlevel\n, whereas both Vicuna-13B-v1.5 [5] and GPT-3.5-\nturbo [23] score 0.00%.\nLeveraging memory leads to better planning.\nIn our\nPerformer Memory, we store previously successful sub-\nobjectives and their corresponding execution actions. When\nplanning in similar scenarios, Performer Memory can pro-\nvide the Planner with similar execution action plans for\ncompleting the sub-objectives. While the plans may not be\nidentical, they can effectively assist the Planner in perform-\ning situation-aware planning. Comparing the first and last\nrows of Tab. 7, we find that without the Performer Memory,\nthe success rate of tasks at all levels decreases (Diamond\nlevel\ndrops from 22.00% to 16.67%). However, the de-\ncrease is not significant as the Performer Memory primarily\nserves a reference function, with specific action planning\nstill heavily reliant on the Planner’s capabilities.\nRobustness is essential in open-world settings. To en-\nhance the robustness evaluation of our system, we introduce\na “Random Drop” setting. In this setting, we randomly dis-\nTable 7.\nSuccess rates on different modules within Process-\nDependent Tasks: We study the roles of the Performer Mem-\nory (PM) and the check part of Patroller (P), with ’RD’ denoting\n“Random Drop” setting. ✓denotes the inclusion of the module or\nsetting, and ✗indicates its absence.\nPM\nP\nRD\nAverage Success Rate(%)\nBasic\nWooden\nStone\nIron\nDiamond\n✗\n✓\n✗\n96.00\n87.33\n67.33\n47.33\n16.67\n✓\n✗\n✓\n70.00\n7.33\n0.67\n0.00\n0.00\n✓\n✓\n✓\n87.33\n76.67\n45.33\n18.67\n1.33\n✓\n✓\n✗\n96.00\n88.67\n76.00\n52.00\n22.00\ncard one complete sub-objective from the inventory at the\nstart of each new sub-objective, which deliberately induces\nexecution errors for the agent. Comparing the second and\nthird lines in Tab. 7, we observe the critical role of the Pa-\ntroller in recognizing feedback errors. The Patroller’s abil-\nity to integrate current environmental information with error\ninformation is essential for enabling the planner to re-plan.\nThe significance of this robustness is evident when exam-\nining the success rates. Without the Patroller’s robustness,\nthe agent’s success rate on the Wooden level\nplummets\nfrom 76.67% to 7.33%, while success rates on the Iron\n,\nand Diamond\nlevels drop to 0.00%. Details regarding the\n“Random Drop” setting can be found in Sup.D.3.\n5. Conclusion\nIn this paper, we propose a novel multi-modal embodied\nsystem termed MP5 which is driven by frequently ego-\ncentric scene perception for task planning and execution. In\npractice, it is designed by integrating five functional mod-\nules to accomplish task planning and execution via actively\nacquiring essential visual information from the scene. The\nexperimental results suggest that our system represents an\neffective integration of perception, planning, and execu-\ntion, skillfully crafted to handle both context- and process-\ndependent tasks within an open-ended environment.\nLimitation and Future Work. Despite the impressive re-\nsults of our approach, two major limitations need to be clar-\nified. Firstly, the reliance on GPT-3.5-turbo [23] or GPT-\n4 [26] limits the system’s usability, as not everyone has ac-\ncess to these APIs. Secondly, the scope of the applied sim-\nulation platform is limited. Despite showing promising per-\nformance in Minecraft, we haven’t extended our exploration\nto other simulation platforms, which is a potential area for\nfurther research.\nAcknowledgement.\nThis work was supported by the\nNational Key R&D Program of China (2021YFB1714300),\nthe\nNational\nNatural\nScience\nFoundation\nof\nChina\n(62106154,\n62132001),\nthe Natural Science Founda-\ntion of Guangdong Province, China (2022A1515011524).","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nMP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception\n```\n#### 2. 论文摘要\n```\nIt is a long-lasting goal to design an embodied system that can solve\nlong-horizon open-world tasks in human-like ways. However, existing approaches\nusually struggle with compound difficulties caused by the logic-aware\ndecomposition and context-aware execution of these tasks. To this end, we\nintroduce MP5, an open-ended multimodal embodied system built upon the\nchallenging Minecraft simulator, which can decompose feasible sub-objectives,\ndesign sophisticated situation-aware plans, and perform embodied action\ncontrol, with frequent communication with a goal-conditioned active perception\nscheme. Specifically, MP5 is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs), and the system is modulated into functional\nmodules that can be scheduled and collaborated to ultimately solve pre-defined\ncontext- and process-dependent tasks. Extensive experiments prove that MP5 can\nachieve a 22% success rate on difficult process-dependent tasks and a 91%\nsuccess rate on tasks that heavily depend on the context. Moreover, MP5\nexhibits a remarkable ability to address many open-ended tasks that are\nentirely novel.\n```\n\n#### 3. 论文全文\n```\nMP5: A Multi-modal Open-ended Embodied System\nin Minecraft via Active Perception\nYiran Qin1,2*,\nEnshen Zhou1,3*,\nQichang Liu1,4*,\nZhenfei Yin1,5,\nLu Sheng3†,\nRuimao Zhang2†,\nYu Qiao1,\nJing Shao1‡\n1Shanghai Artificial Intelligence Laboratory\n2The Chinese University of Hong Kong, Shenzhen\n3School of Software, Beihang University\n4Tsinghua University\n5The University of Sydney\nyiranqin@link.cuhk.edu.cn\nzhouenshen@buaa.edu.cn\nlsheng@buaa.edu.cn\nruimao.zhang@ieee.org\nshaojing@pjlab.org.cn\nforest\nday\ngrass\nProcess\nContext\nDay\nStone\nWood\nWater\nPig\nGrass\nTask: Kill a pig with a stone sword during the \ndaytime near the water with grass next to it .\nForest\nPlains\nlog\npig\nplains\nwater\nstone\nnull\n: log\n: plank\n: stick\n: crafting table\n: wooden pickaxe\n: stone\n: stone sword\n: pig\n0\n1\n2\n3\n4\nProcess\nContext #\n(a)\n(b)\n(c)\nFigure 1. The process of finishing the task “kill a pig with a stone sward during the daytime near the water with grass next to it.”. (a)\nTo achieve the final goal (i.e., O8: “kill a pig\n”), a player should accomplish a list of sub-objectives {Oi}7\ni=1 sequentially. During this\nprocess, the player should also be aware of some items in the environment, e.g., “grass\n”, “day\n” and etc. (b) This diagram shows\nthe number of these necessary items in the context that should be perceived for each sub-objective, during the task execution process. (c)\nImages marked by O1 and O6 show the observed ego-centric views in the process of achieving the corresponding sub-objectives. Images\nmarked by O1\n8 and O2\n8 indicate how the player executes the action about the last sub-objective “kill a pig\n”. This exemplar process tells\nthat such long-horizon open-world embodied tasks in Minecraft should be solved both in process-dependent and context-dependent way.\nAbstract\nIt is a long-lasting goal to design an embodied system\nthat can solve long-horizon open-world tasks in human-like\nways. However, existing approaches usually struggle with\ncompound difficulties caused by the logic-aware decompo-\nsition and context-aware execution of these tasks. To this\nend, we introduce MP5, an open-ended multimodal em-\nbodied system built upon the challenging Minecraft sim-\nulator, which can decompose feasible sub-objectives, de-\nsign sophisticated situation-aware plans, and perform em-\nbodied action control, with frequent communication with\n∗Equal contribution\n† Corresponding author\n‡ Project leader\na goal-conditioned active perception scheme. Specifically,\nMP5 is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs), and the system is mod-\nulated into functional modules that can be scheduled and\ncollaborated to ultimately solve pre-defined context- and\nprocess-dependent tasks. Extensive experiments prove that\nMP5 can achieve a 22% success rate on difficult process-\ndependent tasks and a 91% success rate on tasks that heav-\nily depend on the context. Moreover, MP5 exhibits a re-\nmarkable ability to address many open-ended tasks that are\nentirely novel.\nPlease see the project page at https:\n\/\/iranqin.github.io\/MP5.github.io\/.\narXiv:2312.07472v4  [cs.CV]  26 Mar 2024\n1. Introduction\nOne of the core objectives of current embodied intelligence\nis to construct generalist agents that can solve long-horizon\nopen-world embodied tasks, approaching the behavior pat-\nterns of human beings [1, 19, 22, 31]. However, the process\ndependency and context dependency in these tasks, such as\nthose in Minecraft depicted in Fig. 1, hinder recent agents\nfrom achieving the aforementioned goal. To be specific, the\nformer emphasizes the inherent dependency among the sub-\nobjectives of one task or an action sequence to fulfill one\nsub-objective (such as “craft a stone sword\n” should be\nsolved before “kill a pig\n”). The latter highlights that the\nexecution of each sub-objective or even each action depends\non the contextual information of the environment (such as\n“kill a pig\n” requires to find the target “pig\n” and its sur-\nrounding items “grass\n” and “water\n” during the “day-\ntime\n” in the observed images, as shown in Fig. 1).\nThe recent success of Large Language Models (LLMs)\nhas\nattempted\nto\nsolve\nthe\nprocess-dependent\nchal-\nlenge, by using LLMs to break down a long-horizon\nprocess-dependent task into a sequence of feasible sub-\nobjectives [34, 36, 43]. These methods [34, 43] simplify\nthe context-dependent challenge by assuming the agents are\nall-seeing, i.e., knowing everything about their state and the\nenvironment it locates in. However, to solve the context-\ndependent challenge, an embodied agent should addition-\nally have: (1) the perception capability is open-ended, se-\nlective and give results tailored to diverse purposes (e.g.,\nfor task planning or action execution), (2) the perception\nmodule can be compatibly scheduled along with the other\nmodules (e.g., planning and execution modules) by a uni-\nfied interface, as an integrated system.\nTo this end, we introduce MP5, a novel embodied sys-\ntem developed within Minecraft, to meet the above expec-\ntations. Specifically, MP5 comprises five interacting mod-\nules, i.e., Parser decomposes a long-horizon task into a\nsequence of sub-objectives that should be completed one\nby one; Percipient answers various questions about the ob-\nserved images, as the reference for the other modules; Plan-\nner schedules the action sequences of a sub-objective, as\nwell as refines the following sub-objectives, given the cur-\nrent situation; Performer executes the actions along with\nfrequent interaction with the environment; and Patroller\nchecks the responses from the Percipient, Planner, and Per-\nformer, for the purpose of verifying current plans\/actions, or\nfeedback on potential better strategies. In our work, Percipi-\nent is a LoRA-enabled Multimodal LLM (MLLM). Among\nthe pre-trained LLMs, Parser and Planner are augmented\nwith external Memory, while Patroller is not.\nNotably, MP5 includes an active perception scheme by\nmeans of multi-round interaction between Percipient and\nPatroller, which is to actively perceive the contextual in-\nformation in the observed images, with respect to vari-\nous queries raised by Planner and Performer.\nIt is the\nkey enabler to solve context-dependent tasks.\nPatroller\nin this scheme relays compatible feedback to Planner and\nPerformer accordingly, while eventually strengthening the\nplanning skill in awareness of the situations and improving\nthe action execution correctness in an embodied manner.\nExtensive experiments prove that MP5 can robustly\ncomplete tasks needed for long-horizon reasoning and com-\nplex context understanding. It achieved a 22% success rate\non diamond-level tasks (i.e., one of the hardest long-horizon\ntasks) and a 91% success rate on tasks requiring complex\nscene understanding (i.e., need to perceive around 4 ∼6\nkey items in the observed images). Moreover, in Sec. 4.2.3,\nMP5 can surprisingly address more open-end tasks both\nwith heavy process dependency and context dependency.\n2. Related Work\n2.1. Multi-modal Large Language Models\nWith the development of Large Language Models (LLMs)\nlike the GPT series [2, 27, 29], as well as open-source LLMs\nsuch as the LLaMA series [32, 33] and Vicuna [5], Multi-\nmodal Large Language Models (MLLMs) have emerged.\nExamples of such MLLMs include LLaVA [21], Instruct-\nBLIP [6], and LAMM [39], among others [4, 10, 15, 28, 38,\n42]. In this work, we introduce MineLLM, which is specif-\nically designed and trained for Minecraft, and leverage its\nperception, interaction, and analysis capabilities to build\nup Percipient for MP5, and further enable an objective-\nconditioned active perception scheme.\n2.2. Agents in Minecraft\nPrevious works[3, 7, 9, 11, 19, 22, 40, 41] attempt to use\napproaches such as hierarchical RL, goal-based RL, and re-\nward shaping to train an agent in Minecraft. MineCLIP [9]\nenables the resolution of various open-ended tasks speci-\nfied in free language, even without any manually designed\ndense rewards. DreamerV3 [12] succeeds in training agents\nin Minecraft with a learned world model. VPT [1] builds\na foundation model for Minecraft by learning from massive\nvideos. Based on VPT, Steve-1 [18] also explores bring-\ning in MineCLIP [9] to get an instruction following policy\nwith high performance. The development of recent large\nlanguage model-related work Voyager [34], DEPS [36],\nGITM [43] further promote the advancement of agents\nin long-horizon tasks. These works use pre-trained large\nlanguage models as the zero-shot planners[14] for agents,\nleveraging the powerful reasoning capabilities of large lan-\nguage models to obtain continuous operation instructions or\nexecutable policy lists.\nWe take advantage of the reasoning capability of LLM\nto build up our own agent. Existing LLM agents [43, 43] in\nMinecraft feed scene data from simulation platforms [9, 11]\nPerformer Memory\nKnowledge\nMemory\nPatroller\nPercipient \nObtain Env. \nInfo. for \nPlanning\nObtain Env. \nInfo. for \nPerformer\nMulti-round\nSingle-round\n<Sub-Objective>\nParser\nTask: Kill a pig with a wooden sword during the daytime near \nthe water with grass next to it.\nActive \nPerception\n…\nError \nFeedback\nRe-plan\nPerformer: Having completed a move in “Find” action, based on my current view, tell \nme if I should continue this action or if the next action is ready to execute. \n<Sub-Objective>\nPatroller: I conduct Active Perception with Percipient with your current observation, \nyou must continue with the current action since there is no river near the pig.\nPatroller: I conduct Active Perception with Percipient with your current observation, \nyou can execute the next action since all conditions are satisfied.\nPlanner: Can you tell me what important environmental information I need to know?\nPatroller: I conduct Active Perception with Percipient with your current observation, \nthere is no pig based on the scene.\nPlanner: 1. Equip(       )  2. Find(        )  3. Move(     )  4. Fight(        )  \nPerformer: Having completed a move in “Find” action, based on my current view, tell \nme if I should continue this action or if the next action is ready to execute. \nPerformer: Start executing “Equip”.\nPerformer: Continue executing “Find”.\nSub-Objectives\n{\n}\n…\n…\n…\n…\n…\n…\nMove\nMine\nEquip\nFight\nFind\nCraft\n…\nFigure 2. Overview of module interaction in MP5. After receiving the task instruction, MP5 first utilizes Parser to generate a sub-objective\nlist. Once a sub-objective is passed to the Planner, the Planner Obtaining Env. Info. for Perception-aware Planning. The performer takes\nfrequently Perception-aware Execution to interact with the environment by interacting with the Patroller. Both Perception-aware Planning\nand Execution rely on the Active Perception between the Percipient and the Patroller. Once there are execution failures, the Planner will\nre-schedule the action sequence of the current sub-objective. Mechanisms for collaboration and inspection of multiple modules guarantee\nthe correctness and robustness when MP5 is solving an open-ended embodied task.\ninto large language models for task planning. However, for\nembodied agents in real scenes, it is clearly unrealistic to\nuse accurate scene data directly. Therefore, agents need to\nbe robust to make decision corrections despite inaccurate or\nerroneous perception information. Moreover, open-ended\ntasks need hierarchical reasoning [22] and complex open-\nended context understanding [1, 9], classical perception net-\nworks can only output fixed perception results and cannot\nprovide corresponding perception information according to\nthe task, making it impossible to understand open-ended\nscenarios. Therefore, we design MP5, an embodied agent\nwith open-ended capabilities that can solve the problem of\nopen-ended tasks.\n3. Method\nIn this section, we first give an overview of our proposed\nMP5, for solving context-dependent and process-dependent\ntasks in an open-world and embodied environment, such as\nMinecraft (Sec. 3.1). Next, we elaborate on how to imple-\nment an active perception scheme (Sec. 3.2). This scheme\nplays a vital role in MP5 to solve context-dependent tasks,\nsince it reliably grounds the visual content according to dif-\nferent kinds of objectives, and thus strengthens the plan-\nning skill and execution correctness with respect to context-\ndependent tasks. Then, we show how to plan and update\naction sequences in awareness of the situations, and how to\nreliably execute these actions in an embodied environment\n(Sec. 3.3). Finally, we give necessary implementation de-\ntails about MP5 in Sec. 3.4.\n3.1. Overview\nAs demonstrated in Fig. 2, our MP5 includes five major\nmodules, i.e., Parser, Percipient, Planner, Performer, and\nPatroller. Specifically, Percipient is a parameter-efficiently\nfine-tuned Multimodal Large Language Model (MLLM)\nthat is specified to the Minecraft environment. The Parser,\nPlanner, and Patroller are pre-trained Large-language Mod-\nels (LLMs). We also include retrieval-augmented genera-\ntion (RAG) to enhance the quality of responses generated\nby Parser and Planner. Performer is an interface that ex-\nplains each action from the action sequence into executable\ncommands that directly control the game character.\nWhy can MP5 solve context-dependent and process-\ndependent tasks?\nMP5 includes an active perception\nscheme by means of multi-round interactions between Per-\ncipient and Patroller, which is to actively perceive the envi-\nronmental information in the observed images, with respect\nto various objectives raised by Planner or Performer. With\nthe help of this scheme, Planner can schedule or update ac-\ntion sequences in awareness of the observed images, inven-\ntory status and etc., resulting in a situation-aware planning;\nPerformer can execute actions that are adapted to the em-\nbodied environment, resulting in a embodied action execu-\ntion. Patroller in this scheme can also feedback on better\nchoices of plans\/actions based on the visual evidence so that\nthe process-dependent tasks are solved with fewer chances\nof context-dependent execution failures. Moreover, Percip-\nient can understand open-ended visual concepts, therefore\nit allows MP5 to solve tasks that are never seen before.\nHow does MP5 function?\nIn Fig. 2, upon receiving a\nResponse\nThis picture is showing a pig, \ntypes of Minecraft mobs.\nQuestion\nWhat types of Minecraft mobs is \nthis <image> showing?\nPerformer : Current sub-objective <Kill a pig with a wooden sword during the daytime near the water with grass next to it>    &    Current execution action: Find (          )\nEnv. Info.\nResponse\nThe blocks shown in the image \nare water, grass …\nQuestion\nWhat types of blocks are shown in \nthe given <image> ?\nResponse\nThe image depicts daytime in \nthe Minecraft world.\nQuestion\nWhat time of day does the <image>\ndepict in the Minecraft world?\nResult\nI have seen a pig during the \ndaytime near the water with \ngrass next to it. You can execute \nthe next action.\nAsk question(s) of \nsignificant necessity\nAnswer the question\nAdd answer to Env. Info.\nPatroller\nPatroller\nPatroller\nPerceived env info.\nTemporary Env. Info. Set\nEnv. Info.\nEnv. Info.\nEnv. Info.\nEnv. Info.\nPatroller\nReset\nQuery Env. Info. \nOr Return\nPercipient \nPercipient \nPercipient \nFigure 3. A demonstration of the process of Active Perception scheme. Temporary Env. Info. Set saves information collected in the current\nscenario, so it should be reset at the beginning of Active Perception scheme. Performer then invokes Patroller to start asking Percipient\nquestions with respect to the description of the sub-objective and the current execution action round by round. The responses of Percipient\nare saved in Temporary Env. Info. Set and are also gathered as the context for the next question-answering round. After finishing asking all\nsignificant necessary questions, Patroller will check whether the current execution action is complete by analyzing the current sub-objective\nwith Perceived env info. saved in Temporary Env. Info. Set, therefore complex Context-Dependent Tasks could be solved smoothly.\nhigh-level task, MP5 first utilizes the Parser to generate a\nsequence of short-horizon sub-objectives, as a list of rich\ninstructions in natural languages. The feasibility of the gen-\nerated sub-objectives is augmented by retrieving an external\nKnowledge Memory. This knowledge mainly comes from\nthree sources: part of it is from the online wiki, another part\nis from the crafting recipes of items in MineDojo [9], and\nsome are user tips from Reddit. To one sub-objective, Plan-\nner schedules the action sequence that is grounded by the\nenvironmental information gathered by the active percep-\ntion scheme. In this case, Performer will execute the actual\nactions by explaining the action sequence that is adapted\nto the embodied environment, via frequent interaction with\nthe active perception scheme. Once there are execution fail-\nures (determined by Patroller), Planner will re-schedule the\naction sequence of the current sub-objective, or even up-\ndate the following sub-objectives if some necessary sub-\nobjectives are missing.\nOtherwise, the agent will go to\nthe next sub-objective and schedule new action sequences,\nwhilst the successful action sequence of the current sub-\nobjective will be stored in the external memory of Planner\n(called Performer Memory), along with the agent situation\nwhen it was planned. In the end, the agent will stop when\nthe final sub-objective of the task has been reached.\n3.2. Active Perception\nLet’s take the example shown in Fig. 3 to demonstrate how\nthe active perception scheme works. In this example, the\nactive perception scheme is communicated with Performer\nto enable an embodied action execution.\nAt first, Performer invokes Patroller to start asking Per-\ncipient questions with respect to the description of the\nsub-objective and the current execution action, while si-\nmultaneously resetting the set of environmental informa-\ntion to be gathered.\nThen Patroller progressively asks\nPercipient whether the observed image contains necessary\nitems\/factors (e.g., mobs\n, blocks\n, time\n)\nrelated to recent sub-objective (e.g., pig\n) and the execut-\ning action (e.g., “find pig\n”). The responses of Percipient\nare also progressively gathered and act as the context for\nthe next question-answering round. Note that in each round,\nPatroller also checks whether all the necessary items\/factors\nhave been collected - If yes, Patroller stops the interaction\nand returns all the environmental information as natural lan-\nguage, and invokes Performer to execute the next action. If\nPatroller eventually fails to gather enough items\/factors, it\nwill tell Performer what items\/factors are missing in the ob-\nserved images, which suggests Performer keeps executing\nthe current action. Please also check the example shown in\nFig. 2.\nSimilarly, active perception used in situation-aware plan-\nning is similar to what is explained here, except that the ap-\nplied instructions do not contain the executable action. For\nmore details please check the Sup. E.\n3.3. Perception-aware Planning and Execution\nSituation-aware Planning. Given one sub-objective, Plan-\nner will generate the action sequence based on the descrip-\ntion of the situation, such as the objective-conditioned envi-\nronmental information from the active perception scheme,\nthe inventory status and localization, and etc. Moreover,\nPlanner will retrieve previous successful action sequences\nas the demonstration prompt to augment the aforemen-\ntioned planning results. If the active perception scheme fails\nto find the key items\/factors about the current sub-objective\nin the observed image, the generated action sequences will\ninclude more actions to reach them. Moreover, if Performer\nencounters execution failures determined by Patroller (such\nImage Encoder\n🔥Alignment Net\n🔥\nLoRA\nLarge Language Model\nInstruction\nWhat types of Minecraft mobs is \nthis <image> showing?\nResponse\nThis picture is showing a pig, \ntypes of Minecraft mobs.\n🔥\nFinetune\nFrozen\nFigure 4. The model architecture of MineLLM. Image is encoded\nby a pre-trained vision encoder and decoded by LLM. Only the\nparameters of Alignment Net and LoRA are trainable.\nas failure of “equip wooden sword\n”), Planner will re-\nschedule the action sequence or even update the following\nsub-objectives, with the help of external memories.\nEmbodied Action Perception. As indicated in Sec. 3.2,\nPerformer would like to communicate with the active per-\nception scheme in every round of action execution, so as\nto enhance the ego-centric awareness of the agent. The new\naction will be executed if Patroller identifies necessary envi-\nronmental information in the observed images that matches\nboth the sub-objective and the goal of the current action.\nOtherwise, the current action is kept executing until en-\ncountering execution failures or the end of the episode. The\nsuccessful action sequence about one sub-objective will be\nstored in the Performer Memory, together with necessary\nsituational information of the agent when it was planned.\nFor more details about the planning and execution process,\nplease check Sup. G.2 and Sup. B.2.\n3.4. Implementation Details\nPercipient.\nThe network of Percipient is depicted in\nFig. 4. Images are processed by a frozen vision encoder\nMineCLIP [9], whose features are projected by an Align-\nment Net(we use two-layer MLP like LLaVA-1.5 [20]) to\nthe same feature space as the text embeddings of the ap-\nplied LLM (we use Vicuna-13B-v1.5 [5]). Then the vision\nand text tokens are concatenated to feed into a LoRA-based\nfine-tuned LLM [13]. We add LoRA [13] parameters to\nall projection layers of the self-attention layers in the LLM.\nOnly the parameters of the Alignment Net and the LoRA\nmodule are optimized during training. The construction of\nthe training data with respect to Percipient is in the Sup. B.1.\nParser, Planner, and Patroller. We utilize OpenAI’s GPT-\n4 [26] as LLMs in Parser, Patroller, and Planner. We also\nevaluate other alternatives of GPT-4 [26], such as open-\nsource models like Vicuna-13B-v1.5 [5] and LLaMA2-\n70B-Chat [33] in Sup.D.3.\nPerformer. It is important to clarify that the actions gen-\nerated by Planner are not low-level commands such as key-\nboard and mouse operations [1], but a set of simple actions\n(such as equip, move, craft). Inspired by GITM [43], we\nimplement these actions appropriately through basic oper-\nations provided by the MineDojo [9] simulator. For more\ndetails, please check the Sup. B.2.\n4. Experiments\nAt first, we depict the setup of the Minecraft simulation en-\nvironment that we build and validate MP5, and give the\ndefinition of the evaluated tasks and how to set them in\nSec. 4.1. In Sec. 4.2, we present the quantitative and quali-\ntative performance of MP5, as well as in-depth discussions\non these tasks, and demonstrate that MP5 can even success-\nfully accomplish tasks that are more open-ended and never\nseen before. At last, we investigate how different modules\naffect the performance of MP5 and analyze the impact of\nvarious module choices within our system in Sec. 4.3.\n4.1. Experimental Setup\nEnvironment Setting. We employ MineDojo [9] as our\nsimulation environment to build and validate MP5. We cap-\nture player ego-view images provided by MineDojo [9] as\ninput of MP5, and further construct a dataset for training\nMineLLM. As for the output of MP5, we encapsulate Mine-\nDojo’s [9] actions to create our own action space.\nTask Setting. To evaluate how our MP5 can integrate per-\nception information with planning and execution, we define\ntwo types of tasks: Context-Dependent Tasks and Process-\nDependent Tasks as illustrated in Tab. 1 and Tab. 2.\n1) Context-Dependent Tasks primarily study how Active\nPerception enables the agent to better perceive low-level\ncontext information in the environment.\nWe first estab-\nlish 6 aspects of environmental information derived from\nthe Minecraft game environment: [Object, Mob, Ecology,\nTime, Weather, Brightness]. Each aspect has multiple op-\ntions.\nFor example, pigs\n, cows\n, and sheep\nare\nall elements belonging to Mob.\nBased on this, we de-\nfine 16 tasks and organize their difficulty into four levels\nby taking into account the number of information elements\nthat require perception, as is shown in Tab. 1. For exam-\nple, Easy tasks necessitate the perception of only one el-\nement, whereas Complex tasks involve the perception of\n4 to 6 elements. We rigorously assess MP5’s proficiency\nin environmental context perception across these 16 tasks.\nIn Context-Dependent Tasks, our environment details are\npredetermined (e.g., biomes\n, weather\n, and\netc.), as certain targets are exclusive to specific environ-\nments.\nWithout this environmental specificity, the agent\nmight never encounter the intended target. We retain each\nobservation of active perception throughout the task, using\nthem as references to ascertain the agent’s successful com-\npletion of the task.\n2) Process-Dependent Tasks focus on exploring the con-\nTable 1. Context-Dependent Tasks. 16 tasks are defined and di-\nvided into 4 difficulty levels based on the minimum number of\ninformation types needed. Underlines label the environmental in-\nformation, reflecting the complexity varies at each level.\nTask Level\nExample Task\nEasy\nFind a tree\nMid\nFind a tree\nin the forest\nHard\nFind a tree\nin the forest\nduring the nighttime\nComplex\nFind a pig\nnear a grass\nin the forest\nduring the daytime\ntributions of situation-aware planning, embodied action ex-\necution, and the integration with Active Perception in ac-\ncomplishing long-term tasks while constantly perceiving\nthe environment and dynamically adjusting actions. We se-\nlect 25 tasks from the technology tree and define their dif-\nficulty levels as Basic level\nto Diamond level\nbased\non the number of reasoning steps required to complete\nthe tasks. All environmental factors (e.g., biomes\n,\nweather\n, and etc.) are randomized in Process-\nDependent Tasks. More details can be found in Sup.D.1.\nEvaluation Metrics. For different tasks, the agent’s initial\nposition and environment seed are randomized. The agent\nbegins in survival mode, commencing with an empty inven-\ntory, and faces the challenge of hostile mob generation. It\nstarts from scratch, with a game time limit of 10 minutes, a\ntime period equivalent to 12,000 steps at a control frequency\nof 20Hz. More details can be found in Sup. C.\nFor the Context-Dependency Tasks, each assignment is\nopen-ended.\nTherefore, we conduct manual evaluations\nwhen the agent determines it has completed the task or\nexceeds the time limit.\nTwo cases are ruled as failures:\n1)There is an observation that meets all the conditions, but\nthe agent does not end the task; 2) The last observation\ndoes not meet all the conditions, yet the agent ends the task.\nOtherwise, we believe that the agent correctly perceives all\nthe context according to the task and determines that the\ntask is successfully completed. For the Process-Dependent\nTasks, any accidental deaths of the agent during the game\nare counted as failures, as are instances where the agent\ndoes not accomplish the task within the time limit.\nIn practice, we conduct 50 games on Context-Dependent\nTasks and 30 games on Process-Dependent Tasks, averaging\nthe success rates for both. The results are grouped accord-\ning to the previously defined difficulty levels, and report\nthe group means. For detailed definitions of the evaluation,\nplease refer to Sup. D.\n4.2. Main Results\n4.2.1\nResults of Context-Dependent Tasks\nIn Context-Dependent Tasks, we primarily investigate how\nto enhance an agent’s perception of context information\nTable 2. Process-Dependent Tasks. 25 tasks are defined and di-\nvided into 5 difficulty levels based on incrementally increasing\nreasoning steps. A higher difficulty level implies that the agent\nneeds to engage in longer reasoning and planning with the envi-\nronment.\nTask Level\nReasoning Step\nExample Task\nBasic\n1-3\ncraft crafting table\nWooden\n4-5\ncraft wooden sword\nStone\n6-9\nmine stone\nIron\n10-11\nsmelt iron ingot\nDiamond\n>11\nobtain diamond\nwithin the environment. We demonstrate the performance\ndifference between Active Perception and other percep-\ntion methods. We compare them with pre-trained multi-\nmodal large language models LLaVA-1.5 [20] and GPT-\n4V [25], and analyze the performance of both active and\nfine-grained global perception on the tasks in Tab. 3. Al-\nthough fine-grained global perception can obtain compre-\nhensive perceptual information, due to the lack of objective-\nconditioned attention, the objective-related information ob-\ntained may be lacking or incorrect. Active perception only\nfocuses on objective-related information and ignores other\nuseless information, so that more accurate objective-related\ninformation can be obtained and better performance in\nContext-Dependent Tasks can be achieved. For the compari-\nson, we use MineLLM, which is fine-tuned on the Minecraft\ninstruction dataset we collect, slightly better than GPT-\n4V [25], which is trained on massive data, and substantially\nbetter than LLaVA-1.5 [20], which is not fine-tuned on in-\nstruction data. The complete results of Context-Dependent\nTasks can be found in Sup.D.2.\n4.2.2\nResults of Process-Dependent Tasks\nIn Process-Dependent Tasks, we report the performance\nof the agent in completing long-horizon tasks by contin-\nuously perceiving the environment context and dynami-\ncally adjusting its actions. We also investigate the agent’s\nbehavior in scenarios of non-situation-aware planning and\nnon-embodied action execution. The complete results of\nProcess-Dependent Tasks can be found in Sup.D.2.\nIn considering the landscape of related works [1, 12, 34,\n36, 43], we refrain from making direct comparisons due to\nthe substantial variations in the observation space, action\nspace, environmental setup, and game termination con-\nditions. Notably, VPT [1] emulates human players’ key-\nboard and mouse controls, DreamerV3 [12] is trained from\nscratch for diamond collection\nin a modified Minecraft\nenvironment with altered block-breaking mechanics using\nworld models, DEPS [36] integrates LLM planning and a\nlearning-based control policy based on MineDojo [9] ac-\ntions, GITM [43] employs privileged information such as\nlidar perception, and Voyager [34] utilizes purely text-based\nTable 3.\nPerformance on Context-Dependent Tasks.\nWe com-\npare the success rate of different Methods and different Perception\nstrategies. We set up special prompt to make the output of the cap-\ntion as comprehensive as possible, this perception method is called\nFine-Grained Global Perception. We use A to denote Active Per-\nception, and G to denote Fine-Grained Global Perception.\nMethod\nStrategy\nAverage Success Rate(%)\nEasy\nMid\nHard\nComplex\nLLaVA-1.5 [20]\nG\n47.5\n22.5\n5.0\n0.0\nA\n72.5\n50.0\n11.0\n0.0\nGPT-4V [25]\nG\n97.5\n85.0\n75.0\n60.0\nA\n100.0\n94.5\n92.5\n87.5\nMP5(Ours)\nG\n90.0\n82.5\n77.5\n67.5\nA\n98.5\n94.5\n93.0\n91.0\ninformation perception in collaboration with the Mineflayer\nAPI for action. Given that our experiments aim to showcase\nthe system’s capability to adapt both process-dependent rea-\nsoning and complex context-understanding tasks, our focus\nturns to presenting two key insights drawn from the sys-\ntem’s performance, as detailed below.\nEmbodied action execution is critical for open-ended\ntasks. Comparing MP5 w\/o E. and MP5 in Tab. 4, we\ncan observe that when an agent is unable to interact with\nthe environment and access low-level environment contex-\ntual information during action execution, it essentially be-\ncomes “blind”, unable to determine the termination of its\nactions based on environment. Therefore, the success rate\nin Process-Dependent Tasks is 0.00%.\nSituation-aware\nplanning\nleads\nto\nmore\nscenario-\nappropriate strategies. Comparing MP5 w\/o P. and MP5\nin Tab. 4, we observe that the lack of environment con-\ntextual information during the agent’s planning process can\nlead to erroneous or redundant actions, thereby reducing the\nsuccess rate (for example, the success rate in diamond-level\ntasks decrease from 22.00% to 14.00%). Consider a sce-\nnario where the current sub-objective is “kill a pig\n”. If\na pig\nis already present, the agent should directly ex-\necute “move” to approach without the need to first “find”\nthen “move”. However, the relatively small decrease in the\nsuccess rate can be attributed to the dynamic adjustment of\nperception and action execution offered by embodied ac-\ntion execution. Simultaneously, when errors are detected,\nthe perceived environmental information and the erroneous\nactions can be fed back to the planner for re-planning.\n4.2.3\nOpen-Ended Tasks\nProcessing long-horizon reasoning and understanding com-\nplex contexts are interconnected in the real world. For sim-\nplicity and comparability of the experimental setup, the first\ntwo task settings do not consider the intersection of pro-\ncess and context, as we cannot exhaust all combinations\nTable 4. Performance on Process-Dependent Tasks. We compare\nthe success rate when interacting or not interacting with the en-\nvironment during the planning or execution. w\/o P. and w\/o E.\nindicates non-situation-aware planning and non-embodied action\nexecution.\nMethod\nAverage Success Rate(%)\nBasic\nWooden\nStone\nIron\nDiamond\nMP5 w\/o P.\n0.00\n0.00\n0.00\n0.00\n0.00\nMP5 w\/o E.\n92.00\n86.00\n68.67\n45.33\n14.00\nMP5\n96.00\n88.67\n76.00\n52.00\n22.00\n1. Mine 2 logs\n2. Craft 8 planks\n3. Craft 4 sticks\n4. Craft 1 \nCrafting table\n5. Craft 1 wooden \nshovel\n6. Dig a block of sand near the water at \nnight, with a wooden shovel\n0\n1\n2\n3\n4\nProcess\nContext #\nTask: Dig a block of sand\nnear the water at night,\nwith a wooden shovel\nFigure 5. Screenshots of “Dig a block of sand\nnear the water\nat night\nwith a wooden shovel\n”. In Open-Ended Tasks,\nthe agent needs to better integrate low-level context information\nand high-level decision-making, making it extremely challenging.\nTable 5. Success rates for different MLLMs and pre-trained visual\nencoders in the percipient on Context-Dependent Tasks\nMethod\nVisual\nAverage Success Rate(%)\nEncoder\nEasy\nMid\nHard\nComplex\nLLaVA-1.5 [20]\nCLIP [30]\n72.50\n50.00\n11.00\n0.00\nMineLLM\nCLIP [30]\n95.00\n90.00\n87.00\n80.00\nMineLLM\nMineCLIP [9]\n98.50\n94.50\n93.00\n91.00\nthat these two task dimensions can form. Therefore, we\nrefer to tasks that incorporate both Process-Dependent and\nContext-Dependent elements as Open-Ended Tasks. Specif-\nically, these tasks require the agent to perceive different in-\nformation of the environment at multiple stages of complet-\ning sub-objectives. As shown in Fig. 5, we present an exam-\nple of an Open-Ended Task, named “Dig a block of sand\nnear the water\nat night\nwith a wooden shovel\n”. We\nconduct extensive validations on this type of task, proving\nthat MP5 can complete long-sequential tasks in challeng-\ning environments. More demonstrations and experimental\nresults of Open-Ended Tasks can be found in Sup.F.3.\n4.3. Ablation Study\nWe conduct ablation studies to evaluate the effectiveness of\nvarious modules. The experimental setup and the associated\nsuccess rates are in Sec. 4.1. More detailed ablation studies\nare listed in Sup.D.3. The following paragraphs present the\nanalyses derived from our ablation studies.\nModel pre-trained on massive data of Minecraft can bet-\nter comprehend the Minecraft appearance styles. We\nconduct ablation studies on the multi-modal large language\nmodel (MLLM) part within Context-Dependent Tasks in\nTable 6. Success rates for different LLMs as zero-shot Planner on\nProcess-Dependent Tasks\nPlanner\nAverage Success Rate(%)\nBasic\nWooden\nStone\nIron\nDiamond\nVicuna-13B-v1.5 [5]\n1.33\n0.00\n0.00\n0.00\n0.00\nGPT-3.5-turbo [23]\n95.33\n86.67\n42.00\n2.67\n0.00\nGPT-4 [26]\n96.00\n88.67\n76.00\n52.00\n22.00\nTab. 5, comparing the performance outcomes of different\nMLLMs and different pre-trained visual encoders in the\npercipient.\nWe find the performance of the open-source\nmodel LLaVA-1.5 [20] to be relatively weak, with a suc-\ncess rate of merely 50.00% at the Mid level and 11.00% on\nthe Hard level. This is primarily due to the model’s train-\ning predominantly on real-world data, causing it to strug-\ngle with the pixel-style image recognition characteristic of\nMinecraft. We also discover that, when the visual encoder is\nfrozen, the MineLLM with CLIP [30] as its visual encoder\nconsistently performs worse across all levels compared to\nMineLLM with MineCLIP’s [9] pre-trained single image\nvisual encoder. It may caused by, in the case of a frozen vi-\nsual encoder, a visual encoder pretrained on massive data of\nMinecraft can align with pixel-style images more rapidly.\nEnhanced reasoning ability results in improved plan-\nning.\nWe compare the performance of open-source\nlarge language models, OpenAI’s GPT-3.5-turbo [23] in\nTab. 6, and GPT-4 [26] as zero-shot Planners on Process-\nDependent Tasks. We find that as the models’ inferential\ncapabilities increase, the Planner produces better results\nby planning in a situation-aware method, yielding more\nconcise and accurate execution actions. The Vicuna-13B-\nv1.5 [5] model, when used as a Planner, struggles to pro-\nduce effective plans, achieving only a 1.33% accuracy rate\nat the Basic level\n.\nGPT-4 [26] exhibits the best per-\nformance, attaining a 22.00% success rate at the Diamond\nlevel\n, whereas both Vicuna-13B-v1.5 [5] and GPT-3.5-\nturbo [23] score 0.00%.\nLeveraging memory leads to better planning.\nIn our\nPerformer Memory, we store previously successful sub-\nobjectives and their corresponding execution actions. When\nplanning in similar scenarios, Performer Memory can pro-\nvide the Planner with similar execution action plans for\ncompleting the sub-objectives. While the plans may not be\nidentical, they can effectively assist the Planner in perform-\ning situation-aware planning. Comparing the first and last\nrows of Tab. 7, we find that without the Performer Memory,\nthe success rate of tasks at all levels decreases (Diamond\nlevel\ndrops from 22.00% to 16.67%). However, the de-\ncrease is not significant as the Performer Memory primarily\nserves a reference function, with specific action planning\nstill heavily reliant on the Planner’s capabilities.\nRobustness is essential in open-world settings. To en-\nhance the robustness evaluation of our system, we introduce\na “Random Drop” setting. In this setting, we randomly dis-\nTable 7.\nSuccess rates on different modules within Process-\nDependent Tasks: We study the roles of the Performer Mem-\nory (PM) and the check part of Patroller (P), with ’RD’ denoting\n“Random Drop” setting. ✓denotes the inclusion of the module or\nsetting, and ✗indicates its absence.\nPM\nP\nRD\nAverage Success Rate(%)\nBasic\nWooden\nStone\nIron\nDiamond\n✗\n✓\n✗\n96.00\n87.33\n67.33\n47.33\n16.67\n✓\n✗\n✓\n70.00\n7.33\n0.67\n0.00\n0.00\n✓\n✓\n✓\n87.33\n76.67\n45.33\n18.67\n1.33\n✓\n✓\n✗\n96.00\n88.67\n76.00\n52.00\n22.00\ncard one complete sub-objective from the inventory at the\nstart of each new sub-objective, which deliberately induces\nexecution errors for the agent. Comparing the second and\nthird lines in Tab. 7, we observe the critical role of the Pa-\ntroller in recognizing feedback errors. The Patroller’s abil-\nity to integrate current environmental information with error\ninformation is essential for enabling the planner to re-plan.\nThe significance of this robustness is evident when exam-\nining the success rates. Without the Patroller’s robustness,\nthe agent’s success rate on the Wooden level\nplummets\nfrom 76.67% to 7.33%, while success rates on the Iron\n,\nand Diamond\nlevels drop to 0.00%. Details regarding the\n“Random Drop” setting can be found in Sup.D.3.\n5. Conclusion\nIn this paper, we propose a novel multi-modal embodied\nsystem termed MP5 which is driven by frequently ego-\ncentric scene perception for task planning and execution. In\npractice, it is designed by integrating five functional mod-\nules to accomplish task planning and execution via actively\nacquiring essential visual information from the scene. The\nexperimental results suggest that our system represents an\neffective integration of perception, planning, and execu-\ntion, skillfully crafted to handle both context- and process-\ndependent tasks within an open-ended environment.\nLimitation and Future Work. Despite the impressive re-\nsults of our approach, two major limitations need to be clar-\nified. Firstly, the reliance on GPT-3.5-turbo [23] or GPT-\n4 [26] limits the system’s usability, as not everyone has ac-\ncess to these APIs. Secondly, the scope of the applied sim-\nulation platform is limited. Despite showing promising per-\nformance in Minecraft, we haven’t extended our exploration\nto other simulation platforms, which is a potential area for\nfurther research.\nAcknowledgement.\nThis work was supported by the\nNational Key R&D Program of China (2021YFB1714300),\nthe\nNational\nNatural\nScience\nFoundation\nof\nChina\n(62106154,\n62132001),\nthe Natural Science Founda-\ntion of Guangdong Province, China (2022A1515011524).\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | MP5：基于主动感知的多模态开放式具身系统\n\n## 📌 背景痛点\/本文动机\n在人工智能领域，设计一个能够以人类方式解决开放世界长时任务的具身系统一直是长期目标。然而，现有的方法通常难以应对这些任务中逻辑感知分解和上下文感知执行所带来的复合困难。为了解决这个问题，本文提出了MP5，一个基于Minecraft模拟器的开放式多模态具身系统，它能够分解可行的子目标，设计复杂的情境感知计划，并执行具身动作控制，同时与目标条件下的主动感知方案进行频繁通信。\n\n## 🚀 核心方法\n💡 创新点1：MP5基于最新的多模态大型语言模型（MLLMs）构建，并将系统分解为可调度和协作的功能模块，以解决预定义的上下文和过程依赖任务。\n💡 创新点2：MP5包括一个主动感知方案，通过感知器与巡逻器之间的多轮交互，主动感知观察到的图像中的上下文信息，以解决上下文依赖任务。\n\n## 📈 实验结果\nMP5在困难的过程依赖任务上实现了22%的成功率，在高度依赖上下文的任务上实现了91%的成功率。此外，MP5表现出解决许多完全新颖的开放式任务的能力。\n\n## 💬 可借鉴之处\nMP5的设计和实现为解决开放世界长时任务提供了新的思路和方法，其主动感知方案和模块化设计对于开发更智能、更灵活的具身系统具有重要的参考价值。","llm_summary_res_status":200}
{"title":"OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents","authors":"Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, Yitao Liang","summary":"This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the\ncontrol command directly, OmniJARVIS seeks a different path to ensure both\nstrong reasoning and efficient decision-making capabilities via unified\ntokenization of multimodal interaction data. First, we introduce a\nself-supervised approach to learn a behavior encoder that produces discretized\ntokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation\nlearning policy decoder conditioned on these tokens. These additional behavior\ntokens will be augmented to the vocabulary of pretrained Multimodal Language\nModels. With this encoder, we then pack long-term multimodal interactions\ninvolving task instructions, memories, thoughts, observations, textual\nresponses, behavior trajectories, etc into unified token sequences and model\nthem with autoregressive transformers. Thanks to the semantically meaningful\nbehavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing\nchain-of-thoughts), plan, answer questions, and act (by producing behavior\ntokens for the imitation learning policy decoder). OmniJARVIS demonstrates\nexcellent performances on a comprehensive collection of atomic, programmatic,\nand open-ended tasks in open-world Minecraft. Our analysis further unveils the\ncrucial design principles in interaction data formation, unified tokenization,\nand its scaling potentials. The dataset, models, and code will be released at\nhttps:\/\/craftjarvis.org\/OmniJARVIS.","url":"http:\/\/arxiv.org\/abs\/2407.00114v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2407.00114v2","published":1719495971000,"comment":"accepted on NeurIPS 2024","pdf_text":"OmniJARVIS\nUnified Vision-Language-Action Tokenization Enables\nOpen-World Instruction Following Agents\nZihao Wang1, Shaofei Cai1, Zhancun Mu2, Haowei Lin1, Ceyao Zhang3, Xuejie Liu1\nQing Li3, Anji Liu4, Xiaojian Ma3, Yitao Liang1∗\nTeam CraftJarvis\n1Institute for Artificial Intelligence, Peking University\n2Yuanpei College, Peking University\n3Beijing Institute for General Artificial Intelligence (BIGAI)\n4University of California, Los Angeles\n{zhwang,caishaofei}@stu.pku.edu.cn\nxiaojian.ma@ucla.edu,liuanji@cs.ucla.edu,yitaol@pku.edu.cn\nAbstract\nThis paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the control\ncommand directly, OmniJARVIS seeks a different path to ensure both strong\nreasoning and efficient decision-making capabilities via unified tokenization of\nmultimodal interaction data. First, we introduce a self-supervised approach to\nlearn a behavior encoder that produces discretized tokens for behavior trajectories\nτ = {o0, a0, . . . } and an imitation learning policy decoder conditioned on these\ntokens. These additional behavior tokens will be augmented to the vocabulary\nof pretrained Multimodal Language Models. With this encoder, we then pack\nlong-term multimodal interactions involving task instructions, memories, thoughts,\nobservations, textual responses, behavior trajectories, etc. into unified token se-\nquences and model them with autoregressive transformers. Thanks to the semanti-\ncally meaningful behavior tokens, the resulting VLA model, OmniJARVIS, can\nreason (by producing chain-of-thoughts), plan, answer questions, and act (by pro-\nducing behavior tokens for the imitation learning policy decoder). OmniJARVIS\ndemonstrates excellent performances on a comprehensive collection of atomic,\nprogrammatic, and open-ended tasks in open-world Minecraft. Our analysis further\nunveils the crucial design principles in interaction data formation, unified tokeniza-\ntion, and its scaling potentials. The dataset, models, and code will be released at\nhttps:\/\/craftjarvis.org\/OmniJARVIS\/.\n1\nIntroduction\nUpon the success of pretrained Large Language Models (LLMs) [7, 35, 40, 17, 13] and Multimodal\nLangauge Models (MLMs) [31, 20, 1, 53, 33], some recent works have been venturing into developing\nVision-Language-Action (VLA) models [6, 22, 47, 38], a promising pathway towards the ultimate\ngoal of building autonomous agents that can follow and even self-generated instructions to fulfill\nvarious reasoning and acting tasks in open world environments. Among them, two most prominent\n∗Corresponding Author.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2407.00114v2  [cs.LG]  31 Oct 2024\nFigure 1: Illustration of multi-modal interaction data for decision-making. A canonical interaction sequence\ndepicting the human decision-making process starts from a given task instruction and memory, followed by\na series of sub-task completion which involves initial observations, chain-of-thought reasoning, and behavior\ntrajectories. Our proposed VLA model OmniJARVIS jointly models the vision (observations), language\n(instructions, memories, thoughts), and actions (behavior trajectories) as unified autoregressive sequence\nprediction. A self-supervised behavior encoder (detailed in Section 2 and Figure 2) converts the actions into\nbehavior tokens while the other modalities are tokenized following the practices of MLMs [31, 3, 1].\narchitectures have been proposed: 1) Combining an off-the-shelf MLM [31, 1] with separate goal-\nconditioned controllers [28, 10, 9], where MLM reasons, plans and pilots the controllers by producing\ntextual goal instructions, e.g. DEPS [46], JARVIS-1 [47], voyager [44]; 2) Tuning a pretrained MLM\ninto producing control commands directly, while maintaining the reasoning and language capabilities,\ne.g. RT-2 [6], LEO [22]. However, these two designs could still have significant drawbacks when it\ncomes to open-world environments. First, an open world (e.g., Minecraft) usually teams up with an\ninfinite number of complex and highly contextualized tasks [16, 29], and it can be fairly challenging\nto depict them in text only. Therefore, VLA models that solely depend on text to communicate with\nthe text-conditioned policies [47, 46] may fail to correctly pilot these controllers. On the other side,\nemitting the control command directly [6, 22] without invoking separate controllers could alleviate the\naforementioned communication problem but given the long-horizon nature of open-world tasks, it is\nless practical to perform long-term control with a large VLA model as the context length requirement,\ncomputation cost and inference efficiency could become unaffordable.\nIn this paper, we aim to tackle the aforementioned issues of existing VLA models when facing\nopen-world environments: complex & context-dependent tasks and long-term tasks. Our key\ninsight originates from the observation of human decision-making: Given these open-world tasks,\nhumans can make informed decisions via multi-round mental, verbal, and physical interactions\n(an illustration can be found in Figure 1). Therefore, if the VLA model can manage to learn from\nsuch interaction data, it may master the underlying human decision-making procedures. However,\nmodeling interaction data is non-trivial: it is multi-modal, encloses vision (mostly observations),\nlanguage (instructions, thoughts, etc.), and actions (behavior trajectories). Compared to the fruitful\nexplorations on jointly tokenizing vision and language [31, 3, 43, 1] into sequences for autoregressive\nmodeling [7], tokenizing behavior trajectories (actions) is hard due to the following reasons. On the\none hand, directly using low-level actions from the environment would pose huge challenges to the\nmodel’s ability to process long sequences, which significantly hurts performance. It also hinders us\nfrom leveraging the planning ability of generative models. On the other hand, language-level action\ntokens require significantly more supervision and cannot accurately describe all possible actions.\nTo this end, we propose OmniJARVIS, a novel VLA model that jointly models vision, language,\nand actions in interaction data with unified tokenization. OmniJARVIS comprises two key ideas:\n1) Behavior Tokenization. We introduce a self-supervised approach to learn a behavior encoder\n2\nBehavior Encoder\nDecoder as Policy\nObservations\nLearnable Tokens\n⋯\n⋯\n⋯\nFinite Scalar Quantizer\nDiscrete Behavior Tokens\nTokenizer\n𝑐!\n𝑐\"\n𝑐̂!\n𝑐̂\"\n𝑜!\n𝑜#\n𝑜$\n⋯\n𝑠!\n𝑠\"\nObservations\n⋯\n𝑜!\n𝑜#\n𝑜$\n⋯\n𝑎!\n𝑎#\n𝑎$\nBehavior Cloning\nDeTokenizer\nFigure 2: Self-supervised learning for behavior tokenizer of OmniJARVIS. We modify the VAE-based\nself-supervised learning of behavior trajectories in [10] to train the behavior tokenizer and de-tokenizer in\nOmniJARVIS. Specifically, we adopt the auto-encoding objective but replace the Gaussian latent with a discrete\nrepresentation based on Finite Scalar Quantizer [34]. The encoder will then be used as the behavior tokenizer\nto produce discrete tokens from the actions (behavior trajectories) in multimodal interaction data, while the\nbehavior tokens emitted by OmniJARVIS will be sent to the policy decoder to perform motor control.\nthat produces discretized tokens for actions (behavior trajectories) and an imitation learning policy\ndecoder conditioned on these tokens (Section 2); 2) Autoregressive Modeling. By augmenting these\nbehavior tokens into the vocabulary of pretrained MLMs, we pack the multimodal interaction data into\nunified token sequences and learn a transformer on these sequences with an autoregressive modeling\nobjective. We conduct comprehensive evaluations in the open-world Minecraft Universe [29].\nOmniJARVIS demonstrates impressive performances on a wide range of atomic, programmatic, and\nopen-ended Minecraft tasks. Our analysis confirms several critical design choices in data formation,\ntokenization, and the scaling potential of OmniJARVIS. Our contributions are as follows:\n• We propose OmniJARVIS, a novel VLA model capable of following instructions to reason,\nplan, and act in open-world environments by jointly modeling vision, language, and actions in\nmultimodal interaction data for decision-making.\n• We propose a self-supervised approach to learn a behavior encoder to tokenize actions and an\nimitation learning policy decoder to produce control commands from behavior tokens emitted by\nOmniJARVIS, allowing joint learning of VLA and smooth action readout.\n• We conduct extensive evaluations in open-world Minecraft to demonstrate OmniJARVIS’s profi-\nciency across various tasks and present in-depth analyses to reveal valuable insights.\n2\nA Tokenizer for Behaviors\nAs illustrated in Section 1, a key challenge for VLA is the mismatch between the action modality and\nother modalities such as the language instructions. A key insight is that a good amount of knowledge\nabout the effects of actions can be learned directly from behavior trajectories {τ (i)}i. We propose to\nlearn a behavior tokenizer in addition to the well-studied vision and language tokenizers to achieve\nunified tokenization of the vision , language , and actions in multimodal interaction data (Figure 1).\nWe pose two main requirements to the behavior tokens. First, they should be able to express complete\nand diverse behavior from (short) trajectories. Further, the tokens should contain semantic information\nso that they are compatible with the other modalities, which enables the reasoning and planning\nability of LLMs (e.g., by conducting chain-of-thought reasoning).\nSpecifically, we aim at producing a set of N discrete behavior tokens sbhv\n1 , . . . , sbhv\nN from a behavior\ntrajectory τ = {o0, a0, . . . }. Further, a de-tokenizer is needed to map these tokens back to an action\nrollout in the environment that reproduces the goal achieved in τ. GROOT [10] explores a VAE-based\napproach to jointly learn a latent representation of behavior trajectories and an imitation learning\npolicy decoder that conditions the latent as goal. However, the continuous latent cannot be used\nas the behavior tokens as they can be more difficult to learn and decode with the existing discrete\ntokens of pretrained MLMs [22, 32]. Therefore, we replace the Gaussian latent in GROOT with an\nimproved vector quantized discrete latent called Finite Scalar Quantization (FSQ) [34]. We adopt a\nquantization configuration of [8, 8, 8, 6, 5], which means a code with a length=5 and a codebook size\nof 8 × 8 × 8 × 6 × 5 = 15360 is produced. The configuration is selected by a simple grid search.\nOverall, the behavior tokenizer (behavior encoder) eϕ(o1;T ) and the de-tokenizer (IL policy decoder)\n3\nFigure 3: Architecture and Inference of OmniJARVIS. The main body of OmniJARVIS is a multimodal\nlanguage model (MLM) augmented with additional behavior tokens. Given a task instruction, initial memory,\nand observation, OmniJARVIS will iteratively perform chain-of-thought reasoning and produce behavior tokens\nas a means of control via the decoder policy (behavior de-tokenizer). Every 128 steps, OmniJARVIS is forced\nto reason again and produce new behavior tokens with the latest observation. (Not shown above) OmniJARVIS\ncan also make textual responses, e.g. answering questions.\nπθ(at|o1:t) is learned with the following objective:\nargmin\n(ϕ,θ)\nEτ∼D\n\" T\nX\nt=1\n−log πθ(at|o1:t, f(eϕ(o1:T )))\n#\n,\n(1)\nwhere f(·) denotes the finite scalar quantizer. We choose a non-causal (bidirectional) transformer\nand a causal transformer to parameterize the encoder eϕ(o1;T ) and the policy decoder πθ(at|o1:t),\nrespectively. In practice, we set T = 128 as the trunk size of the behavior trajectory to be encoded.\nWe will discuss how to handle trajectories longer than 128 in the next section.\nCompared to our behavior tokenization, most prior work in VLA models, either represents the\nbehavior trajectories in interaction data as a textual goal description and invokes a separate goal-\nconditioned controller [47, 46], or represents the state-action sequence {o0, a0, . . . } directly as in\nDecision Transformers (DT) [11, 22, 38, 6]. Our approach offers a more compact but still informative\nrepresentation of the actions part in multimodal interaction data. Moreover, the action readout, i.e.\nsimply sending the behavior tokens to the policy decoder, is also more efficient than the DT-style\ndirect control from VLA models [38, 6, 22].\n3\nMultimodal Interaction Data and OmniJARVIS\nAs illustrated in Figure 1, canonical multimodal interaction data comprises vision (observations),\nlanguage (instructions, memories, thoughts), and actions (behavior trajectories). However, it can be\ndifficult to directly collect such interaction data from human annotators. Therefore, we propose to\nconvert an existing Minecraft gameplay dataset [2] into the multimodal interaction data required by\nOmniJARVIS. We begin with a formal definition of the interaction data, followed by our approach\nfor data conversion and augmentation from existing datasets, and finish up with the architecture,\nformulation of learning on such interaction data, and inference procedure of OmniJARVIS. An\noverview of OmniJARVIS architecture and inference can be found in Figure 3.\n3.1\nData Formation\nAn interaction sequence of decision-making D = {Dt}T\nt=0 comprises T segments. Each segment Dt\ncan be a sentence of text words {wi}N\ni=1, i.e. the language part such as instructions Dinst\nt , memory\nDmem\nt\nor thoughts Dtht\nt . Dt can also be an image I, i.e. the vision part such as observations Dobs\nt\n= I.\nFinally, Dt may belong to the action (behavior trajectory) part, i.e. Dbhv\nt\n= {o0, a0, . . . }. We assume\nthese segments follow the ordering below (Figure 1):\nDinst\n0 , Dmem\n1\n|\n{z\n}\nContext\n, Dobs\n2 , Dtht\n3 , Dbhv\n4\n|\n{z\n}\nsub-task 1\n, Dobs\n5 , Dtht\n6 , Dbhv\n7\n|\n{z\n}\nsub-task 2\n, . . .\n(2)\nWe tokenize such a sequence of segments into a series of tokens {s0, . . . , sM} using the vision and\nlanguage tokenizer offered by a pretrained MLM and the behavior tokenizer introduced in Section 2.\n4\n3.2\nPreparing Multimodal Interaction Data\nIn reality, many segments of the multimodal interaction D can be missing in public datasets. We\nconsider the Minecraft contractor data released by OpenAI [2] and it only contains behavior trajec-\ntories Dbhv\nt\n. Therefore, we need to properly augment the data with the additional textual segments\nincluding instructions Dinst\nt , memory Dmem\nt\n, and thoughts Dtht\nt . We follow the prior practices [22, 31]\nto synthesize the required text using LLMs. Below, we detail how each type of segment is constructed.\nMore details can be found in appendix.\nSynthesis of instruction Dinst\nt\n. The instruction is a high-level description of what task is being\nperformed in the current interaction sequence. The considered OpenAI Minecraft data includes meta\ninformation of each gameplay video, which depicts fundamental events that happened during in\nMinecraft gameplay, e.g. what block was just destroyed, what entity was just killed, what item was\njust crafted, etc. Such meta-information can provide a basic overview of what the player has been\nthrough in the gameplay. We therefore prompt an LLM into summarizing the gameplay with the meta\ninformation. The summary will be used as the instruction Dinst\nt\nof the current trajectory.\nSynthesis of memory Dmem\nt\n. The memory is the summary of what agents have finished in the\nprevious interaction sequences. Due to the limited sequence length that the auto-regressive model\ncan handle, the model needs to learn to summarize key information related to the task in historical\ninteractions and ignore behaviors unrelated to instructions. The memory will be updated based on the\nresults of each episode trunk and used for subsequent episode trunks. We therefore prompt an LLM\ninto summarizing the gameplay with the meta information. The summary will then be used as the\nmemory Dmem\nt\nof the current interaction trajectory. The memory prompt can be found in Appendix F.\nSynthesis of thought Dtht\nt . The thought is the agent’s reasoning and explanation of its own decisions.\nPrevious methods have confirmed that using thought-enhanced interaction data helps language models\nunderstand decision-making [21]. Compared to labeling thoughts by humans [50, 6], we assume\nthat thought is an intermediate variable that can be determined by the actions taken and observations\nmade before and after the action, which is similar to an Inverse Dynamics Model [2]. We therefore\nprompt an LLM into estimating the thought of decisions with in-context learning, which will then be\nused as the thought Dtht\nt of the current behavior. Details can be found in Appendix E.\n3.3\nArchitecture, Training, and Inference of OmniJARVIS\nAs illustrated in Figure 3, OmniJARVIS is built upon a pretrained MLM. We augment the original\nvocabulary of the MLM with additional tokens from the behavior tokenizer. Specifically, as we\nadopted the [a, b, c] FSQ configuration (Section 2), we augment with a + b + c new tokens as each\nbehavior comprises n behavior tokens sbhv\n1\n, . . . , sbhv\nn\ncorresponding to n FSQ levels. We formulate\nthe learning objective of OmniJARVIS following [7, 37] in a prefix language modeling fashion. For\na batch B of token sequence s, we optimize OmniJARVIS via:\nL(θ, B) = −\n|B|\nX\nb=1\nT\nX\nt=1\nlog pθ(s(b,t)\nres |s(b,<t)\nres\n, s(b,1)\nprefix, ..., s(b,L)\nprefix ),\n(3)\nwhere sprefix denotes the prefix token, which is tokenized from the segments that served as context for\nreasoning and decision-making, i.e. instruction Dinst\nt , memory Dmem\nt\nand observation Dobs\nt\nwithin the\ninteraction sequence (Equation 2). The remaining tokens (tokenized from thought Dtht\nt and behavior\ntrajectory Dbhv\nt\n) will be predicted in an autoregressive fashion. From a high level, OmniJARVIS is\ntrained to reason (producing thought tokens) and act (producing behavior tokens) from contexts with\ntask instructions, memory, and current observations. During inference, we begin with the feeding\nOmniJARVIS with a task instruction, an empty memory, and an initial observation. OmniJARVIS\nwill produce a chain-of-thought as a means of reasoning and subsequently, emit behavior tokens for\ncontrol. Every N steps, it is forced to reason again to produce new behavior tokens with the latest\nobservation. We empirically set N = 32.\n4\nCapabilities and Analysis\n4.1\nOverview\nTraining details and Datasets. The training of the OmniJARVIS is divided into two stages. In\nthe first step, we use a self-supervised training method to train a Behavior Tokenizer, including\nthe Encoder and Decoder jointly. We use FSQ as a quantization method and build a codebook\n5\nwith 8*8*8*6*5 discrete codes. The training data for Behavior Tokenizer comes from Contractor\nDataset [2], which is a collection of Minecraft gameplay videos. The training parameters and details\nremain consistent with GROOT, which can be found in Appendix A.\nIn the second stage, we use this behavior tokenizer to process Minecraft offline trajectories to obtain\nbehavior token sequences. We add 35 (8+8+8+6+5) additional tokens to the MLM tokenizer as\nbehavior tokens for unified representation, so each time the VLA needs to output a continuous\nsequence of 5 tokens to represent a complete behavior. We use GPT-3.5 to synthesize thought,\nmemory, and instruction to raw offline datasets to build complete interaction data. The specific\nprompt can be found in Appendix E. These data collectively constitute the embodied instruction-\nfollowing dataset of OmniJARVIS, including 600k trajectories and about 900M tokens.\nThe training dataset of OmniJARVIS further includes a large amount of QA data about Minecraft.\nWe generate a large number of seed questions about these texts using web pages on the Minecraft\nwiki. Then, we use the self-instruct method to generate a large number of creative questions and\ninstructions. This constructed QA dataset consists of 300k conversations with about 90M tokens.\nDuring the training process, the QA data and instruction-following data are mixed, with a total of\nabout 1T tokens, to train OmniJARVIS. In specific, we SFT (supervised finetune) LLaVA-7B [31].\nThe details can be found in Appendix A. To further demonstrate the generalizability of the method,\nwe also fine-tune LLaVA at different scales and VLM Fuyu-8B with different architectures. The\nrelevant results are presented in Section 4.5 and Section 4.6.\nExperimental Setups. We conduct experiments in the complex and open-world environment\nof Minecraft, a voxel-based 3D video game that has garnered significant attention from real-life\nresearch due to its popularity and diverse mechanics [18, 16]. We first evaluate OmniJARVIS with\natomic tasks, which are skill-level tasks, testing VLAs’ ability to follow simple and straightforward\ninstructions. Then we evaluate OmniJARVIS with programmatic tasks, which require the agent\nto obtain an item starting from an empty inventory. The success of these tasks requires VLAs to\ndecompose the provided instruction into atomic-level subtasks, and hence tests VLAs’ complex\nreasoning ability. Finally, we test OmniJARVIS with open-world embodied question-answering\nbenchmarks and creative free-form instruction-following. We also conduct ablation experiments of\nOmniJARVIS with different behavior tokenizers, different training dataset formats, and different\nvision tokenizations. Finally, we explore the generalization abilities of OmniJARVIS of Atari Games\nand the scaling potential of OmniJARVIS with different models and data scales.\n4.2\nMain Results I: Short-horizon Atomic Tasks\nAtom tasks are various simple skills that agents in Minecraft need to master. They are basic tasks\nyet are fundamental skills that agents need to master during the learning process. We first evaluate\nOmniJARVIS with our learned behavior tokenizer on these tasks.\nWe select “chopping trees”\n, “digging dirt”\n, “mining stones”\n, and “collecting wheat\nseeds”\nas the evaluation tasks. We directly take those short task descriptions as instructions\nfor agents. We use text-conditioned VPT [2], Open-world Control [9], STEVE-I [28], and video-\ninstructed GROOT [10] as baselines. We compute the average rewards of different agents on\nevery task in Table 1 across 10 runs. By observing the environment and adjusting action tokens\ndynamically, OmniJARVIS effectively follows straightforward instructions across various scenarios.\nIt consistently achieves a high average reward with minimal standard deviation.\n4.3\nMain Results II: Long-horizon Programmatic Tasks\nTo further verify the ability of OmniJARVIS to complete tasks with long sequences, we use 30\nprogrammatic tasks to evaluate the performance of different agents. These tasks require the agent to\nstart from an empty inventory in a new world until obtaining the final required items, which is usually\na chain of atom tasks. These tasks are divided into five groups based on difficulty: wooden, food,\nstone, iron, and diamond. For example, the prompt for task “Obtain a diamond pickaxe”\nis “Give\nyou nothing in the inventory, obtain a diamond pickaxe.” This task requires more game time and\nmore complex planning for up to 10 different intermediate items [2]. We list all programmatic tasks\nand its corresponding instructions in the Appendix C.1.\nBaselines are divided into two types: 1) directly outputs actions, namely the native behavior tokenizer,\nincluding STEVE-I [28] and GROOT [10]. 2) using pretrained LLM as a planner to output language\ngoals and connect the STEVE-I to execute these goals, including Zero-Shot Planner (GPT) [23],\n6\nTable 1: Evaluation results (rewards) on short-horizon\natom tasks.\nThe text-conditioned VPT [2] (“VPT\n(text)∗”) is from Appendix I of its paper.\nMethod\nCondition\n↑\n↑\n↑\n↑\nVPT∗[text] [2]\nLanguage\n2.6±0.3\n9.2±0.7\n-\n0.8±0.1\nSTEVE-I [28]\nLanguage\n11.0±3.0\n10.0±2.5\n3.2±1.6\n5.1±2.5\nGROOT [10]\nVideo\n14.3±4.7\n19.7±8.7\n19.0±11.3\n7.3±0.6\nOmniJARVIS\nLanguage\n10.8±5.2\n20.3±9.2\n25.8±2.9\n8.2±3.6\nTable 2: Results on open-ended instruction following.\nVPT\nSTEVE-1\nVoyager\nDEPS\nOurs\nInstruction\nFollowing ↓\n975.9\n972.7\n932.1\n929.5\n886.2\nTable 3: Results on open-ended question answering.\nWe use LLM-as-judge [35] to evaluate the accuracy.\nVicuna-7B\nVicuna-13B\nLLaMA2-70B\nGPT-3.5\nOurs\nQA ↑\n2.34\n2.85\n2.50\n7.50\n8.40\nTable 4: Success rate of different agents on long-horizon programmatic tasks.\nMethod\nAction Tokenizer\nWooden (10)\nFood (5)\nStone (5)\nIron (5)\nDiamond (5)\nAverage\nSTEVE-I [28]\nNative\n0.04±0.07\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.01\nGROOT [10]\nNative\n0.05±0.08\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.02\nGPT [23]\nLanguage\n0.26±0.14\n0.08±0.04\n0.24±0.05\n0.04±0.05\n0.00±0.00\n0.15\nReAct [50]\nLanguage\n0.44±0.11\n0.12±0.04\n0.30±0.10\n0.06±0.05\n0.00±0.00\n0.23\nDEPS [46]\nLanguage\n0.78±0.11\n0.12±0.04\n0.68±0.08\n0.16±0.05\n0.04±0.05\n0.43\nOmniJARVIS\nFSQ GROOT\n0.95±0.07\n0.44±0.05\n0.82±0.08\n0.32±0.11\n0.08±0.04\n0.59\nReAct [50], and DEPS [46]. We use success rate to evaluate the completion of tasks, that is, whether\nthe task is completed within the specified time. The experimental results are listed in Table 6.\nProgrammatic Tasks usually require complex reasoning for planning. While STEVE-I and GROOT\ncan only finish short skill-level tasks in atom tasks, is difficult to finish these programmatic tasks.\nAgents based on Language behavior tokenizer can complete complex tasks including diamond group\nones, but with a low success rate. This is because these in-context learning methods leverage the\npretrained LLM which may lack the necessary knowledge about this world. It is worth noting that in\nthe Food group, agents based on Language Tokenizer have an average success rate of around 10%,\nas this set of tasks does not require complex reasoning. This indicates that Language-conditioned\nTokenizers need additional language-conditioned trajectories as supervision for training while there\nwas less such data available during STEVE-I’s training phase leading to significant performance gaps.\nMeanwhile, OmniJARVIS uses a self-supervised trained behavior tokenizer which does not require\nextra language labels and hence receives more training resulting in good performance across a wider\nrange of tasks. We will further prove this in the next set of Creative Task experiments.\n4.4\nMain Results III: Open-ended Question-Answering and Instruction Following Tasks\nThe open-ended tasks differ from programmatic tasks due to the lack of straightforward success\ncriteria [16]. We select the long-term open-ended tasks which usually need at least 5 minutes of\nhuman-playing time to finish. The task prompts can be found in Appendix C.3. Following image\ngeneration and video generation tasks [19, 41], we take the Fréchet Sequence Distance (FSD) metrics\nto evaluate the correlation between agent rollout video and creative instruction. Specifically, we first\nask human experts to finish the creative task prompts under randomly generated worlds and record\nthe game-playing videos Vhuman. Then, we provided the task prompts for different Minecraft agents,\nand obtained a rollout video set Vagent. Similar to FID [19], we used MineCLIP [16] to calculate the\nembedding of video clips and computed FSD for the embedding distributions of human and agent\nrollout videos. The analysis of the metrics can be found in Appendix B.\nWe further conduct open-ended embodied question-answering benchmarks to evaluate the ability of\nthe agent to complete open-ended instructions and grasp world knowledge. The questions answering\ninstructions set can be found in Appendix C.2. The evaluation results can be found in Table 3\nand Table 2. OmniJARVIS is the agent that can simultaneously complete both types of tasks\nand has achieved the best performance in different task sets, surpassing strong baselines including\nVoyager [44] and DEPS [46]. Also, it maintains strong reasoning capability, especially on embodied\nquestion answering compared to LLM baselines (with image captions as visual context).\n4.5\nInsights and Analysis\nInteractive Dataset Format. We explore the crucial roles played by the different type of segments\nin interaction data, including the instruction, memory, thought, and caption tokens. The results can\nbe found in Table 4, where we evaluate the loss on predicting the behavior tokens. It can be seen that\ninstruction and thought can be more critical to the successful prediction of behavior tokens. This is\nconsistent with our hypothesis – making informed decisions requires task instruction and reasoning.\n7\nFigure 4: Ablation experiments on OmniJARVIS with different behav-\nior tokenizers, vision tokenizers, and training on different interactive\ndatasets. The first line is training on the unconditional interactive dataset,\ni.e., without instructions on the trajectories. OmniJARVIS with VQ-\nGROOT [42, 10] shows no results because of training collapse.\nDataset Format\nLoss\nBehavior\nTokenizer\nVision\nTokenizer\nInstruction\nCaption\nThought\nMemory\nTrain\nEval\n✗(unconditional)\n0.33\n0.67\n✓\n✗\n✗\n✗\n0.46\n0.51\n✓\n✓\n✗\n✗\n0.44\n0.48\n✓\n✓\n✓\n✗\n0.32\n0.33\nFSQ GROOT\nLLaVA\n✓\n✓\n✓\n✓\n0.16\n0.17\nFSQ GROOT\nCaptioner+\n✓\n✓\n✗\n✗\n0.49\n0.52\nFSQ GROOT\nFUYU\n✓\n✓\n✗\n✗\n0.42\n0.44\nGROOT\nLLaVA\n✓\n✓\n✓\n✓\n0.44\n0.48\nVQ GROOT\nLLaVA\n✓\n✓\n✓\n✓\n-\n-\nFigure 5:\nScaling potential\nof OmniJARVIS. Its evaluation\nloss continues to drop with the\ngrowth of data and model parame-\nters. The Pearson coefficients for\nthe 2B, 7B, and 13B models are\n0.9991, 0.9999, and 0.9989.\nTable 5: Ablation experiments on behavior tokenizer with different code vocabulary size.\nCodebook\nsize\nFSQ\nLevels\nTraining\nIterations\nTrain\nLoss\nEval\nLoss\nReconstruction\nFSD ↓\nSampling\nFSD ↓\nAverage\nRewards ↑\nCodebook\nUsage\ne8\n[8,6,5]\n180k\n2.746\n3.161\n46.57\n68.90\n0.63±0.67\n93.75%\ne10\n[8,5,5,5]\n180k\n3.011\n3.148\n43.67\n61.85\n0.54±1.21\n97.65%\ne14\n[8,8,8,6,5]\n240k\n3.092\n3.116\n42.72\n57.37\n2.27±2.45\n92.36%\nVision Tokenization. We also evaluate training OmniJARVIS with different vision tokenization,\nincluding ImageCaptioner + LLaMA2-7B [12, 40] (basically converting the vision input into textual\ncaptions), fuyu-8b [3], and LLaVA-7B [31] architecture. For the ImageCaptioner+, we fix the\nImageCaptioner models and only fine-tune the language model, i.e., LLaMA2-7B. We use the\nprediction loss of behavior tokens as the evaluation criterion, namely eval loss. We found that the\nmodel trained with LLaVA-7B architecture has the lowest evaluation loss, so we chose this model as\nthe default model.\nBehavior Tokenizer. We explore OmniJARVIS with different behavior tokenizers, including the\ndefault setting using FSQ codebook, a variant of using VQ-VAE instead of FSQ [42], and simply\nusing sub-goal language annotation as behavior “tokens”. The evaluation results on 4 programmatic\ntasks are listed in Table 4. Using an FSQ tokenizer is generally better than a language goal, which\nconfirms the advantages of using a tokenized behavior over language descriptions of behavior. The\nuse of VQ-VAE as a quantized behavior tokenizer collapsed during the training process, so there\nwere no results in all test tasks.\nBehavior Codebook. We conduct an in-depth investigation of behavior tokenizers with varying\ncodebook sizes, utilizing recommended sets of FSQ levels to approximate specified codebook\ndimensions [34] as delineated in Table 5. We evaluate performance across multiple metrics for\neach codebook size. Codebook Usage is quantified as the proportion of codewords utilized at\nleast once when encoding the validation datasets. Reconstruction FSD is measured by the FSD\nscores derived from the MineCLIP encoder [16], processing 1,000 different demonstration videos\nthrough the FSQ-GROOT and subsequent rollout in a randomly generated environment. Additionally,\nwe measure Resampling FSD, which is the FSD score obtained when the environment rollout is\nconditioned on representations sampled from the codebook. Finally, we assess the average rewards\nfor the task “collect wood” using OmniJARVIS across varying codebook sizes. Our findings indicate\nthat increases in codebook size correlate with enhanced average rewards and reduced FSD scores,\nsuggesting a scalable performance in OmniJARVIS with larger codebooks.\nBehavior Semantics. We provide some qualitative analysis on the learned FSQ-based behavior\ntokenizer. In Figure 6, we tokenize several reference videos, then feed the behavior tokens to the\npolicy decoder and see if it can accomplish the same task as in reference videos. The results indicate\nthat our behavior tokenizer is able to capture such behavior semantics and offers rich task information.\n4.6\nGeneralization and Scaling Potential of OmniJARVIS\nWe first explore adapting OmniJARVIS to the Atari game Montezuma’s Revenge. We created\na dataset from 500 episodes played by an agent trained with Random Network Distillation [8],\n8\nProvided\nGenerated\nFigure 6: Examples of behavior tokenization-detokeinzation. Left: the reference video to be tokenized by\nour FSQ-based behavior tokenizer (encoder). Right: the behavior of the policy decoder is conditioned on the\nbehavior tokens. The policy decoder can reproduce the task being accomplished in the reference video.\nFigure 7: OmniJARVIS plays Montezuma’s Revenge and gets a reward of 3600.\nsupplemented by random actions in early frames to enhance diversity. This dataset contains 1,823,699\ntransitions. We then trained the FSQ-GROOT tokenizer on this new dataset and subsequently trained\nOmniJARVIS on the tokenized data. The finetuned OmniJARVIS achieved a score of 3600 in\nMontezuma’s Revenge, indicating promising transferability. A rollout trajectory is in Figure 7.\nWe also investigate the scaling effect [25, 30] of data and model in OmniJARVIS by monitoring the\ninstruction-following loss on the validation set as the amount of data increases. In addition to fine-\ntuning from the default LLaVA-7B, we include two additional scales: OmniJARVIS-2B (fine-tuned\nfrom LLaVA-2B with Gemma-2B language models [20]) and OmniJARVIS-13B (fine-tuned from\nLLaVA-13B with LLaMA2-13B language models [31]).\nThe validation loss curves in Figure 5 reveal the following insights: 1) When using Omni-Tokenizer,\nOmniJARVIS’s instruction tuning aligns with the scaling law [25]. All curves exhibit a log-linear\ndecrease as the data scale increases. 2) Scaling up VLM consistently enhances performance. Notably,\nOmniJARVIS-7B demonstrates significantly lower losses compared to OmniJARVIS-2B. However,\nwhile improvements are consistent, the difference between OmniJARVIS-7B and OmniJARVIS-\n13B seems less pronounced, hinting at potential saturation when further scaling up VLM. This\nunderscores both the scalability of OmniJARVIS and the importance of increasing data volume to\nmatch the model.\n5\nRelated Works\nPretrained Language Models for Decision-making. Several works have explored leveraging LLMs\nto generate action plans for high-level tasks in embodied environments [23, 27, 5, 52]. To better\nperform complex planning in the environment, existing methods usually utilize chain-of-thought [49]\nor related methods [50]. To better cope with uncertainties in open worlds, some LLM-based methods\ngenerate plans interactively with human and environmental feedback [39, 46, 24] and retrieving\nfrom memory [47] or internet corpus [48]. However, those plans can only be executed in a language\nenvironment or require an additional controller or code executor to interact in an open world.\nVision-Language-Action Models. In order to better utilize the knowledge inside the language model\nfor decision-making, some methods tend to use decision datasets to fine-tune pretrained language\nmodels [15, 14]. Gato [38] was among the first to tokenize environment-provided actions to enable\njoint sequential modeling across modalities. PaLM-E [14] generates high-level instructions as texts\nand uses dedicated controllers to perform the task described by the output instructions. The RT\nseries focuses more on robotics settings. Specifically, RT-1 pairs a VLM with a language-conditioned\ncontroller; RT-2 extends the VLM to directly include control tokens; RT-X generalizes to new robots\nand environments. A recent VLA model LEO [22] expands the perception from 2D images to 3D\nworld and enables rich scene-level reasoning and control tasks.\n9\nFigure 8: Comparative Framework of Vision-Language Action Models. (a) depicts a model where upon\nreceiving a language instruction, actions are directly output based on the environmental state, facilitating\nimmediate interaction with the environment at a unified frequency. Smaller models with <1B parameters like\nVPT [2] maintain higher frequencies (>20Hz), though their capability for complex reasoning tasks is limited.\nLarger models with >7B parameters such as RT-2 [6], offer enhanced performance but operate at significantly\nreduced frequencies (2-3Hz). (b) illustrates a common approach utilizing large vision-language models for\nplanning, subsequently outputting language goals [46, 14, 4]. A language-conditioned policy then translates\nthese language goals into actions at a real-time interaction rate of 20Hz, with high-level models re-planning\nat less than 1Hz. This hierarchical structure balances interaction frequency and performance, while it requires\nlanguage as an intermediary and additional language labels. The training process of high-level vision-language\nmodels and language-conditioned policies are separate, thus performing poorly on tasks that can not be easily\nconnected by language. (c) (ours) mirrors the hierarchical structure of (b) but differentiates by employing a\nself-supervised encoder-decoder policy [10] and FSQ quantization [34] as a behavior tokenizer. The upper-level\nvision-language models produce self-supervised behavior tokens, which are then conditioned by a policy decoder\nto output actions, facilitating environment interaction. The behavior tokens are injected into the training corpus\nof vision-language-action models, which enables end-to-end inference. This approach also eliminates the need\nfor external language supervision and scales efficiently.\nOpen-world Agents in Minecraft. As LLMs have achieved remarkable reasoning results and\nunderstanding capabilities across various domains, the year 2023 has witnessed researchers adopting\nmultiple LLM-based approaches to create open-world agents in Minecraft [46, 55, 47, 44]. Some\nmethods focus on building policies for low-level skills [10, 28, 2]. Building upon the low-level\npolicies to interact with the Minecraft environment, Wang et al. [46], Yuan et al. [51] and Wang et al.\n[47] focus on leveraging the pre-trained language models as planners to finish programmatic tasks\nwith in-context learning. Wang et al. [44] adopts the life-long learning scheme and generates code\nas policies to enable continual exploration. Some use expert trajectories and Minecraft corpus to\nfine-tune pre-trained vision language models for better embodied planning [36, 54].\n6\nConclusion\nWe’ve presented OmniJARVIS, a novel VLA model that encompasses strong reasoning and ef-\nficient decision-making capabilities via unified tokenization of vision, language, and actions in\nmultimodal interaction data. The key ideas are learning behavior tokenizer (trajectory encoder)\nand de-tokenizer (IL policy decoder) using self-supervised learning on behavior trajectories and\nautoregressive modeling of tokenized multimodal interaction data using a pretrained multimodal lan-\nguage model (MLM). Evaluations on the open-world Minecraft Universe demonstrate its impressive\ninstruction-following capabilities. Possible future directions include a more in-depth investigation of\nbehavior tokenization, language capabilities after VLA fine-tuning, and alignment concerns emerging\nfrom the unified interaction modeling and VLA capabilities.\nAcknowledgments\nThis work is funded in part by the National Key R&D Program of China #2022ZD0160301. We\nthank a grant from CCF-Tencent Rhino-Bird Open Research Fund. One author is funded in part by\nNSF grants #IIS-1943641, #IIS-1956441, #CCF-1837129, an SRA from Meta and a research gift\nfrom Amazon Alexa AI, and a gift from RelationalAI.\n10","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nOmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents\n```\n#### 2. 论文摘要\n```\nThis paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the\ncontrol command directly, OmniJARVIS seeks a different path to ensure both\nstrong reasoning and efficient decision-making capabilities via unified\ntokenization of multimodal interaction data. First, we introduce a\nself-supervised approach to learn a behavior encoder that produces discretized\ntokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation\nlearning policy decoder conditioned on these tokens. These additional behavior\ntokens will be augmented to the vocabulary of pretrained Multimodal Language\nModels. With this encoder, we then pack long-term multimodal interactions\ninvolving task instructions, memories, thoughts, observations, textual\nresponses, behavior trajectories, etc into unified token sequences and model\nthem with autoregressive transformers. Thanks to the semantically meaningful\nbehavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing\nchain-of-thoughts), plan, answer questions, and act (by producing behavior\ntokens for the imitation learning policy decoder). OmniJARVIS demonstrates\nexcellent performances on a comprehensive collection of atomic, programmatic,\nand open-ended tasks in open-world Minecraft. Our analysis further unveils the\ncrucial design principles in interaction data formation, unified tokenization,\nand its scaling potentials. The dataset, models, and code will be released at\nhttps:\/\/craftjarvis.org\/OmniJARVIS.\n```\n\n#### 3. 论文全文\n```\nOmniJARVIS\nUnified Vision-Language-Action Tokenization Enables\nOpen-World Instruction Following Agents\nZihao Wang1, Shaofei Cai1, Zhancun Mu2, Haowei Lin1, Ceyao Zhang3, Xuejie Liu1\nQing Li3, Anji Liu4, Xiaojian Ma3, Yitao Liang1∗\nTeam CraftJarvis\n1Institute for Artificial Intelligence, Peking University\n2Yuanpei College, Peking University\n3Beijing Institute for General Artificial Intelligence (BIGAI)\n4University of California, Los Angeles\n{zhwang,caishaofei}@stu.pku.edu.cn\nxiaojian.ma@ucla.edu,liuanji@cs.ucla.edu,yitaol@pku.edu.cn\nAbstract\nThis paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the control\ncommand directly, OmniJARVIS seeks a different path to ensure both strong\nreasoning and efficient decision-making capabilities via unified tokenization of\nmultimodal interaction data. First, we introduce a self-supervised approach to\nlearn a behavior encoder that produces discretized tokens for behavior trajectories\nτ = {o0, a0, . . . } and an imitation learning policy decoder conditioned on these\ntokens. These additional behavior tokens will be augmented to the vocabulary\nof pretrained Multimodal Language Models. With this encoder, we then pack\nlong-term multimodal interactions involving task instructions, memories, thoughts,\nobservations, textual responses, behavior trajectories, etc. into unified token se-\nquences and model them with autoregressive transformers. Thanks to the semanti-\ncally meaningful behavior tokens, the resulting VLA model, OmniJARVIS, can\nreason (by producing chain-of-thoughts), plan, answer questions, and act (by pro-\nducing behavior tokens for the imitation learning policy decoder). OmniJARVIS\ndemonstrates excellent performances on a comprehensive collection of atomic,\nprogrammatic, and open-ended tasks in open-world Minecraft. Our analysis further\nunveils the crucial design principles in interaction data formation, unified tokeniza-\ntion, and its scaling potentials. The dataset, models, and code will be released at\nhttps:\/\/craftjarvis.org\/OmniJARVIS\/.\n1\nIntroduction\nUpon the success of pretrained Large Language Models (LLMs) [7, 35, 40, 17, 13] and Multimodal\nLangauge Models (MLMs) [31, 20, 1, 53, 33], some recent works have been venturing into developing\nVision-Language-Action (VLA) models [6, 22, 47, 38], a promising pathway towards the ultimate\ngoal of building autonomous agents that can follow and even self-generated instructions to fulfill\nvarious reasoning and acting tasks in open world environments. Among them, two most prominent\n∗Corresponding Author.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2407.00114v2  [cs.LG]  31 Oct 2024\nFigure 1: Illustration of multi-modal interaction data for decision-making. A canonical interaction sequence\ndepicting the human decision-making process starts from a given task instruction and memory, followed by\na series of sub-task completion which involves initial observations, chain-of-thought reasoning, and behavior\ntrajectories. Our proposed VLA model OmniJARVIS jointly models the vision (observations), language\n(instructions, memories, thoughts), and actions (behavior trajectories) as unified autoregressive sequence\nprediction. A self-supervised behavior encoder (detailed in Section 2 and Figure 2) converts the actions into\nbehavior tokens while the other modalities are tokenized following the practices of MLMs [31, 3, 1].\narchitectures have been proposed: 1) Combining an off-the-shelf MLM [31, 1] with separate goal-\nconditioned controllers [28, 10, 9], where MLM reasons, plans and pilots the controllers by producing\ntextual goal instructions, e.g. DEPS [46], JARVIS-1 [47], voyager [44]; 2) Tuning a pretrained MLM\ninto producing control commands directly, while maintaining the reasoning and language capabilities,\ne.g. RT-2 [6], LEO [22]. However, these two designs could still have significant drawbacks when it\ncomes to open-world environments. First, an open world (e.g., Minecraft) usually teams up with an\ninfinite number of complex and highly contextualized tasks [16, 29], and it can be fairly challenging\nto depict them in text only. Therefore, VLA models that solely depend on text to communicate with\nthe text-conditioned policies [47, 46] may fail to correctly pilot these controllers. On the other side,\nemitting the control command directly [6, 22] without invoking separate controllers could alleviate the\naforementioned communication problem but given the long-horizon nature of open-world tasks, it is\nless practical to perform long-term control with a large VLA model as the context length requirement,\ncomputation cost and inference efficiency could become unaffordable.\nIn this paper, we aim to tackle the aforementioned issues of existing VLA models when facing\nopen-world environments: complex & context-dependent tasks and long-term tasks. Our key\ninsight originates from the observation of human decision-making: Given these open-world tasks,\nhumans can make informed decisions via multi-round mental, verbal, and physical interactions\n(an illustration can be found in Figure 1). Therefore, if the VLA model can manage to learn from\nsuch interaction data, it may master the underlying human decision-making procedures. However,\nmodeling interaction data is non-trivial: it is multi-modal, encloses vision (mostly observations),\nlanguage (instructions, thoughts, etc.), and actions (behavior trajectories). Compared to the fruitful\nexplorations on jointly tokenizing vision and language [31, 3, 43, 1] into sequences for autoregressive\nmodeling [7], tokenizing behavior trajectories (actions) is hard due to the following reasons. On the\none hand, directly using low-level actions from the environment would pose huge challenges to the\nmodel’s ability to process long sequences, which significantly hurts performance. It also hinders us\nfrom leveraging the planning ability of generative models. On the other hand, language-level action\ntokens require significantly more supervision and cannot accurately describe all possible actions.\nTo this end, we propose OmniJARVIS, a novel VLA model that jointly models vision, language,\nand actions in interaction data with unified tokenization. OmniJARVIS comprises two key ideas:\n1) Behavior Tokenization. We introduce a self-supervised approach to learn a behavior encoder\n2\nBehavior Encoder\nDecoder as Policy\nObservations\nLearnable Tokens\n⋯\n⋯\n⋯\nFinite Scalar Quantizer\nDiscrete Behavior Tokens\nTokenizer\n𝑐!\n𝑐\"\n𝑐̂!\n𝑐̂\"\n𝑜!\n𝑜#\n𝑜$\n⋯\n𝑠!\n𝑠\"\nObservations\n⋯\n𝑜!\n𝑜#\n𝑜$\n⋯\n𝑎!\n𝑎#\n𝑎$\nBehavior Cloning\nDeTokenizer\nFigure 2: Self-supervised learning for behavior tokenizer of OmniJARVIS. We modify the VAE-based\nself-supervised learning of behavior trajectories in [10] to train the behavior tokenizer and de-tokenizer in\nOmniJARVIS. Specifically, we adopt the auto-encoding objective but replace the Gaussian latent with a discrete\nrepresentation based on Finite Scalar Quantizer [34]. The encoder will then be used as the behavior tokenizer\nto produce discrete tokens from the actions (behavior trajectories) in multimodal interaction data, while the\nbehavior tokens emitted by OmniJARVIS will be sent to the policy decoder to perform motor control.\nthat produces discretized tokens for actions (behavior trajectories) and an imitation learning policy\ndecoder conditioned on these tokens (Section 2); 2) Autoregressive Modeling. By augmenting these\nbehavior tokens into the vocabulary of pretrained MLMs, we pack the multimodal interaction data into\nunified token sequences and learn a transformer on these sequences with an autoregressive modeling\nobjective. We conduct comprehensive evaluations in the open-world Minecraft Universe [29].\nOmniJARVIS demonstrates impressive performances on a wide range of atomic, programmatic, and\nopen-ended Minecraft tasks. Our analysis confirms several critical design choices in data formation,\ntokenization, and the scaling potential of OmniJARVIS. Our contributions are as follows:\n• We propose OmniJARVIS, a novel VLA model capable of following instructions to reason,\nplan, and act in open-world environments by jointly modeling vision, language, and actions in\nmultimodal interaction data for decision-making.\n• We propose a self-supervised approach to learn a behavior encoder to tokenize actions and an\nimitation learning policy decoder to produce control commands from behavior tokens emitted by\nOmniJARVIS, allowing joint learning of VLA and smooth action readout.\n• We conduct extensive evaluations in open-world Minecraft to demonstrate OmniJARVIS’s profi-\nciency across various tasks and present in-depth analyses to reveal valuable insights.\n2\nA Tokenizer for Behaviors\nAs illustrated in Section 1, a key challenge for VLA is the mismatch between the action modality and\nother modalities such as the language instructions. A key insight is that a good amount of knowledge\nabout the effects of actions can be learned directly from behavior trajectories {τ (i)}i. We propose to\nlearn a behavior tokenizer in addition to the well-studied vision and language tokenizers to achieve\nunified tokenization of the vision , language , and actions in multimodal interaction data (Figure 1).\nWe pose two main requirements to the behavior tokens. First, they should be able to express complete\nand diverse behavior from (short) trajectories. Further, the tokens should contain semantic information\nso that they are compatible with the other modalities, which enables the reasoning and planning\nability of LLMs (e.g., by conducting chain-of-thought reasoning).\nSpecifically, we aim at producing a set of N discrete behavior tokens sbhv\n1 , . . . , sbhv\nN from a behavior\ntrajectory τ = {o0, a0, . . . }. Further, a de-tokenizer is needed to map these tokens back to an action\nrollout in the environment that reproduces the goal achieved in τ. GROOT [10] explores a VAE-based\napproach to jointly learn a latent representation of behavior trajectories and an imitation learning\npolicy decoder that conditions the latent as goal. However, the continuous latent cannot be used\nas the behavior tokens as they can be more difficult to learn and decode with the existing discrete\ntokens of pretrained MLMs [22, 32]. Therefore, we replace the Gaussian latent in GROOT with an\nimproved vector quantized discrete latent called Finite Scalar Quantization (FSQ) [34]. We adopt a\nquantization configuration of [8, 8, 8, 6, 5], which means a code with a length=5 and a codebook size\nof 8 × 8 × 8 × 6 × 5 = 15360 is produced. The configuration is selected by a simple grid search.\nOverall, the behavior tokenizer (behavior encoder) eϕ(o1;T ) and the de-tokenizer (IL policy decoder)\n3\nFigure 3: Architecture and Inference of OmniJARVIS. The main body of OmniJARVIS is a multimodal\nlanguage model (MLM) augmented with additional behavior tokens. Given a task instruction, initial memory,\nand observation, OmniJARVIS will iteratively perform chain-of-thought reasoning and produce behavior tokens\nas a means of control via the decoder policy (behavior de-tokenizer). Every 128 steps, OmniJARVIS is forced\nto reason again and produce new behavior tokens with the latest observation. (Not shown above) OmniJARVIS\ncan also make textual responses, e.g. answering questions.\nπθ(at|o1:t) is learned with the following objective:\nargmin\n(ϕ,θ)\nEτ∼D\n\" T\nX\nt=1\n−log πθ(at|o1:t, f(eϕ(o1:T )))\n#\n,\n(1)\nwhere f(·) denotes the finite scalar quantizer. We choose a non-causal (bidirectional) transformer\nand a causal transformer to parameterize the encoder eϕ(o1;T ) and the policy decoder πθ(at|o1:t),\nrespectively. In practice, we set T = 128 as the trunk size of the behavior trajectory to be encoded.\nWe will discuss how to handle trajectories longer than 128 in the next section.\nCompared to our behavior tokenization, most prior work in VLA models, either represents the\nbehavior trajectories in interaction data as a textual goal description and invokes a separate goal-\nconditioned controller [47, 46], or represents the state-action sequence {o0, a0, . . . } directly as in\nDecision Transformers (DT) [11, 22, 38, 6]. Our approach offers a more compact but still informative\nrepresentation of the actions part in multimodal interaction data. Moreover, the action readout, i.e.\nsimply sending the behavior tokens to the policy decoder, is also more efficient than the DT-style\ndirect control from VLA models [38, 6, 22].\n3\nMultimodal Interaction Data and OmniJARVIS\nAs illustrated in Figure 1, canonical multimodal interaction data comprises vision (observations),\nlanguage (instructions, memories, thoughts), and actions (behavior trajectories). However, it can be\ndifficult to directly collect such interaction data from human annotators. Therefore, we propose to\nconvert an existing Minecraft gameplay dataset [2] into the multimodal interaction data required by\nOmniJARVIS. We begin with a formal definition of the interaction data, followed by our approach\nfor data conversion and augmentation from existing datasets, and finish up with the architecture,\nformulation of learning on such interaction data, and inference procedure of OmniJARVIS. An\noverview of OmniJARVIS architecture and inference can be found in Figure 3.\n3.1\nData Formation\nAn interaction sequence of decision-making D = {Dt}T\nt=0 comprises T segments. Each segment Dt\ncan be a sentence of text words {wi}N\ni=1, i.e. the language part such as instructions Dinst\nt , memory\nDmem\nt\nor thoughts Dtht\nt . Dt can also be an image I, i.e. the vision part such as observations Dobs\nt\n= I.\nFinally, Dt may belong to the action (behavior trajectory) part, i.e. Dbhv\nt\n= {o0, a0, . . . }. We assume\nthese segments follow the ordering below (Figure 1):\nDinst\n0 , Dmem\n1\n|\n{z\n}\nContext\n, Dobs\n2 , Dtht\n3 , Dbhv\n4\n|\n{z\n}\nsub-task 1\n, Dobs\n5 , Dtht\n6 , Dbhv\n7\n|\n{z\n}\nsub-task 2\n, . . .\n(2)\nWe tokenize such a sequence of segments into a series of tokens {s0, . . . , sM} using the vision and\nlanguage tokenizer offered by a pretrained MLM and the behavior tokenizer introduced in Section 2.\n4\n3.2\nPreparing Multimodal Interaction Data\nIn reality, many segments of the multimodal interaction D can be missing in public datasets. We\nconsider the Minecraft contractor data released by OpenAI [2] and it only contains behavior trajec-\ntories Dbhv\nt\n. Therefore, we need to properly augment the data with the additional textual segments\nincluding instructions Dinst\nt , memory Dmem\nt\n, and thoughts Dtht\nt . We follow the prior practices [22, 31]\nto synthesize the required text using LLMs. Below, we detail how each type of segment is constructed.\nMore details can be found in appendix.\nSynthesis of instruction Dinst\nt\n. The instruction is a high-level description of what task is being\nperformed in the current interaction sequence. The considered OpenAI Minecraft data includes meta\ninformation of each gameplay video, which depicts fundamental events that happened during in\nMinecraft gameplay, e.g. what block was just destroyed, what entity was just killed, what item was\njust crafted, etc. Such meta-information can provide a basic overview of what the player has been\nthrough in the gameplay. We therefore prompt an LLM into summarizing the gameplay with the meta\ninformation. The summary will be used as the instruction Dinst\nt\nof the current trajectory.\nSynthesis of memory Dmem\nt\n. The memory is the summary of what agents have finished in the\nprevious interaction sequences. Due to the limited sequence length that the auto-regressive model\ncan handle, the model needs to learn to summarize key information related to the task in historical\ninteractions and ignore behaviors unrelated to instructions. The memory will be updated based on the\nresults of each episode trunk and used for subsequent episode trunks. We therefore prompt an LLM\ninto summarizing the gameplay with the meta information. The summary will then be used as the\nmemory Dmem\nt\nof the current interaction trajectory. The memory prompt can be found in Appendix F.\nSynthesis of thought Dtht\nt . The thought is the agent’s reasoning and explanation of its own decisions.\nPrevious methods have confirmed that using thought-enhanced interaction data helps language models\nunderstand decision-making [21]. Compared to labeling thoughts by humans [50, 6], we assume\nthat thought is an intermediate variable that can be determined by the actions taken and observations\nmade before and after the action, which is similar to an Inverse Dynamics Model [2]. We therefore\nprompt an LLM into estimating the thought of decisions with in-context learning, which will then be\nused as the thought Dtht\nt of the current behavior. Details can be found in Appendix E.\n3.3\nArchitecture, Training, and Inference of OmniJARVIS\nAs illustrated in Figure 3, OmniJARVIS is built upon a pretrained MLM. We augment the original\nvocabulary of the MLM with additional tokens from the behavior tokenizer. Specifically, as we\nadopted the [a, b, c] FSQ configuration (Section 2), we augment with a + b + c new tokens as each\nbehavior comprises n behavior tokens sbhv\n1\n, . . . , sbhv\nn\ncorresponding to n FSQ levels. We formulate\nthe learning objective of OmniJARVIS following [7, 37] in a prefix language modeling fashion. For\na batch B of token sequence s, we optimize OmniJARVIS via:\nL(θ, B) = −\n|B|\nX\nb=1\nT\nX\nt=1\nlog pθ(s(b,t)\nres |s(b,<t)\nres\n, s(b,1)\nprefix, ..., s(b,L)\nprefix ),\n(3)\nwhere sprefix denotes the prefix token, which is tokenized from the segments that served as context for\nreasoning and decision-making, i.e. instruction Dinst\nt , memory Dmem\nt\nand observation Dobs\nt\nwithin the\ninteraction sequence (Equation 2). The remaining tokens (tokenized from thought Dtht\nt and behavior\ntrajectory Dbhv\nt\n) will be predicted in an autoregressive fashion. From a high level, OmniJARVIS is\ntrained to reason (producing thought tokens) and act (producing behavior tokens) from contexts with\ntask instructions, memory, and current observations. During inference, we begin with the feeding\nOmniJARVIS with a task instruction, an empty memory, and an initial observation. OmniJARVIS\nwill produce a chain-of-thought as a means of reasoning and subsequently, emit behavior tokens for\ncontrol. Every N steps, it is forced to reason again to produce new behavior tokens with the latest\nobservation. We empirically set N = 32.\n4\nCapabilities and Analysis\n4.1\nOverview\nTraining details and Datasets. The training of the OmniJARVIS is divided into two stages. In\nthe first step, we use a self-supervised training method to train a Behavior Tokenizer, including\nthe Encoder and Decoder jointly. We use FSQ as a quantization method and build a codebook\n5\nwith 8*8*8*6*5 discrete codes. The training data for Behavior Tokenizer comes from Contractor\nDataset [2], which is a collection of Minecraft gameplay videos. The training parameters and details\nremain consistent with GROOT, which can be found in Appendix A.\nIn the second stage, we use this behavior tokenizer to process Minecraft offline trajectories to obtain\nbehavior token sequences. We add 35 (8+8+8+6+5) additional tokens to the MLM tokenizer as\nbehavior tokens for unified representation, so each time the VLA needs to output a continuous\nsequence of 5 tokens to represent a complete behavior. We use GPT-3.5 to synthesize thought,\nmemory, and instruction to raw offline datasets to build complete interaction data. The specific\nprompt can be found in Appendix E. These data collectively constitute the embodied instruction-\nfollowing dataset of OmniJARVIS, including 600k trajectories and about 900M tokens.\nThe training dataset of OmniJARVIS further includes a large amount of QA data about Minecraft.\nWe generate a large number of seed questions about these texts using web pages on the Minecraft\nwiki. Then, we use the self-instruct method to generate a large number of creative questions and\ninstructions. This constructed QA dataset consists of 300k conversations with about 90M tokens.\nDuring the training process, the QA data and instruction-following data are mixed, with a total of\nabout 1T tokens, to train OmniJARVIS. In specific, we SFT (supervised finetune) LLaVA-7B [31].\nThe details can be found in Appendix A. To further demonstrate the generalizability of the method,\nwe also fine-tune LLaVA at different scales and VLM Fuyu-8B with different architectures. The\nrelevant results are presented in Section 4.5 and Section 4.6.\nExperimental Setups. We conduct experiments in the complex and open-world environment\nof Minecraft, a voxel-based 3D video game that has garnered significant attention from real-life\nresearch due to its popularity and diverse mechanics [18, 16]. We first evaluate OmniJARVIS with\natomic tasks, which are skill-level tasks, testing VLAs’ ability to follow simple and straightforward\ninstructions. Then we evaluate OmniJARVIS with programmatic tasks, which require the agent\nto obtain an item starting from an empty inventory. The success of these tasks requires VLAs to\ndecompose the provided instruction into atomic-level subtasks, and hence tests VLAs’ complex\nreasoning ability. Finally, we test OmniJARVIS with open-world embodied question-answering\nbenchmarks and creative free-form instruction-following. We also conduct ablation experiments of\nOmniJARVIS with different behavior tokenizers, different training dataset formats, and different\nvision tokenizations. Finally, we explore the generalization abilities of OmniJARVIS of Atari Games\nand the scaling potential of OmniJARVIS with different models and data scales.\n4.2\nMain Results I: Short-horizon Atomic Tasks\nAtom tasks are various simple skills that agents in Minecraft need to master. They are basic tasks\nyet are fundamental skills that agents need to master during the learning process. We first evaluate\nOmniJARVIS with our learned behavior tokenizer on these tasks.\nWe select “chopping trees”\n, “digging dirt”\n, “mining stones”\n, and “collecting wheat\nseeds”\nas the evaluation tasks. We directly take those short task descriptions as instructions\nfor agents. We use text-conditioned VPT [2], Open-world Control [9], STEVE-I [28], and video-\ninstructed GROOT [10] as baselines. We compute the average rewards of different agents on\nevery task in Table 1 across 10 runs. By observing the environment and adjusting action tokens\ndynamically, OmniJARVIS effectively follows straightforward instructions across various scenarios.\nIt consistently achieves a high average reward with minimal standard deviation.\n4.3\nMain Results II: Long-horizon Programmatic Tasks\nTo further verify the ability of OmniJARVIS to complete tasks with long sequences, we use 30\nprogrammatic tasks to evaluate the performance of different agents. These tasks require the agent to\nstart from an empty inventory in a new world until obtaining the final required items, which is usually\na chain of atom tasks. These tasks are divided into five groups based on difficulty: wooden, food,\nstone, iron, and diamond. For example, the prompt for task “Obtain a diamond pickaxe”\nis “Give\nyou nothing in the inventory, obtain a diamond pickaxe.” This task requires more game time and\nmore complex planning for up to 10 different intermediate items [2]. We list all programmatic tasks\nand its corresponding instructions in the Appendix C.1.\nBaselines are divided into two types: 1) directly outputs actions, namely the native behavior tokenizer,\nincluding STEVE-I [28] and GROOT [10]. 2) using pretrained LLM as a planner to output language\ngoals and connect the STEVE-I to execute these goals, including Zero-Shot Planner (GPT) [23],\n6\nTable 1: Evaluation results (rewards) on short-horizon\natom tasks.\nThe text-conditioned VPT [2] (“VPT\n(text)∗”) is from Appendix I of its paper.\nMethod\nCondition\n↑\n↑\n↑\n↑\nVPT∗[text] [2]\nLanguage\n2.6±0.3\n9.2±0.7\n-\n0.8±0.1\nSTEVE-I [28]\nLanguage\n11.0±3.0\n10.0±2.5\n3.2±1.6\n5.1±2.5\nGROOT [10]\nVideo\n14.3±4.7\n19.7±8.7\n19.0±11.3\n7.3±0.6\nOmniJARVIS\nLanguage\n10.8±5.2\n20.3±9.2\n25.8±2.9\n8.2±3.6\nTable 2: Results on open-ended instruction following.\nVPT\nSTEVE-1\nVoyager\nDEPS\nOurs\nInstruction\nFollowing ↓\n975.9\n972.7\n932.1\n929.5\n886.2\nTable 3: Results on open-ended question answering.\nWe use LLM-as-judge [35] to evaluate the accuracy.\nVicuna-7B\nVicuna-13B\nLLaMA2-70B\nGPT-3.5\nOurs\nQA ↑\n2.34\n2.85\n2.50\n7.50\n8.40\nTable 4: Success rate of different agents on long-horizon programmatic tasks.\nMethod\nAction Tokenizer\nWooden (10)\nFood (5)\nStone (5)\nIron (5)\nDiamond (5)\nAverage\nSTEVE-I [28]\nNative\n0.04±0.07\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.01\nGROOT [10]\nNative\n0.05±0.08\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.02\nGPT [23]\nLanguage\n0.26±0.14\n0.08±0.04\n0.24±0.05\n0.04±0.05\n0.00±0.00\n0.15\nReAct [50]\nLanguage\n0.44±0.11\n0.12±0.04\n0.30±0.10\n0.06±0.05\n0.00±0.00\n0.23\nDEPS [46]\nLanguage\n0.78±0.11\n0.12±0.04\n0.68±0.08\n0.16±0.05\n0.04±0.05\n0.43\nOmniJARVIS\nFSQ GROOT\n0.95±0.07\n0.44±0.05\n0.82±0.08\n0.32±0.11\n0.08±0.04\n0.59\nReAct [50], and DEPS [46]. We use success rate to evaluate the completion of tasks, that is, whether\nthe task is completed within the specified time. The experimental results are listed in Table 6.\nProgrammatic Tasks usually require complex reasoning for planning. While STEVE-I and GROOT\ncan only finish short skill-level tasks in atom tasks, is difficult to finish these programmatic tasks.\nAgents based on Language behavior tokenizer can complete complex tasks including diamond group\nones, but with a low success rate. This is because these in-context learning methods leverage the\npretrained LLM which may lack the necessary knowledge about this world. It is worth noting that in\nthe Food group, agents based on Language Tokenizer have an average success rate of around 10%,\nas this set of tasks does not require complex reasoning. This indicates that Language-conditioned\nTokenizers need additional language-conditioned trajectories as supervision for training while there\nwas less such data available during STEVE-I’s training phase leading to significant performance gaps.\nMeanwhile, OmniJARVIS uses a self-supervised trained behavior tokenizer which does not require\nextra language labels and hence receives more training resulting in good performance across a wider\nrange of tasks. We will further prove this in the next set of Creative Task experiments.\n4.4\nMain Results III: Open-ended Question-Answering and Instruction Following Tasks\nThe open-ended tasks differ from programmatic tasks due to the lack of straightforward success\ncriteria [16]. We select the long-term open-ended tasks which usually need at least 5 minutes of\nhuman-playing time to finish. The task prompts can be found in Appendix C.3. Following image\ngeneration and video generation tasks [19, 41], we take the Fréchet Sequence Distance (FSD) metrics\nto evaluate the correlation between agent rollout video and creative instruction. Specifically, we first\nask human experts to finish the creative task prompts under randomly generated worlds and record\nthe game-playing videos Vhuman. Then, we provided the task prompts for different Minecraft agents,\nand obtained a rollout video set Vagent. Similar to FID [19], we used MineCLIP [16] to calculate the\nembedding of video clips and computed FSD for the embedding distributions of human and agent\nrollout videos. The analysis of the metrics can be found in Appendix B.\nWe further conduct open-ended embodied question-answering benchmarks to evaluate the ability of\nthe agent to complete open-ended instructions and grasp world knowledge. The questions answering\ninstructions set can be found in Appendix C.2. The evaluation results can be found in Table 3\nand Table 2. OmniJARVIS is the agent that can simultaneously complete both types of tasks\nand has achieved the best performance in different task sets, surpassing strong baselines including\nVoyager [44] and DEPS [46]. Also, it maintains strong reasoning capability, especially on embodied\nquestion answering compared to LLM baselines (with image captions as visual context).\n4.5\nInsights and Analysis\nInteractive Dataset Format. We explore the crucial roles played by the different type of segments\nin interaction data, including the instruction, memory, thought, and caption tokens. The results can\nbe found in Table 4, where we evaluate the loss on predicting the behavior tokens. It can be seen that\ninstruction and thought can be more critical to the successful prediction of behavior tokens. This is\nconsistent with our hypothesis – making informed decisions requires task instruction and reasoning.\n7\nFigure 4: Ablation experiments on OmniJARVIS with different behav-\nior tokenizers, vision tokenizers, and training on different interactive\ndatasets. The first line is training on the unconditional interactive dataset,\ni.e., without instructions on the trajectories. OmniJARVIS with VQ-\nGROOT [42, 10] shows no results because of training collapse.\nDataset Format\nLoss\nBehavior\nTokenizer\nVision\nTokenizer\nInstruction\nCaption\nThought\nMemory\nTrain\nEval\n✗(unconditional)\n0.33\n0.67\n✓\n✗\n✗\n✗\n0.46\n0.51\n✓\n✓\n✗\n✗\n0.44\n0.48\n✓\n✓\n✓\n✗\n0.32\n0.33\nFSQ GROOT\nLLaVA\n✓\n✓\n✓\n✓\n0.16\n0.17\nFSQ GROOT\nCaptioner+\n✓\n✓\n✗\n✗\n0.49\n0.52\nFSQ GROOT\nFUYU\n✓\n✓\n✗\n✗\n0.42\n0.44\nGROOT\nLLaVA\n✓\n✓\n✓\n✓\n0.44\n0.48\nVQ GROOT\nLLaVA\n✓\n✓\n✓\n✓\n-\n-\nFigure 5:\nScaling potential\nof OmniJARVIS. Its evaluation\nloss continues to drop with the\ngrowth of data and model parame-\nters. The Pearson coefficients for\nthe 2B, 7B, and 13B models are\n0.9991, 0.9999, and 0.9989.\nTable 5: Ablation experiments on behavior tokenizer with different code vocabulary size.\nCodebook\nsize\nFSQ\nLevels\nTraining\nIterations\nTrain\nLoss\nEval\nLoss\nReconstruction\nFSD ↓\nSampling\nFSD ↓\nAverage\nRewards ↑\nCodebook\nUsage\ne8\n[8,6,5]\n180k\n2.746\n3.161\n46.57\n68.90\n0.63±0.67\n93.75%\ne10\n[8,5,5,5]\n180k\n3.011\n3.148\n43.67\n61.85\n0.54±1.21\n97.65%\ne14\n[8,8,8,6,5]\n240k\n3.092\n3.116\n42.72\n57.37\n2.27±2.45\n92.36%\nVision Tokenization. We also evaluate training OmniJARVIS with different vision tokenization,\nincluding ImageCaptioner + LLaMA2-7B [12, 40] (basically converting the vision input into textual\ncaptions), fuyu-8b [3], and LLaVA-7B [31] architecture. For the ImageCaptioner+, we fix the\nImageCaptioner models and only fine-tune the language model, i.e., LLaMA2-7B. We use the\nprediction loss of behavior tokens as the evaluation criterion, namely eval loss. We found that the\nmodel trained with LLaVA-7B architecture has the lowest evaluation loss, so we chose this model as\nthe default model.\nBehavior Tokenizer. We explore OmniJARVIS with different behavior tokenizers, including the\ndefault setting using FSQ codebook, a variant of using VQ-VAE instead of FSQ [42], and simply\nusing sub-goal language annotation as behavior “tokens”. The evaluation results on 4 programmatic\ntasks are listed in Table 4. Using an FSQ tokenizer is generally better than a language goal, which\nconfirms the advantages of using a tokenized behavior over language descriptions of behavior. The\nuse of VQ-VAE as a quantized behavior tokenizer collapsed during the training process, so there\nwere no results in all test tasks.\nBehavior Codebook. We conduct an in-depth investigation of behavior tokenizers with varying\ncodebook sizes, utilizing recommended sets of FSQ levels to approximate specified codebook\ndimensions [34] as delineated in Table 5. We evaluate performance across multiple metrics for\neach codebook size. Codebook Usage is quantified as the proportion of codewords utilized at\nleast once when encoding the validation datasets. Reconstruction FSD is measured by the FSD\nscores derived from the MineCLIP encoder [16], processing 1,000 different demonstration videos\nthrough the FSQ-GROOT and subsequent rollout in a randomly generated environment. Additionally,\nwe measure Resampling FSD, which is the FSD score obtained when the environment rollout is\nconditioned on representations sampled from the codebook. Finally, we assess the average rewards\nfor the task “collect wood” using OmniJARVIS across varying codebook sizes. Our findings indicate\nthat increases in codebook size correlate with enhanced average rewards and reduced FSD scores,\nsuggesting a scalable performance in OmniJARVIS with larger codebooks.\nBehavior Semantics. We provide some qualitative analysis on the learned FSQ-based behavior\ntokenizer. In Figure 6, we tokenize several reference videos, then feed the behavior tokens to the\npolicy decoder and see if it can accomplish the same task as in reference videos. The results indicate\nthat our behavior tokenizer is able to capture such behavior semantics and offers rich task information.\n4.6\nGeneralization and Scaling Potential of OmniJARVIS\nWe first explore adapting OmniJARVIS to the Atari game Montezuma’s Revenge. We created\na dataset from 500 episodes played by an agent trained with Random Network Distillation [8],\n8\nProvided\nGenerated\nFigure 6: Examples of behavior tokenization-detokeinzation. Left: the reference video to be tokenized by\nour FSQ-based behavior tokenizer (encoder). Right: the behavior of the policy decoder is conditioned on the\nbehavior tokens. The policy decoder can reproduce the task being accomplished in the reference video.\nFigure 7: OmniJARVIS plays Montezuma’s Revenge and gets a reward of 3600.\nsupplemented by random actions in early frames to enhance diversity. This dataset contains 1,823,699\ntransitions. We then trained the FSQ-GROOT tokenizer on this new dataset and subsequently trained\nOmniJARVIS on the tokenized data. The finetuned OmniJARVIS achieved a score of 3600 in\nMontezuma’s Revenge, indicating promising transferability. A rollout trajectory is in Figure 7.\nWe also investigate the scaling effect [25, 30] of data and model in OmniJARVIS by monitoring the\ninstruction-following loss on the validation set as the amount of data increases. In addition to fine-\ntuning from the default LLaVA-7B, we include two additional scales: OmniJARVIS-2B (fine-tuned\nfrom LLaVA-2B with Gemma-2B language models [20]) and OmniJARVIS-13B (fine-tuned from\nLLaVA-13B with LLaMA2-13B language models [31]).\nThe validation loss curves in Figure 5 reveal the following insights: 1) When using Omni-Tokenizer,\nOmniJARVIS’s instruction tuning aligns with the scaling law [25]. All curves exhibit a log-linear\ndecrease as the data scale increases. 2) Scaling up VLM consistently enhances performance. Notably,\nOmniJARVIS-7B demonstrates significantly lower losses compared to OmniJARVIS-2B. However,\nwhile improvements are consistent, the difference between OmniJARVIS-7B and OmniJARVIS-\n13B seems less pronounced, hinting at potential saturation when further scaling up VLM. This\nunderscores both the scalability of OmniJARVIS and the importance of increasing data volume to\nmatch the model.\n5\nRelated Works\nPretrained Language Models for Decision-making. Several works have explored leveraging LLMs\nto generate action plans for high-level tasks in embodied environments [23, 27, 5, 52]. To better\nperform complex planning in the environment, existing methods usually utilize chain-of-thought [49]\nor related methods [50]. To better cope with uncertainties in open worlds, some LLM-based methods\ngenerate plans interactively with human and environmental feedback [39, 46, 24] and retrieving\nfrom memory [47] or internet corpus [48]. However, those plans can only be executed in a language\nenvironment or require an additional controller or code executor to interact in an open world.\nVision-Language-Action Models. In order to better utilize the knowledge inside the language model\nfor decision-making, some methods tend to use decision datasets to fine-tune pretrained language\nmodels [15, 14]. Gato [38] was among the first to tokenize environment-provided actions to enable\njoint sequential modeling across modalities. PaLM-E [14] generates high-level instructions as texts\nand uses dedicated controllers to perform the task described by the output instructions. The RT\nseries focuses more on robotics settings. Specifically, RT-1 pairs a VLM with a language-conditioned\ncontroller; RT-2 extends the VLM to directly include control tokens; RT-X generalizes to new robots\nand environments. A recent VLA model LEO [22] expands the perception from 2D images to 3D\nworld and enables rich scene-level reasoning and control tasks.\n9\nFigure 8: Comparative Framework of Vision-Language Action Models. (a) depicts a model where upon\nreceiving a language instruction, actions are directly output based on the environmental state, facilitating\nimmediate interaction with the environment at a unified frequency. Smaller models with <1B parameters like\nVPT [2] maintain higher frequencies (>20Hz), though their capability for complex reasoning tasks is limited.\nLarger models with >7B parameters such as RT-2 [6], offer enhanced performance but operate at significantly\nreduced frequencies (2-3Hz). (b) illustrates a common approach utilizing large vision-language models for\nplanning, subsequently outputting language goals [46, 14, 4]. A language-conditioned policy then translates\nthese language goals into actions at a real-time interaction rate of 20Hz, with high-level models re-planning\nat less than 1Hz. This hierarchical structure balances interaction frequency and performance, while it requires\nlanguage as an intermediary and additional language labels. The training process of high-level vision-language\nmodels and language-conditioned policies are separate, thus performing poorly on tasks that can not be easily\nconnected by language. (c) (ours) mirrors the hierarchical structure of (b) but differentiates by employing a\nself-supervised encoder-decoder policy [10] and FSQ quantization [34] as a behavior tokenizer. The upper-level\nvision-language models produce self-supervised behavior tokens, which are then conditioned by a policy decoder\nto output actions, facilitating environment interaction. The behavior tokens are injected into the training corpus\nof vision-language-action models, which enables end-to-end inference. This approach also eliminates the need\nfor external language supervision and scales efficiently.\nOpen-world Agents in Minecraft. As LLMs have achieved remarkable reasoning results and\nunderstanding capabilities across various domains, the year 2023 has witnessed researchers adopting\nmultiple LLM-based approaches to create open-world agents in Minecraft [46, 55, 47, 44]. Some\nmethods focus on building policies for low-level skills [10, 28, 2]. Building upon the low-level\npolicies to interact with the Minecraft environment, Wang et al. [46], Yuan et al. [51] and Wang et al.\n[47] focus on leveraging the pre-trained language models as planners to finish programmatic tasks\nwith in-context learning. Wang et al. [44] adopts the life-long learning scheme and generates code\nas policies to enable continual exploration. Some use expert trajectories and Minecraft corpus to\nfine-tune pre-trained vision language models for better embodied planning [36, 54].\n6\nConclusion\nWe’ve presented OmniJARVIS, a novel VLA model that encompasses strong reasoning and ef-\nficient decision-making capabilities via unified tokenization of vision, language, and actions in\nmultimodal interaction data. The key ideas are learning behavior tokenizer (trajectory encoder)\nand de-tokenizer (IL policy decoder) using self-supervised learning on behavior trajectories and\nautoregressive modeling of tokenized multimodal interaction data using a pretrained multimodal lan-\nguage model (MLM). Evaluations on the open-world Minecraft Universe demonstrate its impressive\ninstruction-following capabilities. Possible future directions include a more in-depth investigation of\nbehavior tokenization, language capabilities after VLA fine-tuning, and alignment concerns emerging\nfrom the unified interaction modeling and VLA capabilities.\nAcknowledgments\nThis work is funded in part by the National Key R&D Program of China #2022ZD0160301. We\nthank a grant from CCF-Tencent Rhino-Bird Open Research Fund. One author is funded in part by\nNSF grants #IIS-1943641, #IIS-1956441, #CCF-1837129, an SRA from Meta and a research gift\nfrom Amazon Alexa AI, and a gift from RelationalAI.\n10\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | OmniJARVIS：开启开放世界指令跟随代理的新篇章\n\n## 📌 背景痛点\/本文动机\n随着预训练大型语言模型（LLMs）和多模态语言模型（MLMs）的成功，构建能够理解和执行指令的自主代理成为人工智能领域的重要目标。然而，现有的视觉-语言-行动（VLA）模型在开放世界环境中面临着两大挑战：复杂且高度依赖上下文的任务，以及需要长期规划的任务。现有的VLA模型要么依赖于文本指令与控制器进行通信，要么直接输出控制命令，这两种方法都存在局限性。\n\n## 🚀 核心方法\nOmniJARVIS提出了一种新的VLA模型，通过统一的多模态交互数据标记化方法，实现了强大的推理能力和高效的决策能力。其核心创新点包括：\n\n💡 行为标记化：OmniJARVIS引入了一种自监督学习方法，学习一个行为编码器，将行为轨迹转换为离散的标记，并使用模仿学习策略解码器根据这些标记生成控制命令。\n\n💡 自回归建模：通过将行为标记添加到预训练的多模态语言模型的词汇表中，OmniJARVIS将多模态交互数据打包成统一的标记序列，并使用自回归Transformer模型进行建模。\n\n## 📈 实验结果\nOmniJARVIS在开放世界的Minecraft环境中进行了广泛的评估，包括原子任务、程序性任务和开放性任务。结果表明，OmniJARVIS在各种任务中都表现出色，能够有效地进行推理、规划和行动。\n\n## 💬 可借鉴之处\nOmniJARVIS的设计为构建开放世界指令跟随代理提供了新的思路。其行为标记化和自回归建模方法可以有效地处理多模态交互数据，并实现强大的推理和决策能力。此外，OmniJARVIS的可扩展性也为未来的研究提供了广阔的空间。\n\n## 🌟 总结\nOmniJARVIS是一种创新的VLA模型，通过统一的多模态交互数据标记化方法，实现了强大的推理能力和高效的决策能力。它在开放世界的Minecraft环境中表现出色，为构建能够理解和执行指令的自主代理提供了新的可能性。","llm_summary_res_status":200}
{"title":"MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory","authors":"Junyeong Park, Junmo Cho, Sungjin Ahn","summary":"Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMrSteve (Memory Recall Steve-1), a novel low-level controller equipped with\nPlace Event Memory (PEM), a form of episodic memory that captures what, where,\nand when information from episodes. This directly addresses the main limitation\nof the popular low-level controller, Steve-1. Unlike previous models that rely\non short-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https:\/\/sites.google.com\/view\/mr-steve.","url":"http:\/\/arxiv.org\/abs\/2411.06736v4","pdf_url":"http:\/\/arxiv.org\/pdf\/2411.06736v4","published":1731305093000,"comment":null,"pdf_text":"Preprint\nMRSTEVE: INSTRUCTION-FOLLOWING AGENTS\nIN MINECRAFT WITH WHAT-WHERE-WHEN MEMORY\nJunyeong Park1∗, Junmo Cho1∗, Sungjin Ahn1\n1KAIST\nABSTRACT\nSignificant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented hierar-\nchical approaches. While these approaches, which combine high-level planners\nwith low-level controllers, show promise, low-level controllers frequently become\nperformance bottlenecks due to repeated failures. In this paper, we argue that the\nprimary cause of failure in many low-level controllers is the absence of an episodic\nmemory system. To address this, we introduce MrSteve (Memory Recall Steve-1),\na novel low-level controller equipped with Place Event Memory (PEM), a form of\nepisodic memory that captures what, where, and when information from episodes.\nThis directly addresses the main limitation of the popular low-level controller,\nSteve-1. Unlike previous models that rely on short-term memory, PEM organizes\nspatial and event-based data, enabling efficient recall and navigation in long-horizon\ntasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented\nTask Solving Framework, allowing agents to alternate between exploration and\ntask-solving based on recalled events. Our approach significantly improves task-\nsolving and exploration efficiency compared to existing methods. We will release\nour code and demos on the project page: https:\/\/sites.google.com\/view\/mr-steve.\n1\nINTRODUCTION\nThe emergence of large-scale foundation models has driven significant advances in developing\ngeneral-purpose embodied AI agents capable of generalizing across a broad spectrum of tasks in\ncomplex, open, and real-world-like environments (Johnson et al., 2016; Guss et al., 2019; Fan et al.,\n2022b; Hafner; Albrecht et al., 2022; Voudouris et al., 2023). While simulating such environments for\neffective learning and evaluation remains a major challenge, Minecraft has become a leading testbed,\noffering a demanding, open-ended environment with rich interaction possibilities. Its procedurally\ngenerated world presents agents with challenges like exploration, resource management, tool crafting,\nand survival, all requiring advanced decision-making and long-horizon planning. For instance, the\ntask of obtaining a diamond\nrequires agents to locate diamond ore\n, and craft an iron pickaxe\n. This process involves finding, mining, and refining iron ore\n, requiring the agent to execute\ndetailed long-term planning over roughly 24,000 environmental steps (Li et al., 2024).\nSolving such tasks through Reinforcement Learning (RL) approaches from scratch is nearly infeasible;\nhowever, recent LLM-augmented hierarchical methods have demonstrated a promising avenue (Huang\net al., 2022a;b; Wang et al., 2023a). These methods feature a division between high-level planners\nand low-level controller policies. High-level planners, driven by Large Language Models (LLMs) or\nMultimodal Large Language Models (MLLMs), propose subgoals by utilizing the reasoning abilities\nand prior knowledge inherent in LLMs (Brown et al., 2020; Touvron et al., 2023; OpenAI, 2024).\nThese subgoals, conveyed in textual instruction form, are then sequentially passed to a learned,\ninstruction-following low-level controller for execution (Wang et al., 2023c;b; Li et al., 2024).\nFor this framework to be effective, it is essential that both the high-level planner and the low-level\ncontroller improve in tandem. However, previous research has primarily focused on enhancing\nhigh-level planning, e.g., via maintaining skill library (Zhu et al., 2023; Wang et al., 2023b; Qin et al.,\n∗Equal contribution. Correspondence to Junyeong Park and Sungjin Ahn.\nContact:{jyp10987,sungjin.ahn}@kaist.ac.kr\n1\narXiv:2411.06736v4  [cs.LG]  25 Dec 2024\nPreprint\n2024; Li et al., 2024), often assuming that low-level controllers will efficiently execute the subgoals\nprovided by the high-level planner. However, this assumption frequently does not hold in practice,\nand the low-level controller becomes a significant performance bottleneck (Cai et al., 2023b).\nIn this regard, we specifically focus on limitations in Steve-1 (Lifshitz et al., 2024), the most widely\nused low-level instruction-following controller framework. Steve-1 is an instruction-following policy\nobtained by fine-tuning the Video Pre-Training (VPT) (Baker et al., 2022) model. A primary limitation\nwe focus on is its constrained episodic memory capability. Steve-1 is based on Transformer-XL (Dai\net al., 2019), which leverages relatively short-term memory, retaining only the last 128 hidden states.\nGiven Minecraft’s simulation speed of 20Hz, this memory span amounts to only a few seconds of\ngameplay. While it can be increased, the quadratic complexity and FIFO-only memory structure of\ntransformers make them significantly inefficient for long-horizon tasks.\nAs a result, when the agent requires information beyond this short memory span, it tends to forget past\nevents within the episode and reverts to inefficient random exploration for each new task, consuming\nexcessive time. For example, when given a task like “Find a Cow”, the agent is unable to recall,\n‘I’ve seen it before near the river in the north’. Ideally, a low-level agent would instead maintain an\nepisodic memory of meaningful events and recall relevant information. See Figure 1 for more detail\nillustration. Moreover, Steve-1 not only lacks the ability to recall such memories but also the ability to\nnavigate directly to the associated locations, which could help avoid unnecessary exploration. Instead,\nSteve-1 relies on a “Go Explore” instruction, randomly exploring until it stumbles upon the resource\nby chance. This inefficiency in executing low-level primitives is not addressed by high-level planners,\nwhich focus on optimizing the sequence of high-level skills (i.e., the plan) but do not optimize the\nexecution of the primitives themselves. We elaborate more on this in Appendix A.\nIn this paper, we introduce an enhanced low-level controller agent, MrSteve (Memory Recall Steve-1),\ndesigned to address the limitations of Steve-1. The key innovation of MrSteve is the integration of\nPlace Event Memory (PEM) which is the instantiation of What-Where-When Episodic Memory.\nWhile previous approaches have explored episodic memory, they primarily target high-level planners,\nsuch as building libraries of high-level skills and plans, which do not directly improve the low-level\ncontroller’s performance. We argue it is essential for the low-level controller to possess memory\ncapabilities. To address this, PEM manages memory more effectively, surpassing the limitations of the\nnon-scalable FIFO memory found in transformers. PEM stores spatial and event-based information,\nallowing the agent to hierarchically organize and retrieve details about locations and events it has\npreviously encountered. For PEM to be fully effective, the agent must also move directly to the\ndesired location along with the ability to modulate between exploration and goal-directed navigation-\nand-execution, a capability lacking in Steve-1. Therefore, we introduce the second component of\nMrSteve: the Exploration Strategy and Memory-Augmented Task Solving Framework. Built upon\nthe PEM structure, this framework enables the agent to alternate between exploration—when no\nrelevant information is stored—and task-solving by recalling past events when applicable. This is\nmade possible and effective with our new navigation policy, VPT-Nav.\nOur contributions are as follows. First, we point out the limitations of Steve-1, the most widely\nused instruction-following controller, and show how its bottlenecks can be addressed with MrSteve.\nSecond, we introduce Place Event Memory (PEM), a novel hierarchical memory system that orga-\nnizes spatial and event-based data for efficient querying and storage, even under limited memory\ncapacity. Third, we propose an Exploration Strategy and Task Solving Module built on PEM that\nenables efficient exploration while maintaining high task-solving performance in Minecraft. Last, we\ndemonstrate that our agent significantly outperforms existing baselines in both exploration and long\nsequence of tasks solving. We will release the code for further research.\n2\nRELATED WORKS\nLow-Level Controllers in Minecraft Earlier works (Guss et al., 2019; Lin et al., 2021; Mao\net al., 2022; Cai et al., 2023a; Hafner et al., 2023; Zhou et al., 2024a) introduced policy models\nfor simple tasks in Minecraft. MineCLIP (Fan et al., 2022b) leveraged text-video data to train a\ncontrastive video-language model as a reward model, while VPT (Baker et al., 2022) was pre-trained\non unlabelled videos without text-based instruction input. Steve-1 (Lifshitz et al., 2024) extended\nVPT by incorporating text instructions to generate low-level actions based on human demonstration\ndata. GROOT (Cai et al., 2023b) used reference video instead of text for goal-conditioned behavior\n2\nPreprint\nFigure 1: Sparse Sequential Task Solving Scenario. The first task is to obtain a log. The agent explores to find a\ntree. While searching, the agent observes a cow but continues focusing on acquiring the log. Once the log is\nobtained, the next task is to obtain a water bucket. Remembering that it already explored the forward direction\nwhile searching for the tree, the agent chooses to explore to the right. After gathering the water bucket, the\nfinal task is obtain meat, which can be acquired from the cow. Recalling the cow’s location, the agent navigates\nthere and completes the task by obtaining the meat. Note that each task takes a few thousand steps to achieve.\nThis scenario highlights the significance of episodic memory for efficient exploration and task-solving in an\nopen-ended world where task-relevant resources are sparsely distributed.\ncloning. Recently, MineDreamer (Zhou et al., 2024b) leveraged Steve-1 generating subgoal images\nwith MLLM and Diffusion based on text and current observation for improved control. However,\nthese agents lack episodic memory, forcing agents to start new tasks from scratch. MrSteve addresses\nthis by integrating episodic memory, making it more effective in sequential tasks.\nLLM-Augmented Agents The development of LLMs has significantly advanced agents in Minecraft\n(Wang et al., 2023a;b). These works utilize pre-trained LLMs as zero-shot planners (Brown et al.,\n2020; Touvron et al., 2023), leveraging their powerful reasoning capabilities to generate subgoal plans\nor executable code. Broadly, this line of research can be divided into two approaches: one that uses\nLLMs for code generation to interact with the environment directly (Wang et al., 2023a; Zhu et al.,\n2023; Qin et al., 2024; Liu et al., 2024), and another that generates text-based subgoals which are\nthen executed by a goal-conditioned low-level controller, such as Steve-1 or programmed heuristics\n(Nottingham et al., 2023; Yuan et al., 2023; Li et al., 2024). In the latter approach, to ensure LLMs\nfocus on high-level semantic reasoning, the low-level controller must efficiently execute subgoals.\nWhile combining LLM as a high-level planner with MrSteve is one possible direction, we focus on\nenhancing low-level controller’s capabilities based on the new type of memory in this work.\nMemory in Agents Memory systems in agents primarily aim to retrieve robust and accurate high-\nlevel plans for long-horizon tasks (Zhang et al., 2023; Song et al., 2023; Kagaya et al., 2024; Sun\net al., 2024; Shinn et al., 2024). Existing works store successful task’s text instruction and its plans in\nlanguage often with observations for robust retrieval, which is useful when plans for the new task\nalready exist in memory. Voyager (Wang et al., 2023a) uses an unimodal storage of achieved skill\ncodes in the form of text. GITM (Zhu et al., 2023) integrates text-based knowledge and memory\nfor higher reasoning efficiency and stores entire skill codes after a goal is achieved. Recently, MP5\n(Qin et al., 2024) and JARVIS-1 (Wang et al., 2023b) enhance planning by storing plans and whole\nmultimodal observations in the abstracted memory, allowing for situation-aware retrieval, while\nOptimus-1 (Li et al., 2024) introduces a multimodal experience pool that summarizes all multimodal\ninformation during agent’s execution of the task improving storage and retrieval efficiency. However,\nthese memory systems store the sequence of high-level skills or plans for high-level planners, which\nare not optimized for low-level controllers. We address this problem with Place Event Memory.\n3\nMETHOD\nIn this section, we describe our agent, MrSteve (Memory Recall Steve-1). We begin with the problem\nsetting, followed by step-by-step construction of our agent’s main modules.\nProblem Setting In this work, we define a sparse sequential task scenario where the agent is\ncontinuously given tasks {τn}∞\nn=1 through text instructions (e.g., Obtain water bucket) from the\n3\nPreprint\n(b) Place Event Memory\nE1\nE2\nE1\nE1\n⋯\no12, p12\nT\ntAmjAqSMNQw0g7UgTxgJFWMLpJ\/dYjUZpKcW\/GEfE5GgaUoyMlR5kLzl3J6cwyv5eqexW3AxwkXg5KYMc9V7p9uXOZEGMyQ1h3PjYyfIGUoZmRS7MaRAiP0IB0LBWIE+0n2dYTeGyVPgylsk8YmKl\/OxLEtR7zwFZyZIZ63kvF\/7xObMIrP6Eig0ReDojBk0EqYRwD5VBs2tgRhRe2uEA+RQtjYoGamBHzmhmQkDXlKY\/LmQ1kzbOKV61U7y7Ktes8sAI4BEfgBHjgEtTALaiDBsBAgWfwCt6cF+fd+XA+p6VLTt5zAGbgfP0CRLCgWg=<\/latexit>o30, p30\nk\n6ANGFUkKahpFOpAjiASPtYHyT+u1HojSV4t5MIuJzNBQ0pBgZKz3IflJ1p+cwyv5+qexW3AxwmXg5KYMcjX7pzeQOZEGMyQ1l3PjYyfIGUoZmRa7MWaRAiP0ZB0LRWIE+0n2dZTeGqVAQylsk8YmKl\/OxLEtZ7wFZyZEZ60UvF\/7xubMIrP6Eig0ReDYojBk0EqYRwAFVBs2sQRhRe2uEI+QtjYoOamBHzuhmQsDXlKY\/IWQ1kmrYuKV6vU7qrl+nUeWAEcgxNwBjxwCergFjRAE2CgwDN4BW\/Oi\/PufDifs9IVJ+85AnNwvn4BSAigXA=<\/latexit>o40, p40\nPlace Cluster 1, Event Cluster 1\n⋯\no48, p48\no120, p120\no132, p132\nPlace Cluster 1, Event Cluster 2\n⋯\no287, p287\no295, p295\no300, p300\nPlace Cluster 3, Event Cluster 1\nPlace Cluster\nE\nEvent Cluster\nAgent’s Trajectory\n(a) Agent Overview\nText\nInstruction ⌧n\not\nlt\nEnvironment\nSolver Module\nMemory Module\nPlace Event \nMemory\nExplore\nExecute\nMode \nSelector\nMrSteve\nat\nCount-Based ⇡H-cnt\nVPT-Nav ⇡L-Nav\nVPT-Nav ⇡L-Nav\nSteve-1\nU\nSIJbZGIR7LrY0U5E7QFDjtxpLi0Oe04+v83rngUrFInEPk5i6IR4KFjCQUeWenHzEv7QJ8gvRUKswzq3bNnspaNk5hqhQ0zN\/+oOIJCEVQDhWqufYMbgplsAIp1m5nygaYzLGQ9rTVuCQKjednp5ZJzoZWEk9RNgTdO\/EykOlZqEvu4MYzUYi0P\/6v1Egu3ZSJOAEqyGxRkHALIivnYA2YpAT4RBtMJNO3WmSEJSagac1t8cO5P6TjSNPKMTmLUJZN+6zm1Gv1u\/Nq46oAVkJH6BidIgdoAa6QU3UQgQ9omf0it6MF+Pd+DA+Z60rRjFziOZkfP0CpIyjTQ=<\/latexit>⇡Inst\nFigure 2: MrSteve and Place Event Memory. (a) MrSteve takes agent’s position, first person view, and text\ninstruction, and utilizes Memory Module and Solver Module to follow the instruction. (b) MrSteve leverages\nPlace Event Memory for exploration and task execution, which stores the novel events from visited places.\nenvironment or subgoal plans by LLM. Additionally, we assume that task-relevant resources (e.g.,\nwater, cow) rarely exist and are sparsely distributed in the environment, making it essential to\nmemorize novel events from visited places for future tasks as shown in Figure 1. When an episode\nbegins, for every time step t, the agent is provided with the observation Xt = {it, lt, t}, which consists\nof the pixel observation it ∈RH×W ×C, representing the first person view of the environment, the\npositional information lt = (coordx, coordy, coordz, yaw, pitch) ∈R5, which denotes the agent’s\nrelative 3D position and camera angles with respect to initial position l0, and time t.\nInstruction Following Policy In sparse sequential task, a naive approach is to employ Steve-1\n(Lifshitz et al., 2024), an instruction-following policy πInst(at|ht, τn) that generates low-level controls\n(mouse and keyboard) in Minecraft. Here, ht is a past pixel observation sequence xt−128:t. While past\nobservations are processed by Transformer-XL layers in Steve-1, the model is ineffective at recalling\nobservations from a few thousand steps ago (Lampinen et al., 2021). Additionally, the Transformer’s\nquadratic complexity makes it significantly inefficient to process thousands of observations. This\nmakes Steve-1 poorly suited for sparse sequential task, as it cannot recall visited places or task-\nrelevant resources seen in the past. To address this, we propose MrSteve which stores novel events\nfrom visited places for efficient sparse sequential task-solving.\nMrSteve is a memory-augmented instruction following policy that consists of Memory Mod-\nule and Solver Module, as shown in Figure 2(a).\nIn Memory Module, we use the mem-\nory called Place Event Memory Mt that stores novel events from visited places (Figure 2(b)).\nAlgorithm 1 MrSteve Single Loop\nRequire: Memory Mt, and task τn\n1: candidates ←Read(Mt, τn)\n2: if candidates ̸= ∅then\n3:\nXt, lt = OneOf(candidates)\n4:\nNavigate to lt with πL-Nav\n5:\nExecute τn with πInst\n6: else\n7:\nExplore with πH-Cnt, πL-Nav\n8: end if\nBased on Place Event Memory, Mode Selector in Solver\nModule decides between Explore mode and Execute mode.\nWhen no task-relevant resource exists in the memory, Ex-\nplore mode is selected, and the agent explores with our\nhierarchical exploration method. If a task-relevant resource\nexists in the memory, Execute mode is selected, then the\nagent navigates to the resource’s position and executes πInst\n(i.e., Steve-1) to solve the task. Algorithm 1 outlines the\ntask-solving loop of MrSteve, which repeats every fixed\nstep or when a new task is given (More details in Appendix\nD). With these modules, MrSteve can efficiently explore\nand recall task-relevant resources from the memory to solve\nsparse sequential task in Figure 1. In the following sections, we describe how each module in MrSteve\nis constructed. We begin by constructing Memory Module.\n3.1\nMEMORY MODULE: CONSTRUCTION OF PLACE EVENT MEMORY\nThe simplest form of memory is FIFO Memory, denoted as Mt with capacity N. At every time step,\ninstead of storing Xt in Mt, we can extract a semantic representation from the video it−H:t with a\nvideo encoder to store an experience frame xt = {et, lt, t}, where et = Encv(it−H:t). For simplicity,\nwe term et as the video embedding at time step t. When the memory exceeds its capacity, the oldest\nframe is removed. For memory read, we calculate the cosine similarity between the task embedding\nˆτn = Enct(τn) and the video embedding et in Mt to retrieve task-relevant frames. Here, we use\n4\nPreprint\nFigure 3: Mode Selector and VPT-Nav in MrSteve. (a) Mode Selector with Place Episodic Memory. It decides\nagent’s mode (Explore or Execute) based on whether a task-relevant resource is in the memory. It uses a\nhierarchical read operation. (b) Architecture of Goal-Conditioned VPT Navigator.\nvideo encoder Encv, text encoder Enct, and H = 16 from MineCLIP (Fan et al., 2022a), which is a\nCLIP (Radford et al., 2021) trained on web videos of Minecraft gameplay and associated captions.\nWhile FIFO Memory offers benefits from its simple memory operations, it has two drawbacks. First,\nthe computational complexity of the read operation grows linearly with the memory size. Second, the\nbias toward removing the oldest frames can be problematic in sparse sequential task as in the Figure\n1 scenario, where task-relevant frames from visited places are lost.\nPlace Memory To address these issues, Place Memory (Cho et al., 2024) divides the agent’s positions\n{ℓn}t\nn=0 in the trajectory into clusters of distinct places, where each place cluster is assigned a FIFO\nMemory. Here, we term ℓt = (coordx, coordy, yaw) as agent’s position, which is a concatenation of\nagent’s top-down location and its head direction. Place Memory is represented as Mt = {Mk}K\nk=1,\nwhere Mk is the k-th place cluster with center position ℓMk, and center embedding eMk. Here, eMk\nis the video embedding whose position is closest to ℓMk. This structure improves the efficiency of\nthe read operation by extracting top-k place clusters with their center embeddings first, then fetching\nrelevant frames from these clusters. Furthermore, when memory capacity is limited, the oldest frame\nis removed from the largest place cluster, allowing the agent to retain memories in diverse places.\nWhile Place Memory prioritizes storing experience frames across diverse places, its FIFO structure\nwithin each cluster still loses novel experience frames in the past. For instance, if an agent stays in\na place where zombies burn and disappear for a long time, the place cluster removes the frames of\nburning zombies that can be crucial in upcoming tasks. This highlights the importance of focusing on\nvisually distinct experience frames rather than storing them sequentially, which can be redundant.\nPlace Event Memory To resolve this issue, we introduce Place Event Memory built on Place Memory,\nwhich captures distinct events that occur within each place cluster (Figure 2(b)). While Place Memory\nuses agent’s position to cluster experience frames, there is no criterion for clustering frames to form\nevents. To tackle this, we use the cosine similarity of video embeddings from MineCLIP for criterion.\nSpecifically, each place cluster Mk is subdivided into event clusters, denoted as {Ek\ni }dk\ni=1, where\neach Ek\ni represents the i-th event cluster in k-th place cluster, characterized by a center embedding\nek\nEi. These event clusters are newly created and updated as the place cluster accumulates a certain\nnumber of additional experience frames. For generating event clusters, DP Means algorithm (Dinari\n& Freifeld, 2022) is applied on video embeddings of these frames, generated by MineCLIP, and the\nresulting cluster centers become a center embedding of each cluster. If the cosine similarity of the\ncluster centers between a newly created cluster and an existing cluster is higher than threshold c (we\ndefine that two event clusters are indistinct), the two clusters are merged to prevent redundancy and\nensure distinct event clusters within each place cluster. When memory capacity is exceeded, the\noldest frame in the largest event cluster is removed, thus the memory can retain diverse places and\ndistinct events within each place. More details on Place Event Memory can be found in Appendix E.\n3.2\nSOLVER MODULE: MODE SELECTOR, EXPLORATION, AND NAVIGATION\nIn this section, we introduce the remaining components in Solver Module, which are Mode Selector,\nand hierarchical policies πH-Cnt, πL-Nav for episodic exploration, and goal-reaching navigation.\n5\nPreprint\nMode Selector Mode Selector decides between Explore and Execute mode by checking whether\ntask-relevant resource exists in the memory. If the resource exists, the agent chooses Execute mode,\nor Explore mode otherwise. When Place Event Memory is employed, Mode Selector first picks top-k\nevent clusters with task alignment score sik(τn) = CLIPt(τn)·CLIPv(ek\nEi) between task embedding,\nand center embedding of event cluster. Then, it calculates task alignment scores on experience frames\nin top-k event clusters and gathers frames with scores higher than task threshold h as shown in Figure\n3(a). We note that leveraging Place Event Memory offers computational efficiency with hierarchical\nread operation compared to FIFO Memory, which calculates the alignment scores on whole frames in\nthe memory. We provide a comparison of memory query time in Appendix K for further insights.\nHierarchical Episodic Exploration We propose a memory-based hierarchical exploration method\nthat allows the agent to efficiently explore the environment while minimizing revisits to previously ex-\nplored positions. This is achieved through a high-level goal selector πH-Nav(gt|ℓ′\nt, Mt) and a low-level\ngoal achiever πL-Nav(at|ht, gt), where ht = it−128:t, and ℓ′\nt, and gt are the agent’s current location,\nand goal location in (coordx, coordy), respectively. We introduce a Count-Based exploration strategy\n(Yamauchi, 1998; Tang et al., 2017; Chang et al., 2023) for the high-level goal selector.\nSpecifically, L × L visitation grid map mt is used with the agent’s starting location set as the center\nof the map. The locations of the agent’s trajectory are discretized and marked on the grid. The goal\nselector then divides the visitation map into grid cells of size G × G, and selects the location of\nthe grid cell with the lowest visitation count as the goal, gt. If multiple grid cells have the same\nminimum count, the cell closest to the current location ℓ′\nt is chosen. This approach directs the agent\ntoward unexplored locations, while minimizing unnecessary revisits. The size of grid cell can be\ndynamically adjusted to balance between broader exploration and finer local searches. Additionally,\nin an infinitely large map, the visitation map can be easily expanded by adding new grids, and further\nhierarchies on visitation maps can be introduced for efficiently managing explored locations.\nGoal-Conditioned VPT Navigator Once the goal location is selected by the high-level goal selector,\nit is crucial for the agent to navigate to the goal accurately. However, navigating complex terrains\n(e.g., river, mountain) requires human prior knowledge, where pure RL policy trained from scratch\nin prior work (Yuan et al., 2023) often shows suboptimal navigation ability. To address this, we use\nthe VPT as our starting policy, and fine-tune it for goal-conditioned navigation policy. We name this\npolicy as VPT-Nav. In VPT-Nav, we add goal embedding Gψ(lt, gt) in the output of TrXLθ in VPT\nwith LoRA adaptor (Hu et al., 2021a) as in Figure 3(b). We used PPO (Schulman et al., 2017) for\nfine-tuning goal encoder Gψ, LoRA parameters, policy πψ, and value vψ with reward based on the\ndistance to the goal location. We note that our VPT-Nav introduces several differences from prior\nVPT fine-tuning methods, thoroughly investigated in Appendix L.\n4\nEXPERIMENTS\nThis section presents a step-by-step validation of our agent MrSteve across various environments\nand conditions. We begin by evaluating the exploration and navigation ability of MrSteve, which\nis crucial in sparse sequential tasks (Section 4.1). Then, we demonstrate MrSteve’s capability to\nsolve A-B-A task sequentially where the memory is necessary to solve the task A twice (Section 4.2).\nAdditionally, we show that the proposed Place Event Memory outperforms other memory variants,\nparticularly when memory capacity is limited (Section 4.3). Lastly, we showcase the generalization\nof MrSteve to long-horizon sparse sequential task (Section 4.4). Each baseline and task is explained\nin each of the experiment sections with more details in Appendix C.\n4.1\nEXPLORATION & NAVIGATION FOR SPARSE SEQUENTIAL TASK\nIn this section, we evaluate the exploration and navigation ability of our agent. To verify this, we\nplaced an agent in a 100 × 100 block map with complex terrains such as mountains and a river, and\ngave 6K steps to wander around the map. Since successful exploration in Minecraft involves covering\nas much of the map as possible while minimizing revisits to previously visited locations, we measure\ntwo metrics: Map Coverage and Revisit Count. Map Coverage is calculated by dividing the map into\n11 × 11 grid cells and measuring the percentage of cells covered by the agent’s trajectory. Revisit\nCount measures the average number of times the agent visits the same grid cell.\n6\nPreprint\nTable 1: Map Coverage and Revisit Count of different exploration policies. Our exploration method (High-Level:\nCount-Based, Low-Level: VPT-Nav) performs the best.\nHigh-Level\nCount-based\nRNN-based\nSteve-1\nLow-Level\nVPT-Nav (Ours)\nDQN\nVPT-Nav\nDQN (Plan4MC)\nMap Coverage (↑)\n84.42 ± 0.06\n31.83 ± 0.11\n29.82 ± 0.07\n16.36 ± 0.04\n50.77 ± 0.13\nRevisit Count (↓)\n0.38 ± 0.6\n4.47 ± 1.43\n4.72 ± 1.73\n6.2 ± 1.73\n2.68 ± 1.36\nFigure 4: Agent’s trajectories of length 6K steps on 100 × 100 block map with different exploration methods.\nThe leftmost figure is the agent’s trajectory from our exploration method.\nTo demonstrate that our proposed hierarchical episodic exploration with Count-Based high-level goal\nselector and low-level goal achiever VPT-Nav is more effective for exploration in the given map, we\ncompared our method with the following baselines: Steve-1 (Lifshitz et al., 2024), and exploration\nmethod from Plan4MC (Yuan et al., 2023). For Steve-1, we provided the “Go Explore” instruction to\nassess its exploratory behavior as in prior works (Cai et al., 2023b; Zhou et al., 2024b). Plan4MC, on\nthe other hand, employs a hierarchical approach where the high-level RNN policy selects the next\ngoal location based on past locations, and a low-level DQN policy is used for goal-reaching. Since the\ninput-output space of our method and Plan4MC is identical, interchanging high-level and low-level\npolicies between two approaches is allowed so that we can evaluate the benefits of each component.\nAs shown in Table 1, and Figure 4, we can see that our exploration method outperforms other baselines.\nWhile Steve-1 showed decent performance in Map Coverage, it repeatedly visits previously explored\nplaces because of lack of memory. In the case of hierarchical exploration, high-level RNN policy\nstruggled with memorizing visited places as the trajectory gets longer, resulting in high Revisit\nCounts. Additionally, the low-level DQN policy had difficulty navigating complex terrain, such as\nmountains and rivers, showing low Map Coverage. On the other hand, the Count-Based goal selector\nthat directs the agent to the least-visited locations as goals and the VPT-Nav that effectively reaches\nthose goals resulted in strong exploratory behavior. Furthermore, to show the robustness of VPT-Nav,\nwe report its navigation capability in diverse terrains in Appendix L.\n4.2\nSEQUENTIAL TASK SOLVING WITH MEMORY IN SPARSE CONDITION\nIn this section, we demonstrate our agent’s capability to solve sparse sequential tasks based on\nexploration methods studied in Section 4.1. To evaluate this, we introduce ABA-Sparse task, which\nconsists of A-B-A tasks given sequentially with text instructions. Task A involves gathering a sparse\nresource, which can be either a water bucket\n, beef\n, wool\n, or milk\n. Task B requires\ncollecting a dense resource, chosen from log\n, dirt\n, leaves\n, seeds\n, or sand\n, making\ntotal 20 tasks. The agent spawns in a 100 × 100 block map, and the A resource exists in a single\nlocation, while the B resource can be found in multiple locations. The agent is given 12K steps with\nunlimited memory capacity to complete all tasks. Since finding the sparse resource A is challenging,\nthe task requires an efficient exploration algorithm. Moreover, after solving tasks A and B, memory\nbecomes crucial to return to the location of resource A within the time limit. We measure success\nrates and task duration for evaluation.\nTo verify the benefits of efficient exploration and the memory, we compared the following agents:\nSteve-1, MrSteve with exploration method from Plan4MC and FIFO Memory (PMC-MrSteve-FM),\nand our agent MrSteve with Count-Based goal selector and VPT-Nav for exploration and Place\nEvent Memory. We also test MrSteve with different memory variants, MrSteve-FM, MrSteve-EM,\n7\nPreprint\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Dirt-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWater-Seed-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\nTask A\nTask A′\n1.3\n3.3\n5.3\nTask Duration (×103)\nTask A\nTask A′\n2.2\n3.85\n5.5\nTask A\nTask A′\n2.4\n5.4\n8.4\nTask A\nTask A′\n2.0\n5.35\n7.7\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 5: Success Rate and Task Duration of different agents in ABA-Sparse tasks. Task A refers to the first A\ntask in the A-B-A task sequence, while Task A′ refers to the final A task in the A-B-A task sequence. We note that\nMrSteve, as well as its memory variants, outperforms Steve-1, which lacks the memory. Additionally, while\nSteve-1 takes a similar amount of time to solve both task A and task A′, MrSteve solves task A′ much faster. The\nfull results on all 20 tasks are in Appendix H, and investigations about memory variants are in Appendix O.\nMrSteve-PM, which use FIFO Memory, Event Memory, and Place Memory, respectively. Here,\nEvent Memory is a Place Event Memory without place clusters, which stores the frames based on\nvisual similarity. We explain the details of Event Memory in Appendix E.3.\nAs shown in Figure 5, it is clear that MrSteve outperforms Steve-1. This is because MrSteve can find\ntask-relevant resources faster with efficient exploration and store the location of the task A resource,\nallowing it to revisit the location and solve task A again within a limited time. This is evident from the\ntask duration in Figure 5, where MrSteve shows a shorter task duration than Steve-1 on the first A task.\nWhen solving the second A task, MrSteve exhibits a much shorter task duration compared to the first\nA task, while Steve-1 takes a similar or even greater number of steps. While other memory-augmented\nbaselines showed similar performance to MrSteve, PMC-MrSteve-FM performed worse due to a\nsuboptimal exploration method, making it difficult to find the sparse resource. We report the full\nresults on all 20 tasks in Appendix H.\n4.3\nMEMORY-CONSTRAINED TASK SOLVING WITH MEMORY\nWe demonstrated in Section 4.2 that memory is essential in solving sparse sequential tasks when there\nis no limitation in memory capacity. However, in real-world scenarios where memory capacity is\nlimited, memorizing visited places and novel events becomes important. In this section, we show that\nPlace Event Memory can benefit in this scenario. To verify this, we introduce three Memory Tasks,\nwhich are Find Water, Find Zombies’ Death Spot, and Find First-Visited House. In all tasks, the\nagent begins with an exploration phase, followed by a task phase. In the exploration phase, the agent\nfollows a fixed exploratory behavior. In the subsequent task phase, the agent is given a MineCLIP\nembedding et = Encv({ot}16\nn=1) as a task embedding instead of text instruction, where ot is the pixel\nobservation seen in the exploration phase. Then, this becomes an image goal navigation task where\nan agent should navigate to the location of the given image.\nIn all tasks, the exploration phase is 3K steps, and the agent’s memory capacity is limited to 2K. In\nFind Water task, the agent stays near water for 0.5K steps, then travels to a random location. In the\ntask phase, the agent should return to the water (Figure 6(a)). This task evaluates whether an agent\ncan memorize water frames in the past. In Find Zombies’ Death Spot, the agent sees burning zombies\nfor 1K steps (zombies burn for 0.5K steps then disappear), then travels. The task is to return to where\nzombies burned (Figure 6(b)). This task evaluates whether an agent can memorize distinct events\n(zombies burn and disappear) in the same place. In Find First-Visited House task, the agent sees the\nfirst house for 0.1K steps, then goes to the second house and stays until 2K step, then travels. The\ntask is to return to the first house (Figure 6(c)). This task evaluates whether an agent can memorize\ntwo visually similar houses in two different places.\n8\nPreprint\nstay\ntravel\n(a) Find Water\ntravel\ntravel\nstay for a long time\nbefore travel\n(c) Find First-Visited House\ntravel\nstay for a long time\nbefore travel\nstay\n(t = 0)\nSee Water\n(t = 0)\n1st House\nj\nHBC9k1Br2YEL14xMRFEtiQ2WGACbMzm5leI9nwDXrUH\/FmvPoL\/ocf4AB7ELCSTipV3enuCmPBDbjut5NbWV1b38hvFra2d3b3ivsHDaMSTZlPlVC6GRLDBJfMBw6CNWPNSBQK9hAObyb+wyPThit5D6OYBRHpS97jl\nICV\/DJcuaedYsmtuFPgZeJlpIQy1DvFn3ZX0SRiEqgxrQ8N4YgJRo4FWxcaCeGxYQOSZ+1LJUkYiZIp8eO8YlVurintC0JeKr+nUhJZMwoCm1nRGBgFr2J+J\/XSqB3GaRcxgkwSWeLeonAoPDkc9zlmlEQI0sI1dze\niumAaELB5jO3JYzmfkiHCtjT2MbkLYayTBpnFa9aqd6dl2rXWB5dISOURl56ALV0C2qIx9RxNEzekVvzovz7nw4n7PWnJPNHKI5OF+\/Nyckw=<\/latexit>(t = 0)\nZombies Burn\n(t = 0.5K)\nSee Water\n2nd House (t = 1K)\nZombies Gone\nW\nldW19ZzG\/nNre2d3cLefkPLWFWp1JI1fKIZoKHrA4cBGtFipHAE6zpDa\/H9eYjU5rL8B5GEXMD0g+5zykBE7kluHQ6wJ4guU1PuoWiXbYnwovGyUwRZap1Cz+dnqRxwEKgmjduwI3IQo4FSwN+JNYsIHZI+axsbkoBpN5kcneJjk\/SwL5V5IeBJ+nciIYHWo8AznQGBgZ6vjcP\/au0Y\/As34WEUAwvpdJEfCwSjwngHleMghgZQ6ji5lZMB0QRCobTzBYvmPlDMpQGVWowOfNQFk3jtOxUypW7s2L1KgOWQ4foCJWQg85RFd2gGqojih7\nQM3pFb9aL9W59WJ\/T1iUrmzlAM7K+fgEN0qBI<\/latexit>(t = 1K)\n(t = 3K)\nAfter Travel\n(t = 3K)\nAfter Travel\n(t = 3K)\nAfter Travel\n(b) Find Zombies’ Death Spot\nMrSteve\nMrSteve-PM\nMrSteve-EM\nMrSteve-FM\nSteve-1\nFigure 6: The overview of Memory Tasks, and Success Rate for each Memory Task from different agents.\nMemory Tasks are basically navigation tasks reaching the location of the previously seen experience frame. We\nobserve that MrSteve which uses Place Event Memory shows high success rates in all tasks.\nTo evaluate how each memory type performs in the Memory Tasks, we tested Steve-1, MrSteve,\nMrSteve-FM, MrSteve-EM, and MrSteve-PM. In Figure 6, the performance of each memory type\nis illustrated. In all tasks, Steve-1 which lacks memory, showed worst performance since it has to\nfind the targets from scratch. In Find Water task, MrSteve-FM does not have the water frames in\nmemory in task phase, so it should explore to find the water showing about 60% success rate. In\ncontrast, other agents store the water frames by allocating place cluster or event cluster in water\nlocation, allowing the agent to recall the frames and easily return to water, showing high success\nrate. In Find Zombies’ Death Spot task, MrSteve-PM loses burning zombies’ frames since Place\nMemory removes the frames in the largest place cluster, which is zombies’ place, showing about 60%\nsuccess rate. However, MrSteve-EM and MrSteve store the burning zombies’ frames as a novel event,\nallowing the agent to easily return to zombies’ spot, showing high success rate. In Find First-Visited\nHouse task, MrSteve-EM loses frames of first house, since it clusters two visually similar houses as\nthe same event, showing about 50% success rate. However, MrSteve-PM and MrSteve store the two\nhouses in different place clusters, enabling the agent to return to the first house, showing high success\nrate. These results suggest that Place Event Memory demonstrates its strength in memory-limited\nsettings, where memorizing both visited places and novel events is crucial for task completion.\n4.4\nLONG-HORIZON SPARSE SEQUENTIAL TASK SOLVING WITH MEMORY\nIn this section, to see how MrSteve generalizes to long-horizon tasks, we introduce two sparse\nsequential tasks. For both tasks, the agent plays in a 200 × 200 block map for 500K steps (About 7\nhours of gameplay) with 20K steps of memory capacity. The first is Long-Instruction task, where the\nagent is continuously given random tasks from Obtain X. Here X can be water\n, beef\n, wool\n, log\n, dirt\nor seeds\n. If the agent fails to complete the task within 20K steps, the task is\nchanged. This task requires efficient exploration in a large map, and managing memory to memorize\nplaces with task-relevant resources to continuously solve the given tasks.\nThe second task is Long-Navigation task similar to Memory Tasks in Section 4.3. It has an exploration\nphase of 16K steps and a task phase. In the exploration phase, the agent observes six events in different\nplaces: 1) burning zombies, 2) river, 3) sugarcane blow up, 4) spider spawn, 5) tree, and 6) house,\nspending 2K steps at each place. In the task phase, the image goal is continuously given randomly\nselected from the frames in the early steps of the event. For instance, if the task is to reach sugarcane\nplace, the image of sugarcane place before blow up is set as an image goal. This task requires\nmanaging memory to retain distinct events in different places.\n9\nPreprint\n0\n100K 200K 300K 400K 500K\nTime Steps\n0\n20\n40\n60\n80\n100\nSucceeded Tasks\nLong-Instruction Task\n0\n100K 200K 300K 400K 500K\nTime Steps\n0\n20\n40\n60\n80\nLong-Navigation Task\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 7: The performance in Long-Intruction task and Long-\nNavigation task. MrSteve performs well in both tasks.\nThe results are shown in Figure 7.\nFor Long-Instruction task, we ob-\nserve that MrSteve, and MrSteve-PM\nsolved over 80 tasks, showing their\ncapabilities of retaining task-relevant\nresources in different places effec-\ntively. MrSteve-EM solved around 50\ntasks, suggesting event-based memory\nis less effective than place-based mem-\nory. This is because similar events in\ndifferent places, like cows and sheeps\nliving in visually similar forests, are\nin the same event cluster, possibly los-\ning task-relevant frames. For the remaining baselines, they either have suboptimal exploratory\nbehaviors or keep losing the task-revelant frames, solving less than 50 tasks. For Long-Navigation\ntask, MrSteve, and MrSteve-EM solved around 70 tasks, showing the ability to retain novel events\noccured in different places. In the case of MrSteve-PM, it removes the frames in early time steps\nof each place cluster, losing novel events (e.g., sugarcane before blow up), thus solving less than\n20 tasks. For the remaining baselines, they lose or cannot retain frames in the early stage of an\nepisode solving less than 10 tasks. These results suggest that MrSteve demonstrates its strength in\nlong-horizon tasks.\n5\nLIMITATIONS\nThis work focuses on improving Steve-1 through the introduction of What-Where-When episodic\nmemory, significantly enhancing the agent’s ability to retain and recall past events for more efficient\ntask-solving. Our experiments demonstrate that MrSteve exhibits significantly enhanced performance\nwhen integrated with LLM-augmented agents for high-level planning. However, current limitations\nprevent the high-level planner from accessing PEM in the low-level controller. Future work could\nexplore enabling PEM access for high-level planners, which could generate more accurate plans by\nleveraging the agent’s episodic memories, further enhancing the system’s capabilities for complex,\nlong-horizon tasks.\nWe list up a few more limitations. First, our experiments are limited to surface-level exploration in\nthe Minecraft environment, omitting underground navigation, which is a crucial aspect of the game\nas mining plays a central role. However, our hierarchical exploration methods based on visitation\nmap could easily be extended to include vertical dimensions. Additionally, while our VPT-Nav\ndemonstrated strong navigation abilities in plains biomes, more challenging terrains, such as crossing\ncliffs that require skills like building bridges, were not addressed. Lastly, we used exact position data\nin Minecraft, which may limit the model’s adaptability to robotics tasks where positional information\nis often noisy. One possible direction is adapting MrSteve in environments with noisy positions.\n6\nCONCLUSION\nIn this paper, we introduced MrSteve (Memory Recall Steve-1), a novel low-level controller designed\nto address the limitations of current LLM-augmented hierarchical approaches in general-purpose\nembodied AI environments like Minecraft. We argued that the primary cause of failures in many\nlow-level controllers is the absence of an episodic memory system. To overcome this, we equipped\nMrSteve with Place Event Memory (PEM), a form of episodic memory that captures and organizes\nwhat, where, and when information from episodes. This allows for efficient recall and navigation in\nlong-horizon tasks, directly addressing the limitations of existing low-level controllers like Steve-1,\nwhich rely heavily on short-term memory. Additionally, we proposed an Exploration Strategy and a\nMemory-Augmented Task Solving Framework, enabling agents to effectively switch between explo-\nration and task-solving based on recalled events. Our results demonstrate significant improvements in\nboth task-solving and exploration efficiency compared to existing methods. We believe that MrSteve\nopens new avenues for improving low-level controllers in hierarchical planning and are releasing our\ncode to facilitate further research in this field.\n10\nPreprint\nETHICS STATEMENT\nWe acknowledge potential societal concerns related to our work. While our agent, in its current form,\nis designed for use in virtual environments such as Minecraft, the techniques and advancements made\nhere could be extended to broader autonomous systems. There is a possibility that, if adapted for real-\nworld applications, such systems might be misused for unethical purposes, including unauthorized\nsurveillance or actions that infringe on individual privacy. We encourage responsible usage and\nfurther research into safeguards to prevent such outcomes.\nREPRODUCIBILITY STATEMENT\nTo facilitate the reproducibility of our work, we have provided the pseudo codes, model architecture,\nand hyperparameters in Appendix D, E, and F. We will also release the source code for our models\nand experiments.\nACKNOWLEDGEMENT\nThis work was supported by GRDC (Global Research Development Center) Cooperative Hub Program\n(RS-2024-00436165) and Brain Pool Plus Program (No. 2021H1D3A2A03103645) through the\nNational Research Foundation (NRF) funded by the Ministry of Science and ICT (MSIT). The authors\nwould like to thank the members of Machine Learning and Mind Lab (MLML) for helpful comments.\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in neural information processing systems, 35:23716–23736,\n2022.\nJosh Albrecht, Abraham J. Fetterman, Bryden Fogelman, Ellie Kitanidis, Bartosz Wróblewski, Nicole\nSeo, Michael Rosenthal, Maksis Knutins, Zachary Polizzi, James B. Simon, and Kanjun Qiu.\nAvalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds. In NeurIPS\nDatasets and Benchmarks Track, 2022.\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. In Advances in Neural Information Processing Systems, pp. 24639–24654,\n2022.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nShaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control\nthrough goal-aware representation learning and adaptive horizon prediction. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 13734–13744, 2023a.\nShaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Groot: Learning to\nfollow instructions by watching gameplay videos. arXiv preprint arXiv:2310.08235, 2023b.\nMatthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min,\nKavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, Roozbeh Mottaghi, Jitendra Malik, and\nDevendra Singh Chaplot. Goat: Go to any thing, 2023. URL https:\/\/arxiv.org\/abs\/\n2311.06430.\nJunmo Cho, Jaesik Yoon, and Sungjin Ahn. Spatially-aware transformers for embodied agents.\nIn The Twelfth International Conference on Learning Representations, 2024. URL https:\n\/\/openreview.net\/forum?id=Ts95eXsPBc.\n11\nPreprint\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a fixed-length context. In Annual Meeting of the\nAssociation for Computational Linguistics, 2019. URL https:\/\/api.semanticscholar.\norg\/CorpusID:57759363.\nOr Dinari and Oren Freifeld. Revisiting DP-means: Fast scalable algorithms via parallelism and\ndelayed cluster creation. In The 38th Conference on Uncertainty in Artificial Intelligence, 2022.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2022a. URL https:\/\/openreview.net\/forum?\nid=rc8o_j8I8PX.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2022b. URL https:\/\/openreview.net\/forum?\nid=rc8o_j8I8PX.\nWilliam H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso,\nand Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. arXiv\npreprint arXiv:1907.13440, 2019.\nDanijar Hafner. Benchmarking the spectrum of agent capabilities. In International Conference on\nLearning Representations.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp, 2019.\nURL https:\/\/arxiv.org\/abs\/1902.00751.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021a. URL https:\n\/\/arxiv.org\/abs\/2106.09685.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations, 2021b.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,\n2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda\nLuu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning\nthrough planning with language models, 2022b. URL https:\/\/arxiv.org\/abs\/2207.\n05608.\nMatthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial\nintelligence experimentation. In Ijcai, volume 16, pp. 4246–4247, 2016.\nTomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose,\nKoki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented planning with contextual\nmemory for multimodal llm agents, 2024.\nAndrew Kyle Lampinen, Stephanie C Y Chan, Andrea Banino, and Felix Hill. Towards mental time\ntravel: a hierarchical memory for reinforcement learning agents. 2021.\n12\nPreprint\nZaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-\n1: Hybrid multimodal memory empowered agents excel in long-horizon tasks, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2408.03615.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. Advances in Neural Information Processing Systems, 36,\n2024.\nZichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, and Wei Yang.\nJuewu-mc:\nPlaying minecraft with sample-efficient hierarchical reinforcement learning.\narXiv preprint\narXiv:2112.04907, 2021.\nShunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui, Wenkai Fang, Yuxuan Zheng, Tongya\nZheng, and Mingli Song. Odyssey: Empowering agents with open-world skills. arXiv preprint\narXiv:2407.15325, 2024.\nHangyu Mao, Chao Wang, Xiaotian Hao, Yihuan Mao, Yiming Lu, Chengjie Wu, Jianye Hao, Dong\nLi, and Pingzhong Tang. Seihai: A sample-efficient hierarchical ai for the minerl competition. In\nDistributed Artificial Intelligence: Third International Conference, DAI 2021, Shanghai, China,\nDecember 17–18, 2021, Proceedings 3, pp. 38–51. Springer, 2022.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-\ntersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,\nDaan Wierstra, Shane Legg, and Demis Hassabis.\nHuman-level control through deep rein-\nforcement learning.\nNature, 518(7540):529–533, 2015.\ndoi: 10.1038\/nature14236.\nURL\nhttps:\/\/doi.org\/10.1038\/nature14236.\nKolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer\nSingh, and Roy Fox.\nDo embodied agents dream of pixelated sheep?: Embodied decision\nmaking using language guided world modelling. arXiv preprint arXiv:2301.12050, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2301.12050.\nOpenAI. Gpt-4 technical report, 2024. URL https:\/\/arxiv.org\/abs\/2303.08774.\nYiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing\nShao. Mp5: A multi-modal open-ended embodied system in minecraft via active perception.\nIn Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp.\n16307–16316, 2024.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision, 2021. URL https:\n\/\/arxiv.org\/abs\/2103.00020.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning. Advances in Neural Information Processing\nSystems, 36, 2024.\nChan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su.\nLlm-planner: Few-shot grounded planning for embodied agents with large language models. In\nProceedings of the IEEE\/CVF International Conference on Computer Vision (ICCV), October\n2023.\nZhiyuan Sun, Haochen Shi, Marc-Alexandre Côté, Glen Berseth, Xingdi Yuan, and Bang Liu.\nEnhancing agent learning through world dynamics modeling. arXiv preprint arXiv:2407.17695,\n2024.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,\nFilip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep\nreinforcement learning, 2017. URL https:\/\/arxiv.org\/abs\/1611.04717.\n13\nPreprint\nQwen Team. Qwen2.5: A party of foundation models, September 2024. URL https:\/\/qwenlm.\ngithub.io\/blog\/qwen2.5\/.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels, 2023. URL https:\/\/arxiv.org\/abs\/2302.13971.\nLaurens van der Maaten and Geoffrey Hinton.\nVisualizing data using t-sne.\nJournal of Ma-\nchine Learning Research, 9(86):2579–2605, 2008. URL http:\/\/jmlr.org\/papers\/v9\/\nvandermaaten08a.html.\nKonstantinos Voudouris, Ibrahim Alhas, Wout Schellaert, Matthew Crosby, Joel Holmes, John\nBurden, Niharika Chaubey, Niall Donnelly, Matishalin Patel, Marta Halina, José Hernández-\nOrallo, and Lucy G. Cheke. Animal-ai 3: What’s new & why you should care, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2312.11414.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv\npreprint arXiv: Arxiv-2305.16291, 2023a.\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng\nHe, Zilong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang. Jarvis-1: Open-world multi-task\nagents with memory-augmented multimodal language models. arXiv preprint arXiv: 2311.05997,\n2023b.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select:\nInteractive planning with large language models enables open-world multi-task agents. arXiv\npreprint arXiv:2302.01560, 2023c.\nBrian Yamauchi. Frontier-based exploration using multiple robots. In Proceedings of the Second\nInternational Conference on Autonomous Agents, AGENTS ’98, pp. 47–53, New York, NY, USA,\n1998. Association for Computing Machinery. ISBN 0897919831. doi: 10.1145\/280765.280773.\nURL https:\/\/doi.org\/10.1145\/280765.280773.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4MC: Skill reinforcement learning and planning for open-world Minecraft tasks. arXiv\npreprint arXiv:2303.16563, 2023.\nHaoqi Yuan, Zhancun Mu, Feiyang Xie, and Zongqing Lu. Pre-training goal-based models for\nsample-efficient reinforcement learning. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https:\/\/openreview.net\/forum?id=o2IEmeLL9r.\nJesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, and\nJoseph J Lim. Bootstrap your own skills: Learning to solve new tasks with large language model\nguidance. In 7th Annual Conference on Robot Learning, 2023. URL https:\/\/openreview.\nnet\/forum?id=a0mFRgadGO.\nBohan Zhou, Ke Li, Jiechuan Jiang, and Zongqing Lu. Learning from visual observation via offline\npretrained state-to-go transformer. Advances in Neural Information Processing Systems, 36, 2024a.\nEnshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing\nShao. Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world\ncontrol. arXiv preprint arXiv:2403.12037, 2024b.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft:\nGenerally capable agents for open-world environments via large language models with text-based\nknowledge and memory. arXiv preprint arXiv:2305.17144, 2023.\n14\nPreprint\nA\nLIMITATIONS OF MEMORY SYSTEM IN LLM-AUGMENTED AGENTS\nMemory systems in agents primarily aim to retrieve robust and accurate high-level plans for long-\nhorizon tasks (Wang et al., 2023a;b; Zhu et al., 2023; Qin et al., 2024; Li et al., 2024). Existing works\nuse an abstracted memory that stores the succeeded task with its plans, and often with history of\nobservations for reliable retrieval, which is helpful when the similar plan in other tasks already exist\nin memory. However, these types of memory systems are not well-suited for low-level controllers for\nthe following reasons:\n• Issues with Managing Memory Recent memory systems in Minecraft, when saving successful\nplans or skills, the history of observations for solving the task is all stored in FIFO manner (Wang\net al., 2023b) or only task-relevant frames are stored in the plan (Li et al., 2024). However,\ncomputation complexity for retrieving the experience frames that are relevant to a new task is\ncomputationally expensive or even impossible (because the latter may not store the experience\nframes despite the agent observed it).\n• Lack of Mechanism for Retrieving Experience frames Current memory systems store the text\ninstructions of succeeded tasks as keys and their plans to complete those tasks as values. The\nretrieval process begins by matching a task query to the task keys in memory and then filtering\nfurther using the current scene’s similarity to the stored frames before retrieving the final plan.\nThese memory systems are targetted to retrieve the succeeded plan, but they lack a mechanism for\nutilizing the experience frames in the memory, which could be crucial for future tasks.\nConsider the following example: Suppose the agent is tasked with collecting wood. While searching\nfor a tree, let’s assume the agent came across a cow in the forest. Once the wood is collected, the\nmemory will store the successful plan and its corresponding observations. However, if the agent\nis later tasked with finding the cow, there would be no memory key related to the cow, making it\nimpossible to retrieve the relevant frames. While some heuristics to calculate the similarity of the\ntask embedding for “find cow” and the visual representations in memory using MineCLIP is possible,\nit is computationally expensive since the similarity should be calculated for all stored frames.\nThus, we need a new type of memory system for the low-level controller to efficiently store novel\nevents (such as encountering the cow) as they explore the environment, even when such events are\nnot directly relevant to the current task. Also, the memory should hierarchically organize these novel\nevents so that they can be efficiently retrieved later. In this paper, we propose a memory system called\nPlace Event Memory (PEM), which organizes experiences by both location and event. PEM allows\nthe agent to store diverse novel events across various locations, making future retrieval more efficient.\nWe argue that PEM, when combined with current memory systems that store successful plans, will\nenable more effective retrieval of task-relevant information.\nB\nCOMPUTATION OVERHEAD\nOur study was performed on an Intel server equipped with 8 NVIDIA RTX 4090 GPUs and 512GB of\nmemory. The inference time for tasks under 20K steps for running a single episode was approximately\n30 minutes on a single GPU. For long-horizon tasks that take 500K steps, approximately 12 hours\nwere required for running a single episode on a single GPU. The VPT-Nav training took roughly 23\nhours on a single GPU.\nC\nENVIRONMENTS DETAILS\nC.1\nENVIRONMENTS SETTING\nAll tasks are implemented using MineDojo (Fan et al., 2022b). We utilize MineDojo’s success\nchecker, where the success of each task is determined based on changes in the agent’s inventory.\nHence, the agent succeeds the task if the corrensponding target item is appeared in the inventory. If\nthe agent exceeds the time limit of each task or dies before completing assigned tasks, indicating the\nagent fails on the task.\nFor all tasks, we assume that the agent has access to both first-person view pixels and its positional\ndata. The raw pixel observation it is provided in the shape (160 × 256 × 3). The agent’s position pt\n15\nPreprint\nFigure 8: Topdown View of Three ABA-Sparse Task Maps. The first map was used in the ABA-Sparse tasks,\nincluding the water bucket task. Trees are distributed on the left side of the map, and water exists only in the\nupper right corner. The second map was used in tasks, not including the water bucket and sand task. Trees are\nlocated on the left side of the map, and on the right side, there are cows and sheep, with a mountain separating\nthem. The last map was used in tasks, including the sand task. Its overall layout is identical to the second\nmap, except for an additional water pond at the top. The map’s edges are surrounded by high walls, making it\nimpossible to access anything other than the resources visible in the top-down view.\nis represented as a vector of shape (5, ), where the first three components correspond to the (x, y, z)\ncoordinates, and the last two components represent the pitch and yaw angles of the agent’s camera.\nWe note that no privileged observations, such as LiDAR or voxel data, are provided. The agent\noperates in a keyboard and mouse action space following VPT (Baker et al., 2022). This action space\nconsists of button input states paired with mouse movements.\nCrafting items, which requires long-horizon planning, is not considered in our method. To eliminate\nthe need for crafting, the appropriate item necessary for solving a task is provided to the agent at the\nbeginning of each new task. For instance, if the task is to “obtain water bucket\n,” the agent starts\nwith an empty bucket\nin its main hand. Additionally, we apply the following rules:\n• \/difficulty peaceful: This rule prevents the occurrence of hostile mobs, such as\nzombies and spiders, and death by starvation.\n• \/gamerule doWeatherCycle false: This rule keeps the weather clear to reduce\nthe noise from heavy rain.\nC.2\nTASK DETAILS\nIn this section, we describe the details for each task in Experiment section (Section 4). The basic\nenvironment settings follow those outlined in Appendix C.1, unless specified otherwise.\nC.2.1\nEVALUATION PROTOCOLS\nExcept for the Long-Horizon tasks in Section 4.4, we ran 100 episodes for each agent using different\nrandom seeds to evaluate performance. For the Long-Horizon tasks, we reported the average success\nrate with the standard error over 5 episode runs for each agent.\nC.2.2\nEXPLORATION & NAVIGATION TASK DETAILS\nFor this task, we used the Map 1 in Figure 8 with 100 × 100 size, surrounded by high walls. The\nmap includes complex terrains such as mountains and a water pond, which requires robust low-level\nnavigation policy for successful exploration. For each episode, the agent spawns in the center of the\nmap and explores for 6,000 steps.\n16\nPreprint\nC.2.3\nABA-Sparse TASKS DETAILS\nIn these tasks, the agent is asked to complete three sequentially given tasks, where the first and third\ntasks are identical. The target item for the first task, denoted as A, is one of four sparsly distributed\nitems: water\n, beef\n, wool\n, or milk\n. In the second task, denoted as B, the taget item is\none of five items: log\n, dirt\n, seeds\n, leaves\n, or sand\n. The agent has unlimited memory\ncapacity and is allowed a maximum of 12,000 steps to complete three tasks. The task only changes to\nthe next one upon successful completion of the current task.\nIf the first task, A, is to obtain a water bucket\n, the agents spawn at the center of Map 1, shown in\nFigure 8. On this map, most of the surface is covered with dirt, while grass, which provides seeds, is\nwidely distributed. A water pond is located in the upper right corner of the map, but it only becomes\nvisible when agents are nearby.\nIf the first task, A, is to collect beef\nor wool\n, the agents spawn at the center of Map 2 in Figure\n8. Similarly, if the second task, B is to collect sand\n, the agents start at the center of Map 3 in\nFigure 8. In both maps, trees are scattered on the left side of the map, while sheep\nand cows\nare found on the right side. A sand mountain runs through the middle, separating the trees from the\nsheep and cows. Dirt is only present on the far left and right sides of the map.\nC.2.4\nMEMORY TASKS\nIn the three Memory Tasks, the agent explores for 3,000 steps before being asked to complete an\nimage goal navigation. Unlike the ABA-Sparse Tasks, the agent has a limited memory capacity of\n2,000 frames. Consequently, an agent utilizing the FIFO memory forgets the memory from the first\n1,000 frames after the exploration phase.\nFind Water Task The agent is initially spawned near a water pond and remains in its vicinity for 500\nsteps. During this period, one observation is selected as a goal image for a subsequent navigation task.\nAfter the initial 500 steps, the agent moves to a random location for the remainder of the exploration\nphase. After 3K steps, the agent begins to navigate back to the water pond it observed at the start of\nthe episode.\nFind Zombies’ Death Spot Task At the beginning, the agent sees burning zombies for the first 1K\nsteps. Approximately 500 steps after the start of the episode, the zombies disappear, resulting in the\nagent observes two distinct scenes in the same location. A goal image for navigation is selected from\nthe observation where the zombies are burning. After 1K steps, the agent starts to travel for the rest\nof the exploration phase. Once the exploration phase finishes, the agent returns to the place where the\nzombies were burning.\nFind First-Visited House Task In this task, there are two distinct houses that look similar to one\nanother. The agent starts near one of the houses, where it stays for 100 steps before moving to the\nother house, where it remains for 2K steps. For the remainder of the exploration phase, the agent\ntravels to a random location. After the exploration phase, the agent is asked to go to the first house it\nhas visited.\nC.2.5\nLONG-HORIZON TASKS\nLong-Instruction Task In this task, the agent is required to complete a series of tasks sequentially on\na 200 × 200 block-sized map in Figure 9. The order of tasks within the sequence is randomized, with\neach task being one of six possible types: water bucket\n, beef\n, wool\n, wood\n, dirt\n, and\nseeds\n. We evaluated the agent’s performance by measuring the number of successfully completed\ntasks over 500K steps. If the agent fails to complete a given task within 20,000 steps, that task is\ncanceled, and a new one is assigned.\nLong-Navigation Task This task is basically image goal navigation in a 200 × 200 block sized\nmap in Figure 9. Before the task phase, the agent undergoes a 16K-step exploration phase where it\nobserves six landmarks: 1) zombie burning\n, 2) water\n, 3) sugarcane explosion\n, 4) spider\nspawn\n, 5) tree\n, and 6) house, spending 2K steps at each location. In the task phase, the agent is\ngiven a random start image of one of these landmarks and must navigate to it. The dynamic nature\nof some landmarks, such as the burning zombie, exploding sugarcane, and spider spawn makes it\nimportant to store novel events from the exploration phase.\n17\nPreprint\nFigure 9: Topdown View of Two Long-Horizon Task Maps. Both maps are of size 200 × 200 blocks. (Left)\nThis map is used for the Long-Instruction task. Trees are located in the lower left and middle bottom of the\nmap, while sheep inhabit the upper left area, and cows exist in the right bottom. A water pond can be found\nin the upper right area of the map. (Right) This map is utilized for the Long-Navigation task. Agents traverse\nbetween six scenes in the map. Three of them are dynamic scenes: burning zombies, popping sugarcanes, and\nspawning spiders, which are positioned at the first, third, and fourth places on the map, respectively. The other\nthree are static scenes: a water pond, trees, and a house, located at the second, fifth, and sixth places on the map,\nrespectively.\n18\nPreprint\nD\nMRSTEVE ALGORITHM\nAlgorithm 2 MrSteve Algorithm\nRequire: Xtinit\nn from Environment env, Memory Mtinit\nn , and new task τn at time step tinit\nn .\n1: mode ←null\n2: Tmode ←0\n3: gt ←null\n4: reached ←false\n5: t ←tinit\nn\n6: Xt ←Xtinit\nn ; Mt ←Mtinit\nn\n7: loop\n8:\nWrite(Mt, Xt)\n9:\nif Tmode > L then\n10:\nmode ←null\n11:\nTmode ←0\n12:\ngt ←null\n13:\nreached ←false\n14:\nend if\n15:\n# Mode Selector\n16:\nif mode is null then\n17:\ncandidates ←Read(Mt, τn)\n18:\nif candidates ̸= ∅then\n19:\nX′\nt ←PickOne(candidates)\n20:\ngt ←Position(X′\nt)\n21:\nmode ←EXECUTE\n22:\nelse\n23:\nmode ←EXPLORE\n24:\nend if\n25:\nend if\n26:\nht ←Xt−128:t\n27:\n# Explore Mode\n28:\nif mode is EXPLORE then\n29:\ngt ∼πH-Nav(gt|ℓ′\nt, Mt)\n30:\nat ∼πL-Nav(at|ht, gt)\n31:\nend if\n32:\n# Execute Mode\n33:\nif mode is EXECUTE then\n34:\nif the agent reaches gt then\n35:\nreached ←true\n36:\nend if\n37:\nif reached then\n38:\nat ∼πInst(at|ht, τn)\n39:\nelse\n40:\nat ∼πL-Nav(at|ht, gt)\n41:\ngt+1 ←gt\n42:\nend if\n43:\nend if\n44:\nif the task τn succeeded or timeout then\n45:\nbreak\n46:\nend if\n47:\nTmode ←Tmode + 1\n48:\nXt+1 ←env.step(at); Mt+1 ←Mt\n49:\nt ←t + 1\n50: end loop\nWe provide more details in Algorithm 2. This algorithm is executed when a new task is given to the\nMrSteve and finished if it completes the task or exceeds a time limit (Line 44-46).\n19\nPreprint\nAgent State Variables When a new task τn is given, the agent state variables are initialized (Line\n1-4). mode indicates which module is executed and Tmode denotes elapsed times for the current mode\nexecution. gt is a target location for the navigation, and reached indicates whether the agent has\nreached the target location in the Execute Mode.\nThe Agent State Variables are reset if the agent remains in the current mode for L steps (Lines 9-13).\nIf the Mode Selector retrieves a wrong experience frame, where no task-relevant resource is present\nat the corresponding location, the agent can have trouble and fail to complete the given task for a\nlong time. In addition, the agent might encounter previously unobserved task-relevant resources\nwhile navigating to the location of the previously retrieved memory. In both scenarios, changing\nthe navigation target can be helpful. Consequently, the agent queries the memory to search for new\ncandidates if the agent executes the current mode for L = 600 steps, equivalent to 30 seconds in-game\ntime.\nMemory Write Frequency In Line 8, MrSteve writes the current observation to the memory,\nregardless of the current mode.\nMode Selector The Mode Selector decides between two modes according to the existence of task-\nrelevant resources in the memory. First, if no mode has been selected, MrSteve queries the memory\n(Line 16-17). If a task-relevant resource exists, MrSteve picks one of them and sets the navigation\ntarget gt to the location of the picked one (Line 18-21). Otherwise, the Explore Mode is selected\n(Line 23).\nExplore Mode In the Explore Mode, MrSteve explores the least-visited locations using πH-Nav and\nπL-Nav (Line 28-31). We provide more details in Appendix F.2.\nExecute Mode In the Execute Mode, MrSteve navigates to the selected target location gt first. Once\nit reached, Steve-1 is executed to follow the task instruction τn (Line 33-43).\n20\nPreprint\nE\nPLACE EVENT MEMORY, PLACE MEMORY, EVENT MEMORY DETAILS\nWe provide the Algorithms on Write & Read operations of Place Event Memory, and other memory\nvariants, which are Place Memory, and Event Memory. We also provide the specifications of each\nmemory in the following section E.4.\nE.1\nPLACE EVENT MEMORY WRITE & READ OPERATIONS\nAlgorithm 3 Place Event Memory Write Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk, and each place cluster Mk has event clusters\n{Ek\ni }dk\ni=1 where Ek\ni is i-th event cluster with center embedding ek\nEi. Each place cluster has\ndummy deque Qk that stores recent R frames in that cluster, and update frequency timer rk.\nAdditional variables are Memory Capacity N, MineCLIP image encoder CLIPv, and experience\nXt = {it, lt, t} at time step t. MineCLIP threshold c.\n1: # Memory Add\n2: Get experience frame xt = {et, lt, t} where et = CLIPv(it)\n3: if ℓt ∈PLACE_CLUSTER(Mt) then\n4:\nFind place cluster Mj where PLACE(ℓt) = ℓMj\n5:\nAdd xt to dummy deque Qj\n6:\nUpdate frequency timer rk = rk + 1\n7:\nif rk = R then\n8:\n{Ei, ei}l′\ni=1 = DP-Means(Qj)\n9:\n{Ei, ei}l\ni=1 = MERGE_CLUSTERS({Ei, ei}l′\ni=1)\n10:\nfor u = 1, . . . , l do\n11:\nadd_cluster=True\n12:\nfor p = 1, . . . , dj do\n13:\nif ej\nu · ej\nEp > c then\n14:\nMerge Eu to Ej\np\n15:\nadd_cluster=False; break\n16:\nend if\n17:\nend for\n18:\nif add_cluster=True then\n19:\nCreate new event cluster Ej\ndj+1 = Eu with center embedding ej\nEdj +1 = eu\n20:\nAdd created cluster Ej\ndj+1 to Mj\n21:\nend if\n22:\nend for\n23:\nrk = 0\n24:\nend if\n25: else\n26:\nCreate new place cluster MK+1 with center position pMK+1 = PLACE(ℓt), and center\nembedding eMK+1 = et\n27:\nCreate new dummy deque QK+1 = {xt}\n28:\nAdd created cluster MK+1 = {QK+1} to Mt\n29: end if\n30: # Memory Remove\n31: if len(Mt) > N then\n32:\nFind event cluster Ek\ni where i, k = arg max len(Ek\ni )\n33:\nRemove the oldest frame in Ek\ni\n34: end if\nWe further elaborate on how # Memory Add in Algorithm 3 operates. First, we get the experience\nframe xt, and check if ℓt belongs to one of place clusters in Mt (Line 3). Here, PLACE_CLUSTER()\nindicates the whole 2D space that memory Mt covers. Since we use fixed-size square area for each\nplace cluster, it can be seen as covered area of set of squares. If xt is not in current place clusters,\nnew place cluster is created with dummy deque (Line 26-28). Dummy deque here is short memory\n21\nPreprint\nfor each place cluster used for clustering the events. If xt belongs to some place cluster (Line 4,\nPLACE() maps ℓt to its place), it is added to dummy deque in the place cluster, and increase update\nfrequency timer in that place cluster by 1 (Line 4-6). When update frequency timer equals to R,\nDP-Means algorithm (Dinari & Freifeld, 2022) is applied to experience frames in dummy deque\nfor event clustering. After applying DP-Means algorithm, we get a set of events and its center\nembeddings (Line 8). However, we found that DP-Means tends to make different clusters even when\nthe agent observes the same scenes. Thus, we applied MERGE_CLUSTERS() to DP-Means output\nclusters to merge clusters that have high alignments in center embeddings (Line 9). After this, for\neach newly created event cluster from DP-Means (Line 10), if some existing event cluster is aligned\n(Line 13), two clusters are merged (Line 14). If newly created cluster does not belong to any existing\nevent clusters, then add to the place cluster as a new event cluster (Line 19-20).\nEvent Cluster Details We use the DP-Means algorithm for clustering the events. DP-Means\nalgorithm is a Bayesian non-parametric extension of the K-means algorithm based on small variance\nasymptotic approximation of the Dirichlet Process Mixture Model. It doesn’t require prior knowledge\non the number of clusters K. To run this algorithm, we first set the initial number of clusters K′ and\ncluster the data with K++ Means initialization (K′ can be 1), then DP-Means algorithm automatically\nre-adjust the number of clusters based on the data points and cluster penalty parameter δ. Thus,\nDP-Means algorithm behaves similarly to K-means with the exception that a new cluster is formed\nwhenever a data point is farther than δ away from every existing cluster centroid.\nUsing clustering algorithm in Minecraft was previously done in Yuan et al. (2024), where K-Means\nalgorithm is used to cluster 100 goal states in massive Minecraft dataset. However, they applied the\nclustering on the reduced dimension of MineCLIP representation with t-SNE (van der Maaten &\nHinton, 2008). In this work, we directly apply DP-Means algorithm in MineCLIP representation\nspace setting initial number of clusters K′ = 5, and δ = 1.\nPlace Cluster Details We provide a detailed explanation of how place clusters are formed. Each place\ncluster stores the agent’s experience frames based on its 2D ground position (x, y) and head direction\nangle yaw. The size of a place cluster and its yaw range are defined by parameters C (in Minecraft\nblock) and W ∈(0◦, 180◦), respectively, which distinguish different clusters. A place cluster is\ncentered at a specific position (x, y) with a center yaw w. An experience frame is assigned to a place\ncluster if the agent’s position falls within the range (x−C\/2, x+C\/2) for x and (y −C\/2, y +C\/2)\nfor y, and the yaw angle satisfies (w −W\/2, w + W\/2). We use relative position and yaw for center\nof the place cluster implying that the center of the initial place cluster has position (0, 0), and yaw 0.\nAlgorithm 4 Place Event Memory Read Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk, and each place cluster Mk has event clusters\n{Ek\ni }dk\ni=1 where Ek\ni is i-th event cluster with center embedding ek\nEi. Additional variables are\nMineCLIP image encoder CLIPv, and text encoder CLIPt, task instruction τn at time step t, and\ntask threshold h.\n1: Calculate sik(τn) = CLIPt(τn) · CLIPv(ek\nEi) for all i, k\n2: Sort sik(τn) for all event clusters and select top-K event clusters\n3: Gather all experience frames xt = {et, lt, t} from the top-K event clusters\n4: cand = {}\n5: for each experience frame xt in the selected top-K event clusters do\n6:\nsxt(τn) = CLIPt(τn) · CLIPv(et)\n7:\nif sxt(τn) > h then\n8:\nAdd xt to cand\n9:\nend if\n10: end for\n11: return cand\nExploiting hierarchical structure of Place Event Memory For memory read operation, we can\nexploit hierarchical structure of place event memory for better computation efficiency. Suppose we\nhave N1 place clusters and exactly N2 event clusters for each place cluster. Then, read operation\nin Algorithm 4 has computational complexity of O(N1N2). However, if we first attend only to\nplace clusters (calculate sik from center embeddings eMk from place clusters), then attend to event\n22\nPreprint\nclusters from extracted top-k place clusters, the computational complexity becomes O(N1 + kN2).\nHowever, center embedding eMk may not be sufficient summarization of the place cluster, since\ncenter embedding can not capture all different events occurred in that place. Thus, we use the read\noperation as in Algorithm 4.\n23\nPreprint\nE.2\nPLACE MEMORY WRITE & READ OPERATIONS\nAlgorithm 5 Place Memory Write Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk. Each place cluster Mk is a FIFO Memory.\nAdditional variables are Memory Capacity N, MineCLIP image encoder CLIPv, and experience\nXt = {it, lt, t} at time step t.\n1: # Memory Add\n2: Get experience frame xt = {et, lt, t} where et = CLIPv(it)\n3: if ℓt ∈PLACE_CLUSTER(Mt) then\n4:\nFind place cluster Mj where PLACE(et) = eMj\n5:\nAdd xt to Mj\n6: else\n7:\nCreate new place cluster MK+1 with center position ℓMK+1 = PLACE(ℓt), and center\nembedding eMK+1 = et\n8:\nAdd created cluster MK+1 = {xt} to Mt\n9: end if\n10: # Memory Remove\n11: if len(Mt) > N then\n12:\nFind place cluster Mk where k = arg max len(Mk)\n13:\nRemove the oldest frame in Mk\n14: end if\nAlgorithm 6 Place Memory Read Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk. Each place cluster Mk is a FIFO Memory. Ad-\nditional variables are MineCLIP image encoder CLIPv, and text encoder CLIPt, task instruction\nτn at time step t, and task threshold h.\n1: Calculate sk(τn) = CLIPt(τn) · CLIPv(eMk) for all k\n2: Sort sk(τn) for all place clusters and select top-K place clusters\n3: Gather all experience frames xt = {et, lt, t} from the top-K place clusters\n4: cand = {}\n5: for each experience frame xt in the selected top-K place clusters do\n6:\nsxt(τn) = CLIPt(τn) · CLIPv(et)\n7:\nif sxt(τn) > h then\n8:\nAdd xt to cand\n9:\nend if\n10: end for\n11: return cand\n24\nPreprint\nE.3\nEVENT MEMORY WRITE & READ OPERATIONS\nAlgorithm 7 Event Memory Write Operation at time step t\nRequire: Assume memory at time step t as Mt = {Ek}K\nk=1 where Ek is k-th event cluster with\ncenter embedding eEk, and each event cluster is a FIFO Memory. Memory has a dummy deque\nQ that stores recent R frames, and update frequency timer r. Additional variables are Memory\nCapacity N, MineCLIP image encoder CLIPv, and experience Xt = {it, lt, t} at time step t.\nMineCLIP threshold c.\n1: # Memory Add\n2: Get experience frame xt = {et, lt, t} where et = CLIPv(it)\n3: Add xt to dummy deque Q\n4: Update frequency timer r = r + 1\n5: if r = R then\n6:\n{Ei, ei}l′\ni=1 = DP-Means(Qj)\n7:\n{Ei, ei}l\ni=1 = MERGE_CLUSTERS({Ei, ei}l′\ni=1)\n8:\nfor u = 1, . . . , l do\n9:\nadd_cluster=True\n10:\nfor p = 1, . . . , k do\n11:\nif ej\nu · eEp > c then\n12:\nMerge Eu to Ep\n13:\nadd_cluster=False; break\n14:\nend if\n15:\nend for\n16:\nif add_cluster=True then\n17:\nCreate new event cluster Ek+1 = Eu with center embedding eEk+1 = eu\n18:\nAdd created cluster Ek+1 to Mt\n19:\nend if\n20:\nend for\n21:\nrk = 0\n22: end if\n23: # Memory Remove\n24: if len(Mt) > N then\n25:\nFind event cluster Ek where k = arg max len(Ek)\n26:\nRemove the oldest frame in Ek\n27: end if\nAlgorithm 8 Event Memory Read Operation at time step t\nRequire: Assume memory at time step t as Mt = {Ek}K\nk=1 where Ek is k-th event cluster with\ncenter embedding eEk, and each event cluster Ek is a FIFO Memory. Additional variables are\nMineCLIP image encoder CLIPv, and text encoder CLIPt, task instruction τn at time step t, and\ntask threshold h.\n1: Calculate sk(τn) = CLIPt(τn) · CLIPv(eEk) for all k\n2: Sort sk(τn) for all event clusters and select top-K event clusters\n3: Gather all experience frames xt = {et, lt, t} from the top-K event clusters\n4: cand = {}\n5: for each experience frame xt in the selected top-K event clusters do\n6:\nsxt(τn) = CLIPt(τn) · CLIPv(et)\n7:\nif sxt(τn) > h then\n8:\nAdd xt to cand\n9:\nend if\n10: end for\n11: return cand\nDiscussions on MineCLIP threshold As in Event Memory’s Write Operation in Algorithm 7,\nclusters generated by DP Means algorithm are either merged with an exisiting event cluster, or\nadded as new event cluster. This is determined by MineCLIP threshold c, which is the criterion\n25\nPreprint\nfor separating event clusters. We note that using proper value for threshold is important for Event\nMemory to work reasonably across the tasks. If c is too small, Event Memory will cluster experience\nframes with only few clusters, which may not be visually distinctive. If c is too large, in extreme\ncase, Event Memory will make event cluster for each experience frame. When memory capacity is\nexceeded, it will randomly remove the oldest frame (since all event clusters have same size), which\nbehaves as a FIFO Memory.\nDrawback in Event Memory Event Memory removes the frame in the largest event cluster, when\nmemory capacity is exceeded. This indicates that the memory can retain visually distinct events.\nHowever, since it does not consider position information in clustering, similar visual frames in\ndifferent places can be clustered into same event cluster. This can be fatal since task-relevant frames\nin different places can be removed from the memory which can be crucial in upcoming tasks. This\ndrawback is shown in Find First-Visited House task in Section 4.3, and Long-Instruction task in\nSection 4.4, where MrSteve-EM showed suboptimal performances.\nE.4\nHYPER-PARAMETERS FOR MEMORY\nWe provide the specifications for each memory type used in the experiments. The task threshold h is\nset to 22.74 by default, but it can be adjusted to enhance retrieval accuracy.\nTable 2: Specifications for each memory type.\nMemory Type\nParameter\nValue\nPlace Event Memory\nPlace Cluster Size C\n6\nPlace Cluster Yaw Y\n60\nUpdate Frequency rk\n100\ntop-K\n30\nMineCLIP Threshold c\n73.5\nTask Threshold h\n22.74\nEvent Memory\nUpdate Frequency rk\n100\ntop-K\n30\nMineCLIP Threshold c\n73.5\nTask Threshold h\n22.74\nPlace Memory\nPlace Cluster Size C\n6\nPlace Cluster Yaw Y\n60\ntop-K\n30\nTask Threshold h\n22.74\n26\nPreprint\nFigure 10: The model architectures of Explore Mode and Execute Mode in Solver Module.\nF\nSOLVER MODULE DETAILS\nF.1\nMODE SELECTOR\nIn Section 3.2, we described the Mode Selector when combined with Place Event Memory. When\nthe memory type changes such as Place Memory or Event Memory, different read operation should\nbe employed. We provide the Memory Read operation of Place Memory, and Event Memory in\nAppendix E.2, and E.3, respectively. In case of FIFO Memory, for read operation, task alignment\nscores on whole frames in the memory is required.\nF.2\nEXPLORE MODE\nIn explore mode, hierarchical structure is employed. For high-level goal selector, we take the similar\napproach introduced in Chang et al. (2023), where the target problem is indoor navigation with\nrobots. Their global policy exploits semantic map to output exploration goal. For the goal selection,\nit uses frontier-based exploration (Yamauchi, 1998), which selects the closest unexplored region as\nthe goal. In Figure 10(a), the overview of hierarchical episodic exploration is illustrated. From Place\nEvent Memory, we contruct a visitation map by marking the agent’s visited locations. In addition to\nmarking agent’s visited locations, we also marked the agent’s FoV in the visitation map with sector\nregion towards agent’s head direction (yellow sector shows agent’s current FoV). From the visitation\nmap, the next goal is chosen, and the goal, and current observation, position is given to VPT-Nav\nfor generating low-level action. In experiments, for the tasks with map size 100 × 100, we used\nvisitation map size L = 120, and grid cell size G = 15. For the tasks with map size 200 × 200, we\nused visitation map size L = 240, and grid cell size G = 30.\nF.3\nEXECUTE MODE\nThe model architecture of Execute mode is illustrated in Figure 10(b). When the Mode Selector\nselects the Execute Mode, the experience frame of a task-relevant resource is retrieved from the\nmemory. MrSteve then navigates to the goal location of the experience frame, and then adjust camera\norientation using yaw and pitch from the frame to ensure it faces the observed of the retrieved\nexperience frame again. Once the camera adjustment is complete, Steve-1 is executed to follow the\ntask instruction τn.\nF.4\nMINECLIP & STEVE-1 PROMPTS FOR TASKS\nThe prompts used in our experiments are listed in Table 3. MrSteve sets τn to one of two different\nprompts depending on the agent’s status. When the agent is in the mode selection phase, the\nMineCLIP prompts are used as the query for the memory to determine whether the task-relevant\nresource exists in the memory. In contrast, the Steve-1 prompts are utilized during the execution\nmode.\nRelying solely on Steve-1 prompts can lead to difficulties in calculating alignment score between the\nprompt and memory records. Since MineCLIP (Fan et al., 2022b) computes the alignment between\nvideos and textual descriptions, the alignment score is higher when the textual description well\nreflects the agent’s action shown in the video. If a video contains scenes relevant to the current\n27\nPreprint\nTable 3: Prompts used in MrSteve for each task.\nMineCLIP\nSteve-1\nlog\nnear tree\ncut a tree\nbeef\nnear cow\nkill cow\ndirt\nnear dirt\nget dirt\nsand\nnear sand\nget sand\nseed\nnear grass\ncollect seeds\nwool\nnear sheep\nkill sheep\nleaves\nnear tree\nbreak leaves\nmilk bucket\nnear cow\nget milk bucket\nwater bucket\nnear water\nget water bucket\ntask but lacks behavior indicative of task completion, the alignment score will be low. For instance,\nconsider a scenario where the “obtain water bucket\n” task is given for the first time, and the agent\nhas previously encountered water. Although the agent is Near water in that scene, it has not yet\nperformed the action of Obtaining water bucket. As a result, the prompt “obtain water bucket” would\nnot accurately describe that memory, and that scene could be disregarded during the mode selection\nprocess. In our experiments, we manually defined two types of prompts for each task. However, this\nprocess can be automated using LLMs, which prompts an LLM to extract the more suitable language\ninstructions to complete a task.\nG\nEXPLORATION EXPERIMENTS DETAILS\nFigure 11: Agent’s trajectories of length 6K steps on 100 × 100 sized map with different exploration methods.\nLeft figure is the agent’s trajectory from our exploration method.\nWe provide further details on evaluation metrics and analysis of the exploration results. For evaluation\nmetrics, we measured Map Coverage and Rivisit Count. Map Coverage is calculated by dividing the\nmap into 11 × 11 grid cells and measuring the percentage of cells covered by the agent’s trajectory.\n28\nPreprint\nRevisit Count measures the average number of times the agent visits the same grid cell only for the\ncells that have visitation counts larger than 300. We measured Revisit Count only for highly visited\ngrid cell since the agent needs some amount of time to escape the grid cell.\nSince Steve-1 (Lifshitz et al., 2024) has no memory module, it tends to visit same place multiple times.\nWe also observed that Steve-1 exhibits a behavior of moving straight ahead when task instruction \"Go\nExplore\" is given, often colliding with walls and then following along the wall instead of avoiding it,\nwhich harms the Map Coverage and Revisit Count.\nPlan4MC (Yuan et al., 2023) employs a hierarchical exploration policy. The high-level policy, based\non an LSTM model, takes the agent’s (x, y) coordinates along its trajectory and outputs the next\ndirection to move (north, south, east, or west). The environment is discretized into an 11×11 grid, and\nthe policy is trained using PPO (Schulman et al., 2017) to maximize the number of unique grid cells\nvisited. The low-level policy is trained using DQN (Mnih et al., 2015) and follows the MineAgent\nstructure from the MineDojo framework. It receives a goal position and current observation in pixel\nand outputs actions in the MineDojo action space. However, despite the high-level RNN policy, it\nstruggled to keep track of visited locations as the trajectory grew longer, leading to a high Revisit\nCount. Furthermore, the low-level DQN controller faced difficulties navigating complex terrain, such\nas mountains and rivers, resulting in lower Map Coverage.\nH\nABA-Sparse TASKS FULL RESULTS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Dirt-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Dirt-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Dirt-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Dirt-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Seed-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Seed-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Seed-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Seed-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Log-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Log-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Log-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Leaves-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Leaves-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Leaves-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Leaves-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Sand-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Sand-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Sand-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 12: ABA-Sparse Tasks Full Result.\nWe report the results of all combinations of the ABA-Sparse tasks in Figure 12. The experiments\nwere conducted under the same conditions as described in Section 4.2. We note that MrSteve and\n29\nPreprint\nits memory variants consistently outperform Steve-1 in all tasks. In case of PMC-MrSteve-FM,\nit performed better than Steve-1 in most tasks, while it failed on some tasks due to a suboptimal\nexploration strategy.\nI\nMRSTEVE IN RANDOMLY GENERATED MAPS\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nSuccess Rate\nWater-Log-Water\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nBeef-Log-Beef\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWool-Log-Wool\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nSEQ(4)\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 13: The performance of different agents in ABA-Random tasks. MrSteve consistently outperforms\nSteve-1 in randomly generated map.\nIn this experiment, we investigate whether MrSteve benefits over Steve-1 in sequential task when\nthe map is randomly generated. To verify this, we created random Plains maps using the Minedojo\nenvironment (Fan et al., 2022b), and set up tasks similar to the ABA-Sparse tasks described in Section\n4.2. We call this task, ABA-Random task.\nIn this experiment, task A could involve collecting either water, beef, or wool, while task B involves\ngathering logs. Additionally, we introduce a sequential task named SEQ(4), which requires the agent\nto solve four consecutive tasks: log, water, wool, and beef. For the ABA-Random tasks, the agent was\ngiven 12K steps, and for the SEQ(4) task, 16K steps were allowed. We evaluated following agents:\nMrSteve, MrSteve-EM, MrSteve-PM, MrSteve-FM, PMC-MrSteve-FM, and Steve-1.\nAs shown in Figure 13, MrSteve and its memory variants consistently showed higher success rate\nthan Steve-1 across all tasks. This is because when Steve-1 is finished with task B, it tries to solve the\nfinal task A from scratch making it difficult to complete all tasks in limited time. However, MrSteve\ncould retain the experience frames of task A in memory making it efficient to solve all tasks in time.\nThis result suggests that augmenting memory plays a crucial role in solving sequential tasks, even in\nrandomly generated map.\nJ\nABLATION STUDIES ON PLACE EVENT MEMORY\n1\n2\n4\n10\n30\ntop-k\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\n1\n2\n4\n6\n12\nCluster Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 14: Success Rates of MrSteve with different top-\nk’s, and cluster sizes on Wool-Log-Wool task.\nIn this section, we study the robustness of our\nproposed agent MrSteve by exploring the effects\nof top-k selection in Mode Selector, and the size\nof a place cluster for Place Event Memory. For\nthis, we used one of the tasks from ABA-Sparse\ntask in Section 4.2, which is Wool-Log-Wool\ntask. The success rate of MrSteve with different\ntop-k’s and place cluster sizes are illustrated in\nFigure 14. For top-k experiment, we tested five\nk values, which are 1, 2, 4, 10, and 30. From\nthe results, we found that using small k did not\nlargely affect the performance. For cluster size experiment, we tested five cluster sizes, which\nare 1, 2, 4, 6, and 12 in Minecraft blocks. From the results, we see that using bigger cluster size\nslightly lowered the performance since the center embedding of the place cluster may not be good\nsummarization vector of the place when cluster size is large, making the agent difficult to recall\ntask-relevant frames in the memory.\n30\nPreprint\nTable 4: Querying Time and FLOPs for each memory types and k values. We report the averages and standard\nerrors over 1K query operations.\nMemory Type\ntop-k\nTime per Query (ms)\nFLOPs (×109)\nPlace Event Memory\n1\n6.24 ± 0.01\n1.591 ± 0.001\n2\n6.47 ± 0.02\n1.610 ± 0.002\n4\n6.89 ± 0.02\n1.667 ± 0.003\n10\n7.92 ± 0.03\n1.791 ± 0.004\n30\n11.24 ± 0.07\n2.210 ± 0.010\nEvent Memory\n1\n11.28 ± 0.35\n1.603 ± 0.054\n2\n18.95 ± 0.56\n2.711 ± 0.073\n4\n42.41 ± 1.20\n3.734 ± 0.088\n10\n53.82 ± 1.11\n7.476 ± 0.150\n30\n117.16 ± 2.02\n12.694 ± 0.201\nPlace Memory\n1\n3.86 ± 0.05\n1.251 ± 0.001\n2\n4.21 ± 0.08\n1.281 ± 0.002\n4\n4.72 ± 0.08\n1.338 ± 0.003\n10\n5.98 ± 0.09\n1.504 ± 0.005\n30\n9.85 ± 0.12\n2.024 ± 0.011\nFIFO\n–\n438.46 ± 0.10\n52.481 ± 0.000\nK\nCOMPUTATIONAL COMPLEXITY OF QUERY OPERATION\nWe provide the computational complexity of query operation of each memory type in Table 4. To\nensure a fair comparison, we generated an episode with a 100K-step trajectory, allowing each type\nof memory to process identical observations. After constructing each type of memory with this\ntrajectory, we randomly selected 1,000 observation embeddings from the trajectory for the query\noperations. The query time represents the elapsed time during the query function call. FLOPs denotes\nthe number of floating-point operations required for the MineCLIP score calculation. For FIFO\nmemory, MineCLIP scores are computed once during task alignment score calculation. For the other\nmemory types, MineCLIP scores are calculated twice: during the top-k selection and for the task\nalignment score.\nThe number of clusters affects the complexity of top-k selection and the number of frames per cluster\ninfluences the complexity of the task alignment score calculation. Place Event Memory consists of\napproximately 3,000 clusters, with each cluster containing an average of 30 frames. Place Memory,\non the other hand, includes about 2,300 clusters, each holding an average of 43 frames. Although\nPlace Memory provides the fastest querying time, Place Event Memory has comparable speed while\nachieving similar or superior success rates in most tasks.\nEvent Memory comprises around 582 clusters, with an average of 172 frames per cluster. Its clustering\napproach relies on visual discriminative features, which means visually similar places are grouped\ntogether, even if they are spatially distant. As a result, Event Memory has significantly fewer clusters\ncompared to Place and Place Event Memory.\nOverall, the three hierarchical memory types are computationally efficient and lightweight compared\nto FIFO Memory, which calculates the task alignment scores on whole 100K frames. In contrast,\nPlace Event Memory with k = 30 evaluates roughly 3,900 frames per query (3,000 for top-k selection\nand 30 frames per cluster across 30 clusters), resulting in a performance that is approximately 40\ntimes faster and requires about 24 times fewer FLOPs than FIFO Memory.\nL\nGOAL-CONDITIONED VPT NAVIGATOR DETAILS AND INVESTIGATION\nL.1\nGOAL-CONDITIONED VPT NAVIGATOR FINE-TUNING DETAILS\nWhen the goal location lG is selected by high-level goal selector, it is important for the agent to\nnavigate to the goal location accurately. Since navigating complex terrains (e.g., river, mountain)\nrequires human prior knowledge, we use VPT as our initial policy, and fine-tune it for goal-conditioned\n31\nPreprint\nnavigation policy. We name this model as VPT-Nav. To see how VPT-Nav model works, we first\ndescribe the components of VPT as follows:\nImage Encoder:\nxt = IE\nθ (it)\nTransformer-XL:\nzt−T :t = TrXLθ(xt−T , · · · , xt)\nPolicy Head:\nˆat ∼πθ(at|zt)\n(1)\nIn previous works, there were different approaches in fine-tuning VPT for specific purpose. First is\ngoal-conditioned behavior cloning. In Steve-1 (Lifshitz et al., 2024), authors added linear projection\nof a text embedding to the image embedding xt and fine-tuned the whole VPT model for behavior\ncloning from human demonstration data. This makes Steve-1 a text-conditioned VPT. In GROOT\n(Cai et al., 2023b), authors used gated cross-attention layers (Alayrac et al., 2022) in Transformer-XL\n(Tr-XL) to condition the video of some tasks (e.g., log wood). GROOT was trained for behavior\ncloning from reference videos of human plays working as video-conditioned VPT.\nThe second is RL fine-tuning for the single task. In DECKARD (Nottingham et al., 2023) and PTGM\n(Yuan et al., 2024), VPT was fine-tuned for single task with PPO (Schulman et al., 2017) by attaching\nadaptor (Houlsby et al., 2019) in Tr-XL layers, and value head ˆvt = vθ(zt). When fine-tuning, only\nthe adaptors, policy head, and value head were updated. For learning stability, those works used\ndifferent KL loss in PPO objective, which is KL loss between fine-tuning policy and VPT policy to\nkeep the VPT’s prior knowledge.\nSince our target is to train goal-conditioned navigation policy, one way to achieve this is to naively\ncombining ideas from 1) goal-conditioned behavior cloning, and 2) RL fine-tuning for the single task.\nWe first make goal embedding G(lt, lG) from the agent’s location lt and goal location lG with goal\nencoder G(·), then add it to image embedding xt. Second, we attach the adaptor in Tr-XL layers, and\nvalue head. Then, we fine-tune the whole model or only adaptors and policy, value heads with PPO.\nHowever, we found that this naive approach showed suboptimal navigation behavior in complex\nterrains such as mountain and river. We speculated that this is because RL objective is rather weak\nlearning signal compared to behavior cloning, which causes hardship in giving information of goal\nembedding to the policy head. Also, some information of goal embedding may be corrupted while\nit is added to image embedding, and processed by Tr-XL layers. Thus, we made the following\nmodifications in the model architecture. First, instead of giving goal embedding in Tr-XL input,\nwe added the goal embedding to policy head input. Second, we used recently proposed adaptor for\nTr-XL, which is LoRA (Hu et al., 2021b). The following changes enabled VPT-Nav to exhibit optimal\nnavigation behavior. We provide an investigation of this model architecture search in Appendix L.2.\nWith the changes in previous paragraph, the modified VPT for navigation with the learning parameters\nψ has the following components:\nImage Encoder:\nxt = IE\nθ (it)\nTransformer-XL:\nzt−T :t = TrXLψ(xt−T , · · · , xt)\nGoal Conditioning:\nz′\nt = GE\nψ(lt, lG) + zt\nPolicy Head:\nˆat ∼πψ(at|z′\nt)\nValue Head:\nˆvt = vψ(z′\nt),\n(2)\nHere, goal encoder GE\nψ is 4-layer MLP, and value head is a randomly initialized single linear layer.\nParameter ψ in Tr-XL indicates LoRA parameters. We use PPO for training with reward based on the\nincrease or decrease in Euclidean distance between locations of the goal and the agent. When the\nagent reaches the goal location within 3 blocks, it is considered a success, and an additional reward\nof 100 is given. The detailed hyper-parameters for training will be found in Table 5.\nL.2\nMODEL ARCHITECTURE SEARCH FOR GOAL-CONDITIONED VPT NAVIGATOR\nIn the previous section, we discussed the naive way of combining the idea of goal-conditioned\nbehaivor cloning and RL finetuning for training goal-conditioned navigator. In this section, we\nconduct an model architecture search on VPT model to find the optimal goal-conditioned navigation\nmodel. To do this, we focused on three key design choices: 1) the input location of goal embedding,\n2) training parameters, and 3) different Tr-XL adaptors . For the location of goal embedding, we\n32\nPreprint\nTable 5: Hyper-parameters for the Goal-Conditioned Navigation VPT Training.\nName\nValue\nInitial VPT Model\nrl-from-foundation-2x\nDiscount Factor\n0.999\nRollout Buffer Size\n40\nTraining Epochs per Iteration\n5\nVectorized Environments\n4\nLearning Rate\n10−4\nKL Loss Coefficient\n10−4\nKL Loss Coefficient Decay\n0.999\nTotal Iteration\n400K\nSteps per Iteration\n500\nGAE Lambda\n0.95\nClip Range\n0.2\nconsider the three tactics: (1) add the goal embedding to the image embeddings and input to Tr-XL\n(TrXL-Cond.), (2) add the goal embedding to Tr-XL output (Head-Cond.), and (3) add the goal\nembedding both at the input and output of Tr-XL (Dual-Cond.). For training parameters, we use\nfull-finetuning or finetuning only part of the model. For the Tr-XL adaptors, we consider the adaptor\nfrom Houlsby et al. (2019), and LoRA adaptor (Hu et al., 2021b). In Figure 15, we illustrate the\ndifferent model architectures for goal-conditioned VPT finetuning models.\nFor the model search, we summarized the navigation performance (SPL and Success Rate (SR)) of\ndifferent model architectures in Table 6. Interestingly, we found that adding goal embedding to image\nembeddings (TrXL-Cond.) showed suboptimal behavior, while adding goal embedding to output of\nTr-XL (Head-Cond., Dual-Cond.) showed good performance indicating that propagating learning\nsignal to goal embedding through Tr-XL is difficult from RL objective. We note that using LoRA\nadaptor showed higher performance than adaptors from Houlsby et al. (2019). In conclusion, using\nHead-Cond. with LoRA finetuning performed the best. Additionally, we tested one additional model\nthat does not update Tr-XL, which showed comparable result to the best performing model. This\narchitecture benefits for using smaller number of learning parameters and lower gradient computations.\nThus, we use the VPT-Nav trained with one of two architectures, which are Head-Cond. with LoRA\nfinetuning, and Head-Cond. with No Tr-XL Update.\nFigure 15: Four Types of Navigation Goal-Conditioning.\nL.3\nVPT NAVIGATOR ABLATION STUDY\nWe investigate how KL loss in PPO objective (i.e., KL loss between fine-tunin VPT and original\nVPT) affects the performance of VPT-Nav in diverse environments (Table 7). We measured the\nSPL and Success Rate (SR) of the navigators in navigation tasks where the goal location is within\n10–20 blocks away from agent’s location. While Heuristic method performs best in Flat, Plains, and\nMountain tasks, VPT-Nav with KL coefficient 10−4 showed high performances in all tasks. In case of\nlow-level navigator in Plan4MC (Yuan et al., 2023) which uses DQN policy, we observed suboptimal\n33\nPreprint\nTable 6: VPT-Nav Performance of different Goal-Conditioning Methods. Top-1 performances are bolded.\nConditioning\nFine-tuning\nFlat\nPlains\nMountain\nRiver\nTrXL\nFull\n0.050 (5%)\n0.423 (46%)\n0.000 (0%)\n0.000 (0%)\nHoulsby et al. (2019)\n0.488 (56%)\n0.305 (34%)\n0.052 (32%)\n0.000 (0%)\nLoRA\n0.481 (55%)\n0.475 (51%)\n0.058 (42%)\n0.000 (0%)\nDual\nHoulsby et al. (2019)\n0.883 (95%)\n0.828 (90%)\n0.125 (94%)\n0.066 (31%)\nLoRA\n0.798 (96%)\n0.762 (90%)\n0.169 (100%)\n0.287 (100%)\nHead\nNo TrXL Update\n0.849 (96%)\n0.780 (88%)\n0.157 (100%)\n0.274 (87%)\nHoulsby et al. (2019)\n0.729 (85%)\n0.841 (91%)\n0.159 (100%)\n0.052 (23%)\nLoRA\n0.904 (98%)\n0.880 (95%)\n0.150 (100%)\n0.307 (100%)\nbehavior. We note that VPT-Nav is robust to tasks with difficult terrains such as Mountain, and River\nsince VPT (Baker et al., 2022) is trained from human demonstration data which has high-quality\nnavigation ability. Also, using large KL coefficient (e.g., 10−2) harmed the overall performance while\nusing small KL coefficient (e.g., 0) harmed the robustness of navigator in complex tasks (e.g., River).\nTable 7: Performance of different Low-Level Navigators. Top-2 performances are bolded.\nSPL (Success Rate)\nFlat\nPlains\nMountain\nRiver\nHeuristic\n0.962 (100%)\n0.906 (94%)\n0.188 (100%)\n0.000 (0%)\nLow-level Navigator in Plan4MC\n0.416 (61%)\n0.326 (47%)\n0.010 (10%)\n0.000 (0%)\nVPT-Nav (KL = 0)\n0.806 (90%)\n0.774 (85%)\n0.131 (97%)\n0.034 (13%)\nVPT-Nav (KL = 10−4)\n0.849 (96%)\n0.780 (88%)\n0.157 (100%)\n0.274 (87%)\nVPT-Nav (KL = 10−3)\n0.762 (95%)\n0.651 (78%)\n0.015 (17%)\n0.012 (6%)\nVPT-Nav (KL = 10−2)\n0.692 (84%)\n0.702 (83%)\n0.078 (87%)\n0.014 (7%)\n34\nPreprint\nM\nLLM-AUGMENTED AGENT WITH MRSTEVE\nTable 8: Long-Horizon Planning Tasks details. The required subgoals denotes the length of shortest plan for\neach task. The items that the low-level controller should obtain are listed in Items to Obtain column.\nTask\nMax Steps\nRequired Subgoals\nItems to Obtain\noak_stairs\n3000\n4\nlog×3\nsign\n3000\n5\nlog×3\nfence\n3000\n5\nlog×3\nbed\n6000\n5\nlog×3, wool×3\npainting\n6000\n6\nlog×2, wool×1\ncarpet\n6000\n2\nwool×2\nitem_frame\n6000\n6\nlog×2, leather×1\nleather_boots\n6000\n5\nlog×1, leather×4\nleather_chestplate\n6000\n5\nlog×1, leather×8\nleather_helmet\n6000\n5\nlog×1, leather×5\nleather_leggings\n6000\n5\nlog×1, leather×7\nTable 9: Success Rate of two low-level controllers with DEPS (Wang et al., 2023c) planner.\nTask\nSteve-1\nMrSteve\noak_stairs\n67%\n80%\nsign\n53%\n60%\nfence\n40%\n50%\nbed\n27%\n50%\nIn this section, we investigate synergy between MrSteve and LLM-augmented hierarchical agent\nframework with long-horizon planning tasks listed in Table 8. We follow DEPS (Wang et al., 2023c)\nas LLM-augmented high-level planner with two low-level controllers: Steve-1 and MrSteve.\nOnce the target object for a task is given, the high-level planner asks the LLM to make the initial\nplan Pt = {τ g\nt,i}N\ni=1 for the task, where τ g\nt,i is i-th subgoal at timestep t in the textual form. After the\ninitial plan is given, the low-level controller executes the subgoal sequentially. Each subgoal is mine\nor craft type, where craft subgoals are executed heuristically via MineDojo functional actions,\nwhereas mine subgoals are executed by low-level controllers.\nLLM-generated initial plans can be inaccurate, so the low-level controllers often fails to execute\nsubgoals. For instance, we found that the initial plans frequently omit the subgoal for creating crafting\ntable, which is prerequisite item for the tasks in Table 8 except the carpet task. To address this\nproblem, DEPS framework introduced the replanning procedure with the descriptor and explainer\nmodules. The descriptor module makes a text prompt representing the inventory of the agent. The\nexplainer modules ask the LLM the reason of failure based on the text prompt from the descriptor\nmodule. Based on the explanation, the high-level planner asks the LLM to revise the plan and the\nlow-level controller executes subgoals from the revised plan.\nAdditionally, bacause the LLM does not observe the environment, the order of subgoals in a plan can\nbe suboptimal. When some subgoals share the same prerequisite so they can be executed in any order,\nit is more efficient to execute subgoal, which can complete faster than the other subgoals, first. This\ncan prevent wasting time by giving up subgoals that could be completed quickly, pursuing a subgoal\nwhose completion time is uncertain, and then going back to search for the previously quicker task\nagain after executing the subgoal. In DEPS, this procedure called elector module is implemented with\nthe horizon prediction module in GSB (Cai et al., 2023a). The horizon prediction module µ(st, τ g\nt,i)\ntakes the current observation st and the textual subgoals {τ g\nt,i}N\ni=1 and predicts the time to complete\neach subgoal. Based on the time prediction, a subgoal to be executed is sampled from the distribution\nas follows:\nSelector(τ g\nt,i; st, Pt) =\nexp(−µ(st, τ g\nt,i))\nP\nj exp(−µ(st, τ g\nt,j)).\n(3)\n35\nPreprint\nTable 10: Success Rate of two low-level controllers with the Ground-Truth plan for each task.\nTask\nSteve-1\nMrSteve\noak_stairs\n80%\n83%\nsign\n70%\n67%\nfence\n67%\n70%\nbed\n37%\n60%\npainting\n60%\n73%\ncarpet\n43%\n60%\nitem_frame\n53%\n63%\nleather_boots\n13%\n33%\nleather_chestplate\n3%\n17%\nleather_helmet\n20%\n20%\nleather_leggings\n0%\n13%\nWe report success rates of DEPS framework with Steve-1 and MrSteve in Table 9. We use Qwen2.5-\n72B (Team, 2024) as LLM in the high-level planner. DEPS with MrSteve shows comparable\nperformance or outperforms compared to DEPS with Steve-1. Especially, there is a huge performance\ngap between the two low-level controllers in the bed task, which requires killing three sheeps. We\nobserve that when the agent hit a sheep, it runs away from the agent and the agent chases it until\nthe agent gets wool items. After this, Steve-1 easily forgets the place where other sheeps exist\nand try to find sheep. However, MrSteve avoids this redundant exploration utilizing the episodic\nmemory. Hence, this results highlight the importance of episodic memory for low-level controllers in\nLLM-augmented hierarchical agent frameworks.\nIn Table 10, we also report success rates of hierarchical agents with Steve-1 and MrSteve and\nthe optimal plan for each plan. In this setting, because there is no instability from the LLM, the\nperformance is solely determined by performance of low-level controllers. Based on this experiment,\nalthough the goal is optimal, Steve-1 shows lower performance compared to MrSteve, indicating\nlow-level controllers is a performance bottleneck of hierarchical agent frameworks.\n36\nPreprint\nN\nTASK-CONDITIONED HIERARCHICAL EPISODIC EXPLORATION\nTable 11: Performance Comparison of Two Exploration Methods. We report success rates and the average and\nstandard error of execution times for the explore mode in ABA-Sparse tasks.\nTask-Conditioned Exploration\nTask-Free Exploration\nSuccess Rate\nExplore Mode Length\nSuccess Rate\nExplore Mode Length\nBeef-Log-Beef\n93%\n1202.64 ± 67.49\n92%\n1512.00 ± 62.37\nBeef-Leaves-Beef\n90%\n1269.23 ± 66.57\n91%\n1496.10 ± 69.43\nWool-Sand-Wool\n98%\n924.98 ± 56.85\n93%\n1350.00 ± 78.80\nWool-Dirt-Wool\n84%\n1303.21 ± 117.04\n84%\n1452.00 ± 89.92\nMilk-Sand-Milk\n86%\n1197.29 ± 71.44\n83%\n1526.48 ± 63.31\nMilk-Leaves-Milk\n59%\n1224.24 ± 68.79\n62%\n1579.35 ± 64.61\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTimestep\n0.0\n0.1\n0.2\n0.3\n0.4\nVisited Pasture Ratio\nTask-Free\nTask-Conditioned\nFigure 16: Given the task is “find cow”, the ratio of\npastures among the locations explored over time is pre-\nsented for two agents, each employing a different explo-\nration method.\nIn this section, we investigate an advanced ver-\nsion of hierarchical episodic exploration. The\ncurrent exploration method selects the next ex-\nploring position based on the visitation map gen-\nerated from PEM, while it does not use the infor-\nmation inherited in PEM. Hence, the current ex-\nploration method is task-free exploration. While\nnot using information from PEM worked pretty\nwell in practice, using knowledge from PEM\nmay have better explorative behaviors. Thus,\nwe implemented a new task-conditioned explo-\nration method which exploits information stored\nin PEM and evaluated two exploration methods\non exploration and task-solving tasks.\nThe two exploration methods have the same\nprimary goal: to visit the least visited places\nfirst. The difference between the two explo-\nration methods is how to select one of the least-visited places. The task-free exploration method\nrandomly selects the next exploring position among the least-visited locations. In contrast, the\ntask-conditioned exploration method selects the next exploring position among the least-visited\nlocations that are estimated to be related to the task based on the information inherited in PEM.\nTo implement the task-conditioned exploration method, the high-level exploration policy first accesses\nall event clusters when selecting the next exploring position. Using MineCLIP, the task-relevant score\nfor each event cluster is estimated by calculating the alignment score between the center embedding\nof the event cluster and the text prompt. If we use the text prompts for MineCLIP listed in Table 3,\nthe alignment score for locations where the target object has not yet been observed will tend to be\nlow, even if the location is relevant to the task, potentially hindering the exploration of those areas.\nTo address this problem, we used text prompts that describe places related to the task. For instance,\nthe text prompt for log\nis set to “near pasture”, while the text prompt for sand\nis set to “near\ndesert.” After that, Event clusters with alignment scores exceeding the threshold are collected, and a\ntask-relevance map is constructed to represent the agent’s FoVs of these event clusters, similarly to\nthe visitation map building method described in Appendix F.2.\nAdditionally, the 3 × 3 box blur filter is applied to the task-relevance map to introduce an inductive\nbias, assuming that if something is near X, it is likely to also be X. Finally, based on the task-\nrelevance map, the high-level exploration policy selects the next exploring position by prioritizing\nlocations that are the least visited, most task-relevant, and closest to the agent.\nFigure 16 shows the proportion of task-relevant locations among the places explored by the agent,\ndemonstrating that the task-conditioned exploration method explores task-relevant locations more\nquickly in the early stages. We used Map 2 in Figure 8 and set the task as “find cow,” and therefore\nmeasured the proportion of pastures, which are relevant to this task. In the early stages of the episode,\nthe task-conditioned exploration method explored pastures more than the task-free exploration method.\nAfter approximately 2000 steps, however, the exploration tendencies of both methods became similar.\n37\nPreprint\nTable 11 shows performance comparison between the two exploration methods in ABA-Sparse tasks.\nWe report success rates and execution times for the explore mode from MrSteve with the task-free and\ntask-conditioned exploration methods. The success rates between the two exploration methods are\ncomparable, while the task-conditioned exploration is finished earlier than the task-free exploration.\nThrough the results of the two experiments, we confirmed that fully utilizing PEM not only aids in\ntask-solving but also optimizes exploration more effectively.\nO\nABA-Sparse TASKS WITH MEMORY CONSTRAINTS\n0.1K\n0.5K\n1K\n2K\n12K\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Dirt-Wool\n0.1K\n0.5K\n1K\n2K\n12K\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.1K\n0.5K\n1K\n2K\n12K\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\n0.1K\n0.5K\n1K\n2K\n12K\nMemory Limit\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Sand-Wool\n0.1K\n0.5K\n1K\n2K\n12K\nMemory Limit\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Leaves-Beef\n0.1K\n0.5K\n1K\n2K\n12K\nMemory Limit\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Leaves-Milk\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nSteve-1\nFigure 17: Success Rate Comparison between MrSteve with its memory variants and Steve-1 in ABA-Sparse\ntasks with memory constraints. We tested different memory capacities (0.1K to 12K) for each model. In all tasks,\nthe performance of MrSteve-FM decreases as memory capacity gets smaller. We note that MrSteve is robust\nto memory capacities across tasks, while MrSteve-PM, and MrSteve-EM showed performance degradation in\nBeef-Log-Beef and Beef-Leaves-Beef tasks.\nIn this section, we investigate how MrSteve and its memory variants perform in ABA-Sparse tasks\nin Section 4.2 when memory capacity is limited. We evaluated each model with different memory\ncapacity ranging from 0.1K to 12K which is the maximum episode length. In all tasks, we observe\nthat the performance of MrSteve-FM decreases as the memory capacity gets small. This is because\nFIFO Memory in MrSteve-FM losts relevant frames in first task A while solving task B. While\nMrSteve showed robust performance to constrained memory capacities across tasks, MrSteve-PM,\nand MrSteve-EM showed degraded performances in Beef-Log-Beef and Beef-Leaves-Beef tasks\nwhen memory capacity is 0.1K. This indicates the robustness of PEM to memory capacities in\nABA-Sparse tasks.\nP\nPLACE EVENT MEMORY INVESTIGATION\nAlthough our place event memory enables efficient querying by clustering experience frames, it\nstores not only the center embedding of each event cluster but also all the frames that constitute the\nevent clusters. Since the frames within each cluster contain highly similar information, storing all of\nthem can increase redundancy in the memory system, potentially degrading storage efficiency and\nretrieval performance. Therefore, we attempted to optimize the memory-storing method of PEM in\nthe simplest way possible to investigate whether reducing the storage of redundant information could\nachieve better efficiency.\nWe implemented the modified PEM, which stores only center embeddings. The write and read\noperations of the modified PEM are the same as the original PEM. However, once event clustering\nis executed, the modified PEM stores only the center embedding of each event cluster and removes\nthe embeddings of other frames. In our experiment settings, event clustering is performed when the\ndummy deque of a place cluster has 100 frames, and until then, frames are stored in the dummy\ndeque of a place cluster, with each place cluster capable of holding up to 100 frames. However, even\n38\nPreprint\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Dirt-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Sand-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Leaves-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Leaves-Milk\nMrSteve (Center Embed)\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nSteve-1\nFigure 18: Success Rates Comparison between MrSteve with its memory variants and Steve-1 in ABA-Sparse\ntasks. We additionally evaluated MrSteve with modified place event memory, named MrSteve (Center Embed),\nthat stores center embeddings only.\nin such cases, memory read operation is highly optimized by accessing only the oldest frame in each\nplace cluster during querying operations.\nFigure 18 shows the performance comparison between MrSteve with its memory variants and Steve-1\nin ABA-Sparse tasks. Surprisingly, the performance of the simplest optimized PEM, named MrSteve\n(Center Embed), was not dropped significantly. Nevertheless, the computational cost for the querying\noperation can be reduced. After 12K environmental steps, 305.52 event clusters, on average, were\ngenerated across all tasks, with a standard error of 4.98. The original PEM stores all 12K frames and\neach event cluster in PEM holds approximately 40 frames on average. In the end, PEM calculates\nthe alignment scores between frames from the top-30 relevant event clusters and the task instruction,\nresulting in 1.2K comparisons. However, the modified PEM calculates the alignment scores for\naround 0.3K frames, making it more efficient.\nThis experiment result demonstrates the potential to further optimize the PEM memory-storing\nmethod. However, our simplest approach resulted in a slight performance drop. Hence, optimizing\nmemory to further reduce redundancy without losing important frames is a worthwhile direction for\nfuture work.\n39\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nMrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory\n```\n#### 2. 论文摘要\n```\nSignificant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMrSteve (Memory Recall Steve-1), a novel low-level controller equipped with\nPlace Event Memory (PEM), a form of episodic memory that captures what, where,\nand when information from episodes. This directly addresses the main limitation\nof the popular low-level controller, Steve-1. Unlike previous models that rely\non short-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https:\/\/sites.google.com\/view\/mr-steve.\n```\n\n#### 3. 论文全文\n```\nPreprint\nMRSTEVE: INSTRUCTION-FOLLOWING AGENTS\nIN MINECRAFT WITH WHAT-WHERE-WHEN MEMORY\nJunyeong Park1∗, Junmo Cho1∗, Sungjin Ahn1\n1KAIST\nABSTRACT\nSignificant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented hierar-\nchical approaches. While these approaches, which combine high-level planners\nwith low-level controllers, show promise, low-level controllers frequently become\nperformance bottlenecks due to repeated failures. In this paper, we argue that the\nprimary cause of failure in many low-level controllers is the absence of an episodic\nmemory system. To address this, we introduce MrSteve (Memory Recall Steve-1),\na novel low-level controller equipped with Place Event Memory (PEM), a form of\nepisodic memory that captures what, where, and when information from episodes.\nThis directly addresses the main limitation of the popular low-level controller,\nSteve-1. Unlike previous models that rely on short-term memory, PEM organizes\nspatial and event-based data, enabling efficient recall and navigation in long-horizon\ntasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented\nTask Solving Framework, allowing agents to alternate between exploration and\ntask-solving based on recalled events. Our approach significantly improves task-\nsolving and exploration efficiency compared to existing methods. We will release\nour code and demos on the project page: https:\/\/sites.google.com\/view\/mr-steve.\n1\nINTRODUCTION\nThe emergence of large-scale foundation models has driven significant advances in developing\ngeneral-purpose embodied AI agents capable of generalizing across a broad spectrum of tasks in\ncomplex, open, and real-world-like environments (Johnson et al., 2016; Guss et al., 2019; Fan et al.,\n2022b; Hafner; Albrecht et al., 2022; Voudouris et al., 2023). While simulating such environments for\neffective learning and evaluation remains a major challenge, Minecraft has become a leading testbed,\noffering a demanding, open-ended environment with rich interaction possibilities. Its procedurally\ngenerated world presents agents with challenges like exploration, resource management, tool crafting,\nand survival, all requiring advanced decision-making and long-horizon planning. For instance, the\ntask of obtaining a diamond\nrequires agents to locate diamond ore\n, and craft an iron pickaxe\n. This process involves finding, mining, and refining iron ore\n, requiring the agent to execute\ndetailed long-term planning over roughly 24,000 environmental steps (Li et al., 2024).\nSolving such tasks through Reinforcement Learning (RL) approaches from scratch is nearly infeasible;\nhowever, recent LLM-augmented hierarchical methods have demonstrated a promising avenue (Huang\net al., 2022a;b; Wang et al., 2023a). These methods feature a division between high-level planners\nand low-level controller policies. High-level planners, driven by Large Language Models (LLMs) or\nMultimodal Large Language Models (MLLMs), propose subgoals by utilizing the reasoning abilities\nand prior knowledge inherent in LLMs (Brown et al., 2020; Touvron et al., 2023; OpenAI, 2024).\nThese subgoals, conveyed in textual instruction form, are then sequentially passed to a learned,\ninstruction-following low-level controller for execution (Wang et al., 2023c;b; Li et al., 2024).\nFor this framework to be effective, it is essential that both the high-level planner and the low-level\ncontroller improve in tandem. However, previous research has primarily focused on enhancing\nhigh-level planning, e.g., via maintaining skill library (Zhu et al., 2023; Wang et al., 2023b; Qin et al.,\n∗Equal contribution. Correspondence to Junyeong Park and Sungjin Ahn.\nContact:{jyp10987,sungjin.ahn}@kaist.ac.kr\n1\narXiv:2411.06736v4  [cs.LG]  25 Dec 2024\nPreprint\n2024; Li et al., 2024), often assuming that low-level controllers will efficiently execute the subgoals\nprovided by the high-level planner. However, this assumption frequently does not hold in practice,\nand the low-level controller becomes a significant performance bottleneck (Cai et al., 2023b).\nIn this regard, we specifically focus on limitations in Steve-1 (Lifshitz et al., 2024), the most widely\nused low-level instruction-following controller framework. Steve-1 is an instruction-following policy\nobtained by fine-tuning the Video Pre-Training (VPT) (Baker et al., 2022) model. A primary limitation\nwe focus on is its constrained episodic memory capability. Steve-1 is based on Transformer-XL (Dai\net al., 2019), which leverages relatively short-term memory, retaining only the last 128 hidden states.\nGiven Minecraft’s simulation speed of 20Hz, this memory span amounts to only a few seconds of\ngameplay. While it can be increased, the quadratic complexity and FIFO-only memory structure of\ntransformers make them significantly inefficient for long-horizon tasks.\nAs a result, when the agent requires information beyond this short memory span, it tends to forget past\nevents within the episode and reverts to inefficient random exploration for each new task, consuming\nexcessive time. For example, when given a task like “Find a Cow”, the agent is unable to recall,\n‘I’ve seen it before near the river in the north’. Ideally, a low-level agent would instead maintain an\nepisodic memory of meaningful events and recall relevant information. See Figure 1 for more detail\nillustration. Moreover, Steve-1 not only lacks the ability to recall such memories but also the ability to\nnavigate directly to the associated locations, which could help avoid unnecessary exploration. Instead,\nSteve-1 relies on a “Go Explore” instruction, randomly exploring until it stumbles upon the resource\nby chance. This inefficiency in executing low-level primitives is not addressed by high-level planners,\nwhich focus on optimizing the sequence of high-level skills (i.e., the plan) but do not optimize the\nexecution of the primitives themselves. We elaborate more on this in Appendix A.\nIn this paper, we introduce an enhanced low-level controller agent, MrSteve (Memory Recall Steve-1),\ndesigned to address the limitations of Steve-1. The key innovation of MrSteve is the integration of\nPlace Event Memory (PEM) which is the instantiation of What-Where-When Episodic Memory.\nWhile previous approaches have explored episodic memory, they primarily target high-level planners,\nsuch as building libraries of high-level skills and plans, which do not directly improve the low-level\ncontroller’s performance. We argue it is essential for the low-level controller to possess memory\ncapabilities. To address this, PEM manages memory more effectively, surpassing the limitations of the\nnon-scalable FIFO memory found in transformers. PEM stores spatial and event-based information,\nallowing the agent to hierarchically organize and retrieve details about locations and events it has\npreviously encountered. For PEM to be fully effective, the agent must also move directly to the\ndesired location along with the ability to modulate between exploration and goal-directed navigation-\nand-execution, a capability lacking in Steve-1. Therefore, we introduce the second component of\nMrSteve: the Exploration Strategy and Memory-Augmented Task Solving Framework. Built upon\nthe PEM structure, this framework enables the agent to alternate between exploration—when no\nrelevant information is stored—and task-solving by recalling past events when applicable. This is\nmade possible and effective with our new navigation policy, VPT-Nav.\nOur contributions are as follows. First, we point out the limitations of Steve-1, the most widely\nused instruction-following controller, and show how its bottlenecks can be addressed with MrSteve.\nSecond, we introduce Place Event Memory (PEM), a novel hierarchical memory system that orga-\nnizes spatial and event-based data for efficient querying and storage, even under limited memory\ncapacity. Third, we propose an Exploration Strategy and Task Solving Module built on PEM that\nenables efficient exploration while maintaining high task-solving performance in Minecraft. Last, we\ndemonstrate that our agent significantly outperforms existing baselines in both exploration and long\nsequence of tasks solving. We will release the code for further research.\n2\nRELATED WORKS\nLow-Level Controllers in Minecraft Earlier works (Guss et al., 2019; Lin et al., 2021; Mao\net al., 2022; Cai et al., 2023a; Hafner et al., 2023; Zhou et al., 2024a) introduced policy models\nfor simple tasks in Minecraft. MineCLIP (Fan et al., 2022b) leveraged text-video data to train a\ncontrastive video-language model as a reward model, while VPT (Baker et al., 2022) was pre-trained\non unlabelled videos without text-based instruction input. Steve-1 (Lifshitz et al., 2024) extended\nVPT by incorporating text instructions to generate low-level actions based on human demonstration\ndata. GROOT (Cai et al., 2023b) used reference video instead of text for goal-conditioned behavior\n2\nPreprint\nFigure 1: Sparse Sequential Task Solving Scenario. The first task is to obtain a log. The agent explores to find a\ntree. While searching, the agent observes a cow but continues focusing on acquiring the log. Once the log is\nobtained, the next task is to obtain a water bucket. Remembering that it already explored the forward direction\nwhile searching for the tree, the agent chooses to explore to the right. After gathering the water bucket, the\nfinal task is obtain meat, which can be acquired from the cow. Recalling the cow’s location, the agent navigates\nthere and completes the task by obtaining the meat. Note that each task takes a few thousand steps to achieve.\nThis scenario highlights the significance of episodic memory for efficient exploration and task-solving in an\nopen-ended world where task-relevant resources are sparsely distributed.\ncloning. Recently, MineDreamer (Zhou et al., 2024b) leveraged Steve-1 generating subgoal images\nwith MLLM and Diffusion based on text and current observation for improved control. However,\nthese agents lack episodic memory, forcing agents to start new tasks from scratch. MrSteve addresses\nthis by integrating episodic memory, making it more effective in sequential tasks.\nLLM-Augmented Agents The development of LLMs has significantly advanced agents in Minecraft\n(Wang et al., 2023a;b). These works utilize pre-trained LLMs as zero-shot planners (Brown et al.,\n2020; Touvron et al., 2023), leveraging their powerful reasoning capabilities to generate subgoal plans\nor executable code. Broadly, this line of research can be divided into two approaches: one that uses\nLLMs for code generation to interact with the environment directly (Wang et al., 2023a; Zhu et al.,\n2023; Qin et al., 2024; Liu et al., 2024), and another that generates text-based subgoals which are\nthen executed by a goal-conditioned low-level controller, such as Steve-1 or programmed heuristics\n(Nottingham et al., 2023; Yuan et al., 2023; Li et al., 2024). In the latter approach, to ensure LLMs\nfocus on high-level semantic reasoning, the low-level controller must efficiently execute subgoals.\nWhile combining LLM as a high-level planner with MrSteve is one possible direction, we focus on\nenhancing low-level controller’s capabilities based on the new type of memory in this work.\nMemory in Agents Memory systems in agents primarily aim to retrieve robust and accurate high-\nlevel plans for long-horizon tasks (Zhang et al., 2023; Song et al., 2023; Kagaya et al., 2024; Sun\net al., 2024; Shinn et al., 2024). Existing works store successful task’s text instruction and its plans in\nlanguage often with observations for robust retrieval, which is useful when plans for the new task\nalready exist in memory. Voyager (Wang et al., 2023a) uses an unimodal storage of achieved skill\ncodes in the form of text. GITM (Zhu et al., 2023) integrates text-based knowledge and memory\nfor higher reasoning efficiency and stores entire skill codes after a goal is achieved. Recently, MP5\n(Qin et al., 2024) and JARVIS-1 (Wang et al., 2023b) enhance planning by storing plans and whole\nmultimodal observations in the abstracted memory, allowing for situation-aware retrieval, while\nOptimus-1 (Li et al., 2024) introduces a multimodal experience pool that summarizes all multimodal\ninformation during agent’s execution of the task improving storage and retrieval efficiency. However,\nthese memory systems store the sequence of high-level skills or plans for high-level planners, which\nare not optimized for low-level controllers. We address this problem with Place Event Memory.\n3\nMETHOD\nIn this section, we describe our agent, MrSteve (Memory Recall Steve-1). We begin with the problem\nsetting, followed by step-by-step construction of our agent’s main modules.\nProblem Setting In this work, we define a sparse sequential task scenario where the agent is\ncontinuously given tasks {τn}∞\nn=1 through text instructions (e.g., Obtain water bucket) from the\n3\nPreprint\n(b) Place Event Memory\nE1\nE2\nE1\nE1\n⋯\no12, p12\nT\ntAmjAqSMNQw0g7UgTxgJFWMLpJ\/dYjUZpKcW\/GEfE5GgaUoyMlR5kLzl3J6cwyv5eqexW3AxwkXg5KYMc9V7p9uXOZEGMyQ1h3PjYyfIGUoZmRS7MaRAiP0IB0LBWIE+0n2dYTeGyVPgylsk8YmKl\/OxLEtR7zwFZyZIZ63kvF\/7xObMIrP6Eig0ReDojBk0EqYRwD5VBs2tgRhRe2uEA+RQtjYoGamBHzmhmQkDXlKY\/LmQ1kzbOKV61U7y7Ktes8sAI4BEfgBHjgEtTALaiDBsBAgWfwCt6cF+fd+XA+p6VLTt5zAGbgfP0CRLCgWg=<\/latexit>o30, p30\nk\n6ANGFUkKahpFOpAjiASPtYHyT+u1HojSV4t5MIuJzNBQ0pBgZKz3IflJ1p+cwyv5+qexW3AxwmXg5KYMcjX7pzeQOZEGMyQ1l3PjYyfIGUoZmRa7MWaRAiP0ZB0LRWIE+0n2dZTeGqVAQylsk8YmKl\/OxLEtZ7wFZyZEZ60UvF\/7xubMIrP6Eig0ReDYojBk0EqYRwAFVBs2sQRhRe2uEI+QtjYoOamBHzuhmQsDXlKY\/IWQ1kmrYuKV6vU7qrl+nUeWAEcgxNwBjxwCergFjRAE2CgwDN4BW\/Oi\/PufDifs9IVJ+85AnNwvn4BSAigXA=<\/latexit>o40, p40\nPlace Cluster 1, Event Cluster 1\n⋯\no48, p48\no120, p120\no132, p132\nPlace Cluster 1, Event Cluster 2\n⋯\no287, p287\no295, p295\no300, p300\nPlace Cluster 3, Event Cluster 1\nPlace Cluster\nE\nEvent Cluster\nAgent’s Trajectory\n(a) Agent Overview\nText\nInstruction ⌧n\not\nlt\nEnvironment\nSolver Module\nMemory Module\nPlace Event \nMemory\nExplore\nExecute\nMode \nSelector\nMrSteve\nat\nCount-Based ⇡H-cnt\nVPT-Nav ⇡L-Nav\nVPT-Nav ⇡L-Nav\nSteve-1\nU\nSIJbZGIR7LrY0U5E7QFDjtxpLi0Oe04+v83rngUrFInEPk5i6IR4KFjCQUeWenHzEv7QJ8gvRUKswzq3bNnspaNk5hqhQ0zN\/+oOIJCEVQDhWqufYMbgplsAIp1m5nygaYzLGQ9rTVuCQKjednp5ZJzoZWEk9RNgTdO\/EykOlZqEvu4MYzUYi0P\/6v1Egu3ZSJOAEqyGxRkHALIivnYA2YpAT4RBtMJNO3WmSEJSagac1t8cO5P6TjSNPKMTmLUJZN+6zm1Gv1u\/Nq46oAVkJH6BidIgdoAa6QU3UQgQ9omf0it6MF+Pd+DA+Z60rRjFziOZkfP0CpIyjTQ=<\/latexit>⇡Inst\nFigure 2: MrSteve and Place Event Memory. (a) MrSteve takes agent’s position, first person view, and text\ninstruction, and utilizes Memory Module and Solver Module to follow the instruction. (b) MrSteve leverages\nPlace Event Memory for exploration and task execution, which stores the novel events from visited places.\nenvironment or subgoal plans by LLM. Additionally, we assume that task-relevant resources (e.g.,\nwater, cow) rarely exist and are sparsely distributed in the environment, making it essential to\nmemorize novel events from visited places for future tasks as shown in Figure 1. When an episode\nbegins, for every time step t, the agent is provided with the observation Xt = {it, lt, t}, which consists\nof the pixel observation it ∈RH×W ×C, representing the first person view of the environment, the\npositional information lt = (coordx, coordy, coordz, yaw, pitch) ∈R5, which denotes the agent’s\nrelative 3D position and camera angles with respect to initial position l0, and time t.\nInstruction Following Policy In sparse sequential task, a naive approach is to employ Steve-1\n(Lifshitz et al., 2024), an instruction-following policy πInst(at|ht, τn) that generates low-level controls\n(mouse and keyboard) in Minecraft. Here, ht is a past pixel observation sequence xt−128:t. While past\nobservations are processed by Transformer-XL layers in Steve-1, the model is ineffective at recalling\nobservations from a few thousand steps ago (Lampinen et al., 2021). Additionally, the Transformer’s\nquadratic complexity makes it significantly inefficient to process thousands of observations. This\nmakes Steve-1 poorly suited for sparse sequential task, as it cannot recall visited places or task-\nrelevant resources seen in the past. To address this, we propose MrSteve which stores novel events\nfrom visited places for efficient sparse sequential task-solving.\nMrSteve is a memory-augmented instruction following policy that consists of Memory Mod-\nule and Solver Module, as shown in Figure 2(a).\nIn Memory Module, we use the mem-\nory called Place Event Memory Mt that stores novel events from visited places (Figure 2(b)).\nAlgorithm 1 MrSteve Single Loop\nRequire: Memory Mt, and task τn\n1: candidates ←Read(Mt, τn)\n2: if candidates ̸= ∅then\n3:\nXt, lt = OneOf(candidates)\n4:\nNavigate to lt with πL-Nav\n5:\nExecute τn with πInst\n6: else\n7:\nExplore with πH-Cnt, πL-Nav\n8: end if\nBased on Place Event Memory, Mode Selector in Solver\nModule decides between Explore mode and Execute mode.\nWhen no task-relevant resource exists in the memory, Ex-\nplore mode is selected, and the agent explores with our\nhierarchical exploration method. If a task-relevant resource\nexists in the memory, Execute mode is selected, then the\nagent navigates to the resource’s position and executes πInst\n(i.e., Steve-1) to solve the task. Algorithm 1 outlines the\ntask-solving loop of MrSteve, which repeats every fixed\nstep or when a new task is given (More details in Appendix\nD). With these modules, MrSteve can efficiently explore\nand recall task-relevant resources from the memory to solve\nsparse sequential task in Figure 1. In the following sections, we describe how each module in MrSteve\nis constructed. We begin by constructing Memory Module.\n3.1\nMEMORY MODULE: CONSTRUCTION OF PLACE EVENT MEMORY\nThe simplest form of memory is FIFO Memory, denoted as Mt with capacity N. At every time step,\ninstead of storing Xt in Mt, we can extract a semantic representation from the video it−H:t with a\nvideo encoder to store an experience frame xt = {et, lt, t}, where et = Encv(it−H:t). For simplicity,\nwe term et as the video embedding at time step t. When the memory exceeds its capacity, the oldest\nframe is removed. For memory read, we calculate the cosine similarity between the task embedding\nˆτn = Enct(τn) and the video embedding et in Mt to retrieve task-relevant frames. Here, we use\n4\nPreprint\nFigure 3: Mode Selector and VPT-Nav in MrSteve. (a) Mode Selector with Place Episodic Memory. It decides\nagent’s mode (Explore or Execute) based on whether a task-relevant resource is in the memory. It uses a\nhierarchical read operation. (b) Architecture of Goal-Conditioned VPT Navigator.\nvideo encoder Encv, text encoder Enct, and H = 16 from MineCLIP (Fan et al., 2022a), which is a\nCLIP (Radford et al., 2021) trained on web videos of Minecraft gameplay and associated captions.\nWhile FIFO Memory offers benefits from its simple memory operations, it has two drawbacks. First,\nthe computational complexity of the read operation grows linearly with the memory size. Second, the\nbias toward removing the oldest frames can be problematic in sparse sequential task as in the Figure\n1 scenario, where task-relevant frames from visited places are lost.\nPlace Memory To address these issues, Place Memory (Cho et al., 2024) divides the agent’s positions\n{ℓn}t\nn=0 in the trajectory into clusters of distinct places, where each place cluster is assigned a FIFO\nMemory. Here, we term ℓt = (coordx, coordy, yaw) as agent’s position, which is a concatenation of\nagent’s top-down location and its head direction. Place Memory is represented as Mt = {Mk}K\nk=1,\nwhere Mk is the k-th place cluster with center position ℓMk, and center embedding eMk. Here, eMk\nis the video embedding whose position is closest to ℓMk. This structure improves the efficiency of\nthe read operation by extracting top-k place clusters with their center embeddings first, then fetching\nrelevant frames from these clusters. Furthermore, when memory capacity is limited, the oldest frame\nis removed from the largest place cluster, allowing the agent to retain memories in diverse places.\nWhile Place Memory prioritizes storing experience frames across diverse places, its FIFO structure\nwithin each cluster still loses novel experience frames in the past. For instance, if an agent stays in\na place where zombies burn and disappear for a long time, the place cluster removes the frames of\nburning zombies that can be crucial in upcoming tasks. This highlights the importance of focusing on\nvisually distinct experience frames rather than storing them sequentially, which can be redundant.\nPlace Event Memory To resolve this issue, we introduce Place Event Memory built on Place Memory,\nwhich captures distinct events that occur within each place cluster (Figure 2(b)). While Place Memory\nuses agent’s position to cluster experience frames, there is no criterion for clustering frames to form\nevents. To tackle this, we use the cosine similarity of video embeddings from MineCLIP for criterion.\nSpecifically, each place cluster Mk is subdivided into event clusters, denoted as {Ek\ni }dk\ni=1, where\neach Ek\ni represents the i-th event cluster in k-th place cluster, characterized by a center embedding\nek\nEi. These event clusters are newly created and updated as the place cluster accumulates a certain\nnumber of additional experience frames. For generating event clusters, DP Means algorithm (Dinari\n& Freifeld, 2022) is applied on video embeddings of these frames, generated by MineCLIP, and the\nresulting cluster centers become a center embedding of each cluster. If the cosine similarity of the\ncluster centers between a newly created cluster and an existing cluster is higher than threshold c (we\ndefine that two event clusters are indistinct), the two clusters are merged to prevent redundancy and\nensure distinct event clusters within each place cluster. When memory capacity is exceeded, the\noldest frame in the largest event cluster is removed, thus the memory can retain diverse places and\ndistinct events within each place. More details on Place Event Memory can be found in Appendix E.\n3.2\nSOLVER MODULE: MODE SELECTOR, EXPLORATION, AND NAVIGATION\nIn this section, we introduce the remaining components in Solver Module, which are Mode Selector,\nand hierarchical policies πH-Cnt, πL-Nav for episodic exploration, and goal-reaching navigation.\n5\nPreprint\nMode Selector Mode Selector decides between Explore and Execute mode by checking whether\ntask-relevant resource exists in the memory. If the resource exists, the agent chooses Execute mode,\nor Explore mode otherwise. When Place Event Memory is employed, Mode Selector first picks top-k\nevent clusters with task alignment score sik(τn) = CLIPt(τn)·CLIPv(ek\nEi) between task embedding,\nand center embedding of event cluster. Then, it calculates task alignment scores on experience frames\nin top-k event clusters and gathers frames with scores higher than task threshold h as shown in Figure\n3(a). We note that leveraging Place Event Memory offers computational efficiency with hierarchical\nread operation compared to FIFO Memory, which calculates the alignment scores on whole frames in\nthe memory. We provide a comparison of memory query time in Appendix K for further insights.\nHierarchical Episodic Exploration We propose a memory-based hierarchical exploration method\nthat allows the agent to efficiently explore the environment while minimizing revisits to previously ex-\nplored positions. This is achieved through a high-level goal selector πH-Nav(gt|ℓ′\nt, Mt) and a low-level\ngoal achiever πL-Nav(at|ht, gt), where ht = it−128:t, and ℓ′\nt, and gt are the agent’s current location,\nand goal location in (coordx, coordy), respectively. We introduce a Count-Based exploration strategy\n(Yamauchi, 1998; Tang et al., 2017; Chang et al., 2023) for the high-level goal selector.\nSpecifically, L × L visitation grid map mt is used with the agent’s starting location set as the center\nof the map. The locations of the agent’s trajectory are discretized and marked on the grid. The goal\nselector then divides the visitation map into grid cells of size G × G, and selects the location of\nthe grid cell with the lowest visitation count as the goal, gt. If multiple grid cells have the same\nminimum count, the cell closest to the current location ℓ′\nt is chosen. This approach directs the agent\ntoward unexplored locations, while minimizing unnecessary revisits. The size of grid cell can be\ndynamically adjusted to balance between broader exploration and finer local searches. Additionally,\nin an infinitely large map, the visitation map can be easily expanded by adding new grids, and further\nhierarchies on visitation maps can be introduced for efficiently managing explored locations.\nGoal-Conditioned VPT Navigator Once the goal location is selected by the high-level goal selector,\nit is crucial for the agent to navigate to the goal accurately. However, navigating complex terrains\n(e.g., river, mountain) requires human prior knowledge, where pure RL policy trained from scratch\nin prior work (Yuan et al., 2023) often shows suboptimal navigation ability. To address this, we use\nthe VPT as our starting policy, and fine-tune it for goal-conditioned navigation policy. We name this\npolicy as VPT-Nav. In VPT-Nav, we add goal embedding Gψ(lt, gt) in the output of TrXLθ in VPT\nwith LoRA adaptor (Hu et al., 2021a) as in Figure 3(b). We used PPO (Schulman et al., 2017) for\nfine-tuning goal encoder Gψ, LoRA parameters, policy πψ, and value vψ with reward based on the\ndistance to the goal location. We note that our VPT-Nav introduces several differences from prior\nVPT fine-tuning methods, thoroughly investigated in Appendix L.\n4\nEXPERIMENTS\nThis section presents a step-by-step validation of our agent MrSteve across various environments\nand conditions. We begin by evaluating the exploration and navigation ability of MrSteve, which\nis crucial in sparse sequential tasks (Section 4.1). Then, we demonstrate MrSteve’s capability to\nsolve A-B-A task sequentially where the memory is necessary to solve the task A twice (Section 4.2).\nAdditionally, we show that the proposed Place Event Memory outperforms other memory variants,\nparticularly when memory capacity is limited (Section 4.3). Lastly, we showcase the generalization\nof MrSteve to long-horizon sparse sequential task (Section 4.4). Each baseline and task is explained\nin each of the experiment sections with more details in Appendix C.\n4.1\nEXPLORATION & NAVIGATION FOR SPARSE SEQUENTIAL TASK\nIn this section, we evaluate the exploration and navigation ability of our agent. To verify this, we\nplaced an agent in a 100 × 100 block map with complex terrains such as mountains and a river, and\ngave 6K steps to wander around the map. Since successful exploration in Minecraft involves covering\nas much of the map as possible while minimizing revisits to previously visited locations, we measure\ntwo metrics: Map Coverage and Revisit Count. Map Coverage is calculated by dividing the map into\n11 × 11 grid cells and measuring the percentage of cells covered by the agent’s trajectory. Revisit\nCount measures the average number of times the agent visits the same grid cell.\n6\nPreprint\nTable 1: Map Coverage and Revisit Count of different exploration policies. Our exploration method (High-Level:\nCount-Based, Low-Level: VPT-Nav) performs the best.\nHigh-Level\nCount-based\nRNN-based\nSteve-1\nLow-Level\nVPT-Nav (Ours)\nDQN\nVPT-Nav\nDQN (Plan4MC)\nMap Coverage (↑)\n84.42 ± 0.06\n31.83 ± 0.11\n29.82 ± 0.07\n16.36 ± 0.04\n50.77 ± 0.13\nRevisit Count (↓)\n0.38 ± 0.6\n4.47 ± 1.43\n4.72 ± 1.73\n6.2 ± 1.73\n2.68 ± 1.36\nFigure 4: Agent’s trajectories of length 6K steps on 100 × 100 block map with different exploration methods.\nThe leftmost figure is the agent’s trajectory from our exploration method.\nTo demonstrate that our proposed hierarchical episodic exploration with Count-Based high-level goal\nselector and low-level goal achiever VPT-Nav is more effective for exploration in the given map, we\ncompared our method with the following baselines: Steve-1 (Lifshitz et al., 2024), and exploration\nmethod from Plan4MC (Yuan et al., 2023). For Steve-1, we provided the “Go Explore” instruction to\nassess its exploratory behavior as in prior works (Cai et al., 2023b; Zhou et al., 2024b). Plan4MC, on\nthe other hand, employs a hierarchical approach where the high-level RNN policy selects the next\ngoal location based on past locations, and a low-level DQN policy is used for goal-reaching. Since the\ninput-output space of our method and Plan4MC is identical, interchanging high-level and low-level\npolicies between two approaches is allowed so that we can evaluate the benefits of each component.\nAs shown in Table 1, and Figure 4, we can see that our exploration method outperforms other baselines.\nWhile Steve-1 showed decent performance in Map Coverage, it repeatedly visits previously explored\nplaces because of lack of memory. In the case of hierarchical exploration, high-level RNN policy\nstruggled with memorizing visited places as the trajectory gets longer, resulting in high Revisit\nCounts. Additionally, the low-level DQN policy had difficulty navigating complex terrain, such as\nmountains and rivers, showing low Map Coverage. On the other hand, the Count-Based goal selector\nthat directs the agent to the least-visited locations as goals and the VPT-Nav that effectively reaches\nthose goals resulted in strong exploratory behavior. Furthermore, to show the robustness of VPT-Nav,\nwe report its navigation capability in diverse terrains in Appendix L.\n4.2\nSEQUENTIAL TASK SOLVING WITH MEMORY IN SPARSE CONDITION\nIn this section, we demonstrate our agent’s capability to solve sparse sequential tasks based on\nexploration methods studied in Section 4.1. To evaluate this, we introduce ABA-Sparse task, which\nconsists of A-B-A tasks given sequentially with text instructions. Task A involves gathering a sparse\nresource, which can be either a water bucket\n, beef\n, wool\n, or milk\n. Task B requires\ncollecting a dense resource, chosen from log\n, dirt\n, leaves\n, seeds\n, or sand\n, making\ntotal 20 tasks. The agent spawns in a 100 × 100 block map, and the A resource exists in a single\nlocation, while the B resource can be found in multiple locations. The agent is given 12K steps with\nunlimited memory capacity to complete all tasks. Since finding the sparse resource A is challenging,\nthe task requires an efficient exploration algorithm. Moreover, after solving tasks A and B, memory\nbecomes crucial to return to the location of resource A within the time limit. We measure success\nrates and task duration for evaluation.\nTo verify the benefits of efficient exploration and the memory, we compared the following agents:\nSteve-1, MrSteve with exploration method from Plan4MC and FIFO Memory (PMC-MrSteve-FM),\nand our agent MrSteve with Count-Based goal selector and VPT-Nav for exploration and Place\nEvent Memory. We also test MrSteve with different memory variants, MrSteve-FM, MrSteve-EM,\n7\nPreprint\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Dirt-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWater-Seed-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\nTask A\nTask A′\n1.3\n3.3\n5.3\nTask Duration (×103)\nTask A\nTask A′\n2.2\n3.85\n5.5\nTask A\nTask A′\n2.4\n5.4\n8.4\nTask A\nTask A′\n2.0\n5.35\n7.7\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 5: Success Rate and Task Duration of different agents in ABA-Sparse tasks. Task A refers to the first A\ntask in the A-B-A task sequence, while Task A′ refers to the final A task in the A-B-A task sequence. We note that\nMrSteve, as well as its memory variants, outperforms Steve-1, which lacks the memory. Additionally, while\nSteve-1 takes a similar amount of time to solve both task A and task A′, MrSteve solves task A′ much faster. The\nfull results on all 20 tasks are in Appendix H, and investigations about memory variants are in Appendix O.\nMrSteve-PM, which use FIFO Memory, Event Memory, and Place Memory, respectively. Here,\nEvent Memory is a Place Event Memory without place clusters, which stores the frames based on\nvisual similarity. We explain the details of Event Memory in Appendix E.3.\nAs shown in Figure 5, it is clear that MrSteve outperforms Steve-1. This is because MrSteve can find\ntask-relevant resources faster with efficient exploration and store the location of the task A resource,\nallowing it to revisit the location and solve task A again within a limited time. This is evident from the\ntask duration in Figure 5, where MrSteve shows a shorter task duration than Steve-1 on the first A task.\nWhen solving the second A task, MrSteve exhibits a much shorter task duration compared to the first\nA task, while Steve-1 takes a similar or even greater number of steps. While other memory-augmented\nbaselines showed similar performance to MrSteve, PMC-MrSteve-FM performed worse due to a\nsuboptimal exploration method, making it difficult to find the sparse resource. We report the full\nresults on all 20 tasks in Appendix H.\n4.3\nMEMORY-CONSTRAINED TASK SOLVING WITH MEMORY\nWe demonstrated in Section 4.2 that memory is essential in solving sparse sequential tasks when there\nis no limitation in memory capacity. However, in real-world scenarios where memory capacity is\nlimited, memorizing visited places and novel events becomes important. In this section, we show that\nPlace Event Memory can benefit in this scenario. To verify this, we introduce three Memory Tasks,\nwhich are Find Water, Find Zombies’ Death Spot, and Find First-Visited House. In all tasks, the\nagent begins with an exploration phase, followed by a task phase. In the exploration phase, the agent\nfollows a fixed exploratory behavior. In the subsequent task phase, the agent is given a MineCLIP\nembedding et = Encv({ot}16\nn=1) as a task embedding instead of text instruction, where ot is the pixel\nobservation seen in the exploration phase. Then, this becomes an image goal navigation task where\nan agent should navigate to the location of the given image.\nIn all tasks, the exploration phase is 3K steps, and the agent’s memory capacity is limited to 2K. In\nFind Water task, the agent stays near water for 0.5K steps, then travels to a random location. In the\ntask phase, the agent should return to the water (Figure 6(a)). This task evaluates whether an agent\ncan memorize water frames in the past. In Find Zombies’ Death Spot, the agent sees burning zombies\nfor 1K steps (zombies burn for 0.5K steps then disappear), then travels. The task is to return to where\nzombies burned (Figure 6(b)). This task evaluates whether an agent can memorize distinct events\n(zombies burn and disappear) in the same place. In Find First-Visited House task, the agent sees the\nfirst house for 0.1K steps, then goes to the second house and stays until 2K step, then travels. The\ntask is to return to the first house (Figure 6(c)). This task evaluates whether an agent can memorize\ntwo visually similar houses in two different places.\n8\nPreprint\nstay\ntravel\n(a) Find Water\ntravel\ntravel\nstay for a long time\nbefore travel\n(c) Find First-Visited House\ntravel\nstay for a long time\nbefore travel\nstay\n(t = 0)\nSee Water\n(t = 0)\n1st House\nj\nHBC9k1Br2YEL14xMRFEtiQ2WGACbMzm5leI9nwDXrUH\/FmvPoL\/ocf4AB7ELCSTipV3enuCmPBDbjut5NbWV1b38hvFra2d3b3ivsHDaMSTZlPlVC6GRLDBJfMBw6CNWPNSBQK9hAObyb+wyPThit5D6OYBRHpS97jl\nICV\/DJcuaedYsmtuFPgZeJlpIQy1DvFn3ZX0SRiEqgxrQ8N4YgJRo4FWxcaCeGxYQOSZ+1LJUkYiZIp8eO8YlVurintC0JeKr+nUhJZMwoCm1nRGBgFr2J+J\/XSqB3GaRcxgkwSWeLeonAoPDkc9zlmlEQI0sI1dze\niumAaELB5jO3JYzmfkiHCtjT2MbkLYayTBpnFa9aqd6dl2rXWB5dISOURl56ALV0C2qIx9RxNEzekVvzovz7nw4n7PWnJPNHKI5OF+\/Nyckw=<\/latexit>(t = 0)\nZombies Burn\n(t = 0.5K)\nSee Water\n2nd House (t = 1K)\nZombies Gone\nW\nldW19ZzG\/nNre2d3cLefkPLWFWp1JI1fKIZoKHrA4cBGtFipHAE6zpDa\/H9eYjU5rL8B5GEXMD0g+5zykBE7kluHQ6wJ4guU1PuoWiXbYnwovGyUwRZap1Cz+dnqRxwEKgmjduwI3IQo4FSwN+JNYsIHZI+axsbkoBpN5kcneJjk\/SwL5V5IeBJ+nciIYHWo8AznQGBgZ6vjcP\/au0Y\/As34WEUAwvpdJEfCwSjwngHleMghgZQ6ji5lZMB0QRCobTzBYvmPlDMpQGVWowOfNQFk3jtOxUypW7s2L1KgOWQ4foCJWQg85RFd2gGqojih7\nQM3pFb9aL9W59WJ\/T1iUrmzlAM7K+fgEN0qBI<\/latexit>(t = 1K)\n(t = 3K)\nAfter Travel\n(t = 3K)\nAfter Travel\n(t = 3K)\nAfter Travel\n(b) Find Zombies’ Death Spot\nMrSteve\nMrSteve-PM\nMrSteve-EM\nMrSteve-FM\nSteve-1\nFigure 6: The overview of Memory Tasks, and Success Rate for each Memory Task from different agents.\nMemory Tasks are basically navigation tasks reaching the location of the previously seen experience frame. We\nobserve that MrSteve which uses Place Event Memory shows high success rates in all tasks.\nTo evaluate how each memory type performs in the Memory Tasks, we tested Steve-1, MrSteve,\nMrSteve-FM, MrSteve-EM, and MrSteve-PM. In Figure 6, the performance of each memory type\nis illustrated. In all tasks, Steve-1 which lacks memory, showed worst performance since it has to\nfind the targets from scratch. In Find Water task, MrSteve-FM does not have the water frames in\nmemory in task phase, so it should explore to find the water showing about 60% success rate. In\ncontrast, other agents store the water frames by allocating place cluster or event cluster in water\nlocation, allowing the agent to recall the frames and easily return to water, showing high success\nrate. In Find Zombies’ Death Spot task, MrSteve-PM loses burning zombies’ frames since Place\nMemory removes the frames in the largest place cluster, which is zombies’ place, showing about 60%\nsuccess rate. However, MrSteve-EM and MrSteve store the burning zombies’ frames as a novel event,\nallowing the agent to easily return to zombies’ spot, showing high success rate. In Find First-Visited\nHouse task, MrSteve-EM loses frames of first house, since it clusters two visually similar houses as\nthe same event, showing about 50% success rate. However, MrSteve-PM and MrSteve store the two\nhouses in different place clusters, enabling the agent to return to the first house, showing high success\nrate. These results suggest that Place Event Memory demonstrates its strength in memory-limited\nsettings, where memorizing both visited places and novel events is crucial for task completion.\n4.4\nLONG-HORIZON SPARSE SEQUENTIAL TASK SOLVING WITH MEMORY\nIn this section, to see how MrSteve generalizes to long-horizon tasks, we introduce two sparse\nsequential tasks. For both tasks, the agent plays in a 200 × 200 block map for 500K steps (About 7\nhours of gameplay) with 20K steps of memory capacity. The first is Long-Instruction task, where the\nagent is continuously given random tasks from Obtain X. Here X can be water\n, beef\n, wool\n, log\n, dirt\nor seeds\n. If the agent fails to complete the task within 20K steps, the task is\nchanged. This task requires efficient exploration in a large map, and managing memory to memorize\nplaces with task-relevant resources to continuously solve the given tasks.\nThe second task is Long-Navigation task similar to Memory Tasks in Section 4.3. It has an exploration\nphase of 16K steps and a task phase. In the exploration phase, the agent observes six events in different\nplaces: 1) burning zombies, 2) river, 3) sugarcane blow up, 4) spider spawn, 5) tree, and 6) house,\nspending 2K steps at each place. In the task phase, the image goal is continuously given randomly\nselected from the frames in the early steps of the event. For instance, if the task is to reach sugarcane\nplace, the image of sugarcane place before blow up is set as an image goal. This task requires\nmanaging memory to retain distinct events in different places.\n9\nPreprint\n0\n100K 200K 300K 400K 500K\nTime Steps\n0\n20\n40\n60\n80\n100\nSucceeded Tasks\nLong-Instruction Task\n0\n100K 200K 300K 400K 500K\nTime Steps\n0\n20\n40\n60\n80\nLong-Navigation Task\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 7: The performance in Long-Intruction task and Long-\nNavigation task. MrSteve performs well in both tasks.\nThe results are shown in Figure 7.\nFor Long-Instruction task, we ob-\nserve that MrSteve, and MrSteve-PM\nsolved over 80 tasks, showing their\ncapabilities of retaining task-relevant\nresources in different places effec-\ntively. MrSteve-EM solved around 50\ntasks, suggesting event-based memory\nis less effective than place-based mem-\nory. This is because similar events in\ndifferent places, like cows and sheeps\nliving in visually similar forests, are\nin the same event cluster, possibly los-\ning task-relevant frames. For the remaining baselines, they either have suboptimal exploratory\nbehaviors or keep losing the task-revelant frames, solving less than 50 tasks. For Long-Navigation\ntask, MrSteve, and MrSteve-EM solved around 70 tasks, showing the ability to retain novel events\noccured in different places. In the case of MrSteve-PM, it removes the frames in early time steps\nof each place cluster, losing novel events (e.g., sugarcane before blow up), thus solving less than\n20 tasks. For the remaining baselines, they lose or cannot retain frames in the early stage of an\nepisode solving less than 10 tasks. These results suggest that MrSteve demonstrates its strength in\nlong-horizon tasks.\n5\nLIMITATIONS\nThis work focuses on improving Steve-1 through the introduction of What-Where-When episodic\nmemory, significantly enhancing the agent’s ability to retain and recall past events for more efficient\ntask-solving. Our experiments demonstrate that MrSteve exhibits significantly enhanced performance\nwhen integrated with LLM-augmented agents for high-level planning. However, current limitations\nprevent the high-level planner from accessing PEM in the low-level controller. Future work could\nexplore enabling PEM access for high-level planners, which could generate more accurate plans by\nleveraging the agent’s episodic memories, further enhancing the system’s capabilities for complex,\nlong-horizon tasks.\nWe list up a few more limitations. First, our experiments are limited to surface-level exploration in\nthe Minecraft environment, omitting underground navigation, which is a crucial aspect of the game\nas mining plays a central role. However, our hierarchical exploration methods based on visitation\nmap could easily be extended to include vertical dimensions. Additionally, while our VPT-Nav\ndemonstrated strong navigation abilities in plains biomes, more challenging terrains, such as crossing\ncliffs that require skills like building bridges, were not addressed. Lastly, we used exact position data\nin Minecraft, which may limit the model’s adaptability to robotics tasks where positional information\nis often noisy. One possible direction is adapting MrSteve in environments with noisy positions.\n6\nCONCLUSION\nIn this paper, we introduced MrSteve (Memory Recall Steve-1), a novel low-level controller designed\nto address the limitations of current LLM-augmented hierarchical approaches in general-purpose\nembodied AI environments like Minecraft. We argued that the primary cause of failures in many\nlow-level controllers is the absence of an episodic memory system. To overcome this, we equipped\nMrSteve with Place Event Memory (PEM), a form of episodic memory that captures and organizes\nwhat, where, and when information from episodes. This allows for efficient recall and navigation in\nlong-horizon tasks, directly addressing the limitations of existing low-level controllers like Steve-1,\nwhich rely heavily on short-term memory. Additionally, we proposed an Exploration Strategy and a\nMemory-Augmented Task Solving Framework, enabling agents to effectively switch between explo-\nration and task-solving based on recalled events. Our results demonstrate significant improvements in\nboth task-solving and exploration efficiency compared to existing methods. We believe that MrSteve\nopens new avenues for improving low-level controllers in hierarchical planning and are releasing our\ncode to facilitate further research in this field.\n10\nPreprint\nETHICS STATEMENT\nWe acknowledge potential societal concerns related to our work. While our agent, in its current form,\nis designed for use in virtual environments such as Minecraft, the techniques and advancements made\nhere could be extended to broader autonomous systems. There is a possibility that, if adapted for real-\nworld applications, such systems might be misused for unethical purposes, including unauthorized\nsurveillance or actions that infringe on individual privacy. We encourage responsible usage and\nfurther research into safeguards to prevent such outcomes.\nREPRODUCIBILITY STATEMENT\nTo facilitate the reproducibility of our work, we have provided the pseudo codes, model architecture,\nand hyperparameters in Appendix D, E, and F. We will also release the source code for our models\nand experiments.\nACKNOWLEDGEMENT\nThis work was supported by GRDC (Global Research Development Center) Cooperative Hub Program\n(RS-2024-00436165) and Brain Pool Plus Program (No. 2021H1D3A2A03103645) through the\nNational Research Foundation (NRF) funded by the Ministry of Science and ICT (MSIT). The authors\nwould like to thank the members of Machine Learning and Mind Lab (MLML) for helpful comments.\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in neural information processing systems, 35:23716–23736,\n2022.\nJosh Albrecht, Abraham J. Fetterman, Bryden Fogelman, Ellie Kitanidis, Bartosz Wróblewski, Nicole\nSeo, Michael Rosenthal, Maksis Knutins, Zachary Polizzi, James B. Simon, and Kanjun Qiu.\nAvalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds. In NeurIPS\nDatasets and Benchmarks Track, 2022.\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. In Advances in Neural Information Processing Systems, pp. 24639–24654,\n2022.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nShaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control\nthrough goal-aware representation learning and adaptive horizon prediction. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 13734–13744, 2023a.\nShaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Groot: Learning to\nfollow instructions by watching gameplay videos. arXiv preprint arXiv:2310.08235, 2023b.\nMatthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min,\nKavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, Roozbeh Mottaghi, Jitendra Malik, and\nDevendra Singh Chaplot. Goat: Go to any thing, 2023. URL https:\/\/arxiv.org\/abs\/\n2311.06430.\nJunmo Cho, Jaesik Yoon, and Sungjin Ahn. Spatially-aware transformers for embodied agents.\nIn The Twelfth International Conference on Learning Representations, 2024. URL https:\n\/\/openreview.net\/forum?id=Ts95eXsPBc.\n11\nPreprint\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a fixed-length context. In Annual Meeting of the\nAssociation for Computational Linguistics, 2019. URL https:\/\/api.semanticscholar.\norg\/CorpusID:57759363.\nOr Dinari and Oren Freifeld. Revisiting DP-means: Fast scalable algorithms via parallelism and\ndelayed cluster creation. In The 38th Conference on Uncertainty in Artificial Intelligence, 2022.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2022a. URL https:\/\/openreview.net\/forum?\nid=rc8o_j8I8PX.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2022b. URL https:\/\/openreview.net\/forum?\nid=rc8o_j8I8PX.\nWilliam H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso,\nand Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. arXiv\npreprint arXiv:1907.13440, 2019.\nDanijar Hafner. Benchmarking the spectrum of agent capabilities. In International Conference on\nLearning Representations.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp, 2019.\nURL https:\/\/arxiv.org\/abs\/1902.00751.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021a. URL https:\n\/\/arxiv.org\/abs\/2106.09685.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations, 2021b.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,\n2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda\nLuu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning\nthrough planning with language models, 2022b. URL https:\/\/arxiv.org\/abs\/2207.\n05608.\nMatthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial\nintelligence experimentation. In Ijcai, volume 16, pp. 4246–4247, 2016.\nTomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose,\nKoki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented planning with contextual\nmemory for multimodal llm agents, 2024.\nAndrew Kyle Lampinen, Stephanie C Y Chan, Andrea Banino, and Felix Hill. Towards mental time\ntravel: a hierarchical memory for reinforcement learning agents. 2021.\n12\nPreprint\nZaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-\n1: Hybrid multimodal memory empowered agents excel in long-horizon tasks, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2408.03615.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. Advances in Neural Information Processing Systems, 36,\n2024.\nZichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, and Wei Yang.\nJuewu-mc:\nPlaying minecraft with sample-efficient hierarchical reinforcement learning.\narXiv preprint\narXiv:2112.04907, 2021.\nShunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui, Wenkai Fang, Yuxuan Zheng, Tongya\nZheng, and Mingli Song. Odyssey: Empowering agents with open-world skills. arXiv preprint\narXiv:2407.15325, 2024.\nHangyu Mao, Chao Wang, Xiaotian Hao, Yihuan Mao, Yiming Lu, Chengjie Wu, Jianye Hao, Dong\nLi, and Pingzhong Tang. Seihai: A sample-efficient hierarchical ai for the minerl competition. In\nDistributed Artificial Intelligence: Third International Conference, DAI 2021, Shanghai, China,\nDecember 17–18, 2021, Proceedings 3, pp. 38–51. Springer, 2022.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-\ntersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,\nDaan Wierstra, Shane Legg, and Demis Hassabis.\nHuman-level control through deep rein-\nforcement learning.\nNature, 518(7540):529–533, 2015.\ndoi: 10.1038\/nature14236.\nURL\nhttps:\/\/doi.org\/10.1038\/nature14236.\nKolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer\nSingh, and Roy Fox.\nDo embodied agents dream of pixelated sheep?: Embodied decision\nmaking using language guided world modelling. arXiv preprint arXiv:2301.12050, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2301.12050.\nOpenAI. Gpt-4 technical report, 2024. URL https:\/\/arxiv.org\/abs\/2303.08774.\nYiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing\nShao. Mp5: A multi-modal open-ended embodied system in minecraft via active perception.\nIn Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp.\n16307–16316, 2024.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision, 2021. URL https:\n\/\/arxiv.org\/abs\/2103.00020.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning. Advances in Neural Information Processing\nSystems, 36, 2024.\nChan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su.\nLlm-planner: Few-shot grounded planning for embodied agents with large language models. In\nProceedings of the IEEE\/CVF International Conference on Computer Vision (ICCV), October\n2023.\nZhiyuan Sun, Haochen Shi, Marc-Alexandre Côté, Glen Berseth, Xingdi Yuan, and Bang Liu.\nEnhancing agent learning through world dynamics modeling. arXiv preprint arXiv:2407.17695,\n2024.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,\nFilip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep\nreinforcement learning, 2017. URL https:\/\/arxiv.org\/abs\/1611.04717.\n13\nPreprint\nQwen Team. Qwen2.5: A party of foundation models, September 2024. URL https:\/\/qwenlm.\ngithub.io\/blog\/qwen2.5\/.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels, 2023. URL https:\/\/arxiv.org\/abs\/2302.13971.\nLaurens van der Maaten and Geoffrey Hinton.\nVisualizing data using t-sne.\nJournal of Ma-\nchine Learning Research, 9(86):2579–2605, 2008. URL http:\/\/jmlr.org\/papers\/v9\/\nvandermaaten08a.html.\nKonstantinos Voudouris, Ibrahim Alhas, Wout Schellaert, Matthew Crosby, Joel Holmes, John\nBurden, Niharika Chaubey, Niall Donnelly, Matishalin Patel, Marta Halina, José Hernández-\nOrallo, and Lucy G. Cheke. Animal-ai 3: What’s new & why you should care, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2312.11414.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv\npreprint arXiv: Arxiv-2305.16291, 2023a.\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng\nHe, Zilong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang. Jarvis-1: Open-world multi-task\nagents with memory-augmented multimodal language models. arXiv preprint arXiv: 2311.05997,\n2023b.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select:\nInteractive planning with large language models enables open-world multi-task agents. arXiv\npreprint arXiv:2302.01560, 2023c.\nBrian Yamauchi. Frontier-based exploration using multiple robots. In Proceedings of the Second\nInternational Conference on Autonomous Agents, AGENTS ’98, pp. 47–53, New York, NY, USA,\n1998. Association for Computing Machinery. ISBN 0897919831. doi: 10.1145\/280765.280773.\nURL https:\/\/doi.org\/10.1145\/280765.280773.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4MC: Skill reinforcement learning and planning for open-world Minecraft tasks. arXiv\npreprint arXiv:2303.16563, 2023.\nHaoqi Yuan, Zhancun Mu, Feiyang Xie, and Zongqing Lu. Pre-training goal-based models for\nsample-efficient reinforcement learning. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https:\/\/openreview.net\/forum?id=o2IEmeLL9r.\nJesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, and\nJoseph J Lim. Bootstrap your own skills: Learning to solve new tasks with large language model\nguidance. In 7th Annual Conference on Robot Learning, 2023. URL https:\/\/openreview.\nnet\/forum?id=a0mFRgadGO.\nBohan Zhou, Ke Li, Jiechuan Jiang, and Zongqing Lu. Learning from visual observation via offline\npretrained state-to-go transformer. Advances in Neural Information Processing Systems, 36, 2024a.\nEnshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing\nShao. Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world\ncontrol. arXiv preprint arXiv:2403.12037, 2024b.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft:\nGenerally capable agents for open-world environments via large language models with text-based\nknowledge and memory. arXiv preprint arXiv:2305.17144, 2023.\n14\nPreprint\nA\nLIMITATIONS OF MEMORY SYSTEM IN LLM-AUGMENTED AGENTS\nMemory systems in agents primarily aim to retrieve robust and accurate high-level plans for long-\nhorizon tasks (Wang et al., 2023a;b; Zhu et al., 2023; Qin et al., 2024; Li et al., 2024). Existing works\nuse an abstracted memory that stores the succeeded task with its plans, and often with history of\nobservations for reliable retrieval, which is helpful when the similar plan in other tasks already exist\nin memory. However, these types of memory systems are not well-suited for low-level controllers for\nthe following reasons:\n• Issues with Managing Memory Recent memory systems in Minecraft, when saving successful\nplans or skills, the history of observations for solving the task is all stored in FIFO manner (Wang\net al., 2023b) or only task-relevant frames are stored in the plan (Li et al., 2024). However,\ncomputation complexity for retrieving the experience frames that are relevant to a new task is\ncomputationally expensive or even impossible (because the latter may not store the experience\nframes despite the agent observed it).\n• Lack of Mechanism for Retrieving Experience frames Current memory systems store the text\ninstructions of succeeded tasks as keys and their plans to complete those tasks as values. The\nretrieval process begins by matching a task query to the task keys in memory and then filtering\nfurther using the current scene’s similarity to the stored frames before retrieving the final plan.\nThese memory systems are targetted to retrieve the succeeded plan, but they lack a mechanism for\nutilizing the experience frames in the memory, which could be crucial for future tasks.\nConsider the following example: Suppose the agent is tasked with collecting wood. While searching\nfor a tree, let’s assume the agent came across a cow in the forest. Once the wood is collected, the\nmemory will store the successful plan and its corresponding observations. However, if the agent\nis later tasked with finding the cow, there would be no memory key related to the cow, making it\nimpossible to retrieve the relevant frames. While some heuristics to calculate the similarity of the\ntask embedding for “find cow” and the visual representations in memory using MineCLIP is possible,\nit is computationally expensive since the similarity should be calculated for all stored frames.\nThus, we need a new type of memory system for the low-level controller to efficiently store novel\nevents (such as encountering the cow) as they explore the environment, even when such events are\nnot directly relevant to the current task. Also, the memory should hierarchically organize these novel\nevents so that they can be efficiently retrieved later. In this paper, we propose a memory system called\nPlace Event Memory (PEM), which organizes experiences by both location and event. PEM allows\nthe agent to store diverse novel events across various locations, making future retrieval more efficient.\nWe argue that PEM, when combined with current memory systems that store successful plans, will\nenable more effective retrieval of task-relevant information.\nB\nCOMPUTATION OVERHEAD\nOur study was performed on an Intel server equipped with 8 NVIDIA RTX 4090 GPUs and 512GB of\nmemory. The inference time for tasks under 20K steps for running a single episode was approximately\n30 minutes on a single GPU. For long-horizon tasks that take 500K steps, approximately 12 hours\nwere required for running a single episode on a single GPU. The VPT-Nav training took roughly 23\nhours on a single GPU.\nC\nENVIRONMENTS DETAILS\nC.1\nENVIRONMENTS SETTING\nAll tasks are implemented using MineDojo (Fan et al., 2022b). We utilize MineDojo’s success\nchecker, where the success of each task is determined based on changes in the agent’s inventory.\nHence, the agent succeeds the task if the corrensponding target item is appeared in the inventory. If\nthe agent exceeds the time limit of each task or dies before completing assigned tasks, indicating the\nagent fails on the task.\nFor all tasks, we assume that the agent has access to both first-person view pixels and its positional\ndata. The raw pixel observation it is provided in the shape (160 × 256 × 3). The agent’s position pt\n15\nPreprint\nFigure 8: Topdown View of Three ABA-Sparse Task Maps. The first map was used in the ABA-Sparse tasks,\nincluding the water bucket task. Trees are distributed on the left side of the map, and water exists only in the\nupper right corner. The second map was used in tasks, not including the water bucket and sand task. Trees are\nlocated on the left side of the map, and on the right side, there are cows and sheep, with a mountain separating\nthem. The last map was used in tasks, including the sand task. Its overall layout is identical to the second\nmap, except for an additional water pond at the top. The map’s edges are surrounded by high walls, making it\nimpossible to access anything other than the resources visible in the top-down view.\nis represented as a vector of shape (5, ), where the first three components correspond to the (x, y, z)\ncoordinates, and the last two components represent the pitch and yaw angles of the agent’s camera.\nWe note that no privileged observations, such as LiDAR or voxel data, are provided. The agent\noperates in a keyboard and mouse action space following VPT (Baker et al., 2022). This action space\nconsists of button input states paired with mouse movements.\nCrafting items, which requires long-horizon planning, is not considered in our method. To eliminate\nthe need for crafting, the appropriate item necessary for solving a task is provided to the agent at the\nbeginning of each new task. For instance, if the task is to “obtain water bucket\n,” the agent starts\nwith an empty bucket\nin its main hand. Additionally, we apply the following rules:\n• \/difficulty peaceful: This rule prevents the occurrence of hostile mobs, such as\nzombies and spiders, and death by starvation.\n• \/gamerule doWeatherCycle false: This rule keeps the weather clear to reduce\nthe noise from heavy rain.\nC.2\nTASK DETAILS\nIn this section, we describe the details for each task in Experiment section (Section 4). The basic\nenvironment settings follow those outlined in Appendix C.1, unless specified otherwise.\nC.2.1\nEVALUATION PROTOCOLS\nExcept for the Long-Horizon tasks in Section 4.4, we ran 100 episodes for each agent using different\nrandom seeds to evaluate performance. For the Long-Horizon tasks, we reported the average success\nrate with the standard error over 5 episode runs for each agent.\nC.2.2\nEXPLORATION & NAVIGATION TASK DETAILS\nFor this task, we used the Map 1 in Figure 8 with 100 × 100 size, surrounded by high walls. The\nmap includes complex terrains such as mountains and a water pond, which requires robust low-level\nnavigation policy for successful exploration. For each episode, the agent spawns in the center of the\nmap and explores for 6,000 steps.\n16\nPreprint\nC.2.3\nABA-Sparse TASKS DETAILS\nIn these tasks, the agent is asked to complete three sequentially given tasks, where the first and third\ntasks are identical. The target item for the first task, denoted as A, is one of four sparsly distributed\nitems: water\n, beef\n, wool\n, or milk\n. In the second task, denoted as B, the taget item is\none of five items: log\n, dirt\n, seeds\n, leaves\n, or sand\n. The agent has unlimited memory\ncapacity and is allowed a maximum of 12,000 steps to complete three tasks. The task only changes to\nthe next one upon successful completion of the current task.\nIf the first task, A, is to obtain a water bucket\n, the agents spawn at the center of Map 1, shown in\nFigure 8. On this map, most of the surface is covered with dirt, while grass, which provides seeds, is\nwidely distributed. A water pond is located in the upper right corner of the map, but it only becomes\nvisible when agents are nearby.\nIf the first task, A, is to collect beef\nor wool\n, the agents spawn at the center of Map 2 in Figure\n8. Similarly, if the second task, B is to collect sand\n, the agents start at the center of Map 3 in\nFigure 8. In both maps, trees are scattered on the left side of the map, while sheep\nand cows\nare found on the right side. A sand mountain runs through the middle, separating the trees from the\nsheep and cows. Dirt is only present on the far left and right sides of the map.\nC.2.4\nMEMORY TASKS\nIn the three Memory Tasks, the agent explores for 3,000 steps before being asked to complete an\nimage goal navigation. Unlike the ABA-Sparse Tasks, the agent has a limited memory capacity of\n2,000 frames. Consequently, an agent utilizing the FIFO memory forgets the memory from the first\n1,000 frames after the exploration phase.\nFind Water Task The agent is initially spawned near a water pond and remains in its vicinity for 500\nsteps. During this period, one observation is selected as a goal image for a subsequent navigation task.\nAfter the initial 500 steps, the agent moves to a random location for the remainder of the exploration\nphase. After 3K steps, the agent begins to navigate back to the water pond it observed at the start of\nthe episode.\nFind Zombies’ Death Spot Task At the beginning, the agent sees burning zombies for the first 1K\nsteps. Approximately 500 steps after the start of the episode, the zombies disappear, resulting in the\nagent observes two distinct scenes in the same location. A goal image for navigation is selected from\nthe observation where the zombies are burning. After 1K steps, the agent starts to travel for the rest\nof the exploration phase. Once the exploration phase finishes, the agent returns to the place where the\nzombies were burning.\nFind First-Visited House Task In this task, there are two distinct houses that look similar to one\nanother. The agent starts near one of the houses, where it stays for 100 steps before moving to the\nother house, where it remains for 2K steps. For the remainder of the exploration phase, the agent\ntravels to a random location. After the exploration phase, the agent is asked to go to the first house it\nhas visited.\nC.2.5\nLONG-HORIZON TASKS\nLong-Instruction Task In this task, the agent is required to complete a series of tasks sequentially on\na 200 × 200 block-sized map in Figure 9. The order of tasks within the sequence is randomized, with\neach task being one of six possible types: water bucket\n, beef\n, wool\n, wood\n, dirt\n, and\nseeds\n. We evaluated the agent’s performance by measuring the number of successfully completed\ntasks over 500K steps. If the agent fails to complete a given task within 20,000 steps, that task is\ncanceled, and a new one is assigned.\nLong-Navigation Task This task is basically image goal navigation in a 200 × 200 block sized\nmap in Figure 9. Before the task phase, the agent undergoes a 16K-step exploration phase where it\nobserves six landmarks: 1) zombie burning\n, 2) water\n, 3) sugarcane explosion\n, 4) spider\nspawn\n, 5) tree\n, and 6) house, spending 2K steps at each location. In the task phase, the agent is\ngiven a random start image of one of these landmarks and must navigate to it. The dynamic nature\nof some landmarks, such as the burning zombie, exploding sugarcane, and spider spawn makes it\nimportant to store novel events from the exploration phase.\n17\nPreprint\nFigure 9: Topdown View of Two Long-Horizon Task Maps. Both maps are of size 200 × 200 blocks. (Left)\nThis map is used for the Long-Instruction task. Trees are located in the lower left and middle bottom of the\nmap, while sheep inhabit the upper left area, and cows exist in the right bottom. A water pond can be found\nin the upper right area of the map. (Right) This map is utilized for the Long-Navigation task. Agents traverse\nbetween six scenes in the map. Three of them are dynamic scenes: burning zombies, popping sugarcanes, and\nspawning spiders, which are positioned at the first, third, and fourth places on the map, respectively. The other\nthree are static scenes: a water pond, trees, and a house, located at the second, fifth, and sixth places on the map,\nrespectively.\n18\nPreprint\nD\nMRSTEVE ALGORITHM\nAlgorithm 2 MrSteve Algorithm\nRequire: Xtinit\nn from Environment env, Memory Mtinit\nn , and new task τn at time step tinit\nn .\n1: mode ←null\n2: Tmode ←0\n3: gt ←null\n4: reached ←false\n5: t ←tinit\nn\n6: Xt ←Xtinit\nn ; Mt ←Mtinit\nn\n7: loop\n8:\nWrite(Mt, Xt)\n9:\nif Tmode > L then\n10:\nmode ←null\n11:\nTmode ←0\n12:\ngt ←null\n13:\nreached ←false\n14:\nend if\n15:\n# Mode Selector\n16:\nif mode is null then\n17:\ncandidates ←Read(Mt, τn)\n18:\nif candidates ̸= ∅then\n19:\nX′\nt ←PickOne(candidates)\n20:\ngt ←Position(X′\nt)\n21:\nmode ←EXECUTE\n22:\nelse\n23:\nmode ←EXPLORE\n24:\nend if\n25:\nend if\n26:\nht ←Xt−128:t\n27:\n# Explore Mode\n28:\nif mode is EXPLORE then\n29:\ngt ∼πH-Nav(gt|ℓ′\nt, Mt)\n30:\nat ∼πL-Nav(at|ht, gt)\n31:\nend if\n32:\n# Execute Mode\n33:\nif mode is EXECUTE then\n34:\nif the agent reaches gt then\n35:\nreached ←true\n36:\nend if\n37:\nif reached then\n38:\nat ∼πInst(at|ht, τn)\n39:\nelse\n40:\nat ∼πL-Nav(at|ht, gt)\n41:\ngt+1 ←gt\n42:\nend if\n43:\nend if\n44:\nif the task τn succeeded or timeout then\n45:\nbreak\n46:\nend if\n47:\nTmode ←Tmode + 1\n48:\nXt+1 ←env.step(at); Mt+1 ←Mt\n49:\nt ←t + 1\n50: end loop\nWe provide more details in Algorithm 2. This algorithm is executed when a new task is given to the\nMrSteve and finished if it completes the task or exceeds a time limit (Line 44-46).\n19\nPreprint\nAgent State Variables When a new task τn is given, the agent state variables are initialized (Line\n1-4). mode indicates which module is executed and Tmode denotes elapsed times for the current mode\nexecution. gt is a target location for the navigation, and reached indicates whether the agent has\nreached the target location in the Execute Mode.\nThe Agent State Variables are reset if the agent remains in the current mode for L steps (Lines 9-13).\nIf the Mode Selector retrieves a wrong experience frame, where no task-relevant resource is present\nat the corresponding location, the agent can have trouble and fail to complete the given task for a\nlong time. In addition, the agent might encounter previously unobserved task-relevant resources\nwhile navigating to the location of the previously retrieved memory. In both scenarios, changing\nthe navigation target can be helpful. Consequently, the agent queries the memory to search for new\ncandidates if the agent executes the current mode for L = 600 steps, equivalent to 30 seconds in-game\ntime.\nMemory Write Frequency In Line 8, MrSteve writes the current observation to the memory,\nregardless of the current mode.\nMode Selector The Mode Selector decides between two modes according to the existence of task-\nrelevant resources in the memory. First, if no mode has been selected, MrSteve queries the memory\n(Line 16-17). If a task-relevant resource exists, MrSteve picks one of them and sets the navigation\ntarget gt to the location of the picked one (Line 18-21). Otherwise, the Explore Mode is selected\n(Line 23).\nExplore Mode In the Explore Mode, MrSteve explores the least-visited locations using πH-Nav and\nπL-Nav (Line 28-31). We provide more details in Appendix F.2.\nExecute Mode In the Execute Mode, MrSteve navigates to the selected target location gt first. Once\nit reached, Steve-1 is executed to follow the task instruction τn (Line 33-43).\n20\nPreprint\nE\nPLACE EVENT MEMORY, PLACE MEMORY, EVENT MEMORY DETAILS\nWe provide the Algorithms on Write & Read operations of Place Event Memory, and other memory\nvariants, which are Place Memory, and Event Memory. We also provide the specifications of each\nmemory in the following section E.4.\nE.1\nPLACE EVENT MEMORY WRITE & READ OPERATIONS\nAlgorithm 3 Place Event Memory Write Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk, and each place cluster Mk has event clusters\n{Ek\ni }dk\ni=1 where Ek\ni is i-th event cluster with center embedding ek\nEi. Each place cluster has\ndummy deque Qk that stores recent R frames in that cluster, and update frequency timer rk.\nAdditional variables are Memory Capacity N, MineCLIP image encoder CLIPv, and experience\nXt = {it, lt, t} at time step t. MineCLIP threshold c.\n1: # Memory Add\n2: Get experience frame xt = {et, lt, t} where et = CLIPv(it)\n3: if ℓt ∈PLACE_CLUSTER(Mt) then\n4:\nFind place cluster Mj where PLACE(ℓt) = ℓMj\n5:\nAdd xt to dummy deque Qj\n6:\nUpdate frequency timer rk = rk + 1\n7:\nif rk = R then\n8:\n{Ei, ei}l′\ni=1 = DP-Means(Qj)\n9:\n{Ei, ei}l\ni=1 = MERGE_CLUSTERS({Ei, ei}l′\ni=1)\n10:\nfor u = 1, . . . , l do\n11:\nadd_cluster=True\n12:\nfor p = 1, . . . , dj do\n13:\nif ej\nu · ej\nEp > c then\n14:\nMerge Eu to Ej\np\n15:\nadd_cluster=False; break\n16:\nend if\n17:\nend for\n18:\nif add_cluster=True then\n19:\nCreate new event cluster Ej\ndj+1 = Eu with center embedding ej\nEdj +1 = eu\n20:\nAdd created cluster Ej\ndj+1 to Mj\n21:\nend if\n22:\nend for\n23:\nrk = 0\n24:\nend if\n25: else\n26:\nCreate new place cluster MK+1 with center position pMK+1 = PLACE(ℓt), and center\nembedding eMK+1 = et\n27:\nCreate new dummy deque QK+1 = {xt}\n28:\nAdd created cluster MK+1 = {QK+1} to Mt\n29: end if\n30: # Memory Remove\n31: if len(Mt) > N then\n32:\nFind event cluster Ek\ni where i, k = arg max len(Ek\ni )\n33:\nRemove the oldest frame in Ek\ni\n34: end if\nWe further elaborate on how # Memory Add in Algorithm 3 operates. First, we get the experience\nframe xt, and check if ℓt belongs to one of place clusters in Mt (Line 3). Here, PLACE_CLUSTER()\nindicates the whole 2D space that memory Mt covers. Since we use fixed-size square area for each\nplace cluster, it can be seen as covered area of set of squares. If xt is not in current place clusters,\nnew place cluster is created with dummy deque (Line 26-28). Dummy deque here is short memory\n21\nPreprint\nfor each place cluster used for clustering the events. If xt belongs to some place cluster (Line 4,\nPLACE() maps ℓt to its place), it is added to dummy deque in the place cluster, and increase update\nfrequency timer in that place cluster by 1 (Line 4-6). When update frequency timer equals to R,\nDP-Means algorithm (Dinari & Freifeld, 2022) is applied to experience frames in dummy deque\nfor event clustering. After applying DP-Means algorithm, we get a set of events and its center\nembeddings (Line 8). However, we found that DP-Means tends to make different clusters even when\nthe agent observes the same scenes. Thus, we applied MERGE_CLUSTERS() to DP-Means output\nclusters to merge clusters that have high alignments in center embeddings (Line 9). After this, for\neach newly created event cluster from DP-Means (Line 10), if some existing event cluster is aligned\n(Line 13), two clusters are merged (Line 14). If newly created cluster does not belong to any existing\nevent clusters, then add to the place cluster as a new event cluster (Line 19-20).\nEvent Cluster Details We use the DP-Means algorithm for clustering the events. DP-Means\nalgorithm is a Bayesian non-parametric extension of the K-means algorithm based on small variance\nasymptotic approximation of the Dirichlet Process Mixture Model. It doesn’t require prior knowledge\non the number of clusters K. To run this algorithm, we first set the initial number of clusters K′ and\ncluster the data with K++ Means initialization (K′ can be 1), then DP-Means algorithm automatically\nre-adjust the number of clusters based on the data points and cluster penalty parameter δ. Thus,\nDP-Means algorithm behaves similarly to K-means with the exception that a new cluster is formed\nwhenever a data point is farther than δ away from every existing cluster centroid.\nUsing clustering algorithm in Minecraft was previously done in Yuan et al. (2024), where K-Means\nalgorithm is used to cluster 100 goal states in massive Minecraft dataset. However, they applied the\nclustering on the reduced dimension of MineCLIP representation with t-SNE (van der Maaten &\nHinton, 2008). In this work, we directly apply DP-Means algorithm in MineCLIP representation\nspace setting initial number of clusters K′ = 5, and δ = 1.\nPlace Cluster Details We provide a detailed explanation of how place clusters are formed. Each place\ncluster stores the agent’s experience frames based on its 2D ground position (x, y) and head direction\nangle yaw. The size of a place cluster and its yaw range are defined by parameters C (in Minecraft\nblock) and W ∈(0◦, 180◦), respectively, which distinguish different clusters. A place cluster is\ncentered at a specific position (x, y) with a center yaw w. An experience frame is assigned to a place\ncluster if the agent’s position falls within the range (x−C\/2, x+C\/2) for x and (y −C\/2, y +C\/2)\nfor y, and the yaw angle satisfies (w −W\/2, w + W\/2). We use relative position and yaw for center\nof the place cluster implying that the center of the initial place cluster has position (0, 0), and yaw 0.\nAlgorithm 4 Place Event Memory Read Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk, and each place cluster Mk has event clusters\n{Ek\ni }dk\ni=1 where Ek\ni is i-th event cluster with center embedding ek\nEi. Additional variables are\nMineCLIP image encoder CLIPv, and text encoder CLIPt, task instruction τn at time step t, and\ntask threshold h.\n1: Calculate sik(τn) = CLIPt(τn) · CLIPv(ek\nEi) for all i, k\n2: Sort sik(τn) for all event clusters and select top-K event clusters\n3: Gather all experience frames xt = {et, lt, t} from the top-K event clusters\n4: cand = {}\n5: for each experience frame xt in the selected top-K event clusters do\n6:\nsxt(τn) = CLIPt(τn) · CLIPv(et)\n7:\nif sxt(τn) > h then\n8:\nAdd xt to cand\n9:\nend if\n10: end for\n11: return cand\nExploiting hierarchical structure of Place Event Memory For memory read operation, we can\nexploit hierarchical structure of place event memory for better computation efficiency. Suppose we\nhave N1 place clusters and exactly N2 event clusters for each place cluster. Then, read operation\nin Algorithm 4 has computational complexity of O(N1N2). However, if we first attend only to\nplace clusters (calculate sik from center embeddings eMk from place clusters), then attend to event\n22\nPreprint\nclusters from extracted top-k place clusters, the computational complexity becomes O(N1 + kN2).\nHowever, center embedding eMk may not be sufficient summarization of the place cluster, since\ncenter embedding can not capture all different events occurred in that place. Thus, we use the read\noperation as in Algorithm 4.\n23\nPreprint\nE.2\nPLACE MEMORY WRITE & READ OPERATIONS\nAlgorithm 5 Place Memory Write Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk. Each place cluster Mk is a FIFO Memory.\nAdditional variables are Memory Capacity N, MineCLIP image encoder CLIPv, and experience\nXt = {it, lt, t} at time step t.\n1: # Memory Add\n2: Get experience frame xt = {et, lt, t} where et = CLIPv(it)\n3: if ℓt ∈PLACE_CLUSTER(Mt) then\n4:\nFind place cluster Mj where PLACE(et) = eMj\n5:\nAdd xt to Mj\n6: else\n7:\nCreate new place cluster MK+1 with center position ℓMK+1 = PLACE(ℓt), and center\nembedding eMK+1 = et\n8:\nAdd created cluster MK+1 = {xt} to Mt\n9: end if\n10: # Memory Remove\n11: if len(Mt) > N then\n12:\nFind place cluster Mk where k = arg max len(Mk)\n13:\nRemove the oldest frame in Mk\n14: end if\nAlgorithm 6 Place Memory Read Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk. Each place cluster Mk is a FIFO Memory. Ad-\nditional variables are MineCLIP image encoder CLIPv, and text encoder CLIPt, task instruction\nτn at time step t, and task threshold h.\n1: Calculate sk(τn) = CLIPt(τn) · CLIPv(eMk) for all k\n2: Sort sk(τn) for all place clusters and select top-K place clusters\n3: Gather all experience frames xt = {et, lt, t} from the top-K place clusters\n4: cand = {}\n5: for each experience frame xt in the selected top-K place clusters do\n6:\nsxt(τn) = CLIPt(τn) · CLIPv(et)\n7:\nif sxt(τn) > h then\n8:\nAdd xt to cand\n9:\nend if\n10: end for\n11: return cand\n24\nPreprint\nE.3\nEVENT MEMORY WRITE & READ OPERATIONS\nAlgorithm 7 Event Memory Write Operation at time step t\nRequire: Assume memory at time step t as Mt = {Ek}K\nk=1 where Ek is k-th event cluster with\ncenter embedding eEk, and each event cluster is a FIFO Memory. Memory has a dummy deque\nQ that stores recent R frames, and update frequency timer r. Additional variables are Memory\nCapacity N, MineCLIP image encoder CLIPv, and experience Xt = {it, lt, t} at time step t.\nMineCLIP threshold c.\n1: # Memory Add\n2: Get experience frame xt = {et, lt, t} where et = CLIPv(it)\n3: Add xt to dummy deque Q\n4: Update frequency timer r = r + 1\n5: if r = R then\n6:\n{Ei, ei}l′\ni=1 = DP-Means(Qj)\n7:\n{Ei, ei}l\ni=1 = MERGE_CLUSTERS({Ei, ei}l′\ni=1)\n8:\nfor u = 1, . . . , l do\n9:\nadd_cluster=True\n10:\nfor p = 1, . . . , k do\n11:\nif ej\nu · eEp > c then\n12:\nMerge Eu to Ep\n13:\nadd_cluster=False; break\n14:\nend if\n15:\nend for\n16:\nif add_cluster=True then\n17:\nCreate new event cluster Ek+1 = Eu with center embedding eEk+1 = eu\n18:\nAdd created cluster Ek+1 to Mt\n19:\nend if\n20:\nend for\n21:\nrk = 0\n22: end if\n23: # Memory Remove\n24: if len(Mt) > N then\n25:\nFind event cluster Ek where k = arg max len(Ek)\n26:\nRemove the oldest frame in Ek\n27: end if\nAlgorithm 8 Event Memory Read Operation at time step t\nRequire: Assume memory at time step t as Mt = {Ek}K\nk=1 where Ek is k-th event cluster with\ncenter embedding eEk, and each event cluster Ek is a FIFO Memory. Additional variables are\nMineCLIP image encoder CLIPv, and text encoder CLIPt, task instruction τn at time step t, and\ntask threshold h.\n1: Calculate sk(τn) = CLIPt(τn) · CLIPv(eEk) for all k\n2: Sort sk(τn) for all event clusters and select top-K event clusters\n3: Gather all experience frames xt = {et, lt, t} from the top-K event clusters\n4: cand = {}\n5: for each experience frame xt in the selected top-K event clusters do\n6:\nsxt(τn) = CLIPt(τn) · CLIPv(et)\n7:\nif sxt(τn) > h then\n8:\nAdd xt to cand\n9:\nend if\n10: end for\n11: return cand\nDiscussions on MineCLIP threshold As in Event Memory’s Write Operation in Algorithm 7,\nclusters generated by DP Means algorithm are either merged with an exisiting event cluster, or\nadded as new event cluster. This is determined by MineCLIP threshold c, which is the criterion\n25\nPreprint\nfor separating event clusters. We note that using proper value for threshold is important for Event\nMemory to work reasonably across the tasks. If c is too small, Event Memory will cluster experience\nframes with only few clusters, which may not be visually distinctive. If c is too large, in extreme\ncase, Event Memory will make event cluster for each experience frame. When memory capacity is\nexceeded, it will randomly remove the oldest frame (since all event clusters have same size), which\nbehaves as a FIFO Memory.\nDrawback in Event Memory Event Memory removes the frame in the largest event cluster, when\nmemory capacity is exceeded. This indicates that the memory can retain visually distinct events.\nHowever, since it does not consider position information in clustering, similar visual frames in\ndifferent places can be clustered into same event cluster. This can be fatal since task-relevant frames\nin different places can be removed from the memory which can be crucial in upcoming tasks. This\ndrawback is shown in Find First-Visited House task in Section 4.3, and Long-Instruction task in\nSection 4.4, where MrSteve-EM showed suboptimal performances.\nE.4\nHYPER-PARAMETERS FOR MEMORY\nWe provide the specifications for each memory type used in the experiments. The task threshold h is\nset to 22.74 by default, but it can be adjusted to enhance retrieval accuracy.\nTable 2: Specifications for each memory type.\nMemory Type\nParameter\nValue\nPlace Event Memory\nPlace Cluster Size C\n6\nPlace Cluster Yaw Y\n60\nUpdate Frequency rk\n100\ntop-K\n30\nMineCLIP Threshold c\n73.5\nTask Threshold h\n22.74\nEvent Memory\nUpdate Frequency rk\n100\ntop-K\n30\nMineCLIP Threshold c\n73.5\nTask Threshold h\n22.74\nPlace Memory\nPlace Cluster Size C\n6\nPlace Cluster Yaw Y\n60\ntop-K\n30\nTask Threshold h\n22.74\n26\nPreprint\nFigure 10: The model architectures of Explore Mode and Execute Mode in Solver Module.\nF\nSOLVER MODULE DETAILS\nF.1\nMODE SELECTOR\nIn Section 3.2, we described the Mode Selector when combined with Place Event Memory. When\nthe memory type changes such as Place Memory or Event Memory, different read operation should\nbe employed. We provide the Memory Read operation of Place Memory, and Event Memory in\nAppendix E.2, and E.3, respectively. In case of FIFO Memory, for read operation, task alignment\nscores on whole frames in the memory is required.\nF.2\nEXPLORE MODE\nIn explore mode, hierarchical structure is employed. For high-level goal selector, we take the similar\napproach introduced in Chang et al. (2023), where the target problem is indoor navigation with\nrobots. Their global policy exploits semantic map to output exploration goal. For the goal selection,\nit uses frontier-based exploration (Yamauchi, 1998), which selects the closest unexplored region as\nthe goal. In Figure 10(a), the overview of hierarchical episodic exploration is illustrated. From Place\nEvent Memory, we contruct a visitation map by marking the agent’s visited locations. In addition to\nmarking agent’s visited locations, we also marked the agent’s FoV in the visitation map with sector\nregion towards agent’s head direction (yellow sector shows agent’s current FoV). From the visitation\nmap, the next goal is chosen, and the goal, and current observation, position is given to VPT-Nav\nfor generating low-level action. In experiments, for the tasks with map size 100 × 100, we used\nvisitation map size L = 120, and grid cell size G = 15. For the tasks with map size 200 × 200, we\nused visitation map size L = 240, and grid cell size G = 30.\nF.3\nEXECUTE MODE\nThe model architecture of Execute mode is illustrated in Figure 10(b). When the Mode Selector\nselects the Execute Mode, the experience frame of a task-relevant resource is retrieved from the\nmemory. MrSteve then navigates to the goal location of the experience frame, and then adjust camera\norientation using yaw and pitch from the frame to ensure it faces the observed of the retrieved\nexperience frame again. Once the camera adjustment is complete, Steve-1 is executed to follow the\ntask instruction τn.\nF.4\nMINECLIP & STEVE-1 PROMPTS FOR TASKS\nThe prompts used in our experiments are listed in Table 3. MrSteve sets τn to one of two different\nprompts depending on the agent’s status. When the agent is in the mode selection phase, the\nMineCLIP prompts are used as the query for the memory to determine whether the task-relevant\nresource exists in the memory. In contrast, the Steve-1 prompts are utilized during the execution\nmode.\nRelying solely on Steve-1 prompts can lead to difficulties in calculating alignment score between the\nprompt and memory records. Since MineCLIP (Fan et al., 2022b) computes the alignment between\nvideos and textual descriptions, the alignment score is higher when the textual description well\nreflects the agent’s action shown in the video. If a video contains scenes relevant to the current\n27\nPreprint\nTable 3: Prompts used in MrSteve for each task.\nMineCLIP\nSteve-1\nlog\nnear tree\ncut a tree\nbeef\nnear cow\nkill cow\ndirt\nnear dirt\nget dirt\nsand\nnear sand\nget sand\nseed\nnear grass\ncollect seeds\nwool\nnear sheep\nkill sheep\nleaves\nnear tree\nbreak leaves\nmilk bucket\nnear cow\nget milk bucket\nwater bucket\nnear water\nget water bucket\ntask but lacks behavior indicative of task completion, the alignment score will be low. For instance,\nconsider a scenario where the “obtain water bucket\n” task is given for the first time, and the agent\nhas previously encountered water. Although the agent is Near water in that scene, it has not yet\nperformed the action of Obtaining water bucket. As a result, the prompt “obtain water bucket” would\nnot accurately describe that memory, and that scene could be disregarded during the mode selection\nprocess. In our experiments, we manually defined two types of prompts for each task. However, this\nprocess can be automated using LLMs, which prompts an LLM to extract the more suitable language\ninstructions to complete a task.\nG\nEXPLORATION EXPERIMENTS DETAILS\nFigure 11: Agent’s trajectories of length 6K steps on 100 × 100 sized map with different exploration methods.\nLeft figure is the agent’s trajectory from our exploration method.\nWe provide further details on evaluation metrics and analysis of the exploration results. For evaluation\nmetrics, we measured Map Coverage and Rivisit Count. Map Coverage is calculated by dividing the\nmap into 11 × 11 grid cells and measuring the percentage of cells covered by the agent’s trajectory.\n28\nPreprint\nRevisit Count measures the average number of times the agent visits the same grid cell only for the\ncells that have visitation counts larger than 300. We measured Revisit Count only for highly visited\ngrid cell since the agent needs some amount of time to escape the grid cell.\nSince Steve-1 (Lifshitz et al., 2024) has no memory module, it tends to visit same place multiple times.\nWe also observed that Steve-1 exhibits a behavior of moving straight ahead when task instruction \"Go\nExplore\" is given, often colliding with walls and then following along the wall instead of avoiding it,\nwhich harms the Map Coverage and Revisit Count.\nPlan4MC (Yuan et al., 2023) employs a hierarchical exploration policy. The high-level policy, based\non an LSTM model, takes the agent’s (x, y) coordinates along its trajectory and outputs the next\ndirection to move (north, south, east, or west). The environment is discretized into an 11×11 grid, and\nthe policy is trained using PPO (Schulman et al., 2017) to maximize the number of unique grid cells\nvisited. The low-level policy is trained using DQN (Mnih et al., 2015) and follows the MineAgent\nstructure from the MineDojo framework. It receives a goal position and current observation in pixel\nand outputs actions in the MineDojo action space. However, despite the high-level RNN policy, it\nstruggled to keep track of visited locations as the trajectory grew longer, leading to a high Revisit\nCount. Furthermore, the low-level DQN controller faced difficulties navigating complex terrain, such\nas mountains and rivers, resulting in lower Map Coverage.\nH\nABA-Sparse TASKS FULL RESULTS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Dirt-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Dirt-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Dirt-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Dirt-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Seed-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Seed-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Seed-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Seed-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Log-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Log-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Log-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Leaves-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Leaves-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Leaves-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Leaves-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Sand-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Sand-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Sand-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 12: ABA-Sparse Tasks Full Result.\nWe report the results of all combinations of the ABA-Sparse tasks in Figure 12. The experiments\nwere conducted under the same conditions as described in Section 4.2. We note that MrSteve and\n29\nPreprint\nits memory variants consistently outperform Steve-1 in all tasks. In case of PMC-MrSteve-FM,\nit performed better than Steve-1 in most tasks, while it failed on some tasks due to a suboptimal\nexploration strategy.\nI\nMRSTEVE IN RANDOMLY GENERATED MAPS\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nSuccess Rate\nWater-Log-Water\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nBeef-Log-Beef\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWool-Log-Wool\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nSEQ(4)\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 13: The performance of different agents in ABA-Random tasks. MrSteve consistently outperforms\nSteve-1 in randomly generated map.\nIn this experiment, we investigate whether MrSteve benefits over Steve-1 in sequential task when\nthe map is randomly generated. To verify this, we created random Plains maps using the Minedojo\nenvironment (Fan et al., 2022b), and set up tasks similar to the ABA-Sparse tasks described in Section\n4.2. We call this task, ABA-Random task.\nIn this experiment, task A could involve collecting either water, beef, or wool, while task B involves\ngathering logs. Additionally, we introduce a sequential task named SEQ(4), which requires the agent\nto solve four consecutive tasks: log, water, wool, and beef. For the ABA-Random tasks, the agent was\ngiven 12K steps, and for the SEQ(4) task, 16K steps were allowed. We evaluated following agents:\nMrSteve, MrSteve-EM, MrSteve-PM, MrSteve-FM, PMC-MrSteve-FM, and Steve-1.\nAs shown in Figure 13, MrSteve and its memory variants consistently showed higher success rate\nthan Steve-1 across all tasks. This is because when Steve-1 is finished with task B, it tries to solve the\nfinal task A from scratch making it difficult to complete all tasks in limited time. However, MrSteve\ncould retain the experience frames of task A in memory making it efficient to solve all tasks in time.\nThis result suggests that augmenting memory plays a crucial role in solving sequential tasks, even in\nrandomly generated map.\nJ\nABLATION STUDIES ON PLACE EVENT MEMORY\n1\n2\n4\n10\n30\ntop-k\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\n1\n2\n4\n6\n12\nCluster Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 14: Success Rates of MrSteve with different top-\nk’s, and cluster sizes on Wool-Log-Wool task.\nIn this section, we study the robustness of our\nproposed agent MrSteve by exploring the effects\nof top-k selection in Mode Selector, and the size\nof a place cluster for Place Event Memory. For\nthis, we used one of the tasks from ABA-Sparse\ntask in Section 4.2, which is Wool-Log-Wool\ntask. The success rate of MrSteve with different\ntop-k’s and place cluster sizes are illustrated in\nFigure 14. For top-k experiment, we tested five\nk values, which are 1, 2, 4, 10, and 30. From\nthe results, we found that using small k did not\nlargely affect the performance. For cluster size experiment, we tested five cluster sizes, which\nare 1, 2, 4, 6, and 12 in Minecraft blocks. From the results, we see that using bigger cluster size\nslightly lowered the performance since the center embedding of the place cluster may not be good\nsummarization vector of the place when cluster size is large, making the agent difficult to recall\ntask-relevant frames in the memory.\n30\nPreprint\nTable 4: Querying Time and FLOPs for each memory types and k values. We report the averages and standard\nerrors over 1K query operations.\nMemory Type\ntop-k\nTime per Query (ms)\nFLOPs (×109)\nPlace Event Memory\n1\n6.24 ± 0.01\n1.591 ± 0.001\n2\n6.47 ± 0.02\n1.610 ± 0.002\n4\n6.89 ± 0.02\n1.667 ± 0.003\n10\n7.92 ± 0.03\n1.791 ± 0.004\n30\n11.24 ± 0.07\n2.210 ± 0.010\nEvent Memory\n1\n11.28 ± 0.35\n1.603 ± 0.054\n2\n18.95 ± 0.56\n2.711 ± 0.073\n4\n42.41 ± 1.20\n3.734 ± 0.088\n10\n53.82 ± 1.11\n7.476 ± 0.150\n30\n117.16 ± 2.02\n12.694 ± 0.201\nPlace Memory\n1\n3.86 ± 0.05\n1.251 ± 0.001\n2\n4.21 ± 0.08\n1.281 ± 0.002\n4\n4.72 ± 0.08\n1.338 ± 0.003\n10\n5.98 ± 0.09\n1.504 ± 0.005\n30\n9.85 ± 0.12\n2.024 ± 0.011\nFIFO\n–\n438.46 ± 0.10\n52.481 ± 0.000\nK\nCOMPUTATIONAL COMPLEXITY OF QUERY OPERATION\nWe provide the computational complexity of query operation of each memory type in Table 4. To\nensure a fair comparison, we generated an episode with a 100K-step trajectory, allowing each type\nof memory to process identical observations. After constructing each type of memory with this\ntrajectory, we randomly selected 1,000 observation embeddings from the trajectory for the query\noperations. The query time represents the elapsed time during the query function call. FLOPs denotes\nthe number of floating-point operations required for the MineCLIP score calculation. For FIFO\nmemory, MineCLIP scores are computed once during task alignment score calculation. For the other\nmemory types, MineCLIP scores are calculated twice: during the top-k selection and for the task\nalignment score.\nThe number of clusters affects the complexity of top-k selection and the number of frames per cluster\ninfluences the complexity of the task alignment score calculation. Place Event Memory consists of\napproximately 3,000 clusters, with each cluster containing an average of 30 frames. Place Memory,\non the other hand, includes about 2,300 clusters, each holding an average of 43 frames. Although\nPlace Memory provides the fastest querying time, Place Event Memory has comparable speed while\nachieving similar or superior success rates in most tasks.\nEvent Memory comprises around 582 clusters, with an average of 172 frames per cluster. Its clustering\napproach relies on visual discriminative features, which means visually similar places are grouped\ntogether, even if they are spatially distant. As a result, Event Memory has significantly fewer clusters\ncompared to Place and Place Event Memory.\nOverall, the three hierarchical memory types are computationally efficient and lightweight compared\nto FIFO Memory, which calculates the task alignment scores on whole 100K frames. In contrast,\nPlace Event Memory with k = 30 evaluates roughly 3,900 frames per query (3,000 for top-k selection\nand 30 frames per cluster across 30 clusters), resulting in a performance that is approximately 40\ntimes faster and requires about 24 times fewer FLOPs than FIFO Memory.\nL\nGOAL-CONDITIONED VPT NAVIGATOR DETAILS AND INVESTIGATION\nL.1\nGOAL-CONDITIONED VPT NAVIGATOR FINE-TUNING DETAILS\nWhen the goal location lG is selected by high-level goal selector, it is important for the agent to\nnavigate to the goal location accurately. Since navigating complex terrains (e.g., river, mountain)\nrequires human prior knowledge, we use VPT as our initial policy, and fine-tune it for goal-conditioned\n31\nPreprint\nnavigation policy. We name this model as VPT-Nav. To see how VPT-Nav model works, we first\ndescribe the components of VPT as follows:\nImage Encoder:\nxt = IE\nθ (it)\nTransformer-XL:\nzt−T :t = TrXLθ(xt−T , · · · , xt)\nPolicy Head:\nˆat ∼πθ(at|zt)\n(1)\nIn previous works, there were different approaches in fine-tuning VPT for specific purpose. First is\ngoal-conditioned behavior cloning. In Steve-1 (Lifshitz et al., 2024), authors added linear projection\nof a text embedding to the image embedding xt and fine-tuned the whole VPT model for behavior\ncloning from human demonstration data. This makes Steve-1 a text-conditioned VPT. In GROOT\n(Cai et al., 2023b), authors used gated cross-attention layers (Alayrac et al., 2022) in Transformer-XL\n(Tr-XL) to condition the video of some tasks (e.g., log wood). GROOT was trained for behavior\ncloning from reference videos of human plays working as video-conditioned VPT.\nThe second is RL fine-tuning for the single task. In DECKARD (Nottingham et al., 2023) and PTGM\n(Yuan et al., 2024), VPT was fine-tuned for single task with PPO (Schulman et al., 2017) by attaching\nadaptor (Houlsby et al., 2019) in Tr-XL layers, and value head ˆvt = vθ(zt). When fine-tuning, only\nthe adaptors, policy head, and value head were updated. For learning stability, those works used\ndifferent KL loss in PPO objective, which is KL loss between fine-tuning policy and VPT policy to\nkeep the VPT’s prior knowledge.\nSince our target is to train goal-conditioned navigation policy, one way to achieve this is to naively\ncombining ideas from 1) goal-conditioned behavior cloning, and 2) RL fine-tuning for the single task.\nWe first make goal embedding G(lt, lG) from the agent’s location lt and goal location lG with goal\nencoder G(·), then add it to image embedding xt. Second, we attach the adaptor in Tr-XL layers, and\nvalue head. Then, we fine-tune the whole model or only adaptors and policy, value heads with PPO.\nHowever, we found that this naive approach showed suboptimal navigation behavior in complex\nterrains such as mountain and river. We speculated that this is because RL objective is rather weak\nlearning signal compared to behavior cloning, which causes hardship in giving information of goal\nembedding to the policy head. Also, some information of goal embedding may be corrupted while\nit is added to image embedding, and processed by Tr-XL layers. Thus, we made the following\nmodifications in the model architecture. First, instead of giving goal embedding in Tr-XL input,\nwe added the goal embedding to policy head input. Second, we used recently proposed adaptor for\nTr-XL, which is LoRA (Hu et al., 2021b). The following changes enabled VPT-Nav to exhibit optimal\nnavigation behavior. We provide an investigation of this model architecture search in Appendix L.2.\nWith the changes in previous paragraph, the modified VPT for navigation with the learning parameters\nψ has the following components:\nImage Encoder:\nxt = IE\nθ (it)\nTransformer-XL:\nzt−T :t = TrXLψ(xt−T , · · · , xt)\nGoal Conditioning:\nz′\nt = GE\nψ(lt, lG) + zt\nPolicy Head:\nˆat ∼πψ(at|z′\nt)\nValue Head:\nˆvt = vψ(z′\nt),\n(2)\nHere, goal encoder GE\nψ is 4-layer MLP, and value head is a randomly initialized single linear layer.\nParameter ψ in Tr-XL indicates LoRA parameters. We use PPO for training with reward based on the\nincrease or decrease in Euclidean distance between locations of the goal and the agent. When the\nagent reaches the goal location within 3 blocks, it is considered a success, and an additional reward\nof 100 is given. The detailed hyper-parameters for training will be found in Table 5.\nL.2\nMODEL ARCHITECTURE SEARCH FOR GOAL-CONDITIONED VPT NAVIGATOR\nIn the previous section, we discussed the naive way of combining the idea of goal-conditioned\nbehaivor cloning and RL finetuning for training goal-conditioned navigator. In this section, we\nconduct an model architecture search on VPT model to find the optimal goal-conditioned navigation\nmodel. To do this, we focused on three key design choices: 1) the input location of goal embedding,\n2) training parameters, and 3) different Tr-XL adaptors . For the location of goal embedding, we\n32\nPreprint\nTable 5: Hyper-parameters for the Goal-Conditioned Navigation VPT Training.\nName\nValue\nInitial VPT Model\nrl-from-foundation-2x\nDiscount Factor\n0.999\nRollout Buffer Size\n40\nTraining Epochs per Iteration\n5\nVectorized Environments\n4\nLearning Rate\n10−4\nKL Loss Coefficient\n10−4\nKL Loss Coefficient Decay\n0.999\nTotal Iteration\n400K\nSteps per Iteration\n500\nGAE Lambda\n0.95\nClip Range\n0.2\nconsider the three tactics: (1) add the goal embedding to the image embeddings and input to Tr-XL\n(TrXL-Cond.), (2) add the goal embedding to Tr-XL output (Head-Cond.), and (3) add the goal\nembedding both at the input and output of Tr-XL (Dual-Cond.). For training parameters, we use\nfull-finetuning or finetuning only part of the model. For the Tr-XL adaptors, we consider the adaptor\nfrom Houlsby et al. (2019), and LoRA adaptor (Hu et al., 2021b). In Figure 15, we illustrate the\ndifferent model architectures for goal-conditioned VPT finetuning models.\nFor the model search, we summarized the navigation performance (SPL and Success Rate (SR)) of\ndifferent model architectures in Table 6. Interestingly, we found that adding goal embedding to image\nembeddings (TrXL-Cond.) showed suboptimal behavior, while adding goal embedding to output of\nTr-XL (Head-Cond., Dual-Cond.) showed good performance indicating that propagating learning\nsignal to goal embedding through Tr-XL is difficult from RL objective. We note that using LoRA\nadaptor showed higher performance than adaptors from Houlsby et al. (2019). In conclusion, using\nHead-Cond. with LoRA finetuning performed the best. Additionally, we tested one additional model\nthat does not update Tr-XL, which showed comparable result to the best performing model. This\narchitecture benefits for using smaller number of learning parameters and lower gradient computations.\nThus, we use the VPT-Nav trained with one of two architectures, which are Head-Cond. with LoRA\nfinetuning, and Head-Cond. with No Tr-XL Update.\nFigure 15: Four Types of Navigation Goal-Conditioning.\nL.3\nVPT NAVIGATOR ABLATION STUDY\nWe investigate how KL loss in PPO objective (i.e., KL loss between fine-tunin VPT and original\nVPT) affects the performance of VPT-Nav in diverse environments (Table 7). We measured the\nSPL and Success Rate (SR) of the navigators in navigation tasks where the goal location is within\n10–20 blocks away from agent’s location. While Heuristic method performs best in Flat, Plains, and\nMountain tasks, VPT-Nav with KL coefficient 10−4 showed high performances in all tasks. In case of\nlow-level navigator in Plan4MC (Yuan et al., 2023) which uses DQN policy, we observed suboptimal\n33\nPreprint\nTable 6: VPT-Nav Performance of different Goal-Conditioning Methods. Top-1 performances are bolded.\nConditioning\nFine-tuning\nFlat\nPlains\nMountain\nRiver\nTrXL\nFull\n0.050 (5%)\n0.423 (46%)\n0.000 (0%)\n0.000 (0%)\nHoulsby et al. (2019)\n0.488 (56%)\n0.305 (34%)\n0.052 (32%)\n0.000 (0%)\nLoRA\n0.481 (55%)\n0.475 (51%)\n0.058 (42%)\n0.000 (0%)\nDual\nHoulsby et al. (2019)\n0.883 (95%)\n0.828 (90%)\n0.125 (94%)\n0.066 (31%)\nLoRA\n0.798 (96%)\n0.762 (90%)\n0.169 (100%)\n0.287 (100%)\nHead\nNo TrXL Update\n0.849 (96%)\n0.780 (88%)\n0.157 (100%)\n0.274 (87%)\nHoulsby et al. (2019)\n0.729 (85%)\n0.841 (91%)\n0.159 (100%)\n0.052 (23%)\nLoRA\n0.904 (98%)\n0.880 (95%)\n0.150 (100%)\n0.307 (100%)\nbehavior. We note that VPT-Nav is robust to tasks with difficult terrains such as Mountain, and River\nsince VPT (Baker et al., 2022) is trained from human demonstration data which has high-quality\nnavigation ability. Also, using large KL coefficient (e.g., 10−2) harmed the overall performance while\nusing small KL coefficient (e.g., 0) harmed the robustness of navigator in complex tasks (e.g., River).\nTable 7: Performance of different Low-Level Navigators. Top-2 performances are bolded.\nSPL (Success Rate)\nFlat\nPlains\nMountain\nRiver\nHeuristic\n0.962 (100%)\n0.906 (94%)\n0.188 (100%)\n0.000 (0%)\nLow-level Navigator in Plan4MC\n0.416 (61%)\n0.326 (47%)\n0.010 (10%)\n0.000 (0%)\nVPT-Nav (KL = 0)\n0.806 (90%)\n0.774 (85%)\n0.131 (97%)\n0.034 (13%)\nVPT-Nav (KL = 10−4)\n0.849 (96%)\n0.780 (88%)\n0.157 (100%)\n0.274 (87%)\nVPT-Nav (KL = 10−3)\n0.762 (95%)\n0.651 (78%)\n0.015 (17%)\n0.012 (6%)\nVPT-Nav (KL = 10−2)\n0.692 (84%)\n0.702 (83%)\n0.078 (87%)\n0.014 (7%)\n34\nPreprint\nM\nLLM-AUGMENTED AGENT WITH MRSTEVE\nTable 8: Long-Horizon Planning Tasks details. The required subgoals denotes the length of shortest plan for\neach task. The items that the low-level controller should obtain are listed in Items to Obtain column.\nTask\nMax Steps\nRequired Subgoals\nItems to Obtain\noak_stairs\n3000\n4\nlog×3\nsign\n3000\n5\nlog×3\nfence\n3000\n5\nlog×3\nbed\n6000\n5\nlog×3, wool×3\npainting\n6000\n6\nlog×2, wool×1\ncarpet\n6000\n2\nwool×2\nitem_frame\n6000\n6\nlog×2, leather×1\nleather_boots\n6000\n5\nlog×1, leather×4\nleather_chestplate\n6000\n5\nlog×1, leather×8\nleather_helmet\n6000\n5\nlog×1, leather×5\nleather_leggings\n6000\n5\nlog×1, leather×7\nTable 9: Success Rate of two low-level controllers with DEPS (Wang et al., 2023c) planner.\nTask\nSteve-1\nMrSteve\noak_stairs\n67%\n80%\nsign\n53%\n60%\nfence\n40%\n50%\nbed\n27%\n50%\nIn this section, we investigate synergy between MrSteve and LLM-augmented hierarchical agent\nframework with long-horizon planning tasks listed in Table 8. We follow DEPS (Wang et al., 2023c)\nas LLM-augmented high-level planner with two low-level controllers: Steve-1 and MrSteve.\nOnce the target object for a task is given, the high-level planner asks the LLM to make the initial\nplan Pt = {τ g\nt,i}N\ni=1 for the task, where τ g\nt,i is i-th subgoal at timestep t in the textual form. After the\ninitial plan is given, the low-level controller executes the subgoal sequentially. Each subgoal is mine\nor craft type, where craft subgoals are executed heuristically via MineDojo functional actions,\nwhereas mine subgoals are executed by low-level controllers.\nLLM-generated initial plans can be inaccurate, so the low-level controllers often fails to execute\nsubgoals. For instance, we found that the initial plans frequently omit the subgoal for creating crafting\ntable, which is prerequisite item for the tasks in Table 8 except the carpet task. To address this\nproblem, DEPS framework introduced the replanning procedure with the descriptor and explainer\nmodules. The descriptor module makes a text prompt representing the inventory of the agent. The\nexplainer modules ask the LLM the reason of failure based on the text prompt from the descriptor\nmodule. Based on the explanation, the high-level planner asks the LLM to revise the plan and the\nlow-level controller executes subgoals from the revised plan.\nAdditionally, bacause the LLM does not observe the environment, the order of subgoals in a plan can\nbe suboptimal. When some subgoals share the same prerequisite so they can be executed in any order,\nit is more efficient to execute subgoal, which can complete faster than the other subgoals, first. This\ncan prevent wasting time by giving up subgoals that could be completed quickly, pursuing a subgoal\nwhose completion time is uncertain, and then going back to search for the previously quicker task\nagain after executing the subgoal. In DEPS, this procedure called elector module is implemented with\nthe horizon prediction module in GSB (Cai et al., 2023a). The horizon prediction module µ(st, τ g\nt,i)\ntakes the current observation st and the textual subgoals {τ g\nt,i}N\ni=1 and predicts the time to complete\neach subgoal. Based on the time prediction, a subgoal to be executed is sampled from the distribution\nas follows:\nSelector(τ g\nt,i; st, Pt) =\nexp(−µ(st, τ g\nt,i))\nP\nj exp(−µ(st, τ g\nt,j)).\n(3)\n35\nPreprint\nTable 10: Success Rate of two low-level controllers with the Ground-Truth plan for each task.\nTask\nSteve-1\nMrSteve\noak_stairs\n80%\n83%\nsign\n70%\n67%\nfence\n67%\n70%\nbed\n37%\n60%\npainting\n60%\n73%\ncarpet\n43%\n60%\nitem_frame\n53%\n63%\nleather_boots\n13%\n33%\nleather_chestplate\n3%\n17%\nleather_helmet\n20%\n20%\nleather_leggings\n0%\n13%\nWe report success rates of DEPS framework with Steve-1 and MrSteve in Table 9. We use Qwen2.5-\n72B (Team, 2024) as LLM in the high-level planner. DEPS with MrSteve shows comparable\nperformance or outperforms compared to DEPS with Steve-1. Especially, there is a huge performance\ngap between the two low-level controllers in the bed task, which requires killing three sheeps. We\nobserve that when the agent hit a sheep, it runs away from the agent and the agent chases it until\nthe agent gets wool items. After this, Steve-1 easily forgets the place where other sheeps exist\nand try to find sheep. However, MrSteve avoids this redundant exploration utilizing the episodic\nmemory. Hence, this results highlight the importance of episodic memory for low-level controllers in\nLLM-augmented hierarchical agent frameworks.\nIn Table 10, we also report success rates of hierarchical agents with Steve-1 and MrSteve and\nthe optimal plan for each plan. In this setting, because there is no instability from the LLM, the\nperformance is solely determined by performance of low-level controllers. Based on this experiment,\nalthough the goal is optimal, Steve-1 shows lower performance compared to MrSteve, indicating\nlow-level controllers is a performance bottleneck of hierarchical agent frameworks.\n36\nPreprint\nN\nTASK-CONDITIONED HIERARCHICAL EPISODIC EXPLORATION\nTable 11: Performance Comparison of Two Exploration Methods. We report success rates and the average and\nstandard error of execution times for the explore mode in ABA-Sparse tasks.\nTask-Conditioned Exploration\nTask-Free Exploration\nSuccess Rate\nExplore Mode Length\nSuccess Rate\nExplore Mode Length\nBeef-Log-Beef\n93%\n1202.64 ± 67.49\n92%\n1512.00 ± 62.37\nBeef-Leaves-Beef\n90%\n1269.23 ± 66.57\n91%\n1496.10 ± 69.43\nWool-Sand-Wool\n98%\n924.98 ± 56.85\n93%\n1350.00 ± 78.80\nWool-Dirt-Wool\n84%\n1303.21 ± 117.04\n84%\n1452.00 ± 89.92\nMilk-Sand-Milk\n86%\n1197.29 ± 71.44\n83%\n1526.48 ± 63.31\nMilk-Leaves-Milk\n59%\n1224.24 ± 68.79\n62%\n1579.35 ± 64.61\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTimestep\n0.0\n0.1\n0.2\n0.3\n0.4\nVisited Pasture Ratio\nTask-Free\nTask-Conditioned\nFigure 16: Given the task is “find cow”, the ratio of\npastures among the locations explored over time is pre-\nsented for two agents, each employing a different explo-\nration method.\nIn this section, we investigate an advanced ver-\nsion of hierarchical episodic exploration. The\ncurrent exploration method selects the next ex-\nploring position based on the visitation map gen-\nerated from PEM, while it does not use the infor-\nmation inherited in PEM. Hence, the current ex-\nploration method is task-free exploration. While\nnot using information from PEM worked pretty\nwell in practice, using knowledge from PEM\nmay have better explorative behaviors. Thus,\nwe implemented a new task-conditioned explo-\nration method which exploits information stored\nin PEM and evaluated two exploration methods\non exploration and task-solving tasks.\nThe two exploration methods have the same\nprimary goal: to visit the least visited places\nfirst. The difference between the two explo-\nration methods is how to select one of the least-visited places. The task-free exploration method\nrandomly selects the next exploring position among the least-visited locations. In contrast, the\ntask-conditioned exploration method selects the next exploring position among the least-visited\nlocations that are estimated to be related to the task based on the information inherited in PEM.\nTo implement the task-conditioned exploration method, the high-level exploration policy first accesses\nall event clusters when selecting the next exploring position. Using MineCLIP, the task-relevant score\nfor each event cluster is estimated by calculating the alignment score between the center embedding\nof the event cluster and the text prompt. If we use the text prompts for MineCLIP listed in Table 3,\nthe alignment score for locations where the target object has not yet been observed will tend to be\nlow, even if the location is relevant to the task, potentially hindering the exploration of those areas.\nTo address this problem, we used text prompts that describe places related to the task. For instance,\nthe text prompt for log\nis set to “near pasture”, while the text prompt for sand\nis set to “near\ndesert.” After that, Event clusters with alignment scores exceeding the threshold are collected, and a\ntask-relevance map is constructed to represent the agent’s FoVs of these event clusters, similarly to\nthe visitation map building method described in Appendix F.2.\nAdditionally, the 3 × 3 box blur filter is applied to the task-relevance map to introduce an inductive\nbias, assuming that if something is near X, it is likely to also be X. Finally, based on the task-\nrelevance map, the high-level exploration policy selects the next exploring position by prioritizing\nlocations that are the least visited, most task-relevant, and closest to the agent.\nFigure 16 shows the proportion of task-relevant locations among the places explored by the agent,\ndemonstrating that the task-conditioned exploration method explores task-relevant locations more\nquickly in the early stages. We used Map 2 in Figure 8 and set the task as “find cow,” and therefore\nmeasured the proportion of pastures, which are relevant to this task. In the early stages of the episode,\nthe task-conditioned exploration method explored pastures more than the task-free exploration method.\nAfter approximately 2000 steps, however, the exploration tendencies of both methods became similar.\n37\nPreprint\nTable 11 shows performance comparison between the two exploration methods in ABA-Sparse tasks.\nWe report success rates and execution times for the explore mode from MrSteve with the task-free and\ntask-conditioned exploration methods. The success rates between the two exploration methods are\ncomparable, while the task-conditioned exploration is finished earlier than the task-free exploration.\nThrough the results of the two experiments, we confirmed that fully utilizing PEM not only aids in\ntask-solving but also optimizes exploration more effectively.\nO\nABA-Sparse TASKS WITH MEMORY CONSTRAINTS\n0.1K\n0.5K\n1K\n2K\n12K\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Dirt-Wool\n0.1K\n0.5K\n1K\n2K\n12K\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.1K\n0.5K\n1K\n2K\n12K\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\n0.1K\n0.5K\n1K\n2K\n12K\nMemory Limit\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Sand-Wool\n0.1K\n0.5K\n1K\n2K\n12K\nMemory Limit\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Leaves-Beef\n0.1K\n0.5K\n1K\n2K\n12K\nMemory Limit\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Leaves-Milk\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nSteve-1\nFigure 17: Success Rate Comparison between MrSteve with its memory variants and Steve-1 in ABA-Sparse\ntasks with memory constraints. We tested different memory capacities (0.1K to 12K) for each model. In all tasks,\nthe performance of MrSteve-FM decreases as memory capacity gets smaller. We note that MrSteve is robust\nto memory capacities across tasks, while MrSteve-PM, and MrSteve-EM showed performance degradation in\nBeef-Log-Beef and Beef-Leaves-Beef tasks.\nIn this section, we investigate how MrSteve and its memory variants perform in ABA-Sparse tasks\nin Section 4.2 when memory capacity is limited. We evaluated each model with different memory\ncapacity ranging from 0.1K to 12K which is the maximum episode length. In all tasks, we observe\nthat the performance of MrSteve-FM decreases as the memory capacity gets small. This is because\nFIFO Memory in MrSteve-FM losts relevant frames in first task A while solving task B. While\nMrSteve showed robust performance to constrained memory capacities across tasks, MrSteve-PM,\nand MrSteve-EM showed degraded performances in Beef-Log-Beef and Beef-Leaves-Beef tasks\nwhen memory capacity is 0.1K. This indicates the robustness of PEM to memory capacities in\nABA-Sparse tasks.\nP\nPLACE EVENT MEMORY INVESTIGATION\nAlthough our place event memory enables efficient querying by clustering experience frames, it\nstores not only the center embedding of each event cluster but also all the frames that constitute the\nevent clusters. Since the frames within each cluster contain highly similar information, storing all of\nthem can increase redundancy in the memory system, potentially degrading storage efficiency and\nretrieval performance. Therefore, we attempted to optimize the memory-storing method of PEM in\nthe simplest way possible to investigate whether reducing the storage of redundant information could\nachieve better efficiency.\nWe implemented the modified PEM, which stores only center embeddings. The write and read\noperations of the modified PEM are the same as the original PEM. However, once event clustering\nis executed, the modified PEM stores only the center embedding of each event cluster and removes\nthe embeddings of other frames. In our experiment settings, event clustering is performed when the\ndummy deque of a place cluster has 100 frames, and until then, frames are stored in the dummy\ndeque of a place cluster, with each place cluster capable of holding up to 100 frames. However, even\n38\nPreprint\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Dirt-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Sand-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Leaves-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Leaves-Milk\nMrSteve (Center Embed)\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nSteve-1\nFigure 18: Success Rates Comparison between MrSteve with its memory variants and Steve-1 in ABA-Sparse\ntasks. We additionally evaluated MrSteve with modified place event memory, named MrSteve (Center Embed),\nthat stores center embeddings only.\nin such cases, memory read operation is highly optimized by accessing only the oldest frame in each\nplace cluster during querying operations.\nFigure 18 shows the performance comparison between MrSteve with its memory variants and Steve-1\nin ABA-Sparse tasks. Surprisingly, the performance of the simplest optimized PEM, named MrSteve\n(Center Embed), was not dropped significantly. Nevertheless, the computational cost for the querying\noperation can be reduced. After 12K environmental steps, 305.52 event clusters, on average, were\ngenerated across all tasks, with a standard error of 4.98. The original PEM stores all 12K frames and\neach event cluster in PEM holds approximately 40 frames on average. In the end, PEM calculates\nthe alignment scores between frames from the top-30 relevant event clusters and the task instruction,\nresulting in 1.2K comparisons. However, the modified PEM calculates the alignment scores for\naround 0.3K frames, making it more efficient.\nThis experiment result demonstrates the potential to further optimize the PEM memory-storing\nmethod. However, our simplest approach resulted in a slight performance drop. Hence, optimizing\nmemory to further reduce redundancy without losing important frames is a worthwhile direction for\nfuture work.\n39\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | MrSteve：Minecraft中具有What-Where-When记忆的指令跟随代理\n\n## 📌 背景痛点\/本文动机\n在Minecraft等复杂环境中，开发通用的具身AI代理面临着巨大挑战。尽管LLM增强的分层方法取得了显著进展，但低级控制器往往成为性能瓶颈，因为它们缺乏对过去事件的记忆，导致重复失败和低效探索。\n\n## 🚀 核心方法\n💡 创新点1：MrSteve (Memory Recall Steve-1)\n本文提出了MrSteve，一个新型的低级控制器，它配备了Place Event Memory (PEM)，一种能够捕获What-Where-When信息的情景记忆系统。PEM超越了Steve-1等现有模型的短期记忆限制，能够有效地组织和检索空间和事件数据，从而在长期任务中实现高效的回忆和导航。\n\n💡 创新点2：探索策略和记忆增强任务解决框架\n为了充分利用PEM，本文还提出了探索策略和记忆增强任务解决框架。该框架允许代理根据回忆的事件在探索和任务解决之间进行切换，从而显著提高任务解决和探索效率。\n\n## 📈 实验结果\n实验结果表明，与现有方法相比，MrSteve在探索和长期任务解决方面都取得了显著的性能提升。例如，在ABA-Sparse任务中，MrSteve能够更快地找到任务相关的资源，并在有限的时间内完成任务。\n\n## 💬 可借鉴之处\n本文提出的PEM和记忆增强框架为开发更高效的低级控制器提供了新的思路。PEM的组织和检索机制可以应用于其他具身AI代理，以提高它们在复杂环境中的性能。此外，探索策略和任务解决框架的设计也可以为其他AI系统提供参考，以实现更灵活和自适应的行为。","llm_summary_res_status":200}
{"title":"Voyager: An Open-Ended Embodied Agent with Large Language Models","authors":"Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar","summary":"We introduce Voyager, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. Voyager consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving\ncomplex behaviors, and 3) a new iterative prompting mechanism that incorporates\nenvironment feedback, execution errors, and self-verification for program\nimprovement. Voyager interacts with GPT-4 via blackbox queries, which bypasses\nthe need for model parameter fine-tuning. The skills developed by Voyager are\ntemporally extended, interpretable, and compositional, which compounds the\nagent's abilities rapidly and alleviates catastrophic forgetting. Empirically,\nVoyager shows strong in-context lifelong learning capability and exhibits\nexceptional proficiency in playing Minecraft. It obtains 3.3x more unique\nitems, travels 2.3x longer distances, and unlocks key tech tree milestones up\nto 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill\nlibrary in a new Minecraft world to solve novel tasks from scratch, while other\ntechniques struggle to generalize. We open-source our full codebase and prompts\nat https:\/\/voyager.minedojo.org\/.","url":"http:\/\/arxiv.org\/abs\/2305.16291v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2305.16291v2","published":1685036798000,"comment":"Project website and open-source codebase:\n  https:\/\/voyager.minedojo.org\/","pdf_text":"VOYAGER: An Open-Ended Embodied Agent\nwith Large Language Models\nGuanzhi Wang1 2 #, Yuqi Xie3, Yunfan Jiang4∗, Ajay Mandlekar1∗,\nChaowei Xiao1 5, Yuke Zhu1 3, Linxi “Jim” Fan1† #, Anima Anandkumar1 2†\n1NVIDIA, 2Caltech, 3UT Austin, 4Stanford, 5UW Madison\n∗Equal contribution\n†Equal advising\n# Corresponding authors\nhttps:\/\/voyager.minedojo.org\nAbstract\nWe introduce VOYAGER, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. VOYAGER consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving complex\nbehaviors, and 3) a new iterative prompting mechanism that incorporates environ-\nment feedback, execution errors, and self-verification for program improvement.\nVOYAGER interacts with GPT-4 via blackbox queries, which bypasses the need for\nmodel parameter fine-tuning. The skills developed by VOYAGER are temporally\nextended, interpretable, and compositional, which compounds the agent’s abilities\nrapidly and alleviates catastrophic forgetting.\nEmpirically, VOYAGER shows\nstrong in-context lifelong learning capability and exhibits exceptional proficiency\nin playing Minecraft. It obtains 3.3× more unique items, travels 2.3× longer\ndistances, and unlocks key tech tree milestones up to 15.3× faster than prior SOTA.\nVOYAGER is able to utilize the learned skill library in a new Minecraft world to\nsolve novel tasks from scratch, while other techniques struggle to generalize.\nFigure 1: VOYAGER discovers new Minecraft items and skills continually by self-driven exploration,\nsignificantly outperforming the baselines. X-axis denotes the number of prompting iterations.\n1\narXiv:2305.16291v2  [cs.AI]  19 Oct 2023\nMine Wood  Log\nMake Crafting Table\nCraft Stone Sword\nCraft Shield\nMake Furnace\nCook Steak\nCombat Zombie\n     Mine Wood Log\nMake Crafting Table\nCombat \nZombie\nMine Diamond\nNew \nTask\nCode as \nActions\nRefine Program\nEnv Feedback\nExecution Errors\nUpdate \nExploration \nProgress\nSkill \nRetrieval\nAdd New Skill\nAutomatic Curriculum\nIterative Prompting Mechanism\nSkill Library\nEnvironment\nSelf-Verification\nFigure 2: VOYAGER consists of three key components: an automatic curriculum for open-ended\nexploration, a skill library for increasingly complex behaviors, and an iterative prompting mechanism\nthat uses code as action space.\n1\nIntroduction\nBuilding generally capable embodied agents that continuously explore, plan, and develop new skills\nin open-ended worlds is a grand challenge for the AI community [1–5]. Classical approaches\nemploy reinforcement learning (RL) [6, 7] and imitation learning [8–10] that operate on primitive\nactions, which could be challenging for systematic exploration [11–15], interpretability [16–18], and\ngeneralization [19–21]. Recent advances in large language model (LLM) based agents harness the\nworld knowledge encapsulated in pre-trained LLMs to generate consistent action plans or executable\npolicies [16, 22, 19]. They are applied to embodied tasks like games and robotics [23–27], as well as\nNLP tasks without embodiment [28–30]. However, these agents are not lifelong learners that can\nprogressively acquire, update, accumulate, and transfer knowledge over extended time spans [31, 32].\nLet us consider Minecraft as an example. Unlike most other games studied in AI [33, 34, 10],\nMinecraft does not impose a predefined end goal or a fixed storyline but rather provides a unique\nplayground with endless possibilities [23]. Minecraft requires players to explore vast, procedurally\ngenerated 3D terrains and unlock a tech tree using gathered resources. Human players typically start\nby learning the basics, such as mining wood and cooking food, before advancing to more complex\ntasks like combating monsters and crafting diamond tools. We argue that an effective lifelong learning\nagent should have similar capabilities as human players: (1) propose suitable tasks based on its\ncurrent skill level and world state, e.g., learn to harvest sand and cactus before iron if it finds itself in\na desert rather than a forest; (2) refine skills based on environmental feedback and commit mastered\nskills to memory for future reuse in similar situations (e.g. fighting zombies is similar to fighting\nspiders); (3) continually explore the world and seek out new tasks in a self-driven manner.\nTowards these goals, we introduce VOYAGER, the first LLM-powered embodied lifelong learning\nagent to drive exploration, master a wide range of skills, and make new discoveries continually\nwithout human intervention in Minecraft. VOYAGER is made possible through three key modules\n(Fig. 2): 1) an automatic curriculum that maximizes exploration; 2) a skill library for storing\nand retrieving complex behaviors; and 3) a new iterative prompting mechanism that generates\nexecutable code for embodied control. We opt to use code as the action space instead of low-level\nmotor commands because programs can naturally represent temporally extended and compositional\nactions [16, 22], which are essential for many long-horizon tasks in Minecraft. VOYAGER interacts\nwith a blackbox LLM (GPT-4 [35]) through prompting and in-context learning [36–38]. Our approach\nbypasses the need for model parameter access and explicit gradient-based training or finetuning.\nMore specifically, VOYAGER attempts to solve progressively harder tasks proposed by the automatic\ncurriculum, which takes into account the exploration progress and the agent’s state. The curriculum\nis generated by GPT-4 based on the overarching goal of “discovering as many diverse things as\npossible”. This approach can be perceived as an in-context form of novelty search [39, 40]. VOYAGER\nincrementally builds a skill library by storing the action programs that help solve a task successfully.\n2\nInventory (5\/36): {'oak_planks': 3, 'stick': \n4, 'crafting_table': 1, 'stone': 3, \n'wooden_pickaxe': 1}\nBiome: river\nInventory (4\/36): {'sugar_cane': 3, 'paper': \n3, 'fishing_rod': 1, 'wooden_pickaxe': 1}\nNearby entities: pig, cat, villager\r\nHealth: 12\/20\r\nHunger: 0\/20\nInventory (6\/36): {'furnace': 1, \n'stone_pickaxe': 1, 'oak_planks': 7, \n'crafting_table': 1, 'raw_iron': 4, 'coal': 1} \nTime: night\nNearby entities: zombie\nEquipment: 'stone_sword', 'shield'\nReasoning: Since you have a wooden pickaxe and some stones, it would be \nbeneficial to upgrade your pickaxe to a stone pickaxe for better efficiency.\r\nTask: Craft 1 stone pickaxe.\nReasoning: Since you have a fishing rod in your inventory and you are near a river \nbiome, it would be a good opportunity to catch some fish for food and experience. \nTask: Catch 1 fish.\nReasoning: Your hunger is at 0, which means you need to find food to replenish \nyour hunger. Since there are pigs nearby, you can kill one to obtain raw porkchops.\nTask: Kill 1 pig.\nReasoning: You have raw iron and coal, and you have a furnace. It's time to smelt \nthe iron to make iron ingots, which can be used to craft better tools and armor.\r\nTask: Smelt 4 raw iron.\nReasoning: Since it's night and there's a zombie nearby, it's a good opportunity to \ntry killing the zombie now that you have a stone sword and a shield equipped.\nTask: Kill 1 zombie.\nGPT-4\nGPT-4\nGPT-4\nGPT-4\nGPT-4\nFigure 3: Tasks proposed by the automatic curriculum. We only display the partial prompt for brevity.\nSee Appendix, Sec. A.3 for the full prompt structure.\nEach program is indexed by the embedding of its description, which can be retrieved in similar\nsituations in the future. Complex skills can be synthesized by composing simpler programs, which\ncompounds VOYAGER’s capabilities rapidly over time and alleviates catastrophic forgetting in other\ncontinual learning methods [31, 32].\nHowever, LLMs struggle to produce the correct action code consistently in one shot [41]. To address\nthis challenge, we propose an iterative prompting mechanism that: (1) executes the generated\nprogram to obtain observations from the Minecraft simulation (such as inventory listing and nearby\ncreatures) and error trace from the code interpreter (if any); (2) incorporates the feedback into GPT-4’s\nprompt for another round of code refinement; and (3) repeats the process until a self-verification\nmodule confirms the task completion, at which point we commit the program to the skill library (e.g.,\ncraftStoneShovel() and combatZombieWithSword()) and query the automatic curriculum for\nthe next milestone (Fig. 2).\nEmpirically, VOYAGER demonstrates strong in-context lifelong learning capabilities. It can construct\nan ever-growing skill library of action programs that are reusable, interpretable, and generalizable\nto novel tasks. We evaluate VOYAGER systematically against other LLM-based agent techniques\n(e.g., ReAct [29], Reflexion [30], AutoGPT [28]) in MineDojo [23], an open-source Minecraft AI\nframework. VOYAGER outperforms prior SOTA by obtaining 3.3× more unique items, unlocking key\ntech tree milestones up to 15.3× faster, and traversing 2.3× longer distances. We further demonstrate\nthat VOYAGER is able to utilize the learned skill library in a new Minecraft world to solve novel tasks\nfrom scratch, while other methods struggle to generalize.\n2\nMethod\nVOYAGER consists of three novel components: (1) an automatic curriculum (Sec. 2.1) that suggests\nobjectives for open-ended exploration, (2) a skill library (Sec. 2.2) for developing increasingly\ncomplex behaviors, and (3) an iterative prompting mechanism (Sec. 2.3) that generates executable\ncode for embodied control. Full prompts are presented in Appendix, Sec. A.\n2.1\nAutomatic Curriculum\nEmbodied agents encounter a variety of objectives with different complexity levels in open-ended\nenvironments. An automatic curriculum offers numerous benefits for open-ended exploration, ensur-\ning a challenging but manageable learning process, fostering curiosity-driven intrinsic motivation\nfor agents to learn and explore, and encouraging the development of general and flexible problem-\nsolving strategies [42–44]. Our automatic curriculum capitalizes on the internet-scale knowledge\ncontained within GPT-4 by prompting it to provide a steady stream of new tasks or challenges. The\ncurriculum unfolds in a bottom-up fashion, allowing for considerable adaptability and responsiveness\nto the exploration progress and the agent’s current state (Fig. 3). As VOYAGER progresses to harder\nself-driven goals, it naturally learns a variety of skills, such as “mining a diamond”.\n3\nProgram Description\nSkill Library\nTop-5 Relevant Skills\nProgram Generated by GPT-4\nTask: Craft Iron Pickaxe\nKey\nAdd\nRetrieve\nValue\nSkill Library\nQuery\nHow to craft an iron pickaxe in \nMinecraft?\nTo craft an iron pickaxe, you \nneed to 3 iron ingots and 2 \nsticks. Once you have gathered \nthe materials, ....\n----------------------------------\n         Environment Feedback\nMine Wood  Log\nMake Crafting Table\nCraft Wooden Pickaxe\nCraft Stone Sword\nMake Furnace\n...\nCombat Cow\nCook Steak\nCraft Iron Axe\nCombat Zombie\nSmelt Iron Ingot\nCraft Stick\nMake Crafting Table\nMake Furnace\nCraft Wooden Pickaxe\nGPT-3.5\nEmbedding\nEmbedding\nGPT-3.5\nFigure 4: Skill library. Top: Adding a new skill. Each time GPT-4 generates and verifies a new\nskill, we add it to the skill library, represented by a vector database. The key is the embedding vector\nof the program description (generated by GPT-3.5), while the value is the program itself. Bottom:\nSkill retrieval. When faced with a new task proposed by the automatic curriculum, we first leverage\nGPT-3.5 to generate a general suggestion for solving the task, which is combined with environment\nfeedback as the query context. Subsequently, we perform querying to identify the top-5 relevant skills.\nThe input prompt to GPT-4 consists of several components:\n(1) Directives encouraging diverse behaviors and imposing constraints,\nsuch as\n“My ultimate goal is to discover as many diverse things as possible\n...\nThe next task should not be too hard since I may not have the\nnecessary resources or have learned enough skills to complete it\nyet.”;\n(2) The agent’s current state, including inventory, equipment, nearby blocks and entities,\nbiome, time, health and hunger bars, and position;\n(3) Previously completed and failed tasks, reflecting the agent’s current exploration progress\nand capabilities frontier;\n(4) Additional context: We also leverage GPT-3.5 to self-ask questions based on the agent’s\ncurrent state and exploration progress and self-answer questions. We opt to use GPT-3.5\ninstead of GPT-4 for standard NLP tasks due to budgetary considerations.\n2.2\nSkill Library\nWith the automatic curriculum consistently proposing increasingly complex tasks, it is essential to\nhave a skill library that serves as a basis for learning and evolution. Inspired by the generality, inter-\npretability, and universality of programs [45], we represent each skill with executable code that scaf-\nfolds temporally extended actions for completing a specific task proposed by the automatic curriculum.\nThe input prompt to GPT-4 consists of the following components:\n(1) Guidelines\nfor\ncode\ngeneration,\nsuch\nas\n“Your function will be reused\nfor building more complex functions.\nTherefore, you should make\nit generic and reusable.”;\n(2) Control primitive APIs, and relevant skills retrieved from the skill library, which are\ncrucial for in-context learning [36–38] to work well;\n(3) The generated code from the last round, environment feedback, execution errors, and\ncritique, based on which GPT-4 can self-improve (Sec. 2.3);\n(4) The agent’s current state, including inventory, equipment, nearby blocks and entities,\nbiome, time, health and hunger bars, and position;\n4\nI cannot make stick because I need:  2 more planks\nI cannot make stone_shovel because I need:  2 more stick\nthrow new Error(`No item named ${name}`);\nNo item named acacia_axe\nat line 18:await craftItem(bot, \"acacia_axe\", 1);\nEnvironment Feedback\nExecution Error\nGPT-4\nGPT-4\nFigure 5: Left: Environment feedback. GPT-4 realizes it needs 2 more planks before crafting sticks.\nRight: Execution error. GPT-4 realizes it should craft a wooden axe instead of an acacia axe since\nthere is no acacia axe in Minecraft. We only display the partial prompt for brevity. The full prompt\nstructure for code generation is in Appendix, Sec. A.4.\n(5) Chain-of-thought prompting [46] to do reasoning before code generation.\nWe iteratively refine the program through a novel iterative prompting mechanism (Sec. 2.3), in-\ncorporate it into the skill library as a new skill, and index it by the embedding of its description\n(Fig. 4, top). For skill retrieval, we query the skill library with the embedding of self-generated task\nplans and environment feedback (Fig. 4, bottom). By continuously expanding and refining the skill\nlibrary, VOYAGER can learn, adapt, and excel in a wide spectrum of tasks, consistently pushing the\nboundaries of its capabilities in the open world.\n2.3\nIterative Prompting Mechanism\nWe introduce an iterative prompting mechanism for self-improvement through three types of feedback:\n(1) Environment feedback, which illustrates the intermediate progress of program execution\n(Fig. 5, left). For example, “I cannot make an iron chestplate because I need:\n7 more iron ingots” highlights the cause of failure in crafting an iron chestplate. We use\nbot.chat() inside control primitive APIs to generate environment feedback and prompt\nGPT-4 to use this function as well during code generation;\n(2) Execution errors from the program interpreter that reveal any invalid operations or syntax\nerrors in programs, which are valuable for bug fixing (Fig. 5, right);\n(3) Self-verification for checking task success. Instead of manually coding success checkers\nfor each new task proposed by the automatic curriculum, we instantiate another GPT-4\nagent for self-verification. By providing VOYAGER’s current state and the task to GPT-4,\nwe ask it to act as a critic [47–49] and inform us whether the program achieves the task.\nIn addition, if the task fails, it provides a critique by suggesting how to complete the task\n(Fig. 6). Hence, our self-verification is more comprehensive than self-reflection [30] by both\nchecking success and reflecting on mistakes.\nDuring each round of code generation, we execute the generated program to obtain environment\nfeedback and execution errors from the code interpreter, which are incorporated into GPT-4’s prompt\nfor the next round of code refinement. This iterative process repeats until self-verification validates\n5\nInventory (8\/36): {'oak_planks': 5, 'cobblestone': 2, \n'porkchop': 2, 'wooden_sword': 1, 'coal': 5, 'wooden_pickaxe': \n1, 'oak_log': 3, 'dirt': 9}\nTask: Mine 5 coal ores\nInventory (10\/36): {'raw_copper': 9, 'copper_ingot': 3, \n'acacia_planks': 1, 'raw_iron': 1, 'stick': 1, 'iron_sword': 1, \n'iron_pickaxe': 1, 'iron_ingot': 3, 'crafting_table': 1, 'furnace': 1}\nTask: Craft a spyglass\nInventory (7\/36): {'oak_log': 5, 'oak_planks': 1, \n'wooden_pickaxe': 1, 'wooden_sword': 1, 'porkchop': 2, \n'white_wool': 2, 'mutton': 6}\nTask: Kill 3 sheep\nInventory (9\/36): {'string': 4, 'coal': 1, 'rotten_flesh': 1, \n'iron_sword': 1, 'furnace': 1, 'dirt': 6, 'stone_shovel': 1, \n'wooden_pickaxe': 1, 'granite': 5}\nTask: Kill 1 zombie\nReasoning: Mining coal_ore in Minecraft will get coal. You have 5 coal in your \ninventory.\nSuccess: True\nReasoning: To craft a spyglass, you need 2 copper ingots and 1 amethyst shard. \nYou have 3 copper ingots, but you don't have any amethyst shards.\nSuccess: False\nCritique: Find and mine an amethyst shard underground.\nReasoning: You have 2 white_wool and 6 mutton in your inventory, which indicates \nthat you killed 2 sheep. You needed to kill 3 sheep.\nSuccess: False\nCritique: Find and kill one more sheep to complete the task.\nReasoning: You have 1 rotten_flesh in your inventory, which means you have killed \nat least 1 zombie.\nSuccess: True\nGPT-4\nGPT-4\nGPT-4\nGPT-4\nFigure 6: Self-verification examples. We only display the partial prompt for brevity. See Appendix,\nSec. A.5 for the full prompt structure.\nthe task’s completion, at which point we add this new skill to the skill library and ask the automatic\ncurriculum for a new objective (Fig. 2). If the agent gets stuck after 4 rounds of code generation, then\nwe query the curriculum for another task. This iterative prompting approach significantly improves\nprogram synthesis for embodied control, enabling VOYAGER to continuously acquire diverse skills\nwithout human intervention.\n3\nExperiments\n3.1\nExperimental Setup\nWe leverage OpenAI’s gpt-4-0314 [35] and gpt-3.5-turbo-0301 [50] APIs for text completion,\nalong with text-embedding-ada-002 [51] API for text embedding. We set all temperatures to\n0 except for the automatic curriculum, which uses temperature = 0.1 to encourage task diversity. Our\nsimulation environment is built on top of MineDojo [23] and leverages Mineflayer [52] JavaScript\nAPIs for motor controls. See Appendix, Sec. B.1 for more details.\n3.2\nBaselines\nBecause there is no LLM-based agents that work out of the box for Minecraft, we make our best\neffort to select a number of representative algorithms as baselines. These methods are originally\ndesigned only for NLP tasks without embodiment, therefore we have to re-interpret them to be\nexecutable in MineDojo and compatible with our experimental setting:\nReAct [29] uses chain-of-thought prompting [46] by generating both reasoning traces and action\nplans with LLMs. We provide it with our environment feedback and the agent states as observations.\nReflexion [30] is built on top of ReAct [29] with self-reflection to infer more intuitive future actions.\nWe provide it with execution errors and our self-verification module.\nAutoGPT [28] is a popular software tool that automates NLP tasks by decomposing a high-level\ngoal into multiple subgoals and executing them in a ReAct-style loop. We re-implement AutoGPT\nby using GPT-4 to do task decomposition and provide it with the agent states, environment feedback,\nand execution errors as observations for subgoal execution. Compared with VOYAGER, AutoGPT\nlacks the skill library for accumulating knowledge, self-verification for assessing task success, and\nautomatic curriculum for open-ended exploration.\nNote that we do not directly compare with prior methods that take Minecraft screen pixels as input\nand output low-level controls [53–55]. It would not be an apple-to-apple comparison, because we rely\non the high-level Mineflayer [52] API to control the agent. Our work’s focus is on pushing the limits\nof GPT-4 for lifelong embodied agent learning, rather than solving the 3D perception or sensorimotor\ncontrol problems. VOYAGER is orthogonal and can be combined with gradient-based approaches like\n6\nTable 1: Tech tree mastery. Fractions indicate the number of successful trials out of three total runs.\n0\/3 means the method fails to unlock a level of the tech tree within the maximal prompting iterations\n(160). Numbers are prompting iterations averaged over three trials. The fewer the iterations, the\nmore efficient the method.\nMethod\nWooden Tool\nStone Tool\nIron Tool\nDiamond Tool\nReAct [29]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nReflexion [30]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nAutoGPT [28]\n92 ± 72 (3\/3)\n94 ± 72 (3\/3)\n135 ± 103 (3\/3)\nN\/A (0\/3)\nVOYAGER w\/o Skill Library\n7 ± 2 (3\/3)\n9 ± 4 (3\/3)\n29 ± 11 (3\/3)\nN\/A (0\/3)\nVOYAGER (Ours)\n6 ± 2 (3\/3)\n11 ± 2 (3\/3)\n21 ± 7 (3\/3)\n102 (1\/3)\nFigure 7: Map coverage: bird’s eye views of Minecraft maps. VOYAGER is able to traverse 2.3×\nlonger distances compared to baselines while crossing diverse terrains.\nVPT [8] as long as the controller provides a code API. We make a system-level comparison between\nVOYAGER and prior Minecraft agents in Table. A.2.\n3.3\nEvaluation Results\nWe systematically evaluate VOYAGER and baselines on their exploration performance, tech tree\nmastery, map coverage, and zero-shot generalization capability to novel tasks in a new world.\nSignificantly better exploration.\nResults of exploration performance are shown in Fig. 1.\nVOYAGER’s superiority is evident in its ability to consistently make new strides, discovering 63\nunique items within 160 prompting iterations, 3.3× many novel items compared to its counterparts.\nOn the other hand, AutoGPT lags considerably in discovering new items, while ReAct and Reflexion\nstruggle to make significant progress, given the abstract nature of the open-ended exploration goal\nthat is challenging to execute without an appropriate curriculum.\nConsistent tech tree mastery. The Minecraft tech tree tests the agent’s ability to craft and use a\nhierarchy of tools. Progressing through this tree (wooden tool →stone tool →iron tool →diamond\ntool) requires the agent to master systematic and compositional skills. Compared with baselines,\nVOYAGER unlocks the wooden level 15.3× faster (in terms of the prompting iterations), the stone\nlevel 8.5× faster, the iron level 6.4× faster, and VOYAGER is the only one to unlock the diamond level\nof the tech tree (Fig. 2 and Table. 1). This underscores the effectiveness of the automatic curriculum,\nwhich consistently presents challenges of suitable complexity to facilitate the agent’s progress.\nExtensive map traversal. VOYAGER is able to navigate distances 2.3× longer compared to baselines\nby traversing a variety of terrains, while the baseline agents often find themselves confined to local\nareas, which significantly hampers their capacity to discover new knowledge (Fig. 7).\n7\nTable 2: Zero-shot generalization to unseen tasks. Fractions indicate the number of successful\ntrials out of three total attempts. 0\/3 means the method fails to solve the task within the maximal\nprompting iterations (50). Numbers are prompting iterations averaged over three trials. The fewer\nthe iterations, the more efficient the method.\nMethod\nDiamond Pickaxe\nGolden Sword\nLava Bucket\nCompass\nReAct [29]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nReflexion [30]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nAutoGPT [28]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nAutoGPT [28] w\/ Our Skill Library\n39 (1\/3)\n30 (1\/3)\nN\/A (0\/3)\n30 (2\/3)\nVOYAGER w\/o Skill Library\n36 (2\/3)\n30 ± 9 (3\/3)\n27 ± 9 (3\/3)\n26 ± 3 (3\/3)\nVOYAGER (Ours)\n19 ± 3 (3\/3)\n18 ± 7 (3\/3)\n21 ± 5 (3\/3)\n18 ± 2 (3\/3)\nFigure 8: Zero-shot generalization to unseen tasks. We visualize the intermediate progress of each\nmethod on two tasks. See Appendix, Sec. B.4.3 for the other two tasks. We do not plot ReAct and\nReflexion since they do not make any meaningful progress.\nEfficient zero-shot generalization to unseen tasks. To evaluate zero-shot generalization, we clear\nthe agent’s inventory, reset it to a newly instantiated world, and test it with unseen tasks. For both\nVOYAGER and AutoGPT, we utilize GPT-4 to break down the task into a series of subgoals. Table. 2\nand Fig. 8 show VOYAGER can consistently solve all the tasks, while baselines cannot solve any task\nwithin 50 prompting iterations. What’s interesting to note is that our skill library constructed from\nlifelong learning not only enhances VOYAGER’s performance but also gives a boost to AutoGPT.\nThis demonstrates that the skill library serves as a versatile tool that can be readily employed by other\nmethods, effectively acting as a plug-and-play asset to enhance performance.\n3.4\nAblation Studies\nWe ablate 6 design choices (automatic curriculum, skill library, environment feedback, execution\nerrors, self-verification, and GPT-4 for code generation) in VOYAGER and study their impact on\nexploration performance (see Appendix, Sec. B.3 for details of each ablated variant). Results are\nshown in Fig. 9. We highlight the key findings below:\n• Automatic curriculum is crucial for the agent’s consistent progress. The discovered item\ncount drops by 93% if the curriculum is replaced with a random one, because certain tasks\nmay be too challenging if attempted out of order. On the other hand, a manually designed\ncurriculum requires significant Minecraft-specific expertise, and does not take into account\nthe agent’s live situation. It falls short in the experimental results compared to our automatic\ncurriculum.\n• VOYAGER w\/o skill library exhibits a tendency to plateau in the later stages. This\nunderscores the pivotal role that the skill library plays in VOYAGER. It helps create more\ncomplex actions and steadily pushes the agent’s boundaries by encouraging new skills to be\nbuilt upon older ones.\n8\nFigure 9: Left: Ablation studies for the automatic curriculum, skill library, and GPT-4. GPT-3.5\nmeans replacing GPT-4 with GPT-3.5 for code generation. VOYAGER outperforms all the alternatives,\ndemonstrating the critical role of each component. Right: Ablation studies for the iterative\nprompting mechanism. VOYAGER surpasses all the other options, thereby highlighting the essential\nsignificance of each type of feedback in the iterative prompting mechanism.\nFigure 10: VOYAGER builds 3D structures with human feedback. The progress of building designs\nthat integrate human input is demonstrated from left to right.\n• Self-verification is the most important among all the feedback types. Removing the\nmodule leads to a significant drop (−73%) in the discovered item count. Self-verification\nserves as a critical mechanism to decide when to move on to a new task or reattempt a\npreviously unsuccessful task.\n• GPT-4 significantly outperforms GPT-3.5 in code generation and obtains 5.7× more\nunique items, as GPT-4 exhibits a quantum leap in coding abilities. This finding corroborates\nrecent studies in the literature [56, 57].\n3.5\nMultimodal Feedback from Humans\nVOYAGER does not currently support visual perception, because the available version of GPT-4 API\nis text-only at the time of this writing. However, VOYAGER has the potential to be augmented by\nmultimodal perception models [58, 59] to achieve more impressive tasks. We demonstrate that given\nhuman feedback, VOYAGER is able to construct complex 3D structures in Minecraft, such as a Nether\nPortal and a house (Fig. 10). There are two ways to integrate human feedback:\n(1) Human as a critic (equivalent to VOYAGER’s self-verification module): humans provide\nvisual critique to VOYAGER, allowing it to modify the code from the previous round. This\nfeedback is essential for correcting certain errors in the spatial details of a 3D structure that\nVOYAGER cannot perceive directly.\n(2) Human as a curriculum (equivalent to VOYAGER’s automatic curriculum module): humans\nbreak down a complex building task into smaller steps, guiding VOYAGER to complete them\nincrementally. This approach improves VOYAGER’s ability to handle more sophisticated 3D\nconstruction tasks.\n9\n4\nLimitations and Future Work\nCost. The GPT-4 API incurs significant costs. It is 15× more expensive than GPT-3.5. Nevertheless,\nVOYAGER requires the quantum leap in code generation quality from GPT-4 (Fig. 9), which GPT-3.5\nand open-source LLMs cannot provide [60].\nInaccuracies. Despite the iterative prompting mechanism, there are still cases where the agent gets\nstuck and fails to generate the correct skill. The automatic curriculum has the flexibility to reattempt\nthis task at a later time. Occasionally, self-verification module may also fail, such as not recognizing\nspider string as a success signal of beating a spider.\nHallucinations. The automatic curriculum occasionally proposes unachievable tasks. For example, it\nmay ask the agent to craft a “copper sword\" or “copper chestplate\", which are items that do not exist\nwithin the game. Hallucinations also occur during the code generation process. For instance, GPT-4\ntends to use cobblestone as a fuel input, despite being an invalid fuel source in the game. Additionally,\nit may call functions absent in the provided control primitive APIs, leading to code execution errors.\nWe are confident that improvements in the GPT API models as well as novel techniques for finetuning\nopen-source LLMs will overcome these limitations in the future.\n5\nRelated work\nDecision-making Agents in Minecraft.\nMinecraft is an open-ended 3D world with incredibly\nflexible game mechanics supporting a broad spectrum of activities. Built upon notable Minecraft\nbenchmarks [23, 61–65], Minecraft learning algorithms can be divided into two categories: 1)\nLow-level controller: Many prior efforts leverage hierarchical reinforcement learning to learn from\nhuman demonstrations [66–68]. Kanitscheider et al. [14] design a curriculum based on success rates,\nbut its objectives are limited to curated items. MineDojo [23] and VPT [8] utilize YouTube videos\nfor large-scale pre-training. DreamerV3 [69], on the other hand, learns a world model to explore\nthe environment and collect diamonds. 2) High-level planner: Volum et al. [70] leverage few-shot\nprompting with Codex [41] to generate executable policies, but they require additional human\ninteraction. Recent works leverage LLMs as a high-level planner in Minecraft by decomposing\na high-level task into several subgoals following Minecraft recipes [55, 53, 71], thus lacking full\nexploration flexibility. Like these latter works, VOYAGER also uses LLMs as a high-level planner by\nprompting GPT-4 and utilizes Mineflayer [52] as a low-level controller following Volum et al. [70].\nUnlike prior works, VOYAGER employs an automatic curriculum that unfolds in a bottom-up manner,\ndriven by curiosity, and therefore enables open-ended exploration.\nLarge Language Models for Agent Planning.\nInspired by the strong emergent capabilities of\nLLMs, such as zero-shot prompting and complex reasoning [72, 37, 38, 36, 73, 74], embodied agent\nresearch [75–78] has witnessed a significant increase in the utilization of LLMs for planning purposes.\nRecent efforts can be roughly classified into two groups. 1) Large language models for robot\nlearning: Many prior works apply LLMs to generate subgoals for robot planning [27, 27, 25, 79, 80].\nInner Monologue [26] incorporates environment feedback for robot planning with LLMs. Code as\nPolicies [16] and ProgPrompt [22] directly leverage LLMs to generate executable robot policies.\nVIMA [19] and PaLM-E [59] fine-tune pre-trained LLMs to support multimodal prompts. 2)\nLarge language models for text agents: ReAct [29] leverages chain-of-thought prompting [46] and\ngenerates both reasoning traces and task-specific actions with LLMs. Reflexion [30] is built upon\nReAct [29] with self-reflection to enhance reasoning. AutoGPT [28] is a popular tool that automates\nNLP tasks by crafting a curriculum of multiple subgoals for completing a high-level goal while\nincorporating ReAct [29]’s reasoning and acting loops. DERA [81] frames a task as a dialogue\nbetween two GPT-4 [35] agents. Generative Agents [82] leverages ChatGPT [50] to simulate human\nbehaviors by storing agents’ experiences as memories and retrieving those for planning, but its agent\nactions are not executable. SPRING [83] is a concurrent work that uses GPT-4 to extract game\nmechanics from game manuals, based on which it answers questions arranged in a directed acyclic\ngraph and predicts the next action. All these works lack a skill library for developing more complex\nbehaviors, which are crucial components for the success of VOYAGER in lifelong learning.\nCode Generation with Execution.\nCode generation has been a longstanding challenge in\nNLP [41, 84, 85, 73, 37], with various works leveraging execution results to improve program\n10\nsynthesis. Execution-guided approaches leverage intermediate execution outcomes to guide program\nsearch [86–88]. Another line of research utilizes majority voting to choose candidates based on their\nexecution performance [89, 90]. Additionally, LEVER [91] trains a verifier to distinguish and reject\nincorrect programs based on execution results. CLAIRIFY [92], on the other hand, generates code\nfor planning chemistry experiments and makes use of a rule-based verifier to iteratively provide\nerror feedback to LLMs. VOYAGER distinguishes itself from these works by integrating environment\nfeedback, execution errors, and self-verification (to assess task success) into an iterative prompting\nmechanism for embodied control.\n6\nConclusion\nIn this work, we introduce VOYAGER, the first LLM-powered embodied lifelong learning agent,\nwhich leverages GPT-4 to explore the world continuously, develop increasingly sophisticated skills,\nand make new discoveries consistently without human intervention. VOYAGER exhibits superior\nperformance in discovering novel items, unlocking the Minecraft tech tree, traversing diverse terrains,\nand applying its learned skill library to unseen tasks in a newly instantiated world. VOYAGER serves\nas a starting point to develop powerful generalist agents without tuning the model parameters.\n7\nBroader Impacts\nOur research is conducted within Minecraft, a safe and harmless 3D video game environment. While\nVOYAGER is designed to be generally applicable to other domains, such as robotics, its application to\nphysical robots would require additional attention and the implementation of safety constraints by\nhumans to ensure responsible and secure deployment.\n8\nAcknowledgements\nWe are extremely grateful to Ziming Zhu, Kaiyu Yang, Rafał Kocielnik, Colin White, Or Sharir, Sahin\nLale, De-An Huang, Jean Kossaifi, Yuncong Yang, Charles Zhang, Minchao Huang, and many other\ncolleagues and friends for their helpful feedback and insightful discussions. This work is done during\nGuanzhi Wang’s internship at NVIDIA. Guanzhi Wang is supported by the Kortschak fellowship in\nComputing and Mathematical Sciences at Caltech.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Voyager: An Open-Ended Embodied Agent with Large Language Models.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nVoyager: An Open-Ended Embodied Agent with Large Language Models\n```\n#### 2. 论文摘要\n```\nWe introduce Voyager, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. Voyager consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving\ncomplex behaviors, and 3) a new iterative prompting mechanism that incorporates\nenvironment feedback, execution errors, and self-verification for program\nimprovement. Voyager interacts with GPT-4 via blackbox queries, which bypasses\nthe need for model parameter fine-tuning. The skills developed by Voyager are\ntemporally extended, interpretable, and compositional, which compounds the\nagent's abilities rapidly and alleviates catastrophic forgetting. Empirically,\nVoyager shows strong in-context lifelong learning capability and exhibits\nexceptional proficiency in playing Minecraft. It obtains 3.3x more unique\nitems, travels 2.3x longer distances, and unlocks key tech tree milestones up\nto 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill\nlibrary in a new Minecraft world to solve novel tasks from scratch, while other\ntechniques struggle to generalize. We open-source our full codebase and prompts\nat https:\/\/voyager.minedojo.org\/.\n```\n\n#### 3. 论文全文\n```\nVOYAGER: An Open-Ended Embodied Agent\nwith Large Language Models\nGuanzhi Wang1 2 #, Yuqi Xie3, Yunfan Jiang4∗, Ajay Mandlekar1∗,\nChaowei Xiao1 5, Yuke Zhu1 3, Linxi “Jim” Fan1† #, Anima Anandkumar1 2†\n1NVIDIA, 2Caltech, 3UT Austin, 4Stanford, 5UW Madison\n∗Equal contribution\n†Equal advising\n# Corresponding authors\nhttps:\/\/voyager.minedojo.org\nAbstract\nWe introduce VOYAGER, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. VOYAGER consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving complex\nbehaviors, and 3) a new iterative prompting mechanism that incorporates environ-\nment feedback, execution errors, and self-verification for program improvement.\nVOYAGER interacts with GPT-4 via blackbox queries, which bypasses the need for\nmodel parameter fine-tuning. The skills developed by VOYAGER are temporally\nextended, interpretable, and compositional, which compounds the agent’s abilities\nrapidly and alleviates catastrophic forgetting.\nEmpirically, VOYAGER shows\nstrong in-context lifelong learning capability and exhibits exceptional proficiency\nin playing Minecraft. It obtains 3.3× more unique items, travels 2.3× longer\ndistances, and unlocks key tech tree milestones up to 15.3× faster than prior SOTA.\nVOYAGER is able to utilize the learned skill library in a new Minecraft world to\nsolve novel tasks from scratch, while other techniques struggle to generalize.\nFigure 1: VOYAGER discovers new Minecraft items and skills continually by self-driven exploration,\nsignificantly outperforming the baselines. X-axis denotes the number of prompting iterations.\n1\narXiv:2305.16291v2  [cs.AI]  19 Oct 2023\nMine Wood  Log\nMake Crafting Table\nCraft Stone Sword\nCraft Shield\nMake Furnace\nCook Steak\nCombat Zombie\n     Mine Wood Log\nMake Crafting Table\nCombat \nZombie\nMine Diamond\nNew \nTask\nCode as \nActions\nRefine Program\nEnv Feedback\nExecution Errors\nUpdate \nExploration \nProgress\nSkill \nRetrieval\nAdd New Skill\nAutomatic Curriculum\nIterative Prompting Mechanism\nSkill Library\nEnvironment\nSelf-Verification\nFigure 2: VOYAGER consists of three key components: an automatic curriculum for open-ended\nexploration, a skill library for increasingly complex behaviors, and an iterative prompting mechanism\nthat uses code as action space.\n1\nIntroduction\nBuilding generally capable embodied agents that continuously explore, plan, and develop new skills\nin open-ended worlds is a grand challenge for the AI community [1–5]. Classical approaches\nemploy reinforcement learning (RL) [6, 7] and imitation learning [8–10] that operate on primitive\nactions, which could be challenging for systematic exploration [11–15], interpretability [16–18], and\ngeneralization [19–21]. Recent advances in large language model (LLM) based agents harness the\nworld knowledge encapsulated in pre-trained LLMs to generate consistent action plans or executable\npolicies [16, 22, 19]. They are applied to embodied tasks like games and robotics [23–27], as well as\nNLP tasks without embodiment [28–30]. However, these agents are not lifelong learners that can\nprogressively acquire, update, accumulate, and transfer knowledge over extended time spans [31, 32].\nLet us consider Minecraft as an example. Unlike most other games studied in AI [33, 34, 10],\nMinecraft does not impose a predefined end goal or a fixed storyline but rather provides a unique\nplayground with endless possibilities [23]. Minecraft requires players to explore vast, procedurally\ngenerated 3D terrains and unlock a tech tree using gathered resources. Human players typically start\nby learning the basics, such as mining wood and cooking food, before advancing to more complex\ntasks like combating monsters and crafting diamond tools. We argue that an effective lifelong learning\nagent should have similar capabilities as human players: (1) propose suitable tasks based on its\ncurrent skill level and world state, e.g., learn to harvest sand and cactus before iron if it finds itself in\na desert rather than a forest; (2) refine skills based on environmental feedback and commit mastered\nskills to memory for future reuse in similar situations (e.g. fighting zombies is similar to fighting\nspiders); (3) continually explore the world and seek out new tasks in a self-driven manner.\nTowards these goals, we introduce VOYAGER, the first LLM-powered embodied lifelong learning\nagent to drive exploration, master a wide range of skills, and make new discoveries continually\nwithout human intervention in Minecraft. VOYAGER is made possible through three key modules\n(Fig. 2): 1) an automatic curriculum that maximizes exploration; 2) a skill library for storing\nand retrieving complex behaviors; and 3) a new iterative prompting mechanism that generates\nexecutable code for embodied control. We opt to use code as the action space instead of low-level\nmotor commands because programs can naturally represent temporally extended and compositional\nactions [16, 22], which are essential for many long-horizon tasks in Minecraft. VOYAGER interacts\nwith a blackbox LLM (GPT-4 [35]) through prompting and in-context learning [36–38]. Our approach\nbypasses the need for model parameter access and explicit gradient-based training or finetuning.\nMore specifically, VOYAGER attempts to solve progressively harder tasks proposed by the automatic\ncurriculum, which takes into account the exploration progress and the agent’s state. The curriculum\nis generated by GPT-4 based on the overarching goal of “discovering as many diverse things as\npossible”. This approach can be perceived as an in-context form of novelty search [39, 40]. VOYAGER\nincrementally builds a skill library by storing the action programs that help solve a task successfully.\n2\nInventory (5\/36): {'oak_planks': 3, 'stick': \n4, 'crafting_table': 1, 'stone': 3, \n'wooden_pickaxe': 1}\nBiome: river\nInventory (4\/36): {'sugar_cane': 3, 'paper': \n3, 'fishing_rod': 1, 'wooden_pickaxe': 1}\nNearby entities: pig, cat, villager\r\nHealth: 12\/20\r\nHunger: 0\/20\nInventory (6\/36): {'furnace': 1, \n'stone_pickaxe': 1, 'oak_planks': 7, \n'crafting_table': 1, 'raw_iron': 4, 'coal': 1} \nTime: night\nNearby entities: zombie\nEquipment: 'stone_sword', 'shield'\nReasoning: Since you have a wooden pickaxe and some stones, it would be \nbeneficial to upgrade your pickaxe to a stone pickaxe for better efficiency.\r\nTask: Craft 1 stone pickaxe.\nReasoning: Since you have a fishing rod in your inventory and you are near a river \nbiome, it would be a good opportunity to catch some fish for food and experience. \nTask: Catch 1 fish.\nReasoning: Your hunger is at 0, which means you need to find food to replenish \nyour hunger. Since there are pigs nearby, you can kill one to obtain raw porkchops.\nTask: Kill 1 pig.\nReasoning: You have raw iron and coal, and you have a furnace. It's time to smelt \nthe iron to make iron ingots, which can be used to craft better tools and armor.\r\nTask: Smelt 4 raw iron.\nReasoning: Since it's night and there's a zombie nearby, it's a good opportunity to \ntry killing the zombie now that you have a stone sword and a shield equipped.\nTask: Kill 1 zombie.\nGPT-4\nGPT-4\nGPT-4\nGPT-4\nGPT-4\nFigure 3: Tasks proposed by the automatic curriculum. We only display the partial prompt for brevity.\nSee Appendix, Sec. A.3 for the full prompt structure.\nEach program is indexed by the embedding of its description, which can be retrieved in similar\nsituations in the future. Complex skills can be synthesized by composing simpler programs, which\ncompounds VOYAGER’s capabilities rapidly over time and alleviates catastrophic forgetting in other\ncontinual learning methods [31, 32].\nHowever, LLMs struggle to produce the correct action code consistently in one shot [41]. To address\nthis challenge, we propose an iterative prompting mechanism that: (1) executes the generated\nprogram to obtain observations from the Minecraft simulation (such as inventory listing and nearby\ncreatures) and error trace from the code interpreter (if any); (2) incorporates the feedback into GPT-4’s\nprompt for another round of code refinement; and (3) repeats the process until a self-verification\nmodule confirms the task completion, at which point we commit the program to the skill library (e.g.,\ncraftStoneShovel() and combatZombieWithSword()) and query the automatic curriculum for\nthe next milestone (Fig. 2).\nEmpirically, VOYAGER demonstrates strong in-context lifelong learning capabilities. It can construct\nan ever-growing skill library of action programs that are reusable, interpretable, and generalizable\nto novel tasks. We evaluate VOYAGER systematically against other LLM-based agent techniques\n(e.g., ReAct [29], Reflexion [30], AutoGPT [28]) in MineDojo [23], an open-source Minecraft AI\nframework. VOYAGER outperforms prior SOTA by obtaining 3.3× more unique items, unlocking key\ntech tree milestones up to 15.3× faster, and traversing 2.3× longer distances. We further demonstrate\nthat VOYAGER is able to utilize the learned skill library in a new Minecraft world to solve novel tasks\nfrom scratch, while other methods struggle to generalize.\n2\nMethod\nVOYAGER consists of three novel components: (1) an automatic curriculum (Sec. 2.1) that suggests\nobjectives for open-ended exploration, (2) a skill library (Sec. 2.2) for developing increasingly\ncomplex behaviors, and (3) an iterative prompting mechanism (Sec. 2.3) that generates executable\ncode for embodied control. Full prompts are presented in Appendix, Sec. A.\n2.1\nAutomatic Curriculum\nEmbodied agents encounter a variety of objectives with different complexity levels in open-ended\nenvironments. An automatic curriculum offers numerous benefits for open-ended exploration, ensur-\ning a challenging but manageable learning process, fostering curiosity-driven intrinsic motivation\nfor agents to learn and explore, and encouraging the development of general and flexible problem-\nsolving strategies [42–44]. Our automatic curriculum capitalizes on the internet-scale knowledge\ncontained within GPT-4 by prompting it to provide a steady stream of new tasks or challenges. The\ncurriculum unfolds in a bottom-up fashion, allowing for considerable adaptability and responsiveness\nto the exploration progress and the agent’s current state (Fig. 3). As VOYAGER progresses to harder\nself-driven goals, it naturally learns a variety of skills, such as “mining a diamond”.\n3\nProgram Description\nSkill Library\nTop-5 Relevant Skills\nProgram Generated by GPT-4\nTask: Craft Iron Pickaxe\nKey\nAdd\nRetrieve\nValue\nSkill Library\nQuery\nHow to craft an iron pickaxe in \nMinecraft?\nTo craft an iron pickaxe, you \nneed to 3 iron ingots and 2 \nsticks. Once you have gathered \nthe materials, ....\n----------------------------------\n         Environment Feedback\nMine Wood  Log\nMake Crafting Table\nCraft Wooden Pickaxe\nCraft Stone Sword\nMake Furnace\n...\nCombat Cow\nCook Steak\nCraft Iron Axe\nCombat Zombie\nSmelt Iron Ingot\nCraft Stick\nMake Crafting Table\nMake Furnace\nCraft Wooden Pickaxe\nGPT-3.5\nEmbedding\nEmbedding\nGPT-3.5\nFigure 4: Skill library. Top: Adding a new skill. Each time GPT-4 generates and verifies a new\nskill, we add it to the skill library, represented by a vector database. The key is the embedding vector\nof the program description (generated by GPT-3.5), while the value is the program itself. Bottom:\nSkill retrieval. When faced with a new task proposed by the automatic curriculum, we first leverage\nGPT-3.5 to generate a general suggestion for solving the task, which is combined with environment\nfeedback as the query context. Subsequently, we perform querying to identify the top-5 relevant skills.\nThe input prompt to GPT-4 consists of several components:\n(1) Directives encouraging diverse behaviors and imposing constraints,\nsuch as\n“My ultimate goal is to discover as many diverse things as possible\n...\nThe next task should not be too hard since I may not have the\nnecessary resources or have learned enough skills to complete it\nyet.”;\n(2) The agent’s current state, including inventory, equipment, nearby blocks and entities,\nbiome, time, health and hunger bars, and position;\n(3) Previously completed and failed tasks, reflecting the agent’s current exploration progress\nand capabilities frontier;\n(4) Additional context: We also leverage GPT-3.5 to self-ask questions based on the agent’s\ncurrent state and exploration progress and self-answer questions. We opt to use GPT-3.5\ninstead of GPT-4 for standard NLP tasks due to budgetary considerations.\n2.2\nSkill Library\nWith the automatic curriculum consistently proposing increasingly complex tasks, it is essential to\nhave a skill library that serves as a basis for learning and evolution. Inspired by the generality, inter-\npretability, and universality of programs [45], we represent each skill with executable code that scaf-\nfolds temporally extended actions for completing a specific task proposed by the automatic curriculum.\nThe input prompt to GPT-4 consists of the following components:\n(1) Guidelines\nfor\ncode\ngeneration,\nsuch\nas\n“Your function will be reused\nfor building more complex functions.\nTherefore, you should make\nit generic and reusable.”;\n(2) Control primitive APIs, and relevant skills retrieved from the skill library, which are\ncrucial for in-context learning [36–38] to work well;\n(3) The generated code from the last round, environment feedback, execution errors, and\ncritique, based on which GPT-4 can self-improve (Sec. 2.3);\n(4) The agent’s current state, including inventory, equipment, nearby blocks and entities,\nbiome, time, health and hunger bars, and position;\n4\nI cannot make stick because I need:  2 more planks\nI cannot make stone_shovel because I need:  2 more stick\nthrow new Error(`No item named ${name}`);\nNo item named acacia_axe\nat line 18:await craftItem(bot, \"acacia_axe\", 1);\nEnvironment Feedback\nExecution Error\nGPT-4\nGPT-4\nFigure 5: Left: Environment feedback. GPT-4 realizes it needs 2 more planks before crafting sticks.\nRight: Execution error. GPT-4 realizes it should craft a wooden axe instead of an acacia axe since\nthere is no acacia axe in Minecraft. We only display the partial prompt for brevity. The full prompt\nstructure for code generation is in Appendix, Sec. A.4.\n(5) Chain-of-thought prompting [46] to do reasoning before code generation.\nWe iteratively refine the program through a novel iterative prompting mechanism (Sec. 2.3), in-\ncorporate it into the skill library as a new skill, and index it by the embedding of its description\n(Fig. 4, top). For skill retrieval, we query the skill library with the embedding of self-generated task\nplans and environment feedback (Fig. 4, bottom). By continuously expanding and refining the skill\nlibrary, VOYAGER can learn, adapt, and excel in a wide spectrum of tasks, consistently pushing the\nboundaries of its capabilities in the open world.\n2.3\nIterative Prompting Mechanism\nWe introduce an iterative prompting mechanism for self-improvement through three types of feedback:\n(1) Environment feedback, which illustrates the intermediate progress of program execution\n(Fig. 5, left). For example, “I cannot make an iron chestplate because I need:\n7 more iron ingots” highlights the cause of failure in crafting an iron chestplate. We use\nbot.chat() inside control primitive APIs to generate environment feedback and prompt\nGPT-4 to use this function as well during code generation;\n(2) Execution errors from the program interpreter that reveal any invalid operations or syntax\nerrors in programs, which are valuable for bug fixing (Fig. 5, right);\n(3) Self-verification for checking task success. Instead of manually coding success checkers\nfor each new task proposed by the automatic curriculum, we instantiate another GPT-4\nagent for self-verification. By providing VOYAGER’s current state and the task to GPT-4,\nwe ask it to act as a critic [47–49] and inform us whether the program achieves the task.\nIn addition, if the task fails, it provides a critique by suggesting how to complete the task\n(Fig. 6). Hence, our self-verification is more comprehensive than self-reflection [30] by both\nchecking success and reflecting on mistakes.\nDuring each round of code generation, we execute the generated program to obtain environment\nfeedback and execution errors from the code interpreter, which are incorporated into GPT-4’s prompt\nfor the next round of code refinement. This iterative process repeats until self-verification validates\n5\nInventory (8\/36): {'oak_planks': 5, 'cobblestone': 2, \n'porkchop': 2, 'wooden_sword': 1, 'coal': 5, 'wooden_pickaxe': \n1, 'oak_log': 3, 'dirt': 9}\nTask: Mine 5 coal ores\nInventory (10\/36): {'raw_copper': 9, 'copper_ingot': 3, \n'acacia_planks': 1, 'raw_iron': 1, 'stick': 1, 'iron_sword': 1, \n'iron_pickaxe': 1, 'iron_ingot': 3, 'crafting_table': 1, 'furnace': 1}\nTask: Craft a spyglass\nInventory (7\/36): {'oak_log': 5, 'oak_planks': 1, \n'wooden_pickaxe': 1, 'wooden_sword': 1, 'porkchop': 2, \n'white_wool': 2, 'mutton': 6}\nTask: Kill 3 sheep\nInventory (9\/36): {'string': 4, 'coal': 1, 'rotten_flesh': 1, \n'iron_sword': 1, 'furnace': 1, 'dirt': 6, 'stone_shovel': 1, \n'wooden_pickaxe': 1, 'granite': 5}\nTask: Kill 1 zombie\nReasoning: Mining coal_ore in Minecraft will get coal. You have 5 coal in your \ninventory.\nSuccess: True\nReasoning: To craft a spyglass, you need 2 copper ingots and 1 amethyst shard. \nYou have 3 copper ingots, but you don't have any amethyst shards.\nSuccess: False\nCritique: Find and mine an amethyst shard underground.\nReasoning: You have 2 white_wool and 6 mutton in your inventory, which indicates \nthat you killed 2 sheep. You needed to kill 3 sheep.\nSuccess: False\nCritique: Find and kill one more sheep to complete the task.\nReasoning: You have 1 rotten_flesh in your inventory, which means you have killed \nat least 1 zombie.\nSuccess: True\nGPT-4\nGPT-4\nGPT-4\nGPT-4\nFigure 6: Self-verification examples. We only display the partial prompt for brevity. See Appendix,\nSec. A.5 for the full prompt structure.\nthe task’s completion, at which point we add this new skill to the skill library and ask the automatic\ncurriculum for a new objective (Fig. 2). If the agent gets stuck after 4 rounds of code generation, then\nwe query the curriculum for another task. This iterative prompting approach significantly improves\nprogram synthesis for embodied control, enabling VOYAGER to continuously acquire diverse skills\nwithout human intervention.\n3\nExperiments\n3.1\nExperimental Setup\nWe leverage OpenAI’s gpt-4-0314 [35] and gpt-3.5-turbo-0301 [50] APIs for text completion,\nalong with text-embedding-ada-002 [51] API for text embedding. We set all temperatures to\n0 except for the automatic curriculum, which uses temperature = 0.1 to encourage task diversity. Our\nsimulation environment is built on top of MineDojo [23] and leverages Mineflayer [52] JavaScript\nAPIs for motor controls. See Appendix, Sec. B.1 for more details.\n3.2\nBaselines\nBecause there is no LLM-based agents that work out of the box for Minecraft, we make our best\neffort to select a number of representative algorithms as baselines. These methods are originally\ndesigned only for NLP tasks without embodiment, therefore we have to re-interpret them to be\nexecutable in MineDojo and compatible with our experimental setting:\nReAct [29] uses chain-of-thought prompting [46] by generating both reasoning traces and action\nplans with LLMs. We provide it with our environment feedback and the agent states as observations.\nReflexion [30] is built on top of ReAct [29] with self-reflection to infer more intuitive future actions.\nWe provide it with execution errors and our self-verification module.\nAutoGPT [28] is a popular software tool that automates NLP tasks by decomposing a high-level\ngoal into multiple subgoals and executing them in a ReAct-style loop. We re-implement AutoGPT\nby using GPT-4 to do task decomposition and provide it with the agent states, environment feedback,\nand execution errors as observations for subgoal execution. Compared with VOYAGER, AutoGPT\nlacks the skill library for accumulating knowledge, self-verification for assessing task success, and\nautomatic curriculum for open-ended exploration.\nNote that we do not directly compare with prior methods that take Minecraft screen pixels as input\nand output low-level controls [53–55]. It would not be an apple-to-apple comparison, because we rely\non the high-level Mineflayer [52] API to control the agent. Our work’s focus is on pushing the limits\nof GPT-4 for lifelong embodied agent learning, rather than solving the 3D perception or sensorimotor\ncontrol problems. VOYAGER is orthogonal and can be combined with gradient-based approaches like\n6\nTable 1: Tech tree mastery. Fractions indicate the number of successful trials out of three total runs.\n0\/3 means the method fails to unlock a level of the tech tree within the maximal prompting iterations\n(160). Numbers are prompting iterations averaged over three trials. The fewer the iterations, the\nmore efficient the method.\nMethod\nWooden Tool\nStone Tool\nIron Tool\nDiamond Tool\nReAct [29]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nReflexion [30]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nAutoGPT [28]\n92 ± 72 (3\/3)\n94 ± 72 (3\/3)\n135 ± 103 (3\/3)\nN\/A (0\/3)\nVOYAGER w\/o Skill Library\n7 ± 2 (3\/3)\n9 ± 4 (3\/3)\n29 ± 11 (3\/3)\nN\/A (0\/3)\nVOYAGER (Ours)\n6 ± 2 (3\/3)\n11 ± 2 (3\/3)\n21 ± 7 (3\/3)\n102 (1\/3)\nFigure 7: Map coverage: bird’s eye views of Minecraft maps. VOYAGER is able to traverse 2.3×\nlonger distances compared to baselines while crossing diverse terrains.\nVPT [8] as long as the controller provides a code API. We make a system-level comparison between\nVOYAGER and prior Minecraft agents in Table. A.2.\n3.3\nEvaluation Results\nWe systematically evaluate VOYAGER and baselines on their exploration performance, tech tree\nmastery, map coverage, and zero-shot generalization capability to novel tasks in a new world.\nSignificantly better exploration.\nResults of exploration performance are shown in Fig. 1.\nVOYAGER’s superiority is evident in its ability to consistently make new strides, discovering 63\nunique items within 160 prompting iterations, 3.3× many novel items compared to its counterparts.\nOn the other hand, AutoGPT lags considerably in discovering new items, while ReAct and Reflexion\nstruggle to make significant progress, given the abstract nature of the open-ended exploration goal\nthat is challenging to execute without an appropriate curriculum.\nConsistent tech tree mastery. The Minecraft tech tree tests the agent’s ability to craft and use a\nhierarchy of tools. Progressing through this tree (wooden tool →stone tool →iron tool →diamond\ntool) requires the agent to master systematic and compositional skills. Compared with baselines,\nVOYAGER unlocks the wooden level 15.3× faster (in terms of the prompting iterations), the stone\nlevel 8.5× faster, the iron level 6.4× faster, and VOYAGER is the only one to unlock the diamond level\nof the tech tree (Fig. 2 and Table. 1). This underscores the effectiveness of the automatic curriculum,\nwhich consistently presents challenges of suitable complexity to facilitate the agent’s progress.\nExtensive map traversal. VOYAGER is able to navigate distances 2.3× longer compared to baselines\nby traversing a variety of terrains, while the baseline agents often find themselves confined to local\nareas, which significantly hampers their capacity to discover new knowledge (Fig. 7).\n7\nTable 2: Zero-shot generalization to unseen tasks. Fractions indicate the number of successful\ntrials out of three total attempts. 0\/3 means the method fails to solve the task within the maximal\nprompting iterations (50). Numbers are prompting iterations averaged over three trials. The fewer\nthe iterations, the more efficient the method.\nMethod\nDiamond Pickaxe\nGolden Sword\nLava Bucket\nCompass\nReAct [29]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nReflexion [30]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nAutoGPT [28]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nAutoGPT [28] w\/ Our Skill Library\n39 (1\/3)\n30 (1\/3)\nN\/A (0\/3)\n30 (2\/3)\nVOYAGER w\/o Skill Library\n36 (2\/3)\n30 ± 9 (3\/3)\n27 ± 9 (3\/3)\n26 ± 3 (3\/3)\nVOYAGER (Ours)\n19 ± 3 (3\/3)\n18 ± 7 (3\/3)\n21 ± 5 (3\/3)\n18 ± 2 (3\/3)\nFigure 8: Zero-shot generalization to unseen tasks. We visualize the intermediate progress of each\nmethod on two tasks. See Appendix, Sec. B.4.3 for the other two tasks. We do not plot ReAct and\nReflexion since they do not make any meaningful progress.\nEfficient zero-shot generalization to unseen tasks. To evaluate zero-shot generalization, we clear\nthe agent’s inventory, reset it to a newly instantiated world, and test it with unseen tasks. For both\nVOYAGER and AutoGPT, we utilize GPT-4 to break down the task into a series of subgoals. Table. 2\nand Fig. 8 show VOYAGER can consistently solve all the tasks, while baselines cannot solve any task\nwithin 50 prompting iterations. What’s interesting to note is that our skill library constructed from\nlifelong learning not only enhances VOYAGER’s performance but also gives a boost to AutoGPT.\nThis demonstrates that the skill library serves as a versatile tool that can be readily employed by other\nmethods, effectively acting as a plug-and-play asset to enhance performance.\n3.4\nAblation Studies\nWe ablate 6 design choices (automatic curriculum, skill library, environment feedback, execution\nerrors, self-verification, and GPT-4 for code generation) in VOYAGER and study their impact on\nexploration performance (see Appendix, Sec. B.3 for details of each ablated variant). Results are\nshown in Fig. 9. We highlight the key findings below:\n• Automatic curriculum is crucial for the agent’s consistent progress. The discovered item\ncount drops by 93% if the curriculum is replaced with a random one, because certain tasks\nmay be too challenging if attempted out of order. On the other hand, a manually designed\ncurriculum requires significant Minecraft-specific expertise, and does not take into account\nthe agent’s live situation. It falls short in the experimental results compared to our automatic\ncurriculum.\n• VOYAGER w\/o skill library exhibits a tendency to plateau in the later stages. This\nunderscores the pivotal role that the skill library plays in VOYAGER. It helps create more\ncomplex actions and steadily pushes the agent’s boundaries by encouraging new skills to be\nbuilt upon older ones.\n8\nFigure 9: Left: Ablation studies for the automatic curriculum, skill library, and GPT-4. GPT-3.5\nmeans replacing GPT-4 with GPT-3.5 for code generation. VOYAGER outperforms all the alternatives,\ndemonstrating the critical role of each component. Right: Ablation studies for the iterative\nprompting mechanism. VOYAGER surpasses all the other options, thereby highlighting the essential\nsignificance of each type of feedback in the iterative prompting mechanism.\nFigure 10: VOYAGER builds 3D structures with human feedback. The progress of building designs\nthat integrate human input is demonstrated from left to right.\n• Self-verification is the most important among all the feedback types. Removing the\nmodule leads to a significant drop (−73%) in the discovered item count. Self-verification\nserves as a critical mechanism to decide when to move on to a new task or reattempt a\npreviously unsuccessful task.\n• GPT-4 significantly outperforms GPT-3.5 in code generation and obtains 5.7× more\nunique items, as GPT-4 exhibits a quantum leap in coding abilities. This finding corroborates\nrecent studies in the literature [56, 57].\n3.5\nMultimodal Feedback from Humans\nVOYAGER does not currently support visual perception, because the available version of GPT-4 API\nis text-only at the time of this writing. However, VOYAGER has the potential to be augmented by\nmultimodal perception models [58, 59] to achieve more impressive tasks. We demonstrate that given\nhuman feedback, VOYAGER is able to construct complex 3D structures in Minecraft, such as a Nether\nPortal and a house (Fig. 10). There are two ways to integrate human feedback:\n(1) Human as a critic (equivalent to VOYAGER’s self-verification module): humans provide\nvisual critique to VOYAGER, allowing it to modify the code from the previous round. This\nfeedback is essential for correcting certain errors in the spatial details of a 3D structure that\nVOYAGER cannot perceive directly.\n(2) Human as a curriculum (equivalent to VOYAGER’s automatic curriculum module): humans\nbreak down a complex building task into smaller steps, guiding VOYAGER to complete them\nincrementally. This approach improves VOYAGER’s ability to handle more sophisticated 3D\nconstruction tasks.\n9\n4\nLimitations and Future Work\nCost. The GPT-4 API incurs significant costs. It is 15× more expensive than GPT-3.5. Nevertheless,\nVOYAGER requires the quantum leap in code generation quality from GPT-4 (Fig. 9), which GPT-3.5\nand open-source LLMs cannot provide [60].\nInaccuracies. Despite the iterative prompting mechanism, there are still cases where the agent gets\nstuck and fails to generate the correct skill. The automatic curriculum has the flexibility to reattempt\nthis task at a later time. Occasionally, self-verification module may also fail, such as not recognizing\nspider string as a success signal of beating a spider.\nHallucinations. The automatic curriculum occasionally proposes unachievable tasks. For example, it\nmay ask the agent to craft a “copper sword\" or “copper chestplate\", which are items that do not exist\nwithin the game. Hallucinations also occur during the code generation process. For instance, GPT-4\ntends to use cobblestone as a fuel input, despite being an invalid fuel source in the game. Additionally,\nit may call functions absent in the provided control primitive APIs, leading to code execution errors.\nWe are confident that improvements in the GPT API models as well as novel techniques for finetuning\nopen-source LLMs will overcome these limitations in the future.\n5\nRelated work\nDecision-making Agents in Minecraft.\nMinecraft is an open-ended 3D world with incredibly\nflexible game mechanics supporting a broad spectrum of activities. Built upon notable Minecraft\nbenchmarks [23, 61–65], Minecraft learning algorithms can be divided into two categories: 1)\nLow-level controller: Many prior efforts leverage hierarchical reinforcement learning to learn from\nhuman demonstrations [66–68]. Kanitscheider et al. [14] design a curriculum based on success rates,\nbut its objectives are limited to curated items. MineDojo [23] and VPT [8] utilize YouTube videos\nfor large-scale pre-training. DreamerV3 [69], on the other hand, learns a world model to explore\nthe environment and collect diamonds. 2) High-level planner: Volum et al. [70] leverage few-shot\nprompting with Codex [41] to generate executable policies, but they require additional human\ninteraction. Recent works leverage LLMs as a high-level planner in Minecraft by decomposing\na high-level task into several subgoals following Minecraft recipes [55, 53, 71], thus lacking full\nexploration flexibility. Like these latter works, VOYAGER also uses LLMs as a high-level planner by\nprompting GPT-4 and utilizes Mineflayer [52] as a low-level controller following Volum et al. [70].\nUnlike prior works, VOYAGER employs an automatic curriculum that unfolds in a bottom-up manner,\ndriven by curiosity, and therefore enables open-ended exploration.\nLarge Language Models for Agent Planning.\nInspired by the strong emergent capabilities of\nLLMs, such as zero-shot prompting and complex reasoning [72, 37, 38, 36, 73, 74], embodied agent\nresearch [75–78] has witnessed a significant increase in the utilization of LLMs for planning purposes.\nRecent efforts can be roughly classified into two groups. 1) Large language models for robot\nlearning: Many prior works apply LLMs to generate subgoals for robot planning [27, 27, 25, 79, 80].\nInner Monologue [26] incorporates environment feedback for robot planning with LLMs. Code as\nPolicies [16] and ProgPrompt [22] directly leverage LLMs to generate executable robot policies.\nVIMA [19] and PaLM-E [59] fine-tune pre-trained LLMs to support multimodal prompts. 2)\nLarge language models for text agents: ReAct [29] leverages chain-of-thought prompting [46] and\ngenerates both reasoning traces and task-specific actions with LLMs. Reflexion [30] is built upon\nReAct [29] with self-reflection to enhance reasoning. AutoGPT [28] is a popular tool that automates\nNLP tasks by crafting a curriculum of multiple subgoals for completing a high-level goal while\nincorporating ReAct [29]’s reasoning and acting loops. DERA [81] frames a task as a dialogue\nbetween two GPT-4 [35] agents. Generative Agents [82] leverages ChatGPT [50] to simulate human\nbehaviors by storing agents’ experiences as memories and retrieving those for planning, but its agent\nactions are not executable. SPRING [83] is a concurrent work that uses GPT-4 to extract game\nmechanics from game manuals, based on which it answers questions arranged in a directed acyclic\ngraph and predicts the next action. All these works lack a skill library for developing more complex\nbehaviors, which are crucial components for the success of VOYAGER in lifelong learning.\nCode Generation with Execution.\nCode generation has been a longstanding challenge in\nNLP [41, 84, 85, 73, 37], with various works leveraging execution results to improve program\n10\nsynthesis. Execution-guided approaches leverage intermediate execution outcomes to guide program\nsearch [86–88]. Another line of research utilizes majority voting to choose candidates based on their\nexecution performance [89, 90]. Additionally, LEVER [91] trains a verifier to distinguish and reject\nincorrect programs based on execution results. CLAIRIFY [92], on the other hand, generates code\nfor planning chemistry experiments and makes use of a rule-based verifier to iteratively provide\nerror feedback to LLMs. VOYAGER distinguishes itself from these works by integrating environment\nfeedback, execution errors, and self-verification (to assess task success) into an iterative prompting\nmechanism for embodied control.\n6\nConclusion\nIn this work, we introduce VOYAGER, the first LLM-powered embodied lifelong learning agent,\nwhich leverages GPT-4 to explore the world continuously, develop increasingly sophisticated skills,\nand make new discoveries consistently without human intervention. VOYAGER exhibits superior\nperformance in discovering novel items, unlocking the Minecraft tech tree, traversing diverse terrains,\nand applying its learned skill library to unseen tasks in a newly instantiated world. VOYAGER serves\nas a starting point to develop powerful generalist agents without tuning the model parameters.\n7\nBroader Impacts\nOur research is conducted within Minecraft, a safe and harmless 3D video game environment. While\nVOYAGER is designed to be generally applicable to other domains, such as robotics, its application to\nphysical robots would require additional attention and the implementation of safety constraints by\nhumans to ensure responsible and secure deployment.\n8\nAcknowledgements\nWe are extremely grateful to Ziming Zhu, Kaiyu Yang, Rafał Kocielnik, Colin White, Or Sharir, Sahin\nLale, De-An Huang, Jean Kossaifi, Yuncong Yang, Charles Zhang, Minchao Huang, and many other\ncolleagues and friends for their helpful feedback and insightful discussions. This work is done during\nGuanzhi Wang’s internship at NVIDIA. Guanzhi Wang is supported by the Kortschak fellowship in\nComputing and Mathematical Sciences at Caltech.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Voyager：基于大型语言模型的开放式具身终身学习智能体\n\n## 📌 背景痛点\/本文动机\n构建能够在开放世界中持续探索、规划和开发新技能的通用具身智能体，是人工智能领域的一大挑战。传统的强化学习和模仿学习方法在探索、可解释性和泛化方面存在局限性。近年来，基于大型语言模型（LLM）的智能体利用预训练LLM中的世界知识生成一致的行动计划或可执行策略，但它们并非终身学习者，无法在长时间跨度内逐步获取、更新、积累和转移知识。\n\n## 🚀 核心方法\nVoyager 是第一个由 LLM 驱动的具身终身学习智能体，能够在 Minecraft 中持续探索世界、获取多样化技能，并在没有人类干预的情况下进行新的发现。Voyager 由三个关键组件组成：\n\n💡 创新点1：自动课程\nVoyager 通过自动课程进行开放式探索，该课程由 GPT-4 生成，旨在“发现尽可能多的多样化事物”。课程会根据探索进度和智能体的状态提出越来越难的任务，从而推动智能体不断学习新技能。\n\n💡 创新点2：技能库\nVoyager 拥有一个不断增长的技能库，用于存储和检索可执行代码，以存储和检索复杂的行为。每个技能都由可执行代码表示，这些代码可以自然地表示时间扩展和组合动作，这对于 Minecraft 中的许多长期任务至关重要。\n\n💡 创新点3：迭代提示机制\nVoyager 通过迭代提示机制生成可执行代码，该机制利用环境反馈、执行错误和自我验证来改进程序。该机制通过执行生成的程序、获取环境反馈和执行错误，并将这些反馈纳入 GPT-4 的提示中，从而进行代码改进。这个过程会重复进行，直到自我验证模块确认任务完成，此时将程序添加到技能库中，并查询自动课程以获取下一个目标。\n\n## 📈 实验结果\nVoyager 在 MineDojo 框架中与其他 LLM 基于智能体技术进行了比较，结果表明 Voyager 在发现新物品、解锁 Minecraft 技术树、穿越各种地形以及将学习到的技能库应用于新世界中的未见任务方面表现出色。Voyager 获得了 3.3 倍的新物品，解锁关键技术树里程碑的速度提高了 15.3 倍，穿越的距离是基线的 2.3 倍。\n\n## 💬 可借鉴之处\nVoyager 的设计为开发强大的通用智能体提供了一个起点，无需调整模型参数。其自动课程、技能库和迭代提示机制为终身学习智能体的开发提供了新的思路。此外，Voyager 的技能库可以作为其他方法的即插即用资产，有效地提高性能。","llm_summary_res_status":200}
{"title":"Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory","authors":"Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, Jifeng Dai","summary":"The captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https:\/\/github.com\/OpenGVLab\/GITM.","url":"http:\/\/arxiv.org\/abs\/2305.17144v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2305.17144v2","published":1685037589000,"comment":null,"pdf_text":"Ghost in the Minecraft: Generally Capable Agents for\nOpen-World Environments via Large Language\nModels with Text-based Knowledge and Memory\nXizhou Zhu1,2∗, Yuntao Chen3∗, Hao Tian2∗, Chenxin Tao1,2∗, Weijie Su2,4∗, Chenyu Yang1∗,\nGao Huang1, Bin Li4, Lewei Lu2, Xiaogang Wang2,5, Yu Qiao6, Zhaoxiang Zhang7, Jifeng Dai1,6 \u00001Tsinghua University\n2SenseTime Research\n3Centre for Artificial Intelligence and Robotics, HKISI, CAS\n4University of Science and Technology of China\n5The Chinese University of Hong Kong\n6Shanghai Artificial Intelligence Laboratory\n7Institute of Automation, Chinese Academy of Science (CASIA)\n{zhuxizhou,gaohuang,daijifeng}@tsinghua.edu.cn, chenyuntao08@gmail.com\ntianhao2@senseauto.com, {tcx20,yangcy19}@mails.tsinghua.edu.cn,\njackroos@mail.ustc.edu.cn, binli@ustc.edu.cn, luotto@sensetime.com\nxgwang@ee.cuhk.edu.hk, qiaoyu@pjlab.org.cn, zhaoxiang.zhang@ia.ac.cn\nAbstract\nThe captivating realm of Minecraft has attracted substantial research interest in\nrecent years, serving as a rich platform for developing intelligent agents capable of\nfunctioning in open-world environments. However, the current research landscape\npredominantly focuses on specific objectives, such as the popular \"ObtainDiamond\"\ntask, and has not yet shown effective generalization to a broader spectrum of\ntasks. Furthermore, the current leading success rate for the \"ObtainDiamond\"\ntask stands at around 20%, highlighting the limitations of Reinforcement Learning\n(RL) based controllers used in existing methods. To tackle these challenges, we\nintroduce Ghost in the Minecraft (GITM), a novel framework integrates Large\nLanguage Models (LLMs) with text-based knowledge and memory, aiming to\ncreate Generally Capable Agents (GCAs) in Minecraft. These agents, equipped\nwith the logic and common sense capabilities of LLMs, can skillfully navigate\ncomplex, sparse-reward environments with text-based interactions. We develop\na set of structured actions and leverage LLMs to generate action plans for the\nagents to execute. The resulting LLM-based agent markedly surpasses previous\nmethods, achieving a remarkable improvement of +47.5% in success rate on the\n\"ObtainDiamond\" task, demonstrating superior robustness compared to traditional\nRL-based controllers. Notably, our agent is the first to procure all items in the\nMinecraft Overworld technology tree, demonstrating its extensive capabilities.\nGITM does not need any GPU for training, but a single CPU node with 32 CPU\ncores is enough. This research shows the potential of LLMs in developing capable\nagents for handling long-horizon, complex tasks and adapting to uncertainties\nin open-world environments. See the project website at https:\/\/github.com\/\nOpenGVLab\/GITM.\n1\nIntroduction\n“What if a cyber brain could possibly generate its own ghost, create a soul all by itself? And if it did,\njust what would be the importance of being human then?”\n— Ghost in the Shell (1995)\n∗Equal contribution. This work is done when Chenxin Tao and Weijie Su are interns at SenseTime Research.\n\u0000 Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.\narXiv:2305.17144v2  [cs.AI]  1 Jun 2023\nFigure 1: Our GITM unlocks the entire technology tree by obtaining all items in Minecraft\nOverworld. Each node represents an individual item in Minecraft. The directed edges between nodes\nrepresent prerequisite relationships for obtaining items. For better readability, we manually merge\nsome similar nodes, e.g., “wooden_pickaxe”, “wooden_axe”, “wooden_hoe”, and ’wooden_shovel’\nare merged into one node, and “wooden_pickaxe” is selected to represent the merged node. Existing\nMinecraft agents [2, 7, 25] only unlocked 78 \/ 262 = 30% items, while our GITM successfully\nunlocked all items.\ngoal\ngoal\nRL Agent\nkeyboard & mouse\nobservation\naction list\nfeedback\nobservation\nkeyboard & mouse\nEnvironment\nexplore\nmine\ncraft \/ smelt\ndig_down\n…\nRL-based method\nOurs\ngoal\nLLM\nDecomposer\nLLM Planner\nstructured actions\naction list\nfeedback\nobservation\nkeyboard & mouse\nEnvironment\nexplore\nmine\ncraft \/ smelt\ndig_down\n…\nGhost In the Minecraft (GITM)\nsub-goal tree\nLLM Interface\nFigure 2: Comparison between RL-based method and our GITM. RL agents try to map an\ncomplex goal directly to a sequence of low-level control signals, while our GITM leverages LLM to\nbreak down the goals and map them to structured actions for final control signals.\nMinecraft, as the world’s best-selling game, boasts over 238 million copies sold and more than\n140 million peak monthly active users [27]. Within the game, hundreds of millions of players have\nexperienced a digital second life by surviving, exploring and creating, closely resembling the human\nworld in many aspects. Given its massive scale, vast success, and unrestricted freedom, Minecraft\nhas established itself as an unparalleled platform for researching autonomous and robust Generally\nCapable Agents (GCAs) [23] in open-world environments brimmed with long-horizon challenges,\nenvironmental disruptions, and uncertainties.\nMinecraft acts as a microcosm of the real world. Developing an automated agent that can mas-\nter all technical challenges in Minecraft is akin to creating an artificial intelligence capable of\nautonomously learning and mastering the entire real-world technology. However, existing re-\nsearches [2, 7, 25] remain narrowly scoped. Prior studies have predominantly focused on the\nspecific goal of ObtainDiamond [18]. Yet, in the process of obtaining diamonds, the number of\ntypes of items involved only accounts for <5% of the entire Minecraft world. ObtainDiamond only\nrequires specialized skills in a specific domain, while obtaining all items in Minecraft demonstrates a\nwide range of knowledge and capabilities, similar to mastering multidisciplinary fields in the real\nworld. As illustrated in Fig. 1, our work endeavors to obtain all items in Minecraft within a reasonable\ncomputation budget. This achievement stands as a significant milestone in the development of GCAs,\nillustrating the potential of intelligent agents to match human performance in terms of versatility and\nadaptability.\nAlthough reinforcement learning (RL) [16] is the most popular paradigm for approaching GCAs, it\nhas shown some staggering limitations in conquering Minecraft. RL-based agents typically require\na vast number of learning steps (e.g., nearly 30 million steps to obtain diamonds as reported in\nDreamerV3 [7]) and exhibit poor scalability when learning new tasks(e.g., VPT [2] uses different\nagents for world exploration and diamond mining). As a consequence, adopting RL-based agents for\n2\ncompleting a wide range of tasks may require an prohibitively high number of training steps, making\nit impractical to obtain all items in Minecraft. This inefficiency and lack of adaptability have hindered\nthe development of generally capable agents in open-world environments.\nAs shown in Fig. 2, the biggest dilemma of previous RL-based agents is how to map an extremely\nlong-horizon and complex goal to a sequence of lowest-level keyboard\/mouse operations. To address\nthis challenge, we propose our framework Ghost In the Minecraft (GITM) 2, which uses Large\nLanguage Model (LLM)-based agents as a new paradigm. Instead of direct mapping like RL agents,\nour LLM-based agents employ a hierarchical approach. It first breaks down the decompose goal\ninto sub-goals, then into structured actions, and finally into keyboard\/mouse operations. Such\ndecomposition is similar to how humans solve complex problems in the real world, enabling mastery\nof Minecraft with efficiency orders of magnitude higher than that of RL. LLM can also leverage\ntext-based knowledge and memory to quickly acquire the ability to interact with the environment\nand accomplish goals, offering immense learning efficiency improvements, unlimited scalability and\nrepresenting a disruptive innovation compared with RL. Our GITM framework has the potential to\nrevolutionize the path to generally capable agents.\nSpecifically, the proposed LLM-based agent consists of an LLM Decomposer, an LLM Planner, and\nan LLM Interface, which are responsible for the decomposition of sub-goals, structured actions,\nand keyboard\/mouse operations, respectively. Given a goal in Minecraft, LLM Decomposer first\ndecomposes it into a series of well-defined sub-goals according to the text-based knowledge collected\nfrom the Internet. Then, LLM Planner plans a sequence of structured actions for each sub-goal. The\nstructured actions are defined with clear semantics and corresponding feedback, enabling LLMs to\nunderstand surrounding environments and make decisions at the cognitive level. LLM Planner also\nrecords and summarizes successful action lists into a text-based memory to enhance future planning.\nFinally, LLM Interface execute the structured actions to interact with the environment by processing\nraw keyboard\/mouse input and receiving raw observations.\nIn this paper, we demonstrate the feasibility of employing Large Language Models (LLMs) to develop\nGenerally Capable Agents (GCAs) within an open-world environment built from Minecraft. By\nexploiting the common sense and reasoning capabilities of LLMs for hierarchical goal decomposition,\nas well as utilizing text-based knowledge and memory, this paradigm shows the possibility of\nenabling agents to address a wide range of challenges within Minecraft and allowing them to\neffectively handle such open-world environment. Consequently, our agent has surpassed all previous\nmethods in achieving the ObtainDiamond goal (+47.5% success rate). Our agent also demonstrates\nsuperior learning efficiency compared to previous methods, reducing the number of environment\ninteraction steps by more than 10,000×. Specifically, VPT [2] needs to be trained for 6,480 GPU days,\nDreamerV3 [7] needs to be trained for 17 GPU days, while our GITM does not require any GPUs\nand can be trained in just 2 days using a single CPU node with 32 CPU cores. More importantly, by\nobtaining all items in Minecraft Overworld as a milestone, this work represents a crucial first step\ntowards achieving GCAs that can handle any task humans can accomplish in Minecraft.\n2\nRelated Work\nMinecraft agents are intelligent programs that can perform various tasks within Minecraft world.\nReinforcement learning has dominated this area for many years. Some initial attempts have tried\nto use hierarchical RL [14, 15, 22] or imitation learning [1] in MineRL competitions [6, 10, 17].\nRecently, with large-scale web data, VPT [2] builds a foundation model for Minecraft by learning\nfrom videos. Based on its success, many works [18] have also explored to finetune foundation\nmodel with human feedback. On the other hand, as Minecraft agents become increasingly proficient\nin handling simple tasks, the importance of multi-task learning becomes more prominent. Some\nprevious works have adopted knowledge distillation [24] and curriculum learning [11], while recent\nworks [3, 5] tried to construct a language-conditioned multi-task agent via feeding the goal description\nembedding into the model.\nRecently, researchers have come to aware the extraordinary general planning ability for LLMs [8].\nMany works [9, 25, 28] have leveraged LLMs for enhancing the high-level planning ability of\nminecraft agents. Inner Monologue [9] leveraged environment feedback to improve the planning\nability of LLM. DEPS [25] further extended this closed-loop interaction by introducing description,\n2The name is chosen to pay tribute to the science fiction movie \"Ghost in the Shell\".\n3\ngoal\nLLM\nPlanner\nstructured actions\naction list\nfeedback\nobservation\nkeyboard & mouse\nenvironment\nexplore\nmine\ncraft \/ smelt\ndig_down\n…\ntext-based\nknowledge\ntext-based\nmemory\n(Object, Count, Material, Tool, Info)\ngoal format\nLLM\nDecomposer\nequip, explore, approach, mine, \nattack, dig_down, go_up, build,\ncraft, smelt, apply, place\nstructured action set\nsub-goal tree\nupdate\nupdate\nLLM Interface\nFigure 3: Overview of our GITM. Given a Minecraft goal, the LLM Decomposer divides the goal\ninto a sub-goal tree. The LLM Planner then plans an action sequence for each sub-goal. Finally,\nthe LLM Interface executes each action in the environment. Our LLM-based agents can be further\nenhanced by leveraging text-based knowledge and memory.\nexplainer and selector. Plan4MC [28] pre-defined basic skills and instructed LLM to extract the\nrelationship between skills to construct a skill graph.\nUnlike previous RL-based or RL with LLM methods, our LLM-native approach brings the minecraft\nagent to another level both in efficiency and robustness by leveraging high-level action abstraction\nand text-based knowledge and memory.\nLarge Language Models with Tools Extending the ability of LLMs by leveraging external tools\nhave drawn a lot of attentions recently. Several works [4, 13, 21] have explored to augment LLMs\nwith robot perception and control abilities. Code as Polices[13] tried to prompt LLM to generate\ncodes that can drive robots. PaLM-E [4] unified robot perception, instruction following, task planning\nand low-level control into a unified framework. Another line of works tries to build external plugins\naround LLMs to enhance its ability. Toolformer [19] tries to teach LLMs to choose and use a wide\nrange of tools like calculator and search engines and incorporate the results from tools into text\ngeneration. HuggingGPT [20] builds an agent for leveraging a combination of vision, language and\naudio models hosted on HuggingFace for completing user request. API Bank [12] proposes a syntheic\nbenchmark suite for evaluating the how good LLMs are for using external tools.\nCompared with these tool-augmented LLMs, our agents are tasks for much more complex goals in a\nhigh uncertain open-world.\n3\nMethod\nTraditional RL-based agents struggle to develop generally capable agents in Minecraft. The core issue\nis that they attempt to map extremely long-horizon and complex goals directly to the lowest-level\nkeyboard and mouse operations. To overcome this, we propose LLM-based agents in Fig. 2 that utilize\nhierarchical goal decomposition. LLM Decomposer, LLM Planner, and LLM Interface are introduced\nto progressively decompose the task goal into sub-goals, structured actions, and keyboard\/mouse\noperations. Moreover, LLM-based agents can leverage text-based knowledge and memory to quickly\nacquire the skills needed to master Minecraft.\n3.1\nLLM Decomposer\nRather than directly assigning the task goal to the agent and expecting a comprehensive and robust\naction plan, this work suggests the more practical strategy of decomposing the task goal into a\nseries of more achievable sub-goals. By addressing each constituent sub-goal, the task goal can\nbe progressively achieved. To this end, an LLM Decomposer is proposed. Goals are fed to the\ndecomposer and recursively decomposed into a sub-goal tree. Text-base knowledge provides the\nnecessary information for decomposition.\nGoal Format. Since we aim to obtain all items in Minecraft, all goals can be defined in the format of\n(Object, Count, Material, Tool, Info),\n(1)\nwhere “Object” denotes the target item, “Count” specifies the target quantity. “Material” and “Tool”\nrefer to prerequisites needed to obtain the target item. “Info” stores the text-based knowledge related\nto this goal. Given a specific goal, its sentence embedding extracted from a pre-trained LLM is used\n4\nAction Interface:\n𝑒𝑥𝑝𝑙𝑜𝑟𝑒(𝑜𝑏𝑗𝑒𝑐𝑡)\n𝑚𝑖𝑛𝑒(𝑜𝑏𝑗𝑒𝑐𝑡, 𝑡𝑜𝑜𝑙)\n𝑑𝑖𝑔_𝑑𝑜𝑤𝑛(𝑦_𝑙𝑒𝑣𝑒𝑙, 𝑡𝑜𝑜𝑙)\n...\nQuery Illustration:\nQuery contains a goal, feedback from the \nagent, and reference plan from the memory\nResponse Format:\n{\n“explanation”: “explain action failure”,\n“thoughts”: “thoughts on your plan”,\n“action_list”: [ … ]\n}\nInteraction Guideline:\nYou will receive feedback messages. If \nyour action failed, please replan …\nInstruction\nQuery\nGoal: obtain 1 diamond given iron pickaxe.\nExternal info: Diamond is usually\nfound at level 12 …\nFeedback\nReference plan\nPlan\n{\n“explanation”: “The last action failed because diamond cannot be found on the ground.”,\n“thoughts”: “First I need to dig down to level 12 where diamond is usually found. \nThen I can explore diamond ore underground with branching strategy.\nOnce I find diamond ore, I can mine it with the tool iron pickaxe.”\n“action_list”: [\n{“name”: “dig_down”, “args”: {“y_level”: “12”, “tool”: “iron_pickaxe”}},\n{“name”: “explore”, “args”: {“object”: “diamond_ore”, “strategy”: “branching”}},\n{“name”: “mine”, “args”: {“object”: “diamond_ore”, “tool”: “iron_pickaxe”}\n]\n}\nLarge \nLanguage \nModel\nFeedback\nThe action {“name”: “mine”, “args”: {“object”: “diamond_ore”, “tool”: “iron_pickaxe”} failed\nError message: the target object “diamond_ore” is too far\n- inventory: {‘iron_pickaxe’: 1, …}\n- environment: {‘biome’: forest, ‘y_level’: 12}\nAgent\nMemory\nGoal object: diamond\nReference plan:\n[\n{“name”: “dig_down”, “args”: {“y_level”: “12”, “tool”: “iron_pickaxe”}},\n{“name”: “explore”, “args”: {“object”: “diamond_ore”, “strategy”: “branching”}},\n{“name”: “approach”, “args”: {“object”: “diamond_ore”}},\n{“name”: “mine”, “args”: {“object”: “diamond_ore”, “tool”: “iron_pickaxe”}\n]\nFigure 4: Illustration of our planning process with the LLM Planner and the agent in the loop.\nGiven a specific goal, the planner generates plans with structured actions under the guidance of\ninstruction, user query, previous feedback, and reference plan from memory. The agent executes the\nactions and provides feedback for the following planning.\nto retrieve the most relevant text-based knowledge from an external knowledge base. Then, the LLM\nidentifies the required material, tools, and related information from the gathered knowledge. The\ncomplete instructions for the LLM are described in Appendix.\nRecursive Decomposition. This goal format enables recursive decomposition of each goal into a sub-\ngoal tree. Specifically, given a goal, all prerequisite items are listed as sub-goals, including materials,\ntools, and their corresponding quantities. Then, the recursive decomposition continues for each\nsub-goal until it has no prerequisites. After the decomposition is completed, the execution sequence\nof the sub-goals is planned through post-order traversal. Such goal decomposition significantly\nenhances the success rate of LLM planning, especially for goals necessitating long-horizon planning.\nText-based Knowledge. External knowledge is essential for automatic goal decomposition. We\nbuild an external knowledge base documented in text from the Minecraft Wiki on the Internet 3 and\nthe item crafting\/smelting recipes, providing an exhaustive source of knowledge about the Minecraft\nworld. For instance, if we need to craft a wooden pickaxe, the item crafting recipe will indicate that\nthe required materials are three planks and two sticks, and the necessary tool is a crafting table. It also\nprovides information about the distribution of raw materials. For example, diamonds are frequently\nfound in levels 10∼12 underground.\n3.2\nLLM Planner\nLLMs excel at language understanding and reasoning but struggle with low-level control and mul-\ntimodal perception. To leverage LLMs’ strengths while addressing their limitations, we develop\nstructured actions and feedback mechanisms as an abstract interface for them to manage agent-\nenvironment interaction. We propose an LLM-based Planner to achieve goals in Minecraft. Given a\ngoal, it generates structured actions to control agents, receives feedback, and revises plans accordingly.\nIt also has a text memory that aids planning by providing solutions for frequent goals.\nStructured Actions. The structured actions are designed with well-defined functions and clear\nsemantics, enabling LLMs to make decisions at the cognitive level. A structured action can be defined\nas follows:\n(Name, Arguments, Description),\n(2)\n3https:\/\/minecraft-archive.fandom.com\/wiki\/Minecraft_Wiki\n5\nTable 1: Examples of structured actions. A structured action contains name and arguments for\nexecution, as well as description to help LLMs understand and decide when to choose this action.\nName\nArguments\nDescription\nequip\nobject\nEquip the object from the inventory: used to equip equipment, including tools, weapons, and armor.\nexplore\nobject, strategy\nMove around to find the object: used to find objects including block items and entities on the ground.\napproach\nobject\nMove close to a visible object: used to approach the object you want to attack or mine.\nmine\/attack\nobject, tool\nAttack \/ Mine the object with the tool: used to attack \/ mine the object within reach.\ndig_down\/go_up\nylevel, tool\nDig down \/ Go up with the tool: used to go down \/ up underground.\nbuild\nblueprint\nBuild according to a blueprint: used to place corresponding objects on locations according to a preset blueprint.\ncraft\/smelt\nobject, tool, material\nCraft \/ Smelt the object with the materials and tool: used to craft new object that is not in the inventory or is not enough.\napply\/place\nobject, tool\nApply \/ Place the tool on the object: used to apply tools or place blocks.\nThe name and arguments defines the action we want the agent to execute, while the action description\nprovides enough information for letting LLMs know when to choose the corresponding actions, as\nshown in Tab. 1.\nWe extract the set of structured actions by leveraging the powerful reasoning capability of LLMs.\nSpecifically, a pre-trained LLM is utilized to decompose the 3141 predefined tasks provided by\nMineDojo [5] into action sequences. Instructions for guiding LLMs on action decomposition are\nprovided in Appendix. Then, we extract the structured actions by selecting frequent actions and\nmerging actions with similar functionalities. See Appendix for the set of structured actions.\nFeedback Mechanism. Open-loop planning cannot guarantee success, especially in open-world\nenvironments, where agents might encounter unexpected events. Feedback is crucial to form an effec-\ntive closed loop. Without appropriate feedback, the LLM has no information about the consequences\nof actions and may repeat failed action plans. Feedback message is designed to present the agent’s\ncurrent state in the environment (i.e., inventory and environment), as well as the success and failure\ninformation for each executed actions, as shown in Fig. 4. By incorporating this feedback message,\nthe LLM can update its understanding of the environment, refine their strategies, and adapt their\nbehavior accordingly.\nPlanning. Once the abstract interface is prepared, a pre-trained LLM is queried to generate goal-\nspecific action sequence. This is achieved through carefully designed instructions and user queries,\nenabling the LLM to efficiently create and revise the plans. Fig. 4 illustrates the planning process.\nSee Appendix for the full description.\nInstruction specifies the guidelines that LLMs must follow when planning, including 1) Action\nInterface provides functional descriptions of the structured actions and their parameters; 2) Query\nIllustration clarifies the structure and meaning of user queries; 3) Response Format requires LLM to\nreturn responses in the format of {Explanation, Thought, Action List}, where “Explanation” requires\nLLMs to explain the reason for action failure, “Thought” requires LLM to use natural language to\nplan before outputting action sequences as a chain-of-thought (CoT) mechanism [26], and “Action\nList” outputs a list of structured actions to be executed; 4) Interaction Guideline guides LLMs to\ncorrect failed actions based on the feedback message, thus enabling the LLM to revise the plan.\nUser Query provides the specific query to LLMs for a given goal, including 1) Goal represents the\nobjective by text as “Obtain Count Item, given Material and Tool. Extra info: Info” according to\nEq. (1); 2) Feedback is the feedback information of the abstract interface; 3) Reference Plan provides\na common reference plan for the current goal retrieved from the text-base memory.\nText-based Memory is designed for LLM to maintain common reference plans for each encountered\nobjective as experiential knowledge. LLMs acquire the experience about controlling agents and\nresolving specific situations through game play and agent interaction. Instead of starting from scratch\nevery time, using prior experience allows LLMs to handle tasks more efficiently, a process similar to\nhuman skill improvement through practice.\nTo this end, we design a text-based memory mechanism for LLM to store and retrieve gained knowl-\nedge. Unlike the RL-based model, which stores knowledge in parameters, this textual knowledge is\nexplicit, logical, and closely aligned with human thought processes. This allows for direct application\nto a wide range of similar tasks, leading to more efficient learning and improved generalization.\nSpecifically, during each game episode, once the goal is achieved, the entirely executed action list\nwould be stored in memory. The LLM may achieve the same goal under various circumstances,\nresulting in a range of different plans. To identify a common reference plan suitable for general\nsituations, essential actions from multiple plans are summarized. This summarization process is\n6\nacacia_boat\nacacia_door\nacacia_fence\nacacia_fence_gate\nacacia_stairs\nbeef\nbirch_boat\nbirch_door\nbirch_fence\nbirch_fence_gate\nbirch_stairs\nboat\nbone\nbowl\nchest\nchicken\ncobblestone\ncobblestone_wall\ncooked_beef\ncooked_chicken\ncooked_mutton\ncooked_porkchop\ncrafting_table\ndark_oak_boat\ndark_oak_door\ndark_oak_fence\ndark_oak_fence_gate\ndark_oak_stairs\ndirt\ndouble_plant\nfence\nfence_gate\nfurnace\nglass\nglass_bottle\nglass_pane\nladder\nlever\nlog\nmutton\noak_stairs\npaper\nplanks\nporkchop\nred_flower\nreeds\nsand\nsandstone\nsapling\nsign\nspruce_boat\nspruce_door\nspruce_fence\nspruce_fence_gate\nspruce_stairs\nstick\nstone\nstone_axe\nstone_brick_stairs\nstone_button\nstone_hoe\nstone_pickaxe\nstone_pressure_plate\nstone_shovel\nstone_stairs\nstone_sword\nstonebrick\nsugar\ntallgrass\ntrapdoor\nwheat\nwheat_seeds\nwooden_axe\nwooden_button\nwooden_door\nwooden_hoe\nwooden_pickaxe\nwooden_pressure_plate\nwooden_shovel\nwooden_slab\nwooden_sword\nyellow_flower\nbone_meal\narmor_stand\nbook\nbread\ncoal\niron_ingot\niron_nugget\niron_ore\niron_shovel\nitem_frame\nleather\nrotten_flesh\nshield\nspider_eye\nstone_slab\ntorch\ntrapped_chest\ntripwire_hook\nfireworks\ngunpowder\nsnow\nsnow_layer\nsnowball\ncarpet\ngrass\neavy_weighted_pressure_plate\niron_hoe\niron_sword\nleather_boots\nleaves\npainting\nshears\nstring\nwool\ncoal_block\nleather_helmet\nbrown_mushroom\nbrown_mushroom_block\nflint\nbed\nbucket\nhay_block\niron_axe\niron_pickaxe\nmilk_bucket\nsandstone_stairs\nwater_bucket\nfermented_spider_eye\nleather_leggings\ngravel\nflint_and_steel\nfishing_rod\niron_boots\niron_trapdoor\nleather_chestplate\nmossy_cobblestone\nvine\nwaterlily\nbone_block\nbow\nchest_minecart\nfurnace_minecart\nhopper\niron_helmet\nminecart\ndiamond\ndiamond_shovel\ndropper\njukebox\nnoteblock\nredstone\nredstone_torch\nbanner\nfeather\niron_bars\niron_door\nrail\nbrick\nclay_ball\nlapis_lazuli\npiston\nemerald\ncauldron\niron_leggings\ntnt\ndiamond_hoe\ndiamond_sword\nflower_pot\niron_chestplate\narrow\ncompass\nhopper_minecart\ngold_ingot\ngold_nugget\ngold_ore\ngolden_shovel\niron_block\nbrick_block\nclay\nhardened_clay\nslime_ball\ndispenser\ndiamond_axe\ndiamond_pickaxe\nlava_bucket\nactivator_rail\ndetector_rail\negg\nrepeater\ntnt_minecart\nbookshelf\ngolden_hoe\ngolden_sword\night_weighted_pressure_plate\nredstone_block\nred_mushroom\nred_mushroom_block\nbeetroot\nbeetroot_seeds\ndiamond_boots\ndiamond_helmet\ngolden_axe\ngolden_pickaxe\nsticky_piston\nink_sac\ndiamond_leggings\ngolden_boots\nmushroom_stew\nmap\nbeetroot_soup\nlead\ngolden_helmet\nrabbit_hide\ncooked_rabbit\nrabbit\nbrick_stairs\ncake\nobsidian\ncactus\ndiamond_chestplate\nclock\ndeadbush\nwritable_book\nlapis_block\ngolden_leggings\ngolden_rail\nbaked_potato\npotato\ndiamond_block\ngolden_chestplate\nemerald_block\ncarrot\npumpkin\npumpkin_seeds\njungle_boat\njungle_door\njungle_fence\njungle_fence_gate\njungle_stairs\nlit_pumpkin\ncarrot_on_a_stick\nmelon\nmelon_block\nmelon_seeds\ngolden_carrot\ngold_block\npumpkin_pie\nred_sandstone\nstone_slab2\nred_sandstone_stairs\nspeckled_melon\nenchanting_table\napple\nanvil\nenchanted_book\npoisonous_potato\nrabbit_foot\nslime\ngolden_apple\nrabbit_stew\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nOurs\nDEPS\nDreamerV3\nVPT\nFigure 5: Success rate for all items in the entire Minecraft Overworld Technology Tree. The x\naxis lists all item names. We overlay the results from our GITM and the best results from baselines.\nalso implemented using LLMs (see Appendix for details). When encountering similar goals, the\nLLM creates new plans based on the summarized reference plans retrieved from memory. Successful\naction sequences from these new plans are also added to memory for future summarization. As the\nLLM-based Planner accumulates summaries, it becomes increasingly effective.\n3.3\nLLM Interface\nUnlike the existing RL-based agents that directly control keyboard and mouse, LLM-based agents\ninteract with the environment through structured actions and feedback messages. The LLM interface\nserves to implement structured actions as keyboard\/mouse operations, and extract observations\nprovided by the environment into feedback messages.\nStructured actions can be implemented in various ways such as hand-written scripts or RL-learned\nmodels. While RL-learned models have been employed in Minecraft previously, they were either\nbroad in functionality but inefficient in practice, or too specific in functionality, limiting their\napplicability to general tasks and actions. Clarifying the capability boundary of RL-learned models is\nchallenging. Instead, in this work, we choose to implement structured actions using hand-written\nscripts. Since structured actions are well-defined and easy to implement, we can manually implement\nthem based on observations (e.g., location, LiDAR, and voxel) and basic operations (e.g., move,\njump, adjust camera angle, click left mouse button, and click right mouse button) provided by the\nMineDojo [5] environment. See Appendix for details.\nFeedback messages can be obtained directly from the environment. These include whether the\nstructured action execution succeeded or failed. If the execution fails, the reason for the failure is\nadditionally notified. It also includes the current state of the agent in the environment, including the\nitems in the inventory, the current biome and depth, etc. See Appendix for details.\n4\nExperiments\nTask Definition and Metrics. We measure the ability of GITM through item collection tasks. We\nonly collect items could be found in the Overworld. We exclude items could only be obtained by\ntrading with villagers, opening treasure chest or find a special structure on the map, using a tool\nenchanted with Silk Touch. This give us a total of 262 tasks. For the assessment of our agent, we\nemploy “Coverage of the Overworld Technology Tree” and“Success Rate” as evaluation metrics.\n7\n4.1\nMain Result\nUnlocking the Entire Technology Tree by Obtaining All Items. Compared with existing Minecraft\nagents [2, 7, 25] which mostly focuses on solving the ObtainDiamond task and could only unlock\na limited part of the full technology tree (13\/262 for Dreamerv3, 15\/262 VPT, 69\/262 for DEPS),\nour approach could collect all 262 items as shown in Fig. 1. There are two major blockers for\nexisting methods. For RL-based methods like VPT [2] and DreamerV3 [7], the goal item(diamond)\nis hard-coded into the model weights, which means there are no easy way to re-task the trained\nRL agents for collecting other items in the inference stage. Moreover, the low training efficiency\nhinders them from solving extremely long-horizon tasks (e.g., obtaining a “enchanted_book”). For\nmethods like DEPS [25] that use an RL controller [3] and LLM planner still rely on pre-trained RL\nagents to execute specific subtasks (e.g. mining 1 “cobblestone”) in the generated plan. So these\napproaches still suffer from the inability of RL-based methods alone to generalize to unseen tasks\n(e.g. obtaining “lapis_lazuli”). In contrast, we extract a well-defined set of structured actions by\nusing LLMs to decompose over 3000 predefined MineDojo tasks. This provides broad, open-world\nMinecraft capability. Combined with LLM planning, it enables solving more complex tasks than\nObtainDiamond - which RL cannot achieve. Our knowledge bases also improve efficiency. To our\nknowledge, we present the first agent to unlock the entire Overworld technology tree - a level of\nopen-world skill RL-based methods have not demonstrated.\nSuccess Rate for the Entire Technology Tree. We show the success rate of our method for collecting\nall Overworld items in Fig. 5. Our methods could achieve 100% success rate for simple tasks like\ncollecting wooden tools. It achieves non-zero success rates for all items which indicates a strong\ncollecting capability. The successful rate for collecting different items change smoothly for our agent,\nwhich showcase the robustness of our method against the highly uncertain open world environment.\n4.2\nComparison with Other Minecraft Agents\nTable 2: Comparison of our GITM with pre-\nvious methods on ObtainDiamond challenge.\nMethod\nSuccess Rate (%)\nDreamerV3\n-\n50.0\n3.0\n0.01\n0.01\nDEPS\n90.0\n80.0\n73.3\n10.0\n0.6\nVPT\n100.0 100.0 100.0\n85.0\n20.0\nOur GITM\n100.0 100.0 100.0\n95.0\n67.5\n101\n103\n105\n107\n109\nStep\n0\n10\n20\n30\n40\n50\n60\n70\nSuccess Rate\nVPT\nDreamV3\nOurs\nFigure 6: Comparison of learning efficiency.\nWe compared our LLM-based method with three existing agents: VPT [2], DreamerV3 [7], and\nDEPS [25] on the well known ObtainDiamond challenge, i.e, obtaining a diamond from scratch in\nMinecraft. Previous methods set different time limits of a single episode of game play (20 minutes\nfor VPT, 30 minutes for Dreamerv3, and 10 minutes for DEPS). For fair comparison, we use the\nstrictest limit of previous methods: 10 minutes (12,000 steps at 20Hz control).\nSuccess Rate for Obtaining Diamond and Other Items.\nSince VPT and Dreamerv3 are not\ntargeted for collecting items other than diamond, we mainly compare our method with DEPS for\nitems not related to obtain diamonds. Overall, our GITM and VPT rank task difficulty similarly, but\nDEPS rankings severely fluctuate for tasks more complex than mining coal. Dreamerv3 also behaves\noddly by having an abnormally low success rate on tasks like obtaining a stone sword. As shown in\nFig. 2, most agents performs generally well for easy tasks relating to make wooden tools. VPT could\neven rival with our GITM for the success rate of obtaining iron axes. But for obtaining diamonds, our\nmethod wins over any other methods by 3.5 times on the succeess rate.\nThis giant improvement comes from the following two aspects: First, we employ the strong long-term\nplanning capability of LLMs to decompose the complex tasks into feasible sub-goals and tackle\nthem step by step. Second, our model can directly leverage external knowledge such as the suitable\nlocations for mining ores, while RL models need to explore themselves and may not acquire reliable\nknowledge.\n8\nTable 3: Ablation study. The milestone items from left to right are crafting table\n, wooden\npickaxe\n, stone pickaxe\n, iron pickaxe\n, and diamond\n. The success rate is calculated\nunder time limit of 12000 steps (total) and query limit of 30 (each sub-goal). “Goal Decomp.” and\n“External Info.” indicates goal decomposition and external knowledge respectively.\nGoal\nDecomp.\nFeedback\nExternal\nInfo.\nMemory\nSuccess Rate (%)\n57.5\n32.5\n5.0\n0.0\n0.0\n✓\n90.0\n90.0\n67.5\n2.5\n0.0\n✓\n✓\n97.5\n95.0\n77.5\n20.0\n5.0\n✓\n✓\n✓\n100.0\n100.0\n100.0\n57.5\n35.0\n✓\n✓\n✓\n✓\n100.0\n100.0\n100.0\n95.0\n67.5\nLearning Efficiency.\nBesides measuring the success rate of each agents, we also compare the\nlearning efficiency of our model with other learnable models. Since DEPS uses a LLM-based planner\nwithout learning mechanism and a pre-trained RL-based controller, its performance could not improve\nwith more episodes played and is excluded from the comparison here.\nIt usually takes tens of millions of steps to train an RL agent by updating parameters before its success\nrate starts to converges to meaningful non-zero numbers. However, the success rate for RL-based\nagents increases rather slowly even after them starts to converge. On the contrary, the learning process\nof our LLM-based agent is considerably faster. As shown in Fig. 6, our method requires several orders\nless episodes than any other methods before doubling its initial success rate. Moreover, our method is\nextremely sample efficient as our success rate raises from 35% to 47.5% by learning from the first five\nthousand steps. By just playing each task several times and summarize successful experience into the\nmemory, the LLM-based agent can acquire explicit experiential knowledge and achieve significantly\nhigher success rate.\n4.3\nAblation Study\nWe conduct ablation experiments on the ObtainDiamond task. We set a time limit of 10 minutes of\ngame play (12000 steps at the control frequency of 20Hz). When leveraging goal decomposition, for\neach sub-goal, we set the maximum number of queries to LLM as 30, and exceeding the query limit\nwill be judged as a failure. For each setting, we run 40 games and calculate the success rate. Tab. 3\nrecords the success rates of achieving the final goal diamond as well as the milestones in this goal,\nincluding crafting table, wooden pickaxe, stone pickaxe, and iron pickaxe.\nGoal Decomposition. Without goal decomposition, the planner can only accomplish several short-\nterm tasks such as obtaining stone axes with rather low success rate of 5%, which indicates the\nnecessity of goal decomposition. Leveraging the powerful long-term planning capabilities of LLMs,\nthe goals are decomposed into sub-goals feasible and practical for the planner, so the success rate for\nobtaining stone axes advances from 5% to 67.5% by leveraging goal decomposition alone.\nFeedback Message. Feedback contains the agent’s state and the execution result of the actions, which\nhelps the planner to understand and make another attempt to correct the mistakes in the previous and\ndeal with special cases. This enables the planner to accomplish a broader range of goals with higher\nsuccess rate. As shown in the 3rd row of Tab. 3, our agent gain the ability to collect diamond by\ncombining feedback with goal decomposition.\nExternal Knowledge Base. External knowledge contains general rules, crafting recipes, and common\ntricks in Minecraft, such as the recipes for crafting iron ingot and iron pickaxe, the suitable location to\nfind diamond ore, and the efficient way to get cobblestone. Providing the planner with this information\ngreatly boosts the success rate of obtaining iron pickaxe and diamond, and the success rate of mining\ndiamond increase by 7 times by learning from the knowledge base that diamonds are more likely to\nappear in specific levels.\nText-based Memory. Leveraging the reference plan recorded in the memory, the planner can handle\nthe task it has encountered more efficiently. The success rates of obtaining iron pickaxe and diamond\nare 95.0% and 67.5%, surpassing the model without memory by 37.5% and 32.5%, respectively.\n9\n5\nConclusion\nWe introduce the GITM framework, which utilizes Large Language Models (LLMs) for hierarchical\ndecomposition of goals. GITM introduces LLM Decomposer, LLM Planner and LLM Interface\nto gradually decompose goals into sub-goals, structured actions and keyboard\/mouse operations.\nThis work makes significant progress towards the ObtainDiamond goal, outperforming all previous\nmethods by a significant margin (+47.5% success rate). This proves the potential inefficiency and\npoorly scalability of Reinforcement Learning (RL) in Minecraft, breaking the traditional reliance\non RL. Moreover, by obtaining all items in Minecraft Overworld, this research marks a critical step\ntoward Generally Capable Agents (GCAs) that match human performance in Minecraft.\nAcknowledgments\nThe work is partially supported by the National Natural Science Foundation of\nChina under grants No.U19B2044, No.61836011, No.62022048, and No.62276150. This work is also\npartially supported by the National Key R&D Program of China under grants NO.2022ZD0114900,\nand the Guoqiang Institute of Tsinghua University.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nGhost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory\n```\n#### 2. 论文摘要\n```\nThe captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https:\/\/github.com\/OpenGVLab\/GITM.\n```\n\n#### 3. 论文全文\n```\nGhost in the Minecraft: Generally Capable Agents for\nOpen-World Environments via Large Language\nModels with Text-based Knowledge and Memory\nXizhou Zhu1,2∗, Yuntao Chen3∗, Hao Tian2∗, Chenxin Tao1,2∗, Weijie Su2,4∗, Chenyu Yang1∗,\nGao Huang1, Bin Li4, Lewei Lu2, Xiaogang Wang2,5, Yu Qiao6, Zhaoxiang Zhang7, Jifeng Dai1,6 \u00001Tsinghua University\n2SenseTime Research\n3Centre for Artificial Intelligence and Robotics, HKISI, CAS\n4University of Science and Technology of China\n5The Chinese University of Hong Kong\n6Shanghai Artificial Intelligence Laboratory\n7Institute of Automation, Chinese Academy of Science (CASIA)\n{zhuxizhou,gaohuang,daijifeng}@tsinghua.edu.cn, chenyuntao08@gmail.com\ntianhao2@senseauto.com, {tcx20,yangcy19}@mails.tsinghua.edu.cn,\njackroos@mail.ustc.edu.cn, binli@ustc.edu.cn, luotto@sensetime.com\nxgwang@ee.cuhk.edu.hk, qiaoyu@pjlab.org.cn, zhaoxiang.zhang@ia.ac.cn\nAbstract\nThe captivating realm of Minecraft has attracted substantial research interest in\nrecent years, serving as a rich platform for developing intelligent agents capable of\nfunctioning in open-world environments. However, the current research landscape\npredominantly focuses on specific objectives, such as the popular \"ObtainDiamond\"\ntask, and has not yet shown effective generalization to a broader spectrum of\ntasks. Furthermore, the current leading success rate for the \"ObtainDiamond\"\ntask stands at around 20%, highlighting the limitations of Reinforcement Learning\n(RL) based controllers used in existing methods. To tackle these challenges, we\nintroduce Ghost in the Minecraft (GITM), a novel framework integrates Large\nLanguage Models (LLMs) with text-based knowledge and memory, aiming to\ncreate Generally Capable Agents (GCAs) in Minecraft. These agents, equipped\nwith the logic and common sense capabilities of LLMs, can skillfully navigate\ncomplex, sparse-reward environments with text-based interactions. We develop\na set of structured actions and leverage LLMs to generate action plans for the\nagents to execute. The resulting LLM-based agent markedly surpasses previous\nmethods, achieving a remarkable improvement of +47.5% in success rate on the\n\"ObtainDiamond\" task, demonstrating superior robustness compared to traditional\nRL-based controllers. Notably, our agent is the first to procure all items in the\nMinecraft Overworld technology tree, demonstrating its extensive capabilities.\nGITM does not need any GPU for training, but a single CPU node with 32 CPU\ncores is enough. This research shows the potential of LLMs in developing capable\nagents for handling long-horizon, complex tasks and adapting to uncertainties\nin open-world environments. See the project website at https:\/\/github.com\/\nOpenGVLab\/GITM.\n1\nIntroduction\n“What if a cyber brain could possibly generate its own ghost, create a soul all by itself? And if it did,\njust what would be the importance of being human then?”\n— Ghost in the Shell (1995)\n∗Equal contribution. This work is done when Chenxin Tao and Weijie Su are interns at SenseTime Research.\n\u0000 Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.\narXiv:2305.17144v2  [cs.AI]  1 Jun 2023\nFigure 1: Our GITM unlocks the entire technology tree by obtaining all items in Minecraft\nOverworld. Each node represents an individual item in Minecraft. The directed edges between nodes\nrepresent prerequisite relationships for obtaining items. For better readability, we manually merge\nsome similar nodes, e.g., “wooden_pickaxe”, “wooden_axe”, “wooden_hoe”, and ’wooden_shovel’\nare merged into one node, and “wooden_pickaxe” is selected to represent the merged node. Existing\nMinecraft agents [2, 7, 25] only unlocked 78 \/ 262 = 30% items, while our GITM successfully\nunlocked all items.\ngoal\ngoal\nRL Agent\nkeyboard & mouse\nobservation\naction list\nfeedback\nobservation\nkeyboard & mouse\nEnvironment\nexplore\nmine\ncraft \/ smelt\ndig_down\n…\nRL-based method\nOurs\ngoal\nLLM\nDecomposer\nLLM Planner\nstructured actions\naction list\nfeedback\nobservation\nkeyboard & mouse\nEnvironment\nexplore\nmine\ncraft \/ smelt\ndig_down\n…\nGhost In the Minecraft (GITM)\nsub-goal tree\nLLM Interface\nFigure 2: Comparison between RL-based method and our GITM. RL agents try to map an\ncomplex goal directly to a sequence of low-level control signals, while our GITM leverages LLM to\nbreak down the goals and map them to structured actions for final control signals.\nMinecraft, as the world’s best-selling game, boasts over 238 million copies sold and more than\n140 million peak monthly active users [27]. Within the game, hundreds of millions of players have\nexperienced a digital second life by surviving, exploring and creating, closely resembling the human\nworld in many aspects. Given its massive scale, vast success, and unrestricted freedom, Minecraft\nhas established itself as an unparalleled platform for researching autonomous and robust Generally\nCapable Agents (GCAs) [23] in open-world environments brimmed with long-horizon challenges,\nenvironmental disruptions, and uncertainties.\nMinecraft acts as a microcosm of the real world. Developing an automated agent that can mas-\nter all technical challenges in Minecraft is akin to creating an artificial intelligence capable of\nautonomously learning and mastering the entire real-world technology. However, existing re-\nsearches [2, 7, 25] remain narrowly scoped. Prior studies have predominantly focused on the\nspecific goal of ObtainDiamond [18]. Yet, in the process of obtaining diamonds, the number of\ntypes of items involved only accounts for <5% of the entire Minecraft world. ObtainDiamond only\nrequires specialized skills in a specific domain, while obtaining all items in Minecraft demonstrates a\nwide range of knowledge and capabilities, similar to mastering multidisciplinary fields in the real\nworld. As illustrated in Fig. 1, our work endeavors to obtain all items in Minecraft within a reasonable\ncomputation budget. This achievement stands as a significant milestone in the development of GCAs,\nillustrating the potential of intelligent agents to match human performance in terms of versatility and\nadaptability.\nAlthough reinforcement learning (RL) [16] is the most popular paradigm for approaching GCAs, it\nhas shown some staggering limitations in conquering Minecraft. RL-based agents typically require\na vast number of learning steps (e.g., nearly 30 million steps to obtain diamonds as reported in\nDreamerV3 [7]) and exhibit poor scalability when learning new tasks(e.g., VPT [2] uses different\nagents for world exploration and diamond mining). As a consequence, adopting RL-based agents for\n2\ncompleting a wide range of tasks may require an prohibitively high number of training steps, making\nit impractical to obtain all items in Minecraft. This inefficiency and lack of adaptability have hindered\nthe development of generally capable agents in open-world environments.\nAs shown in Fig. 2, the biggest dilemma of previous RL-based agents is how to map an extremely\nlong-horizon and complex goal to a sequence of lowest-level keyboard\/mouse operations. To address\nthis challenge, we propose our framework Ghost In the Minecraft (GITM) 2, which uses Large\nLanguage Model (LLM)-based agents as a new paradigm. Instead of direct mapping like RL agents,\nour LLM-based agents employ a hierarchical approach. It first breaks down the decompose goal\ninto sub-goals, then into structured actions, and finally into keyboard\/mouse operations. Such\ndecomposition is similar to how humans solve complex problems in the real world, enabling mastery\nof Minecraft with efficiency orders of magnitude higher than that of RL. LLM can also leverage\ntext-based knowledge and memory to quickly acquire the ability to interact with the environment\nand accomplish goals, offering immense learning efficiency improvements, unlimited scalability and\nrepresenting a disruptive innovation compared with RL. Our GITM framework has the potential to\nrevolutionize the path to generally capable agents.\nSpecifically, the proposed LLM-based agent consists of an LLM Decomposer, an LLM Planner, and\nan LLM Interface, which are responsible for the decomposition of sub-goals, structured actions,\nand keyboard\/mouse operations, respectively. Given a goal in Minecraft, LLM Decomposer first\ndecomposes it into a series of well-defined sub-goals according to the text-based knowledge collected\nfrom the Internet. Then, LLM Planner plans a sequence of structured actions for each sub-goal. The\nstructured actions are defined with clear semantics and corresponding feedback, enabling LLMs to\nunderstand surrounding environments and make decisions at the cognitive level. LLM Planner also\nrecords and summarizes successful action lists into a text-based memory to enhance future planning.\nFinally, LLM Interface execute the structured actions to interact with the environment by processing\nraw keyboard\/mouse input and receiving raw observations.\nIn this paper, we demonstrate the feasibility of employing Large Language Models (LLMs) to develop\nGenerally Capable Agents (GCAs) within an open-world environment built from Minecraft. By\nexploiting the common sense and reasoning capabilities of LLMs for hierarchical goal decomposition,\nas well as utilizing text-based knowledge and memory, this paradigm shows the possibility of\nenabling agents to address a wide range of challenges within Minecraft and allowing them to\neffectively handle such open-world environment. Consequently, our agent has surpassed all previous\nmethods in achieving the ObtainDiamond goal (+47.5% success rate). Our agent also demonstrates\nsuperior learning efficiency compared to previous methods, reducing the number of environment\ninteraction steps by more than 10,000×. Specifically, VPT [2] needs to be trained for 6,480 GPU days,\nDreamerV3 [7] needs to be trained for 17 GPU days, while our GITM does not require any GPUs\nand can be trained in just 2 days using a single CPU node with 32 CPU cores. More importantly, by\nobtaining all items in Minecraft Overworld as a milestone, this work represents a crucial first step\ntowards achieving GCAs that can handle any task humans can accomplish in Minecraft.\n2\nRelated Work\nMinecraft agents are intelligent programs that can perform various tasks within Minecraft world.\nReinforcement learning has dominated this area for many years. Some initial attempts have tried\nto use hierarchical RL [14, 15, 22] or imitation learning [1] in MineRL competitions [6, 10, 17].\nRecently, with large-scale web data, VPT [2] builds a foundation model for Minecraft by learning\nfrom videos. Based on its success, many works [18] have also explored to finetune foundation\nmodel with human feedback. On the other hand, as Minecraft agents become increasingly proficient\nin handling simple tasks, the importance of multi-task learning becomes more prominent. Some\nprevious works have adopted knowledge distillation [24] and curriculum learning [11], while recent\nworks [3, 5] tried to construct a language-conditioned multi-task agent via feeding the goal description\nembedding into the model.\nRecently, researchers have come to aware the extraordinary general planning ability for LLMs [8].\nMany works [9, 25, 28] have leveraged LLMs for enhancing the high-level planning ability of\nminecraft agents. Inner Monologue [9] leveraged environment feedback to improve the planning\nability of LLM. DEPS [25] further extended this closed-loop interaction by introducing description,\n2The name is chosen to pay tribute to the science fiction movie \"Ghost in the Shell\".\n3\ngoal\nLLM\nPlanner\nstructured actions\naction list\nfeedback\nobservation\nkeyboard & mouse\nenvironment\nexplore\nmine\ncraft \/ smelt\ndig_down\n…\ntext-based\nknowledge\ntext-based\nmemory\n(Object, Count, Material, Tool, Info)\ngoal format\nLLM\nDecomposer\nequip, explore, approach, mine, \nattack, dig_down, go_up, build,\ncraft, smelt, apply, place\nstructured action set\nsub-goal tree\nupdate\nupdate\nLLM Interface\nFigure 3: Overview of our GITM. Given a Minecraft goal, the LLM Decomposer divides the goal\ninto a sub-goal tree. The LLM Planner then plans an action sequence for each sub-goal. Finally,\nthe LLM Interface executes each action in the environment. Our LLM-based agents can be further\nenhanced by leveraging text-based knowledge and memory.\nexplainer and selector. Plan4MC [28] pre-defined basic skills and instructed LLM to extract the\nrelationship between skills to construct a skill graph.\nUnlike previous RL-based or RL with LLM methods, our LLM-native approach brings the minecraft\nagent to another level both in efficiency and robustness by leveraging high-level action abstraction\nand text-based knowledge and memory.\nLarge Language Models with Tools Extending the ability of LLMs by leveraging external tools\nhave drawn a lot of attentions recently. Several works [4, 13, 21] have explored to augment LLMs\nwith robot perception and control abilities. Code as Polices[13] tried to prompt LLM to generate\ncodes that can drive robots. PaLM-E [4] unified robot perception, instruction following, task planning\nand low-level control into a unified framework. Another line of works tries to build external plugins\naround LLMs to enhance its ability. Toolformer [19] tries to teach LLMs to choose and use a wide\nrange of tools like calculator and search engines and incorporate the results from tools into text\ngeneration. HuggingGPT [20] builds an agent for leveraging a combination of vision, language and\naudio models hosted on HuggingFace for completing user request. API Bank [12] proposes a syntheic\nbenchmark suite for evaluating the how good LLMs are for using external tools.\nCompared with these tool-augmented LLMs, our agents are tasks for much more complex goals in a\nhigh uncertain open-world.\n3\nMethod\nTraditional RL-based agents struggle to develop generally capable agents in Minecraft. The core issue\nis that they attempt to map extremely long-horizon and complex goals directly to the lowest-level\nkeyboard and mouse operations. To overcome this, we propose LLM-based agents in Fig. 2 that utilize\nhierarchical goal decomposition. LLM Decomposer, LLM Planner, and LLM Interface are introduced\nto progressively decompose the task goal into sub-goals, structured actions, and keyboard\/mouse\noperations. Moreover, LLM-based agents can leverage text-based knowledge and memory to quickly\nacquire the skills needed to master Minecraft.\n3.1\nLLM Decomposer\nRather than directly assigning the task goal to the agent and expecting a comprehensive and robust\naction plan, this work suggests the more practical strategy of decomposing the task goal into a\nseries of more achievable sub-goals. By addressing each constituent sub-goal, the task goal can\nbe progressively achieved. To this end, an LLM Decomposer is proposed. Goals are fed to the\ndecomposer and recursively decomposed into a sub-goal tree. Text-base knowledge provides the\nnecessary information for decomposition.\nGoal Format. Since we aim to obtain all items in Minecraft, all goals can be defined in the format of\n(Object, Count, Material, Tool, Info),\n(1)\nwhere “Object” denotes the target item, “Count” specifies the target quantity. “Material” and “Tool”\nrefer to prerequisites needed to obtain the target item. “Info” stores the text-based knowledge related\nto this goal. Given a specific goal, its sentence embedding extracted from a pre-trained LLM is used\n4\nAction Interface:\n𝑒𝑥𝑝𝑙𝑜𝑟𝑒(𝑜𝑏𝑗𝑒𝑐𝑡)\n𝑚𝑖𝑛𝑒(𝑜𝑏𝑗𝑒𝑐𝑡, 𝑡𝑜𝑜𝑙)\n𝑑𝑖𝑔_𝑑𝑜𝑤𝑛(𝑦_𝑙𝑒𝑣𝑒𝑙, 𝑡𝑜𝑜𝑙)\n...\nQuery Illustration:\nQuery contains a goal, feedback from the \nagent, and reference plan from the memory\nResponse Format:\n{\n“explanation”: “explain action failure”,\n“thoughts”: “thoughts on your plan”,\n“action_list”: [ … ]\n}\nInteraction Guideline:\nYou will receive feedback messages. If \nyour action failed, please replan …\nInstruction\nQuery\nGoal: obtain 1 diamond given iron pickaxe.\nExternal info: Diamond is usually\nfound at level 12 …\nFeedback\nReference plan\nPlan\n{\n“explanation”: “The last action failed because diamond cannot be found on the ground.”,\n“thoughts”: “First I need to dig down to level 12 where diamond is usually found. \nThen I can explore diamond ore underground with branching strategy.\nOnce I find diamond ore, I can mine it with the tool iron pickaxe.”\n“action_list”: [\n{“name”: “dig_down”, “args”: {“y_level”: “12”, “tool”: “iron_pickaxe”}},\n{“name”: “explore”, “args”: {“object”: “diamond_ore”, “strategy”: “branching”}},\n{“name”: “mine”, “args”: {“object”: “diamond_ore”, “tool”: “iron_pickaxe”}\n]\n}\nLarge \nLanguage \nModel\nFeedback\nThe action {“name”: “mine”, “args”: {“object”: “diamond_ore”, “tool”: “iron_pickaxe”} failed\nError message: the target object “diamond_ore” is too far\n- inventory: {‘iron_pickaxe’: 1, …}\n- environment: {‘biome’: forest, ‘y_level’: 12}\nAgent\nMemory\nGoal object: diamond\nReference plan:\n[\n{“name”: “dig_down”, “args”: {“y_level”: “12”, “tool”: “iron_pickaxe”}},\n{“name”: “explore”, “args”: {“object”: “diamond_ore”, “strategy”: “branching”}},\n{“name”: “approach”, “args”: {“object”: “diamond_ore”}},\n{“name”: “mine”, “args”: {“object”: “diamond_ore”, “tool”: “iron_pickaxe”}\n]\nFigure 4: Illustration of our planning process with the LLM Planner and the agent in the loop.\nGiven a specific goal, the planner generates plans with structured actions under the guidance of\ninstruction, user query, previous feedback, and reference plan from memory. The agent executes the\nactions and provides feedback for the following planning.\nto retrieve the most relevant text-based knowledge from an external knowledge base. Then, the LLM\nidentifies the required material, tools, and related information from the gathered knowledge. The\ncomplete instructions for the LLM are described in Appendix.\nRecursive Decomposition. This goal format enables recursive decomposition of each goal into a sub-\ngoal tree. Specifically, given a goal, all prerequisite items are listed as sub-goals, including materials,\ntools, and their corresponding quantities. Then, the recursive decomposition continues for each\nsub-goal until it has no prerequisites. After the decomposition is completed, the execution sequence\nof the sub-goals is planned through post-order traversal. Such goal decomposition significantly\nenhances the success rate of LLM planning, especially for goals necessitating long-horizon planning.\nText-based Knowledge. External knowledge is essential for automatic goal decomposition. We\nbuild an external knowledge base documented in text from the Minecraft Wiki on the Internet 3 and\nthe item crafting\/smelting recipes, providing an exhaustive source of knowledge about the Minecraft\nworld. For instance, if we need to craft a wooden pickaxe, the item crafting recipe will indicate that\nthe required materials are three planks and two sticks, and the necessary tool is a crafting table. It also\nprovides information about the distribution of raw materials. For example, diamonds are frequently\nfound in levels 10∼12 underground.\n3.2\nLLM Planner\nLLMs excel at language understanding and reasoning but struggle with low-level control and mul-\ntimodal perception. To leverage LLMs’ strengths while addressing their limitations, we develop\nstructured actions and feedback mechanisms as an abstract interface for them to manage agent-\nenvironment interaction. We propose an LLM-based Planner to achieve goals in Minecraft. Given a\ngoal, it generates structured actions to control agents, receives feedback, and revises plans accordingly.\nIt also has a text memory that aids planning by providing solutions for frequent goals.\nStructured Actions. The structured actions are designed with well-defined functions and clear\nsemantics, enabling LLMs to make decisions at the cognitive level. A structured action can be defined\nas follows:\n(Name, Arguments, Description),\n(2)\n3https:\/\/minecraft-archive.fandom.com\/wiki\/Minecraft_Wiki\n5\nTable 1: Examples of structured actions. A structured action contains name and arguments for\nexecution, as well as description to help LLMs understand and decide when to choose this action.\nName\nArguments\nDescription\nequip\nobject\nEquip the object from the inventory: used to equip equipment, including tools, weapons, and armor.\nexplore\nobject, strategy\nMove around to find the object: used to find objects including block items and entities on the ground.\napproach\nobject\nMove close to a visible object: used to approach the object you want to attack or mine.\nmine\/attack\nobject, tool\nAttack \/ Mine the object with the tool: used to attack \/ mine the object within reach.\ndig_down\/go_up\nylevel, tool\nDig down \/ Go up with the tool: used to go down \/ up underground.\nbuild\nblueprint\nBuild according to a blueprint: used to place corresponding objects on locations according to a preset blueprint.\ncraft\/smelt\nobject, tool, material\nCraft \/ Smelt the object with the materials and tool: used to craft new object that is not in the inventory or is not enough.\napply\/place\nobject, tool\nApply \/ Place the tool on the object: used to apply tools or place blocks.\nThe name and arguments defines the action we want the agent to execute, while the action description\nprovides enough information for letting LLMs know when to choose the corresponding actions, as\nshown in Tab. 1.\nWe extract the set of structured actions by leveraging the powerful reasoning capability of LLMs.\nSpecifically, a pre-trained LLM is utilized to decompose the 3141 predefined tasks provided by\nMineDojo [5] into action sequences. Instructions for guiding LLMs on action decomposition are\nprovided in Appendix. Then, we extract the structured actions by selecting frequent actions and\nmerging actions with similar functionalities. See Appendix for the set of structured actions.\nFeedback Mechanism. Open-loop planning cannot guarantee success, especially in open-world\nenvironments, where agents might encounter unexpected events. Feedback is crucial to form an effec-\ntive closed loop. Without appropriate feedback, the LLM has no information about the consequences\nof actions and may repeat failed action plans. Feedback message is designed to present the agent’s\ncurrent state in the environment (i.e., inventory and environment), as well as the success and failure\ninformation for each executed actions, as shown in Fig. 4. By incorporating this feedback message,\nthe LLM can update its understanding of the environment, refine their strategies, and adapt their\nbehavior accordingly.\nPlanning. Once the abstract interface is prepared, a pre-trained LLM is queried to generate goal-\nspecific action sequence. This is achieved through carefully designed instructions and user queries,\nenabling the LLM to efficiently create and revise the plans. Fig. 4 illustrates the planning process.\nSee Appendix for the full description.\nInstruction specifies the guidelines that LLMs must follow when planning, including 1) Action\nInterface provides functional descriptions of the structured actions and their parameters; 2) Query\nIllustration clarifies the structure and meaning of user queries; 3) Response Format requires LLM to\nreturn responses in the format of {Explanation, Thought, Action List}, where “Explanation” requires\nLLMs to explain the reason for action failure, “Thought” requires LLM to use natural language to\nplan before outputting action sequences as a chain-of-thought (CoT) mechanism [26], and “Action\nList” outputs a list of structured actions to be executed; 4) Interaction Guideline guides LLMs to\ncorrect failed actions based on the feedback message, thus enabling the LLM to revise the plan.\nUser Query provides the specific query to LLMs for a given goal, including 1) Goal represents the\nobjective by text as “Obtain Count Item, given Material and Tool. Extra info: Info” according to\nEq. (1); 2) Feedback is the feedback information of the abstract interface; 3) Reference Plan provides\na common reference plan for the current goal retrieved from the text-base memory.\nText-based Memory is designed for LLM to maintain common reference plans for each encountered\nobjective as experiential knowledge. LLMs acquire the experience about controlling agents and\nresolving specific situations through game play and agent interaction. Instead of starting from scratch\nevery time, using prior experience allows LLMs to handle tasks more efficiently, a process similar to\nhuman skill improvement through practice.\nTo this end, we design a text-based memory mechanism for LLM to store and retrieve gained knowl-\nedge. Unlike the RL-based model, which stores knowledge in parameters, this textual knowledge is\nexplicit, logical, and closely aligned with human thought processes. This allows for direct application\nto a wide range of similar tasks, leading to more efficient learning and improved generalization.\nSpecifically, during each game episode, once the goal is achieved, the entirely executed action list\nwould be stored in memory. The LLM may achieve the same goal under various circumstances,\nresulting in a range of different plans. To identify a common reference plan suitable for general\nsituations, essential actions from multiple plans are summarized. This summarization process is\n6\nacacia_boat\nacacia_door\nacacia_fence\nacacia_fence_gate\nacacia_stairs\nbeef\nbirch_boat\nbirch_door\nbirch_fence\nbirch_fence_gate\nbirch_stairs\nboat\nbone\nbowl\nchest\nchicken\ncobblestone\ncobblestone_wall\ncooked_beef\ncooked_chicken\ncooked_mutton\ncooked_porkchop\ncrafting_table\ndark_oak_boat\ndark_oak_door\ndark_oak_fence\ndark_oak_fence_gate\ndark_oak_stairs\ndirt\ndouble_plant\nfence\nfence_gate\nfurnace\nglass\nglass_bottle\nglass_pane\nladder\nlever\nlog\nmutton\noak_stairs\npaper\nplanks\nporkchop\nred_flower\nreeds\nsand\nsandstone\nsapling\nsign\nspruce_boat\nspruce_door\nspruce_fence\nspruce_fence_gate\nspruce_stairs\nstick\nstone\nstone_axe\nstone_brick_stairs\nstone_button\nstone_hoe\nstone_pickaxe\nstone_pressure_plate\nstone_shovel\nstone_stairs\nstone_sword\nstonebrick\nsugar\ntallgrass\ntrapdoor\nwheat\nwheat_seeds\nwooden_axe\nwooden_button\nwooden_door\nwooden_hoe\nwooden_pickaxe\nwooden_pressure_plate\nwooden_shovel\nwooden_slab\nwooden_sword\nyellow_flower\nbone_meal\narmor_stand\nbook\nbread\ncoal\niron_ingot\niron_nugget\niron_ore\niron_shovel\nitem_frame\nleather\nrotten_flesh\nshield\nspider_eye\nstone_slab\ntorch\ntrapped_chest\ntripwire_hook\nfireworks\ngunpowder\nsnow\nsnow_layer\nsnowball\ncarpet\ngrass\neavy_weighted_pressure_plate\niron_hoe\niron_sword\nleather_boots\nleaves\npainting\nshears\nstring\nwool\ncoal_block\nleather_helmet\nbrown_mushroom\nbrown_mushroom_block\nflint\nbed\nbucket\nhay_block\niron_axe\niron_pickaxe\nmilk_bucket\nsandstone_stairs\nwater_bucket\nfermented_spider_eye\nleather_leggings\ngravel\nflint_and_steel\nfishing_rod\niron_boots\niron_trapdoor\nleather_chestplate\nmossy_cobblestone\nvine\nwaterlily\nbone_block\nbow\nchest_minecart\nfurnace_minecart\nhopper\niron_helmet\nminecart\ndiamond\ndiamond_shovel\ndropper\njukebox\nnoteblock\nredstone\nredstone_torch\nbanner\nfeather\niron_bars\niron_door\nrail\nbrick\nclay_ball\nlapis_lazuli\npiston\nemerald\ncauldron\niron_leggings\ntnt\ndiamond_hoe\ndiamond_sword\nflower_pot\niron_chestplate\narrow\ncompass\nhopper_minecart\ngold_ingot\ngold_nugget\ngold_ore\ngolden_shovel\niron_block\nbrick_block\nclay\nhardened_clay\nslime_ball\ndispenser\ndiamond_axe\ndiamond_pickaxe\nlava_bucket\nactivator_rail\ndetector_rail\negg\nrepeater\ntnt_minecart\nbookshelf\ngolden_hoe\ngolden_sword\night_weighted_pressure_plate\nredstone_block\nred_mushroom\nred_mushroom_block\nbeetroot\nbeetroot_seeds\ndiamond_boots\ndiamond_helmet\ngolden_axe\ngolden_pickaxe\nsticky_piston\nink_sac\ndiamond_leggings\ngolden_boots\nmushroom_stew\nmap\nbeetroot_soup\nlead\ngolden_helmet\nrabbit_hide\ncooked_rabbit\nrabbit\nbrick_stairs\ncake\nobsidian\ncactus\ndiamond_chestplate\nclock\ndeadbush\nwritable_book\nlapis_block\ngolden_leggings\ngolden_rail\nbaked_potato\npotato\ndiamond_block\ngolden_chestplate\nemerald_block\ncarrot\npumpkin\npumpkin_seeds\njungle_boat\njungle_door\njungle_fence\njungle_fence_gate\njungle_stairs\nlit_pumpkin\ncarrot_on_a_stick\nmelon\nmelon_block\nmelon_seeds\ngolden_carrot\ngold_block\npumpkin_pie\nred_sandstone\nstone_slab2\nred_sandstone_stairs\nspeckled_melon\nenchanting_table\napple\nanvil\nenchanted_book\npoisonous_potato\nrabbit_foot\nslime\ngolden_apple\nrabbit_stew\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nOurs\nDEPS\nDreamerV3\nVPT\nFigure 5: Success rate for all items in the entire Minecraft Overworld Technology Tree. The x\naxis lists all item names. We overlay the results from our GITM and the best results from baselines.\nalso implemented using LLMs (see Appendix for details). When encountering similar goals, the\nLLM creates new plans based on the summarized reference plans retrieved from memory. Successful\naction sequences from these new plans are also added to memory for future summarization. As the\nLLM-based Planner accumulates summaries, it becomes increasingly effective.\n3.3\nLLM Interface\nUnlike the existing RL-based agents that directly control keyboard and mouse, LLM-based agents\ninteract with the environment through structured actions and feedback messages. The LLM interface\nserves to implement structured actions as keyboard\/mouse operations, and extract observations\nprovided by the environment into feedback messages.\nStructured actions can be implemented in various ways such as hand-written scripts or RL-learned\nmodels. While RL-learned models have been employed in Minecraft previously, they were either\nbroad in functionality but inefficient in practice, or too specific in functionality, limiting their\napplicability to general tasks and actions. Clarifying the capability boundary of RL-learned models is\nchallenging. Instead, in this work, we choose to implement structured actions using hand-written\nscripts. Since structured actions are well-defined and easy to implement, we can manually implement\nthem based on observations (e.g., location, LiDAR, and voxel) and basic operations (e.g., move,\njump, adjust camera angle, click left mouse button, and click right mouse button) provided by the\nMineDojo [5] environment. See Appendix for details.\nFeedback messages can be obtained directly from the environment. These include whether the\nstructured action execution succeeded or failed. If the execution fails, the reason for the failure is\nadditionally notified. It also includes the current state of the agent in the environment, including the\nitems in the inventory, the current biome and depth, etc. See Appendix for details.\n4\nExperiments\nTask Definition and Metrics. We measure the ability of GITM through item collection tasks. We\nonly collect items could be found in the Overworld. We exclude items could only be obtained by\ntrading with villagers, opening treasure chest or find a special structure on the map, using a tool\nenchanted with Silk Touch. This give us a total of 262 tasks. For the assessment of our agent, we\nemploy “Coverage of the Overworld Technology Tree” and“Success Rate” as evaluation metrics.\n7\n4.1\nMain Result\nUnlocking the Entire Technology Tree by Obtaining All Items. Compared with existing Minecraft\nagents [2, 7, 25] which mostly focuses on solving the ObtainDiamond task and could only unlock\na limited part of the full technology tree (13\/262 for Dreamerv3, 15\/262 VPT, 69\/262 for DEPS),\nour approach could collect all 262 items as shown in Fig. 1. There are two major blockers for\nexisting methods. For RL-based methods like VPT [2] and DreamerV3 [7], the goal item(diamond)\nis hard-coded into the model weights, which means there are no easy way to re-task the trained\nRL agents for collecting other items in the inference stage. Moreover, the low training efficiency\nhinders them from solving extremely long-horizon tasks (e.g., obtaining a “enchanted_book”). For\nmethods like DEPS [25] that use an RL controller [3] and LLM planner still rely on pre-trained RL\nagents to execute specific subtasks (e.g. mining 1 “cobblestone”) in the generated plan. So these\napproaches still suffer from the inability of RL-based methods alone to generalize to unseen tasks\n(e.g. obtaining “lapis_lazuli”). In contrast, we extract a well-defined set of structured actions by\nusing LLMs to decompose over 3000 predefined MineDojo tasks. This provides broad, open-world\nMinecraft capability. Combined with LLM planning, it enables solving more complex tasks than\nObtainDiamond - which RL cannot achieve. Our knowledge bases also improve efficiency. To our\nknowledge, we present the first agent to unlock the entire Overworld technology tree - a level of\nopen-world skill RL-based methods have not demonstrated.\nSuccess Rate for the Entire Technology Tree. We show the success rate of our method for collecting\nall Overworld items in Fig. 5. Our methods could achieve 100% success rate for simple tasks like\ncollecting wooden tools. It achieves non-zero success rates for all items which indicates a strong\ncollecting capability. The successful rate for collecting different items change smoothly for our agent,\nwhich showcase the robustness of our method against the highly uncertain open world environment.\n4.2\nComparison with Other Minecraft Agents\nTable 2: Comparison of our GITM with pre-\nvious methods on ObtainDiamond challenge.\nMethod\nSuccess Rate (%)\nDreamerV3\n-\n50.0\n3.0\n0.01\n0.01\nDEPS\n90.0\n80.0\n73.3\n10.0\n0.6\nVPT\n100.0 100.0 100.0\n85.0\n20.0\nOur GITM\n100.0 100.0 100.0\n95.0\n67.5\n101\n103\n105\n107\n109\nStep\n0\n10\n20\n30\n40\n50\n60\n70\nSuccess Rate\nVPT\nDreamV3\nOurs\nFigure 6: Comparison of learning efficiency.\nWe compared our LLM-based method with three existing agents: VPT [2], DreamerV3 [7], and\nDEPS [25] on the well known ObtainDiamond challenge, i.e, obtaining a diamond from scratch in\nMinecraft. Previous methods set different time limits of a single episode of game play (20 minutes\nfor VPT, 30 minutes for Dreamerv3, and 10 minutes for DEPS). For fair comparison, we use the\nstrictest limit of previous methods: 10 minutes (12,000 steps at 20Hz control).\nSuccess Rate for Obtaining Diamond and Other Items.\nSince VPT and Dreamerv3 are not\ntargeted for collecting items other than diamond, we mainly compare our method with DEPS for\nitems not related to obtain diamonds. Overall, our GITM and VPT rank task difficulty similarly, but\nDEPS rankings severely fluctuate for tasks more complex than mining coal. Dreamerv3 also behaves\noddly by having an abnormally low success rate on tasks like obtaining a stone sword. As shown in\nFig. 2, most agents performs generally well for easy tasks relating to make wooden tools. VPT could\neven rival with our GITM for the success rate of obtaining iron axes. But for obtaining diamonds, our\nmethod wins over any other methods by 3.5 times on the succeess rate.\nThis giant improvement comes from the following two aspects: First, we employ the strong long-term\nplanning capability of LLMs to decompose the complex tasks into feasible sub-goals and tackle\nthem step by step. Second, our model can directly leverage external knowledge such as the suitable\nlocations for mining ores, while RL models need to explore themselves and may not acquire reliable\nknowledge.\n8\nTable 3: Ablation study. The milestone items from left to right are crafting table\n, wooden\npickaxe\n, stone pickaxe\n, iron pickaxe\n, and diamond\n. The success rate is calculated\nunder time limit of 12000 steps (total) and query limit of 30 (each sub-goal). “Goal Decomp.” and\n“External Info.” indicates goal decomposition and external knowledge respectively.\nGoal\nDecomp.\nFeedback\nExternal\nInfo.\nMemory\nSuccess Rate (%)\n57.5\n32.5\n5.0\n0.0\n0.0\n✓\n90.0\n90.0\n67.5\n2.5\n0.0\n✓\n✓\n97.5\n95.0\n77.5\n20.0\n5.0\n✓\n✓\n✓\n100.0\n100.0\n100.0\n57.5\n35.0\n✓\n✓\n✓\n✓\n100.0\n100.0\n100.0\n95.0\n67.5\nLearning Efficiency.\nBesides measuring the success rate of each agents, we also compare the\nlearning efficiency of our model with other learnable models. Since DEPS uses a LLM-based planner\nwithout learning mechanism and a pre-trained RL-based controller, its performance could not improve\nwith more episodes played and is excluded from the comparison here.\nIt usually takes tens of millions of steps to train an RL agent by updating parameters before its success\nrate starts to converges to meaningful non-zero numbers. However, the success rate for RL-based\nagents increases rather slowly even after them starts to converge. On the contrary, the learning process\nof our LLM-based agent is considerably faster. As shown in Fig. 6, our method requires several orders\nless episodes than any other methods before doubling its initial success rate. Moreover, our method is\nextremely sample efficient as our success rate raises from 35% to 47.5% by learning from the first five\nthousand steps. By just playing each task several times and summarize successful experience into the\nmemory, the LLM-based agent can acquire explicit experiential knowledge and achieve significantly\nhigher success rate.\n4.3\nAblation Study\nWe conduct ablation experiments on the ObtainDiamond task. We set a time limit of 10 minutes of\ngame play (12000 steps at the control frequency of 20Hz). When leveraging goal decomposition, for\neach sub-goal, we set the maximum number of queries to LLM as 30, and exceeding the query limit\nwill be judged as a failure. For each setting, we run 40 games and calculate the success rate. Tab. 3\nrecords the success rates of achieving the final goal diamond as well as the milestones in this goal,\nincluding crafting table, wooden pickaxe, stone pickaxe, and iron pickaxe.\nGoal Decomposition. Without goal decomposition, the planner can only accomplish several short-\nterm tasks such as obtaining stone axes with rather low success rate of 5%, which indicates the\nnecessity of goal decomposition. Leveraging the powerful long-term planning capabilities of LLMs,\nthe goals are decomposed into sub-goals feasible and practical for the planner, so the success rate for\nobtaining stone axes advances from 5% to 67.5% by leveraging goal decomposition alone.\nFeedback Message. Feedback contains the agent’s state and the execution result of the actions, which\nhelps the planner to understand and make another attempt to correct the mistakes in the previous and\ndeal with special cases. This enables the planner to accomplish a broader range of goals with higher\nsuccess rate. As shown in the 3rd row of Tab. 3, our agent gain the ability to collect diamond by\ncombining feedback with goal decomposition.\nExternal Knowledge Base. External knowledge contains general rules, crafting recipes, and common\ntricks in Minecraft, such as the recipes for crafting iron ingot and iron pickaxe, the suitable location to\nfind diamond ore, and the efficient way to get cobblestone. Providing the planner with this information\ngreatly boosts the success rate of obtaining iron pickaxe and diamond, and the success rate of mining\ndiamond increase by 7 times by learning from the knowledge base that diamonds are more likely to\nappear in specific levels.\nText-based Memory. Leveraging the reference plan recorded in the memory, the planner can handle\nthe task it has encountered more efficiently. The success rates of obtaining iron pickaxe and diamond\nare 95.0% and 67.5%, surpassing the model without memory by 37.5% and 32.5%, respectively.\n9\n5\nConclusion\nWe introduce the GITM framework, which utilizes Large Language Models (LLMs) for hierarchical\ndecomposition of goals. GITM introduces LLM Decomposer, LLM Planner and LLM Interface\nto gradually decompose goals into sub-goals, structured actions and keyboard\/mouse operations.\nThis work makes significant progress towards the ObtainDiamond goal, outperforming all previous\nmethods by a significant margin (+47.5% success rate). This proves the potential inefficiency and\npoorly scalability of Reinforcement Learning (RL) in Minecraft, breaking the traditional reliance\non RL. Moreover, by obtaining all items in Minecraft Overworld, this research marks a critical step\ntoward Generally Capable Agents (GCAs) that match human performance in Minecraft.\nAcknowledgments\nThe work is partially supported by the National Natural Science Foundation of\nChina under grants No.U19B2044, No.61836011, No.62022048, and No.62276150. This work is also\npartially supported by the National Key R&D Program of China under grants NO.2022ZD0114900,\nand the Guoqiang Institute of Tsinghua University.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Minecraft中的幽灵：通过大型语言模型和基于文本的知识与记忆，创建开放世界环境中的通用能力智能体\n\n## 📌 背景痛点\/本文动机\nMinecraft作为一款开放世界游戏，吸引了大量研究兴趣，成为开发能够在开放世界中运行的智能体的丰富平台。然而，目前的研究主要集中在特定目标上，如流行的“ObtainDiamond”任务，尚未在更广泛的任务上展现出有效的泛化能力。此外，现有方法在“ObtainDiamond”任务上的最高成功率仅为约20%，突显了现有基于强化学习（RL）的控制器方法的局限性。为了解决这些挑战，本文提出了Ghost in the Minecraft（GITM）框架，该框架将大型语言模型（LLMs）与基于文本的知识和记忆相结合，旨在创建能够在Minecraft中运行的通用能力智能体（GCAs）。\n\n## 🚀 核心方法\n💡 创新点1：LLM分解器\nLLM分解器负责将任务目标分解为一系列更易于实现的子目标。通过解决每个子目标，可以逐步实现任务目标。LLM分解器利用从互联网收集的文本知识，将目标分解为子目标树。\n\n💡 创新点2：LLM规划器\nLLM规划器负责为每个子目标生成一系列结构化操作。结构化操作具有明确的语义和相应的反馈，使LLMs能够在认知层面理解周围环境并做出决策。LLM规划器还记录和总结成功的操作列表，以增强未来的规划。\n\n💡 创新点3：LLM接口\nLLM接口负责将结构化操作转换为键盘\/鼠标操作，并与环境进行交互。它还从环境中提取观察结果，并将其转换为反馈消息。\n\n## 📈 实验结果\n本文的实验结果表明，基于LLM的智能体在“ObtainDiamond”任务上的成功率显著提高，达到了47.5%，超过了现有的方法。此外，该智能体是第一个在Minecraft Overworld中获取所有物品的智能体，展示了其广泛的技能。\n\n## 💬 可借鉴之处\n本文提出的GITM框架为开发能够在开放世界中运行的通用能力智能体提供了一种新的思路。通过利用LLMs的常识和推理能力，以及基于文本的知识和记忆，该框架能够使智能体有效地处理开放世界环境中的各种挑战。此外，该框架还具有高效的学习效率和良好的泛化能力，使其在开发通用能力智能体方面具有巨大的潜力。","llm_summary_res_status":200}
{"title":"Creative Agents: Empowering Agents with Imagination for Creative Tasks","authors":"Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, Zongqing Lu","summary":"We study building embodied agents for open-ended creative tasks. While\nexisting methods build instruction-following agents that can perform diverse\nopen-ended tasks, none of them demonstrates creativity -- the ability to give\nnovel and diverse task solutions implicit in the language instructions. This\nlimitation comes from their inability to convert abstract language instructions\ninto concrete task goals in the environment and perform long-horizon planning\nfor such complicated goals. Given the observation that humans perform creative\ntasks with the help of imagination, we propose a class of solutions for\ncreative agents, where the controller is enhanced with an imaginator that\ngenerates detailed imaginations of task outcomes conditioned on language\ninstructions. We introduce several approaches to implementing the components of\ncreative agents. We implement the imaginator with either a large language model\nfor textual imagination or a diffusion model for visual imagination. The\ncontroller can either be a behavior-cloning policy learned from data or a\npre-trained foundation model generating executable codes in the environment. We\nbenchmark creative tasks with the challenging open-world game Minecraft, where\nthe agents are asked to create diverse buildings given free-form language\ninstructions. In addition, we propose novel evaluation metrics for open-ended\ncreative tasks utilizing GPT-4V, which holds many advantages over existing\nmetrics. We perform a detailed experimental analysis of creative agents,\nshowing that creative agents are the first AI agents accomplishing diverse\nbuilding creation in the survival mode of Minecraft. Our benchmark and models\nare open-source for future research on creative agents\n(https:\/\/github.com\/PKU-RL\/Creative-Agents).","url":"http:\/\/arxiv.org\/abs\/2312.02519v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2312.02519v1","published":1701756052000,"comment":"The first two authors contribute equally","pdf_text":"Creative Agents: Empowering Agents with Imagination for Creative Tasks\nChi Zhang1*\nPenglin Cai1∗\nYuhui Fu2\nHaoqi Yuan1\nZongqing Lu1,3†\n1Peking University\n2Tsinghua University\n3BAAI\nAbstract\nWe study building embodied agents for open-ended cre-\native tasks.\nWhile existing methods build instruction-\nfollowing agents that can perform diverse open-ended\ntasks, none of them demonstrates creativity – the ability\nto give novel and diverse task solutions implicit in the lan-\nguage instructions. This limitation comes from their inabil-\nity to convert abstract language instructions into concrete\ntask goals in the environment and perform long-horizon\nplanning for such complicated goals. Given the observa-\ntion that humans perform creative tasks with the help of\nimagination, we propose a class of solutions for creative\nagents, where the controller is enhanced with an imagina-\ntor that generates detailed imaginations of task outcomes\nconditioned on language instructions. We introduce sev-\neral approaches to implementing the components of cre-\native agents. We implement the imaginator with either a\nlarge language model for textual imagination or a diffu-\nsion model for visual imagination. The controller can ei-\nther be a behavior-cloning policy learned from data or a\npre-trained foundation model generating executable codes\nin the environment. We benchmark creative tasks with the\nchallenging open-world game Minecraft, where the agents\nare asked to create diverse buildings given free-form lan-\nguage instructions. In addition, we propose novel evalu-\nation metrics for open-ended creative tasks utilizing GPT-\n4V, which holds many advantages over existing metrics. We\nperform a detailed experimental analysis of creative agents,\nshowing that creative agents are the first AI agents accom-\nplishing diverse building creation in the survival mode of\nMinecraft. Our benchmark and models are open-source for\nfuture research on creative agents (https:\/\/github.com\/PKU-\nRL\/Creative-Agents).\n1. Introduction\nBuilding open-ended embodied agents has been a long-\nstanding goal of AI research.\nUnlike many existing AI\nagents that perform a fixed set of tasks specified with re-\n*Equal contribution\n†Correspondence to B: zongqing.lu@pku.edu.cn\nwards [1, 19], open-ended agents can perform diverse arbi-\ntrary tasks without such specification. Existing research pri-\nmarily focuses on learning instruction-following agents [4,\n11, 29] that can solve open-ended tasks given free-form\nlanguage instructions, achieving success in robotic do-\nmains [4, 11, 24] and open-world games [6, 29, 45]. How-\never, these agents can only follow clear instructions that rep-\nresent specific goals or behaviors. Creative tasks, where the\ninstructions describe abstract tasks and the agent is required\nto generate complicated, novel, and diverse solutions, bring\nnew challenges to intelligent agents.\nAs an example, in the open-world game Minecraft, ex-\nisting agents can follow simple and clear instructions like\n‘harvest a stone’ [52] and ‘build a snow golem, which stacks\n2 snow blocks and 1 pumpkin’ [7], but they cannot solve\ncreative tasks like ‘build a sandstone palace’. For the latter,\nthe agent can struggle to understand the target outcome of\nthe task implied in the abstract instruction and plan actions\nfor the long-horizon execution where hundreds of blocks\nshould be properly placed. However, empowered with the\nability of imagination, humans can first imagine the appear-\nance and functionality of the building, then plan for a proper\norder to build blocks and realize the imagined house in the\ngame. Such ability enhances humans with strong creativ-\nity, enabling humans to create novel and diverse outcomes.\nImagination also enriches the fuzzy instructions into refined\ntask outcomes grounded in the environment, making the\ntask description more explicit and executable.\nMotivated by this ability, we introduce a framework for\ncreative agents, empowering open-ended agents with imagi-\nnation to solve creative tasks. Figure 1 gives an overview of\nthe framework. Equipped with a text-conditioned imagina-\ntor, creative agents can imagine the details of the task out-\ncome abstracted in the language instruction. These imag-\ninations serve as a blueprint for the controller to interpret\nand act upon. We propose two variants of the imaginator,\nincluding a large language model (LLM) [5] generating text\nimaginations and a finetuned diffusion model [38] generat-\ning grounded visual imaginations. We also introduce two\nvariants of the controller that transform the imagination into\nexecutable plans. The first is a behavior-cloning controller\ntrained on an environment dataset and maps imaginations\n1\narXiv:2312.02519v1  [cs.AI]  5 Dec 2023\n“build a \nsandstone \npalace”\nGPT-4(V)\nCode Gen.\nBC\nController\nImaginator\nController\nCreative Agents\nCreative Task\nCreation\n“1. Size: 10x10x12\n2. Made of sandstone\n3. It has an entrance”\n𝐼𝑔𝑙\n𝜋𝑎𝑠, 𝑔, 𝑙\n①\n②\n③\nInstruction (𝑙)\nImagination (𝑔)\nLLM CoT\nDiffusion\nModel\nFigure 1. Overview of creative agents for open-ended creative tasks. A creative agent consists of two components: an imaginator and a\ncontroller. Given a free-form language instruction describing the creative task, the imaginator first generates the imagination in the form of\ntext\/image by LLM with Chain-of-Thought (CoT)\/diffusion model, then the controller fulfills the imagination by executing actions in the\nenvironment, leveraging the code generation capability of vision-language model (VLM) or a behavior-cloning (BC) policy learned from\ndata. We implement three combinations of the imaginator and controller: ①CoT+GPT-4, ②Diffusion+GPT-4V, and ③Diffusion+BC.\nto actions. The second method leverages the strong abili-\nties in vision-language understanding [51] and code gener-\nation [44] of the large vision-language model (VLM) GPT-\n4V [34]. The VLM controller receives the imagination as\nthe task goal and generates code to perform actions in the\nenvironment.\nDesigning evaluation metrics for open-ended tasks re-\nmains underexplored.\nExisting methods either use some\nsurrogate metrics [44] which may not reflect the language\ninstruction, or use human evaluation [7] which is labori-\nous. Fan et al. [13] proposes to use the similarity of the\nCLIP [37] embedding between vision and language, which\nhowever can only provide some unknown correlation be-\ntween the instruction and task outcome. To address these\nlimitations, we propose novel evaluation metrics based on\nGPT-4V. Leveraging the analytical strength of GPT-4V, our\nmetrics offer an effective, general, and human-independent\nmeans of evaluation. We verify that such metrics are con-\nsistent with human evaluations. Our proposed metrics are\ncrucial for objectively measuring the creativity and effec-\ntiveness of solutions generated by open-ended agents.\nWe benchmark creative tasks with challenging building\ncreation in Minecraft1, following 20 diverse instructions.\nSeveral variants of creative agents demonstrate their abil-\nity to create diverse and visually appealing buildings in the\nsurvival mode of Minecraft, which has never been achieved\nin previous studies. We give a detailed experimental analy-\n1We select the open-world game Minecraft as the benchmark platform\nbecause it is convenient to build various imaginators and controllers and\nalso supports creation in the game. Specifically, we choose the survival\nmode of Minecraft, where it is difficult for the agent to construct buildings\nsince the agent has to move around and go up\/down to place the blocks\nwith diverse materials and colors, making the building process realistic. It\nis worth noting that our framework for creative agents is general and can\nalso be applied to other environments.\nsis of creative agents, discuss the strengths and weaknesses\nof each variant, and provide insights for improving creative\nagents in future work.\nOur main contributions are threefold:\n• We propose creative agents, the first framework that en-\ndows open-ended agents with the ability to perform cre-\native tasks through imagination. Our method builds the\nfirst instruction-following agent that can create diverse\nbuildings in the survival mode of Minecraft.\n• We establish novel evaluation metrics for creative tasks\nin open-ended environments, in which GPT-4V is used as\nthe evaluator.\n• By open-sourcing the datasets and models, our work sets\na new benchmark for future research in the field of open-\nended learning and creative AI agents.\n2. Preliminaries\n2.1. Open-Ended Tasks\nWe formalize the process of the agent interacting with the\nenvironment as a Markov Decision Process (MDP) without\nreward, defined by a tuple M = (S, A, P, ρ) representing\nstates, actions, the transition function of the environment,\nand the initial state distribution, respectively. Starting from\nthe initial state, for each time step, the agent performs an\naction based on the state, then the environment transitions\nto the next state upon the action.\nCompared with traditional reinforcement learning tasks\ndefined with reward functions, open-ended tasks have nei-\nther fixed targets nor optimal solutions. We follow Fan et al.\n[13], formulating open-ended tasks as instruction-following\nproblems T = (L, M), where l ∈L is a free-form language\ninstruction.\nWe aim to acquire an instruction-following\nagent P(a|s, l) which can exhibit behaviors consistent with\n2\nthe instruction to perform the described task.\n2.2. Creative Agents with Imagination\nDue to the abstract nature of language, language instruc-\ntions cannot describe the full details of complicated tasks,\ndrawing high uncertainty on the task completion and requir-\ning the agent to possess creativity.\nThough many open-\nended agents [6, 11, 29] can follow clear instructions that\nrefer to some specific task goals, none of them can follow\nsuch uncertain instructions to perform complicated tasks.\nWe define creative tasks as a challenging case of open-\nended tasks, where language instructions lack information\nto describe the whole task and can refer to diverse, novel,\nand complicated outcomes in the environment. Such in-\nstructions bring uncertainty for the agent and require the\nability to imagine the details unspecified by the instruction.\nIn addition, a short instruction (e.g. ‘build a house’) may re-\nfer to a long-horizon complicated task, increasing the chal-\nlenge for the action planning and execution.\nTo tackle the challenge, we propose to decompose the\nagent into an imaginator and a controller:\nP(a|s, l) =\nX\ng\nI(g|l)π(a|s, g, l).\n(1)\nHere, g ∈G is an imagination of the task outcome, which\ncan be in the form of diverse modalities (e.g. text, image)\nand serves as a description of the target environment state\nof the task. The imaginator I converts the instruction into\nan imagined outcome, providing the controller π with a de-\ntailed task description. Therefore, we leave the uncertainty\nand creativity brought from creative tasks to the imaginator,\nproviding the controller with richer task information to re-\nduce its uncertainty. By disentangling these two models, we\ncan delve deeper into the design choices for each part and\ncombine them together to build creative agents.\n3. Generative Imagination\nGenerative models in natural language processing and com-\nputer vision provide techniques to build the imaginator in\neither text space or image space. In this section, we present\ntwo variants for implementing the imaginator.\n3.1. Language Models for Textual Imagination\nLarge language models (LLMs) have shown marvelous\nabilities in solving diverse tasks [9, 46] as well as high plas-\nticity with prompt engineering [5, 48]. To tackle the prob-\nlems in reasoning logically, Wei et al. [47] proposed Chain-\nof-Thought (CoT), aimed at enhancing the emergence abil-\nity of LLMs.\nFollowing the idea of zero-shot-CoT [28], we design\nan imaginator using GPT-4 [34] as the backbone, with zero-\nshot prompts for imagination in Minecraft building-creation\ndomain (please refer to Appendix B). Specifically, we pro-\nvide the initial text instruction to GPT-4 and ask five ques-\ntions relevant to the imagination, including the material\nused for the building, the approximate size, the significant\nfeatures of the architecture, etc. After GPT-4 generates an-\nswers to these questions indicating that the imagination pro-\ncess has been finished, we then ask the controller to exe-\ncute actions accordingly to construct the building (see Sec-\ntion 4).\n3.2. Diffusion Models for Visual Imagination\nDiffusion models have achieved breakthrough performance\nin generating diverse and high-quality images. Stable Dif-\nfusion [38] models data distribution as the stationary state\nof a diffusion process, learning to generate samples mir-\nroring the true data distribution by reversing this process.\nNoteworthy for its training stability, it addresses issues like\nmode collapse.\nTo better align with the human conception of “imagi-\nnation”, we use images to be the imagination space and\nleverage text-conditioned diffusion models to be the imag-\ninator. We finetune the Stable Diffusion [38] using a text-\nimage dataset to achieve a reasonable and diverse imagina-\ntion of textual input. The text-image pairs in the dataset\nare constructed by automatically annotating the Minecraft\nbuildings in CraftAssist [16] using the multimodal Emu\nmodel [42]. After finetuning, we obtain visually plausible\nand diverse imaginations that align with both the textual de-\nscriptions and the Minecraft world.\n4. Designing Controllers\nAfter the imaginator generates the imagination g, it is the\ncontroller to take actions in the environment, conditioned\non the current state, the imagination, and the language in-\nstruction. In the following, two variants of the controller\nare presented, including a behavior-cloning controller and a\ncontroller based on GPT-4(V).\n4.1. Behavior-Cloning Controller\nTo transform the imagination into a practical construction\nprocess, we introduce a behavior-cloning controller that\nfirst converts the image imagination into a blueprint and\nthen maps the blueprint into tangible actions.\nFor tasks related to constructing buildings in Minecraft,\nwe use voxel information as the basis for blueprints. To\nlearn a module generating voxel blueprints conditioned\non images, we adopt the methodology introduced by\nPix2Vox++ [50], utilizing the image-voxel dataset con-\nstructed through data augmentation from original construc-\ntions in CraftAssist [16] and ISM [14].\nThe module is\ntrained to optimize a combination of the voxel predic-\ntion loss and two regularization terms, including the occu-\npancy rate loss [36] and the total variation loss [39, 49].\n3\nSubsequently, for the construction process, we employ\nResNet3D-CNN[20] and train a behavior-cloning (BC) pol-\nicy on a collected voxel-action dataset. After that, the fi-\nnal construction is executed by the BC policy conditioned\non the voxel information through path-searching and block-\nplacing within the MineDojo simulator [13]. More details\nabout our methods and datasets are available in Appendix B.\n4.2. Vision-Language Models as Controller\nWe also adopt a generative vision-language model (VLM)\nto construct the controller, which can perceive both visual\nimaginations and textual imaginations. Utilizing its abili-\nties in task reasoning and code generation, given an envi-\nronment code interface that wraps actions, the VLM can\ngenerate executable code in the environment for task com-\npletion.\nSpecifically, we use GPT-4(V) which takes as input an\nimage generated by the diffusion imaginator or the textual\nimagination generated by the LLM with CoT. We ask GPT-\n4(V) to generate code that can call Mineflayer [35] APIs to\nexecute environment actions for building creation. Mine-\nflayer implements JavaScript APIs for diverse skill primi-\ntives in the Minecraft world. Following the prompt design\nin Voyager [44], we provide GPT-4(V) with API documen-\ntation to clarify the coding rules and a one-shot example of\ncode generation for in-context learning. More details about\nthe prompts are available in Appendix B.\nWith this controller, we implement creative agents in\nboth two modalities of imaginations.\n5. Experiments\n5.1. Building Creation in Minecraft\nInspired by the creative tasks in MineDojo [13], we set\nup an evaluation benchmark for constructing buildings in\nMinecraft, consisting of 20 diverse language instructions,\nsuch as “a huge Minecraft volcano built of ice” as illustrated\nin Figure 4. Following the text description, the agent takes\nactions to move and place blocks in the game simulator to\ncreate buildings. In the experiment, we aim to investigate\nwhether the agent can construct novel, diverse buildings by\njust following language instructions, which reflects the cre-\nativity of the agent. In the evaluation, we take screenshots\nof its creations in the game. More details can be found in\nAppendix A. We setup various metrics to evaluate the open-\nended building creation tasks and apply two evaluators, in-\ncluding human evaluators and a novel evaluator based on\nGPT-4V. Section 5.3 presents the evaluation details.\n5.2. Implementation\nWe implement several variants of creative agents using dif-\nferent combinations of imaginators and controllers (more\nCorrectness\nComplexity\nQuality\nFunctionality\nRobustness\n6.95\n4.90\n6.35\n4.15\n4.07\nVanilla GPT-4\nCoT+GPT-4\nDiffusion+GPT-4V\nDiffusion+BC\nFigure 2.\nComparison of all variants of creative agents in\nMinecraft building creation. For each evaluation metric, the num-\nber denotes the average score of the best agent over the 20 tasks.\nDiffusion+GPT-4V performs relatively better than other variants.\ndetails are provided in Appendix B) and build a baseline\nmethod to compare with:\n• Vanilla GPT-4.\nThis is the baseline method without\nimagination using GPT-4 as the controller. We simply\nreplace the textual imagination with the original task in-\nstruction and ask GPT-4 to perform code generation.\n• CoT+GPT-4. We implement this agent by adding a CoT-\nimagination on the basis of Vanilla GPT-4, which means\nwe use GPT-4 for both textual imagination and code gen-\neration (method ①in Figure 1).\n• Diffusion+GPT-4V2. We use a finetuned Stable Diffu-\nsion to generate images as imagination and use GPT-4V\nas the controller to generate codes based on the visual\nimagination (method ②in Figure 1).\n• Diffusion+BC. The finetuned Stable Diffusion is used as\nthe imaginator while the behavior-cloning controller is\nused to convert images into voxel blueprints and execute\nactions.(method ③in Figure 1).\n5.3. Evaluation Metrics\nBased on existing evaluation methods in open-ended learn-\ning [30] and content generation in Minecraft [40], we intro-\nduce a set of evaluation aspects, which are important criteria\nfor creative agents:\n• Correctness. Are the creations consistent with the lan-\nguage instruction?\n• Complexity.\nCan the agent create large and complex\nbuildings?\n2We use GPT-4V here to indicate the agent additionally takes an image\nimagination as input.\n4\nDiffusion+GPT-4V CoT+GPT-4\nDiffusion+BC\nVanilla GPT-4\n1000\n1100\n1200\n1300\n1400\n1500\n1600\nElo Rating\n1531.0\n1441.0\n1395.0\n1233.0\nElo Ratings of Different Agents\nVanilla GPT-4\nCoT+GPT-4\nDiffusion+GPT-4V\nDiffusion+BC\n16\n17\n18\n19\n20\n21\n22\n23\n24\nAverage Score\n18.1\n21.2\n22.4\n19.8\n19.1\n21.1\n23.2\n17.8\nAverage Score of Different Agents\nGPT-4V\nHumans\nFigure 3. Evaluation results in Minecraft building creation. Left: The Elo Rating of all agents based on the evaluation of GPT-4V. Right:\nThe average overall score of each agent in all test tasks evaluated by GPT-4V and humans.\n• Quality. Do the creations have a good visual appearance\nfrom the perspective of aesthetics?\n• Functionality. Do the created buildings have the neces-\nsary functions and structures (such as windows and en-\ntrances)?\nTo quantitatively evaluate such metrics, recent work [7]\nrequires humans to perform evaluation, which however is\nlabor-intensive and may be susceptible to subject prefer-\nences. To tackle these issues, we leverage the strong ca-\npabilities of the recent VLMs in vision-language reasoning\nand vision question answering and propose two VLM-based\nevaluation methods.\nIn the first method, given a language instruction, we sam-\nple a pair of creation results from two methods and fill in a\ntemplate to ask the VLM which one is better overall based\non all evaluation metrics:\nYou are a critic with high aesthetic ability. I\nwill provide you a text instruction and two\nbuildings created by different agents\nfollowing this instruction in the game \"\nMinecraft\".\nPlease evaluate their overall performance\naccording to four aspects: $(\nEVALUATION_ASPECT 1˜4). Tell me which\nbuilding in the image is better (left or\nright).\nText: $(INSTRUCTION)\nImage of buildings: $(IMAGE1, IMAGE2)\nWe use the Elo Rating System [12] to measure the relative\nstrength of each agent.\nIn the second method, we fill in a template with the four\nevaluation metrics above to ask the VLM to directly score\nfor each building:\nYou are a critic with high aesthetic ability. I\nwill provide you a text instruction and a\ncreated building following this instruction\nin the game \"Minecraft\".\nAccording to four aspects: $(EVALUATION_ASPECT\n1˜4),\nplease evaluate the building with a score (out of\n10) on each aspect respectively, then give a\ntotal score.\nText: $(INSTRUCTION)\nImage of the building: $IMAGE\nTo verify the reliability of VLMs in evaluation, we also ask\nhumans to participate in both two evaluation methods and\ncompare the difference between VLM and human evalua-\ntions.\n5.4. Results and Analysis\nIn the first evaluation method, every two agents are com-\npared by GPT-4V for each test task. With all these com-\nparison results, we model the four agents into a four-player\nzero-sum game and apply the Elo Rating System to com-\npute the ratings, as shown in Figure 3 (left).\nAccording to the second evaluation method, we record\nratings of each single test over the four metrics (correctness,\ncomplexity, quality, and functionality), the sum of which is\nthe overall score. We calculate the standard deviation of the\noverall scores of all test tasks for each agent, which repre-\nsents Robustness, with a larger standard deviation standing\nfor weaker robustness. To align with the other four metrics\nfor better presentation, we properly transform the standard\ndeviation into a value such that a higher value indicates bet-\nter robustness. Note that better robustness does not neces-\nsarily mean better performance since it merely indicates the\nconsistency of the performance on all test tasks. The results\nare plotted as a polar chart shown in Figure 2. Furthermore,\nthe overall score averaged over all test tasks for each agent\nis shown in Figure 3 (right). Figure 4 shows examples of\nthe language description, the generated visual imagination,\nand the created building of each agent.\nAnalyzing the experimental results, we can draw the fol-\nlowing primal conclusions:\n• CoT for textual imagination can effectively enrich the\ndetailed features of the target buildings.\n5\nComparing CoT+GPT-4 with Vanilla GPT-4, the for-\nmer outperforms the latter in terms of all metrics ex-\ncept robustness in the polar chart by a large margin, and\nCoT+GPT-4 obtains a higher score in the Elo Rating re-\nsults than Vanilla GPT-4. We assign this due to the rich\ninformation brought by Chain-of-Thought, which plays a\nrole in self-reflection. Through this process, the LLM gets\na better understanding of the details of the task, including\nbut not limited to the materials used, the size of the build-\ning, the significant features, etc. Within the context of a\nconversation, when GPT-4 generates the code in the sec-\nond round, it can still perceive the extra information from\nthe first round, thus reaching a better representation of the\ntask goal.\n• For the controller, using VLM instead of LLM leads\nto a marginally better performance in most metrics.\nAs shown in Figure 2, Diffusion+GPT-4V weakly sur-\npasses CoT+GPT-4 in correctness, complexity, quality,\nand robustness. However, Diffusion+GPT-4V strikes a tie\nwith CoT+GPT-4 in functionality. In terms of functional-\nity, Diffusion+GPT-4V behaves no better than CoT+GPT-\n4, which can be owing to the weak ability of GPT-4V\nin 3D reconstruction. Empirically, the images passed to\nGPT-4V are usually a complete building generated by the\ndiffusion-based imaginator, without the sectional view to\nshow the internal structures. Therefore, sometimes GPT-\n4V tends to write code that leads to a solid house instead\nof a hollow one. According to the criteria of functionality,\nsolid houses can result in low ratings.\n• Diffsion+GPT-4V has the best performance overall,\nshowing a strong ability of anti-interference and ro-\nbustness.\nIn Figure 3, both the Elo Rating results and the average\nscore show that the three variants proposed in Figure 1\noutperform the baseline to varying degrees, among which\nDiffsion+GPT-4V ranks the first. Combining the previous\ntwo conclusions, Diffusion+GPT-4V has both the advan-\ntage in visual imagination and the strengths from CoT,\nthus having a better performance. Additionally, we are\nsurprised to find that Diffusion+GPT-4V overcomes the\nmisleading information of the diffusion-based imaginator.\nIn about half of the test tasks, the images generated by the\ndiffusion-based imaginator tend to have obvious noises in\nthe background to some extent. However, GPT-4V seems\nto have the ability of anti-interference, thus capturing the\nmajor essential factors of the images. In contrast, Dif-\nfusion+BC may be susceptible to such noises, leading to\nweaker robustness.\n• The human-rating results coincide with the VLM-\nrating results with a minor gap, indicating that evalu-\nating by vision-language models is reliable.\nWe list the average scores by both VLM and human evalu-\nTable 1. Consistency between VLM and human evaluations. Here,\n“agreement” refers to the proportion of cases where the pairwise\ncomparison between four variants of creative agents has the same\nnumerical relationship in both VLM and human evaluations.\nMetric\n1v1\nOverall\nscore\nCorrect.\nscore\nCompl.\nscore\nQual.\nscore\nFunc.\nscore\nAgreement 62.5%\n67%\n68%\n71%\n62%\n52%\nation in Figure 3 (right), from which we know the human-\nrating results are generally in line with the VLM-rating\nresults. In both evaluations, the first two in the ranking\nare the same - Diffusion+GPT-4V and CoT+GPT-4. The\nlast two are in the opposite order but within a small gap.\nOverall, both two evaluations agree that Diffusion+GPT-\n4V has the best performance.\n• The buildings created by agents are relatively simple,\nlimited by the code written by language models and\nthe trained policy of the behavior-cloning controller.\nIn the analysis of the final creations of different agents,\nwe find that the buildings are relatively simple. For those\nvariants with GPT-4(V) controllers, this may be limited\nby the code written by GPT-4(V). Due to limited APIs\nin Mineflayer, GPT-4(V) tends to generate simple code.\nFor instance, GPT-4(V) tends to use for-loops in a piece\nof code that corresponds to a wall in the building, re-\nsulting in the square shape of the building.\nAddition-\nally, Mineflayer uses a rule-based algorithm for path plan-\nning to place blocks in the Minecraft world, and the agent\nwill always destroy some blocks when not able to find\na proper path toward the goal. Therefore, there can be\nmany demolished walls in the final creations.\nOn the\nother hand, the trained policy of the behavior-cloning\ncontroller has several limitations. When reconstructing\nvoxels from the images generated by the diffusion-based\nimaginator, the Pix2Vox approach can only capture the\nRGB color for each voxel and choose the most similar\nblock in Minecraft, which is not very accurate. To make\nthings worse, the plausible structure of a common build-\ning is missed out during the reconstruction, which makes\nthe voxel look like “a mess”. Some blocks are even float-\ning in the voxel, so they cannot be placed correctly in the\nfinal execution. This also provides a reason why Diffu-\nsion+BC ranks the last in the human evaluation results.\n5.5. The VLM Evaluator vs. Human Evaluators\nIn human evaluation, for the first evaluation method (1v1\ncomparison), we use the majority vote among all humans\n(49 human evaluators in total) to represent human prefer-\nence for each pair of buildings. For the second evaluation\nmethod, the scoring data from each human is standardized\nin each of the four evaluation metrics and the overall score.\n6\n“A huge Minecraft volcano built of ice.”\n“A tall Minecraft tower with glass windows, \nmainly built of leaves.”\n“A big Minecraft house built of colorful \nwools.”\n“A slender Minecraft tower with crystal-\nclear windows, predominantly crafted from \nbirch logs.”\n“A yellow concrete Minecraft house with a \nroof and windows.”\n“A sandstone palace in Minecraft with \nintricate details and towering minarets.”\n“A mystical ice castle in Minecraft, sculpted \nfrom packed ice and frosted blocks, adorned \nwith icicle chandeliers and frosty spires.”\n“A medieval-inspired fortress in Minecraft \nbuilt from cobblestone and mossy stone \nbricks, complete with imposing towers and a \ndrawbridge.”\nDiffusion Image\nVanilla GPT-4\nCoT+GPT-4\nDiffusion+GPT-4V\nDiffusion+BC\nText Description\n“An enchanting Minecraft pagoda adorned \nwith bamboo and built primarily of jungle \nwood planks.”\n“A pyramid in Minecraft built of sandstone.”\nFigure 4. Examples of the language description, the generated visual imagination, and the created building of each variant of creative\nagents. Visual imagination generated by the diffusion model has great diversity, which is an important manifestation of creativity.\nThen we take the average score from all humans as the\nhuman evaluation score for each building created by each\nagent. To measure the consistency of human and VLM eval-\nuators, we use “agreement”, which refers to the proportion\nof cases where the pairwise comparison between four vari-\nants of creative agents has the same numerical relationship\nin both human and VLM evaluations under each evaluation\nmetric.\nThe agreements of human and VLM evaluations are\nlisted in Table 1.\nThe agreement in the overall score is\n67%, greater than two-thirds, indicating that the VLM eval-\nuation is consistent with human evaluation to some extent,\nthough each metric may have a slightly different agreement.\nMoreover, the agreement in 1v1 is 62.5%, relatively lower\nthan that in the overall score, but they are reasonably consis-\ntent. Together with human and VLM evaluations reaching a\nconsensus that Diffusion+GPT-4V has the best performance\nfollowed by CoT+GPT-4, as shown in Figure 3 (right), we\nbelieve the VLM evaluator can be a good choice for creative\ntasks.\n7\n6. Related Work\n6.1. Open-Ended Agents\nIn recent years, task learning in open-ended worlds has at-\ntracted increasing attention, among which Minecraft [25]\nhas become a marvelous test-bed for open-ended learn-\ning. MineRL [18] and MineDojo [13] implemented sim-\nulated environments and organized datasets of a relatively\nlarge scale, and the latter provides tasks for agent train-\ning in the open-ended domain.\nHowever, most previous\nwork [3, 29, 45, 52] mainly focused on unlocking numerous\nskills and tackling long-horizon tasks in Minecraft, which\nhowever are mostly predefined, lacking the open-ended na-\nture, not to mention creativity.\nThe IGLU Competition [27] was a giant leap to solving\ntasks according to instructions in natural language. Skryn-\nnik et al. [41] proposed a pipeline containing a T5-based\nlanguage module, a rule-based transfer module, and the\ndownstream policy module for execution. This agent could\nsolve simple tasks of stacking blocks in Gridworld [53], a\nMinecraft-like open-ended world. However, it depended on\ntoo many step-by-step instructions, thus showing little cre-\nativity. In general, there is a significant gap between previ-\nous work and the true “creative agents”.\n6.2. Generative Models\nIn recent years, many modern generative models have been\nproposed and are used to produce high-quality samples in\nvarious domains [8]. Text-generative models have aroused\nmuch attention due to their wide range of uses. Especially,\nlarge language models (LLMs) are playing more and more\nsignificant roles in decision-making, planning, and reason-\ning.\nAmong LLMs, a representative one is GPT-4 [34],\nwhose emergence has laid a solid foundation for further re-\nsearch. Accompanied by the appearance of LLMs, prompt\nengineering and tuning techniques [5, 48] have been widely\nstudied and applied, including Parameter-Efficient Fine-\nTuning (PEFT) [21] and Chain-of-Thought (CoT) [47]. In\nour work, LLMs with CoT are adopted as textual imagina-\ntors, and we also construct the text-based controller with\nLLM code generation.\nIn the field of computer vision, image-generative mod-\nels are becoming increasingly important.\nProminent ap-\nproaches include variational autoencoders (VAEs) [26],\nGenerative\nAdversarial\nNetworks\n(GANs)\n[15],\nand\nflows [22], demonstrating success in capturing image dis-\ntributions and representations. Recently, diffusion models\n[23] and DALL-E 3 [33] are springing up, accelerating the\nresearch in visual generation. In our work, a finetuned Sta-\nble Diffusion [38] is used for visual imagination, represent-\ning a concrete description of the building-creation task.\nIn the Minecraft world, previous work [2, 17, 40] fo-\ncuses on Procedural Content Generation (PCG). However,\nthey usually generate a pre-defined type of buildings with\na lot of human prior knowledge. In our work, imagination\nfor creative agents is similar to content generation, but our\nimaginator can generate with free-form instructions, in dif-\nferent modalities, and requiring much less human prior.\n6.3. Evaluation for Open-Ended Tasks\nRecent work has gathered many evaluation methods for\nopen-ended tasks. Voyager [44], STEVE-1 [29], and DIP-\nRL [32] use travel distance and collected items as surro-\ngate metrics to evaluate. GROOT [7] and BASALT compe-\ntition [31] use human evaluation, which is relatively labor-\nintensive and may be susceptible to subject preferences. Re-\ncent work [10, 13] proposes to use the CLIP-like model to\ncompute alignment between the behaviors and instructions.\nWe propose a novel evaluation method using VLMs, which\ncan either directly rate the performance in various aspects\nor conduct pairwise comparisons.\nIn terms of evaluation aspects, previous studies have pro-\nposed a variety of metrics. MCU [30] took evaluations from\nthe perspective of planning complexity, time consumption,\nnovelty, and creativity. GDMC Competition [40] required\nhumans as judges, rating the generated contents from adapt-\nability, functionality, evocative narrative, as well as visual\naesthetics. Team et al. [43] evaluated the results in both task\ncoverage and relative performance. Inspired by these stud-\nies, we adopt the evaluation aspects in correctness, com-\nplexity, quality, functionality, and robustness.\n7. Conclusion and Limitations\nIn this paper, we propose creative agents, which is the first\nframework that can handle creative tasks in an open-ended\nworld. Using this framework, we implement various em-\nbodied agents through different combinations of imagina-\ntors and controllers. Additionally, we tap into the potential\nof Vision-Language Models (VLMs), utilizing VLMs for\nevaluation as judges. By comparing the rating results from\nVLM and humans, we illustrate the reliability of VLM eval-\nuation.\nIn the meanwhile, we find a few limitations of these cre-\native agents, to be investigated in further work. First, there\nis much room for improving the BC controller, especially\nfor the performance of Pix2Vox module. Another limitation\nlies in the simplicity of the building created by the agents,\nwhich means the capabilities of these agents are limited.\nHow to enhance the creativity of agents can be a challeng-\ning problem.\nIn the end, we declare that creative agents is an initial\nattempt in this field, aimed at raising the awareness of build-\ning intelligent agents with creativity. We hope this work can\nbe of inspiration for further research.\n8","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Creative Agents: Empowering Agents with Imagination for Creative Tasks.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nCreative Agents: Empowering Agents with Imagination for Creative Tasks\n```\n#### 2. 论文摘要\n```\nWe study building embodied agents for open-ended creative tasks. While\nexisting methods build instruction-following agents that can perform diverse\nopen-ended tasks, none of them demonstrates creativity -- the ability to give\nnovel and diverse task solutions implicit in the language instructions. This\nlimitation comes from their inability to convert abstract language instructions\ninto concrete task goals in the environment and perform long-horizon planning\nfor such complicated goals. Given the observation that humans perform creative\ntasks with the help of imagination, we propose a class of solutions for\ncreative agents, where the controller is enhanced with an imaginator that\ngenerates detailed imaginations of task outcomes conditioned on language\ninstructions. We introduce several approaches to implementing the components of\ncreative agents. We implement the imaginator with either a large language model\nfor textual imagination or a diffusion model for visual imagination. The\ncontroller can either be a behavior-cloning policy learned from data or a\npre-trained foundation model generating executable codes in the environment. We\nbenchmark creative tasks with the challenging open-world game Minecraft, where\nthe agents are asked to create diverse buildings given free-form language\ninstructions. In addition, we propose novel evaluation metrics for open-ended\ncreative tasks utilizing GPT-4V, which holds many advantages over existing\nmetrics. We perform a detailed experimental analysis of creative agents,\nshowing that creative agents are the first AI agents accomplishing diverse\nbuilding creation in the survival mode of Minecraft. Our benchmark and models\nare open-source for future research on creative agents\n(https:\/\/github.com\/PKU-RL\/Creative-Agents).\n```\n\n#### 3. 论文全文\n```\nCreative Agents: Empowering Agents with Imagination for Creative Tasks\nChi Zhang1*\nPenglin Cai1∗\nYuhui Fu2\nHaoqi Yuan1\nZongqing Lu1,3†\n1Peking University\n2Tsinghua University\n3BAAI\nAbstract\nWe study building embodied agents for open-ended cre-\native tasks.\nWhile existing methods build instruction-\nfollowing agents that can perform diverse open-ended\ntasks, none of them demonstrates creativity – the ability\nto give novel and diverse task solutions implicit in the lan-\nguage instructions. This limitation comes from their inabil-\nity to convert abstract language instructions into concrete\ntask goals in the environment and perform long-horizon\nplanning for such complicated goals. Given the observa-\ntion that humans perform creative tasks with the help of\nimagination, we propose a class of solutions for creative\nagents, where the controller is enhanced with an imagina-\ntor that generates detailed imaginations of task outcomes\nconditioned on language instructions. We introduce sev-\neral approaches to implementing the components of cre-\native agents. We implement the imaginator with either a\nlarge language model for textual imagination or a diffu-\nsion model for visual imagination. The controller can ei-\nther be a behavior-cloning policy learned from data or a\npre-trained foundation model generating executable codes\nin the environment. We benchmark creative tasks with the\nchallenging open-world game Minecraft, where the agents\nare asked to create diverse buildings given free-form lan-\nguage instructions. In addition, we propose novel evalu-\nation metrics for open-ended creative tasks utilizing GPT-\n4V, which holds many advantages over existing metrics. We\nperform a detailed experimental analysis of creative agents,\nshowing that creative agents are the first AI agents accom-\nplishing diverse building creation in the survival mode of\nMinecraft. Our benchmark and models are open-source for\nfuture research on creative agents (https:\/\/github.com\/PKU-\nRL\/Creative-Agents).\n1. Introduction\nBuilding open-ended embodied agents has been a long-\nstanding goal of AI research.\nUnlike many existing AI\nagents that perform a fixed set of tasks specified with re-\n*Equal contribution\n†Correspondence to B: zongqing.lu@pku.edu.cn\nwards [1, 19], open-ended agents can perform diverse arbi-\ntrary tasks without such specification. Existing research pri-\nmarily focuses on learning instruction-following agents [4,\n11, 29] that can solve open-ended tasks given free-form\nlanguage instructions, achieving success in robotic do-\nmains [4, 11, 24] and open-world games [6, 29, 45]. How-\never, these agents can only follow clear instructions that rep-\nresent specific goals or behaviors. Creative tasks, where the\ninstructions describe abstract tasks and the agent is required\nto generate complicated, novel, and diverse solutions, bring\nnew challenges to intelligent agents.\nAs an example, in the open-world game Minecraft, ex-\nisting agents can follow simple and clear instructions like\n‘harvest a stone’ [52] and ‘build a snow golem, which stacks\n2 snow blocks and 1 pumpkin’ [7], but they cannot solve\ncreative tasks like ‘build a sandstone palace’. For the latter,\nthe agent can struggle to understand the target outcome of\nthe task implied in the abstract instruction and plan actions\nfor the long-horizon execution where hundreds of blocks\nshould be properly placed. However, empowered with the\nability of imagination, humans can first imagine the appear-\nance and functionality of the building, then plan for a proper\norder to build blocks and realize the imagined house in the\ngame. Such ability enhances humans with strong creativ-\nity, enabling humans to create novel and diverse outcomes.\nImagination also enriches the fuzzy instructions into refined\ntask outcomes grounded in the environment, making the\ntask description more explicit and executable.\nMotivated by this ability, we introduce a framework for\ncreative agents, empowering open-ended agents with imagi-\nnation to solve creative tasks. Figure 1 gives an overview of\nthe framework. Equipped with a text-conditioned imagina-\ntor, creative agents can imagine the details of the task out-\ncome abstracted in the language instruction. These imag-\ninations serve as a blueprint for the controller to interpret\nand act upon. We propose two variants of the imaginator,\nincluding a large language model (LLM) [5] generating text\nimaginations and a finetuned diffusion model [38] generat-\ning grounded visual imaginations. We also introduce two\nvariants of the controller that transform the imagination into\nexecutable plans. The first is a behavior-cloning controller\ntrained on an environment dataset and maps imaginations\n1\narXiv:2312.02519v1  [cs.AI]  5 Dec 2023\n“build a \nsandstone \npalace”\nGPT-4(V)\nCode Gen.\nBC\nController\nImaginator\nController\nCreative Agents\nCreative Task\nCreation\n“1. Size: 10x10x12\n2. Made of sandstone\n3. It has an entrance”\n𝐼𝑔𝑙\n𝜋𝑎𝑠, 𝑔, 𝑙\n①\n②\n③\nInstruction (𝑙)\nImagination (𝑔)\nLLM CoT\nDiffusion\nModel\nFigure 1. Overview of creative agents for open-ended creative tasks. A creative agent consists of two components: an imaginator and a\ncontroller. Given a free-form language instruction describing the creative task, the imaginator first generates the imagination in the form of\ntext\/image by LLM with Chain-of-Thought (CoT)\/diffusion model, then the controller fulfills the imagination by executing actions in the\nenvironment, leveraging the code generation capability of vision-language model (VLM) or a behavior-cloning (BC) policy learned from\ndata. We implement three combinations of the imaginator and controller: ①CoT+GPT-4, ②Diffusion+GPT-4V, and ③Diffusion+BC.\nto actions. The second method leverages the strong abili-\nties in vision-language understanding [51] and code gener-\nation [44] of the large vision-language model (VLM) GPT-\n4V [34]. The VLM controller receives the imagination as\nthe task goal and generates code to perform actions in the\nenvironment.\nDesigning evaluation metrics for open-ended tasks re-\nmains underexplored.\nExisting methods either use some\nsurrogate metrics [44] which may not reflect the language\ninstruction, or use human evaluation [7] which is labori-\nous. Fan et al. [13] proposes to use the similarity of the\nCLIP [37] embedding between vision and language, which\nhowever can only provide some unknown correlation be-\ntween the instruction and task outcome. To address these\nlimitations, we propose novel evaluation metrics based on\nGPT-4V. Leveraging the analytical strength of GPT-4V, our\nmetrics offer an effective, general, and human-independent\nmeans of evaluation. We verify that such metrics are con-\nsistent with human evaluations. Our proposed metrics are\ncrucial for objectively measuring the creativity and effec-\ntiveness of solutions generated by open-ended agents.\nWe benchmark creative tasks with challenging building\ncreation in Minecraft1, following 20 diverse instructions.\nSeveral variants of creative agents demonstrate their abil-\nity to create diverse and visually appealing buildings in the\nsurvival mode of Minecraft, which has never been achieved\nin previous studies. We give a detailed experimental analy-\n1We select the open-world game Minecraft as the benchmark platform\nbecause it is convenient to build various imaginators and controllers and\nalso supports creation in the game. Specifically, we choose the survival\nmode of Minecraft, where it is difficult for the agent to construct buildings\nsince the agent has to move around and go up\/down to place the blocks\nwith diverse materials and colors, making the building process realistic. It\nis worth noting that our framework for creative agents is general and can\nalso be applied to other environments.\nsis of creative agents, discuss the strengths and weaknesses\nof each variant, and provide insights for improving creative\nagents in future work.\nOur main contributions are threefold:\n• We propose creative agents, the first framework that en-\ndows open-ended agents with the ability to perform cre-\native tasks through imagination. Our method builds the\nfirst instruction-following agent that can create diverse\nbuildings in the survival mode of Minecraft.\n• We establish novel evaluation metrics for creative tasks\nin open-ended environments, in which GPT-4V is used as\nthe evaluator.\n• By open-sourcing the datasets and models, our work sets\na new benchmark for future research in the field of open-\nended learning and creative AI agents.\n2. Preliminaries\n2.1. Open-Ended Tasks\nWe formalize the process of the agent interacting with the\nenvironment as a Markov Decision Process (MDP) without\nreward, defined by a tuple M = (S, A, P, ρ) representing\nstates, actions, the transition function of the environment,\nand the initial state distribution, respectively. Starting from\nthe initial state, for each time step, the agent performs an\naction based on the state, then the environment transitions\nto the next state upon the action.\nCompared with traditional reinforcement learning tasks\ndefined with reward functions, open-ended tasks have nei-\nther fixed targets nor optimal solutions. We follow Fan et al.\n[13], formulating open-ended tasks as instruction-following\nproblems T = (L, M), where l ∈L is a free-form language\ninstruction.\nWe aim to acquire an instruction-following\nagent P(a|s, l) which can exhibit behaviors consistent with\n2\nthe instruction to perform the described task.\n2.2. Creative Agents with Imagination\nDue to the abstract nature of language, language instruc-\ntions cannot describe the full details of complicated tasks,\ndrawing high uncertainty on the task completion and requir-\ning the agent to possess creativity.\nThough many open-\nended agents [6, 11, 29] can follow clear instructions that\nrefer to some specific task goals, none of them can follow\nsuch uncertain instructions to perform complicated tasks.\nWe define creative tasks as a challenging case of open-\nended tasks, where language instructions lack information\nto describe the whole task and can refer to diverse, novel,\nand complicated outcomes in the environment. Such in-\nstructions bring uncertainty for the agent and require the\nability to imagine the details unspecified by the instruction.\nIn addition, a short instruction (e.g. ‘build a house’) may re-\nfer to a long-horizon complicated task, increasing the chal-\nlenge for the action planning and execution.\nTo tackle the challenge, we propose to decompose the\nagent into an imaginator and a controller:\nP(a|s, l) =\nX\ng\nI(g|l)π(a|s, g, l).\n(1)\nHere, g ∈G is an imagination of the task outcome, which\ncan be in the form of diverse modalities (e.g. text, image)\nand serves as a description of the target environment state\nof the task. The imaginator I converts the instruction into\nan imagined outcome, providing the controller π with a de-\ntailed task description. Therefore, we leave the uncertainty\nand creativity brought from creative tasks to the imaginator,\nproviding the controller with richer task information to re-\nduce its uncertainty. By disentangling these two models, we\ncan delve deeper into the design choices for each part and\ncombine them together to build creative agents.\n3. Generative Imagination\nGenerative models in natural language processing and com-\nputer vision provide techniques to build the imaginator in\neither text space or image space. In this section, we present\ntwo variants for implementing the imaginator.\n3.1. Language Models for Textual Imagination\nLarge language models (LLMs) have shown marvelous\nabilities in solving diverse tasks [9, 46] as well as high plas-\nticity with prompt engineering [5, 48]. To tackle the prob-\nlems in reasoning logically, Wei et al. [47] proposed Chain-\nof-Thought (CoT), aimed at enhancing the emergence abil-\nity of LLMs.\nFollowing the idea of zero-shot-CoT [28], we design\nan imaginator using GPT-4 [34] as the backbone, with zero-\nshot prompts for imagination in Minecraft building-creation\ndomain (please refer to Appendix B). Specifically, we pro-\nvide the initial text instruction to GPT-4 and ask five ques-\ntions relevant to the imagination, including the material\nused for the building, the approximate size, the significant\nfeatures of the architecture, etc. After GPT-4 generates an-\nswers to these questions indicating that the imagination pro-\ncess has been finished, we then ask the controller to exe-\ncute actions accordingly to construct the building (see Sec-\ntion 4).\n3.2. Diffusion Models for Visual Imagination\nDiffusion models have achieved breakthrough performance\nin generating diverse and high-quality images. Stable Dif-\nfusion [38] models data distribution as the stationary state\nof a diffusion process, learning to generate samples mir-\nroring the true data distribution by reversing this process.\nNoteworthy for its training stability, it addresses issues like\nmode collapse.\nTo better align with the human conception of “imagi-\nnation”, we use images to be the imagination space and\nleverage text-conditioned diffusion models to be the imag-\ninator. We finetune the Stable Diffusion [38] using a text-\nimage dataset to achieve a reasonable and diverse imagina-\ntion of textual input. The text-image pairs in the dataset\nare constructed by automatically annotating the Minecraft\nbuildings in CraftAssist [16] using the multimodal Emu\nmodel [42]. After finetuning, we obtain visually plausible\nand diverse imaginations that align with both the textual de-\nscriptions and the Minecraft world.\n4. Designing Controllers\nAfter the imaginator generates the imagination g, it is the\ncontroller to take actions in the environment, conditioned\non the current state, the imagination, and the language in-\nstruction. In the following, two variants of the controller\nare presented, including a behavior-cloning controller and a\ncontroller based on GPT-4(V).\n4.1. Behavior-Cloning Controller\nTo transform the imagination into a practical construction\nprocess, we introduce a behavior-cloning controller that\nfirst converts the image imagination into a blueprint and\nthen maps the blueprint into tangible actions.\nFor tasks related to constructing buildings in Minecraft,\nwe use voxel information as the basis for blueprints. To\nlearn a module generating voxel blueprints conditioned\non images, we adopt the methodology introduced by\nPix2Vox++ [50], utilizing the image-voxel dataset con-\nstructed through data augmentation from original construc-\ntions in CraftAssist [16] and ISM [14].\nThe module is\ntrained to optimize a combination of the voxel predic-\ntion loss and two regularization terms, including the occu-\npancy rate loss [36] and the total variation loss [39, 49].\n3\nSubsequently, for the construction process, we employ\nResNet3D-CNN[20] and train a behavior-cloning (BC) pol-\nicy on a collected voxel-action dataset. After that, the fi-\nnal construction is executed by the BC policy conditioned\non the voxel information through path-searching and block-\nplacing within the MineDojo simulator [13]. More details\nabout our methods and datasets are available in Appendix B.\n4.2. Vision-Language Models as Controller\nWe also adopt a generative vision-language model (VLM)\nto construct the controller, which can perceive both visual\nimaginations and textual imaginations. Utilizing its abili-\nties in task reasoning and code generation, given an envi-\nronment code interface that wraps actions, the VLM can\ngenerate executable code in the environment for task com-\npletion.\nSpecifically, we use GPT-4(V) which takes as input an\nimage generated by the diffusion imaginator or the textual\nimagination generated by the LLM with CoT. We ask GPT-\n4(V) to generate code that can call Mineflayer [35] APIs to\nexecute environment actions for building creation. Mine-\nflayer implements JavaScript APIs for diverse skill primi-\ntives in the Minecraft world. Following the prompt design\nin Voyager [44], we provide GPT-4(V) with API documen-\ntation to clarify the coding rules and a one-shot example of\ncode generation for in-context learning. More details about\nthe prompts are available in Appendix B.\nWith this controller, we implement creative agents in\nboth two modalities of imaginations.\n5. Experiments\n5.1. Building Creation in Minecraft\nInspired by the creative tasks in MineDojo [13], we set\nup an evaluation benchmark for constructing buildings in\nMinecraft, consisting of 20 diverse language instructions,\nsuch as “a huge Minecraft volcano built of ice” as illustrated\nin Figure 4. Following the text description, the agent takes\nactions to move and place blocks in the game simulator to\ncreate buildings. In the experiment, we aim to investigate\nwhether the agent can construct novel, diverse buildings by\njust following language instructions, which reflects the cre-\nativity of the agent. In the evaluation, we take screenshots\nof its creations in the game. More details can be found in\nAppendix A. We setup various metrics to evaluate the open-\nended building creation tasks and apply two evaluators, in-\ncluding human evaluators and a novel evaluator based on\nGPT-4V. Section 5.3 presents the evaluation details.\n5.2. Implementation\nWe implement several variants of creative agents using dif-\nferent combinations of imaginators and controllers (more\nCorrectness\nComplexity\nQuality\nFunctionality\nRobustness\n6.95\n4.90\n6.35\n4.15\n4.07\nVanilla GPT-4\nCoT+GPT-4\nDiffusion+GPT-4V\nDiffusion+BC\nFigure 2.\nComparison of all variants of creative agents in\nMinecraft building creation. For each evaluation metric, the num-\nber denotes the average score of the best agent over the 20 tasks.\nDiffusion+GPT-4V performs relatively better than other variants.\ndetails are provided in Appendix B) and build a baseline\nmethod to compare with:\n• Vanilla GPT-4.\nThis is the baseline method without\nimagination using GPT-4 as the controller. We simply\nreplace the textual imagination with the original task in-\nstruction and ask GPT-4 to perform code generation.\n• CoT+GPT-4. We implement this agent by adding a CoT-\nimagination on the basis of Vanilla GPT-4, which means\nwe use GPT-4 for both textual imagination and code gen-\neration (method ①in Figure 1).\n• Diffusion+GPT-4V2. We use a finetuned Stable Diffu-\nsion to generate images as imagination and use GPT-4V\nas the controller to generate codes based on the visual\nimagination (method ②in Figure 1).\n• Diffusion+BC. The finetuned Stable Diffusion is used as\nthe imaginator while the behavior-cloning controller is\nused to convert images into voxel blueprints and execute\nactions.(method ③in Figure 1).\n5.3. Evaluation Metrics\nBased on existing evaluation methods in open-ended learn-\ning [30] and content generation in Minecraft [40], we intro-\nduce a set of evaluation aspects, which are important criteria\nfor creative agents:\n• Correctness. Are the creations consistent with the lan-\nguage instruction?\n• Complexity.\nCan the agent create large and complex\nbuildings?\n2We use GPT-4V here to indicate the agent additionally takes an image\nimagination as input.\n4\nDiffusion+GPT-4V CoT+GPT-4\nDiffusion+BC\nVanilla GPT-4\n1000\n1100\n1200\n1300\n1400\n1500\n1600\nElo Rating\n1531.0\n1441.0\n1395.0\n1233.0\nElo Ratings of Different Agents\nVanilla GPT-4\nCoT+GPT-4\nDiffusion+GPT-4V\nDiffusion+BC\n16\n17\n18\n19\n20\n21\n22\n23\n24\nAverage Score\n18.1\n21.2\n22.4\n19.8\n19.1\n21.1\n23.2\n17.8\nAverage Score of Different Agents\nGPT-4V\nHumans\nFigure 3. Evaluation results in Minecraft building creation. Left: The Elo Rating of all agents based on the evaluation of GPT-4V. Right:\nThe average overall score of each agent in all test tasks evaluated by GPT-4V and humans.\n• Quality. Do the creations have a good visual appearance\nfrom the perspective of aesthetics?\n• Functionality. Do the created buildings have the neces-\nsary functions and structures (such as windows and en-\ntrances)?\nTo quantitatively evaluate such metrics, recent work [7]\nrequires humans to perform evaluation, which however is\nlabor-intensive and may be susceptible to subject prefer-\nences. To tackle these issues, we leverage the strong ca-\npabilities of the recent VLMs in vision-language reasoning\nand vision question answering and propose two VLM-based\nevaluation methods.\nIn the first method, given a language instruction, we sam-\nple a pair of creation results from two methods and fill in a\ntemplate to ask the VLM which one is better overall based\non all evaluation metrics:\nYou are a critic with high aesthetic ability. I\nwill provide you a text instruction and two\nbuildings created by different agents\nfollowing this instruction in the game \"\nMinecraft\".\nPlease evaluate their overall performance\naccording to four aspects: $(\nEVALUATION_ASPECT 1˜4). Tell me which\nbuilding in the image is better (left or\nright).\nText: $(INSTRUCTION)\nImage of buildings: $(IMAGE1, IMAGE2)\nWe use the Elo Rating System [12] to measure the relative\nstrength of each agent.\nIn the second method, we fill in a template with the four\nevaluation metrics above to ask the VLM to directly score\nfor each building:\nYou are a critic with high aesthetic ability. I\nwill provide you a text instruction and a\ncreated building following this instruction\nin the game \"Minecraft\".\nAccording to four aspects: $(EVALUATION_ASPECT\n1˜4),\nplease evaluate the building with a score (out of\n10) on each aspect respectively, then give a\ntotal score.\nText: $(INSTRUCTION)\nImage of the building: $IMAGE\nTo verify the reliability of VLMs in evaluation, we also ask\nhumans to participate in both two evaluation methods and\ncompare the difference between VLM and human evalua-\ntions.\n5.4. Results and Analysis\nIn the first evaluation method, every two agents are com-\npared by GPT-4V for each test task. With all these com-\nparison results, we model the four agents into a four-player\nzero-sum game and apply the Elo Rating System to com-\npute the ratings, as shown in Figure 3 (left).\nAccording to the second evaluation method, we record\nratings of each single test over the four metrics (correctness,\ncomplexity, quality, and functionality), the sum of which is\nthe overall score. We calculate the standard deviation of the\noverall scores of all test tasks for each agent, which repre-\nsents Robustness, with a larger standard deviation standing\nfor weaker robustness. To align with the other four metrics\nfor better presentation, we properly transform the standard\ndeviation into a value such that a higher value indicates bet-\nter robustness. Note that better robustness does not neces-\nsarily mean better performance since it merely indicates the\nconsistency of the performance on all test tasks. The results\nare plotted as a polar chart shown in Figure 2. Furthermore,\nthe overall score averaged over all test tasks for each agent\nis shown in Figure 3 (right). Figure 4 shows examples of\nthe language description, the generated visual imagination,\nand the created building of each agent.\nAnalyzing the experimental results, we can draw the fol-\nlowing primal conclusions:\n• CoT for textual imagination can effectively enrich the\ndetailed features of the target buildings.\n5\nComparing CoT+GPT-4 with Vanilla GPT-4, the for-\nmer outperforms the latter in terms of all metrics ex-\ncept robustness in the polar chart by a large margin, and\nCoT+GPT-4 obtains a higher score in the Elo Rating re-\nsults than Vanilla GPT-4. We assign this due to the rich\ninformation brought by Chain-of-Thought, which plays a\nrole in self-reflection. Through this process, the LLM gets\na better understanding of the details of the task, including\nbut not limited to the materials used, the size of the build-\ning, the significant features, etc. Within the context of a\nconversation, when GPT-4 generates the code in the sec-\nond round, it can still perceive the extra information from\nthe first round, thus reaching a better representation of the\ntask goal.\n• For the controller, using VLM instead of LLM leads\nto a marginally better performance in most metrics.\nAs shown in Figure 2, Diffusion+GPT-4V weakly sur-\npasses CoT+GPT-4 in correctness, complexity, quality,\nand robustness. However, Diffusion+GPT-4V strikes a tie\nwith CoT+GPT-4 in functionality. In terms of functional-\nity, Diffusion+GPT-4V behaves no better than CoT+GPT-\n4, which can be owing to the weak ability of GPT-4V\nin 3D reconstruction. Empirically, the images passed to\nGPT-4V are usually a complete building generated by the\ndiffusion-based imaginator, without the sectional view to\nshow the internal structures. Therefore, sometimes GPT-\n4V tends to write code that leads to a solid house instead\nof a hollow one. According to the criteria of functionality,\nsolid houses can result in low ratings.\n• Diffsion+GPT-4V has the best performance overall,\nshowing a strong ability of anti-interference and ro-\nbustness.\nIn Figure 3, both the Elo Rating results and the average\nscore show that the three variants proposed in Figure 1\noutperform the baseline to varying degrees, among which\nDiffsion+GPT-4V ranks the first. Combining the previous\ntwo conclusions, Diffusion+GPT-4V has both the advan-\ntage in visual imagination and the strengths from CoT,\nthus having a better performance. Additionally, we are\nsurprised to find that Diffusion+GPT-4V overcomes the\nmisleading information of the diffusion-based imaginator.\nIn about half of the test tasks, the images generated by the\ndiffusion-based imaginator tend to have obvious noises in\nthe background to some extent. However, GPT-4V seems\nto have the ability of anti-interference, thus capturing the\nmajor essential factors of the images. In contrast, Dif-\nfusion+BC may be susceptible to such noises, leading to\nweaker robustness.\n• The human-rating results coincide with the VLM-\nrating results with a minor gap, indicating that evalu-\nating by vision-language models is reliable.\nWe list the average scores by both VLM and human evalu-\nTable 1. Consistency between VLM and human evaluations. Here,\n“agreement” refers to the proportion of cases where the pairwise\ncomparison between four variants of creative agents has the same\nnumerical relationship in both VLM and human evaluations.\nMetric\n1v1\nOverall\nscore\nCorrect.\nscore\nCompl.\nscore\nQual.\nscore\nFunc.\nscore\nAgreement 62.5%\n67%\n68%\n71%\n62%\n52%\nation in Figure 3 (right), from which we know the human-\nrating results are generally in line with the VLM-rating\nresults. In both evaluations, the first two in the ranking\nare the same - Diffusion+GPT-4V and CoT+GPT-4. The\nlast two are in the opposite order but within a small gap.\nOverall, both two evaluations agree that Diffusion+GPT-\n4V has the best performance.\n• The buildings created by agents are relatively simple,\nlimited by the code written by language models and\nthe trained policy of the behavior-cloning controller.\nIn the analysis of the final creations of different agents,\nwe find that the buildings are relatively simple. For those\nvariants with GPT-4(V) controllers, this may be limited\nby the code written by GPT-4(V). Due to limited APIs\nin Mineflayer, GPT-4(V) tends to generate simple code.\nFor instance, GPT-4(V) tends to use for-loops in a piece\nof code that corresponds to a wall in the building, re-\nsulting in the square shape of the building.\nAddition-\nally, Mineflayer uses a rule-based algorithm for path plan-\nning to place blocks in the Minecraft world, and the agent\nwill always destroy some blocks when not able to find\na proper path toward the goal. Therefore, there can be\nmany demolished walls in the final creations.\nOn the\nother hand, the trained policy of the behavior-cloning\ncontroller has several limitations. When reconstructing\nvoxels from the images generated by the diffusion-based\nimaginator, the Pix2Vox approach can only capture the\nRGB color for each voxel and choose the most similar\nblock in Minecraft, which is not very accurate. To make\nthings worse, the plausible structure of a common build-\ning is missed out during the reconstruction, which makes\nthe voxel look like “a mess”. Some blocks are even float-\ning in the voxel, so they cannot be placed correctly in the\nfinal execution. This also provides a reason why Diffu-\nsion+BC ranks the last in the human evaluation results.\n5.5. The VLM Evaluator vs. Human Evaluators\nIn human evaluation, for the first evaluation method (1v1\ncomparison), we use the majority vote among all humans\n(49 human evaluators in total) to represent human prefer-\nence for each pair of buildings. For the second evaluation\nmethod, the scoring data from each human is standardized\nin each of the four evaluation metrics and the overall score.\n6\n“A huge Minecraft volcano built of ice.”\n“A tall Minecraft tower with glass windows, \nmainly built of leaves.”\n“A big Minecraft house built of colorful \nwools.”\n“A slender Minecraft tower with crystal-\nclear windows, predominantly crafted from \nbirch logs.”\n“A yellow concrete Minecraft house with a \nroof and windows.”\n“A sandstone palace in Minecraft with \nintricate details and towering minarets.”\n“A mystical ice castle in Minecraft, sculpted \nfrom packed ice and frosted blocks, adorned \nwith icicle chandeliers and frosty spires.”\n“A medieval-inspired fortress in Minecraft \nbuilt from cobblestone and mossy stone \nbricks, complete with imposing towers and a \ndrawbridge.”\nDiffusion Image\nVanilla GPT-4\nCoT+GPT-4\nDiffusion+GPT-4V\nDiffusion+BC\nText Description\n“An enchanting Minecraft pagoda adorned \nwith bamboo and built primarily of jungle \nwood planks.”\n“A pyramid in Minecraft built of sandstone.”\nFigure 4. Examples of the language description, the generated visual imagination, and the created building of each variant of creative\nagents. Visual imagination generated by the diffusion model has great diversity, which is an important manifestation of creativity.\nThen we take the average score from all humans as the\nhuman evaluation score for each building created by each\nagent. To measure the consistency of human and VLM eval-\nuators, we use “agreement”, which refers to the proportion\nof cases where the pairwise comparison between four vari-\nants of creative agents has the same numerical relationship\nin both human and VLM evaluations under each evaluation\nmetric.\nThe agreements of human and VLM evaluations are\nlisted in Table 1.\nThe agreement in the overall score is\n67%, greater than two-thirds, indicating that the VLM eval-\nuation is consistent with human evaluation to some extent,\nthough each metric may have a slightly different agreement.\nMoreover, the agreement in 1v1 is 62.5%, relatively lower\nthan that in the overall score, but they are reasonably consis-\ntent. Together with human and VLM evaluations reaching a\nconsensus that Diffusion+GPT-4V has the best performance\nfollowed by CoT+GPT-4, as shown in Figure 3 (right), we\nbelieve the VLM evaluator can be a good choice for creative\ntasks.\n7\n6. Related Work\n6.1. Open-Ended Agents\nIn recent years, task learning in open-ended worlds has at-\ntracted increasing attention, among which Minecraft [25]\nhas become a marvelous test-bed for open-ended learn-\ning. MineRL [18] and MineDojo [13] implemented sim-\nulated environments and organized datasets of a relatively\nlarge scale, and the latter provides tasks for agent train-\ning in the open-ended domain.\nHowever, most previous\nwork [3, 29, 45, 52] mainly focused on unlocking numerous\nskills and tackling long-horizon tasks in Minecraft, which\nhowever are mostly predefined, lacking the open-ended na-\nture, not to mention creativity.\nThe IGLU Competition [27] was a giant leap to solving\ntasks according to instructions in natural language. Skryn-\nnik et al. [41] proposed a pipeline containing a T5-based\nlanguage module, a rule-based transfer module, and the\ndownstream policy module for execution. This agent could\nsolve simple tasks of stacking blocks in Gridworld [53], a\nMinecraft-like open-ended world. However, it depended on\ntoo many step-by-step instructions, thus showing little cre-\nativity. In general, there is a significant gap between previ-\nous work and the true “creative agents”.\n6.2. Generative Models\nIn recent years, many modern generative models have been\nproposed and are used to produce high-quality samples in\nvarious domains [8]. Text-generative models have aroused\nmuch attention due to their wide range of uses. Especially,\nlarge language models (LLMs) are playing more and more\nsignificant roles in decision-making, planning, and reason-\ning.\nAmong LLMs, a representative one is GPT-4 [34],\nwhose emergence has laid a solid foundation for further re-\nsearch. Accompanied by the appearance of LLMs, prompt\nengineering and tuning techniques [5, 48] have been widely\nstudied and applied, including Parameter-Efficient Fine-\nTuning (PEFT) [21] and Chain-of-Thought (CoT) [47]. In\nour work, LLMs with CoT are adopted as textual imagina-\ntors, and we also construct the text-based controller with\nLLM code generation.\nIn the field of computer vision, image-generative mod-\nels are becoming increasingly important.\nProminent ap-\nproaches include variational autoencoders (VAEs) [26],\nGenerative\nAdversarial\nNetworks\n(GANs)\n[15],\nand\nflows [22], demonstrating success in capturing image dis-\ntributions and representations. Recently, diffusion models\n[23] and DALL-E 3 [33] are springing up, accelerating the\nresearch in visual generation. In our work, a finetuned Sta-\nble Diffusion [38] is used for visual imagination, represent-\ning a concrete description of the building-creation task.\nIn the Minecraft world, previous work [2, 17, 40] fo-\ncuses on Procedural Content Generation (PCG). However,\nthey usually generate a pre-defined type of buildings with\na lot of human prior knowledge. In our work, imagination\nfor creative agents is similar to content generation, but our\nimaginator can generate with free-form instructions, in dif-\nferent modalities, and requiring much less human prior.\n6.3. Evaluation for Open-Ended Tasks\nRecent work has gathered many evaluation methods for\nopen-ended tasks. Voyager [44], STEVE-1 [29], and DIP-\nRL [32] use travel distance and collected items as surro-\ngate metrics to evaluate. GROOT [7] and BASALT compe-\ntition [31] use human evaluation, which is relatively labor-\nintensive and may be susceptible to subject preferences. Re-\ncent work [10, 13] proposes to use the CLIP-like model to\ncompute alignment between the behaviors and instructions.\nWe propose a novel evaluation method using VLMs, which\ncan either directly rate the performance in various aspects\nor conduct pairwise comparisons.\nIn terms of evaluation aspects, previous studies have pro-\nposed a variety of metrics. MCU [30] took evaluations from\nthe perspective of planning complexity, time consumption,\nnovelty, and creativity. GDMC Competition [40] required\nhumans as judges, rating the generated contents from adapt-\nability, functionality, evocative narrative, as well as visual\naesthetics. Team et al. [43] evaluated the results in both task\ncoverage and relative performance. Inspired by these stud-\nies, we adopt the evaluation aspects in correctness, com-\nplexity, quality, functionality, and robustness.\n7. Conclusion and Limitations\nIn this paper, we propose creative agents, which is the first\nframework that can handle creative tasks in an open-ended\nworld. Using this framework, we implement various em-\nbodied agents through different combinations of imagina-\ntors and controllers. Additionally, we tap into the potential\nof Vision-Language Models (VLMs), utilizing VLMs for\nevaluation as judges. By comparing the rating results from\nVLM and humans, we illustrate the reliability of VLM eval-\nuation.\nIn the meanwhile, we find a few limitations of these cre-\native agents, to be investigated in further work. First, there\nis much room for improving the BC controller, especially\nfor the performance of Pix2Vox module. Another limitation\nlies in the simplicity of the building created by the agents,\nwhich means the capabilities of these agents are limited.\nHow to enhance the creativity of agents can be a challeng-\ning problem.\nIn the end, we declare that creative agents is an initial\nattempt in this field, aimed at raising the awareness of build-\ning intelligent agents with creativity. We hope this work can\nbe of inspiration for further research.\n8\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 创意智能体：赋予智能体想象力以完成创意任务\n\n## 📌 背景痛点\/本文动机\n现有的智能体大多只能执行预定义的任务，缺乏处理开放性任务的能力，尤其是那些需要创造力的任务。例如，在Minecraft游戏中，现有的智能体可以执行简单的指令，如“收集石头”或“建造一个雪人”，但无法完成更复杂的创意任务，如“建造一个砂岩宫殿”。这是因为它们无法将抽象的语言指令转换为具体的任务目标，并执行长期规划。\n\n## 🚀 核心方法\n本文提出了“创意智能体”的概念，通过赋予智能体想象力来处理开放性创意任务。创意智能体由两个主要部分组成：想象器和控制器。\n\n💡 创新点1：想象器\n想象器负责根据语言指令生成任务结果的详细想象。本文提出了两种实现想象器的方法：\n- **文本想象**：使用大型语言模型（LLM）如GPT-4，通过Chain-of-Thought（CoT）技术生成文本想象。\n- **视觉想象**：使用扩散模型如Stable Diffusion，生成与文本描述相符的视觉想象。\n\n💡 创新点2：控制器\n控制器负责将想象转换为可执行的计划。本文提出了两种实现控制器的方法：\n- **行为克隆控制器**：从环境中学习行为克隆策略，将图像想象转换为建筑蓝图，并执行相应的动作。\n- **基于GPT-4(V)的控制器**：利用GPT-4(V)的视觉语言理解和代码生成能力，直接生成可执行代码来完成任务。\n\n## 📈 实验结果\n本文在Minecraft游戏中进行了实验，结果表明，创意智能体能够根据自由形式的语言指令创建多样化和视觉上吸引人的建筑。其中，使用扩散模型进行视觉想象，并结合GPT-4(V)进行控制的智能体表现最佳。\n\n## 💬 可借鉴之处\n本文提出的创意智能体框架为开放性学习和创意AI智能体研究提供了新的思路和方法。未来可以进一步探索如何提高行为克隆控制器的性能，以及如何增强智能体的创造力。","llm_summary_res_status":200}
{"title":"Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft","authors":"Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai","summary":"Many reinforcement learning environments (e.g., Minecraft) provide only\nsparse rewards that indicate task completion or failure with binary values. The\nchallenge in exploration efficiency in such environments makes it difficult for\nreinforcement-learning-based agents to learn complex tasks. To address this,\nthis paper introduces an advanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically design dense reward\nfunctions, thereby enhancing the learning efficiency. Auto MC-Reward consists\nof three important components: Reward Designer, Reward Critic, and Trajectory\nAnalyzer. Given the environment information and task descriptions, the Reward\nDesigner first design the reward function by coding an executable Python\nfunction with predefined observation inputs. Then, our Reward Critic will be\nresponsible for verifying the code, checking whether the code is\nself-consistent and free of syntax and semantic errors. Further, the Trajectory\nAnalyzer summarizes possible failure causes and provides refinement suggestions\naccording to collected trajectories. In the next round, Reward Designer will\nfurther refine and iterate the dense reward function based on feedback.\nExperiments demonstrate a significant improvement in the success rate and\nlearning efficiency of our agents in complex tasks in Minecraft, such as\nobtaining diamond with the efficient ability to avoid lava, and efficiently\nexplore trees and animals that are sparse in the plains biome.","url":"http:\/\/arxiv.org\/abs\/2312.09238v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2312.09238v2","published":1702580292000,"comment":"Accepted by CVPR2024","pdf_text":"Auto MC-Reward: Automated Dense Reward Design with\nLarge Language Models for Minecraft\nHao Li1,2∗, Xue Yang2∗, Zhaokai Wang2,3∗, Xizhou Zhu4,5,\nJie Zhou4, Yu Qiao2, Xiaogang Wang1,5, Hongsheng Li1, Lewei Lu5, Jifeng Dai4,2B\n1CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong\n2OpenGVLab, Shanghai AI Laboratory\n3Shanghai Jiao Tong University\n4Tsinghua University\n5SenseTime Research\nhttps:\/\/yangxue0827.github.io\/auto_mc-reward.html\nFigure 1. Overview of our Auto MC-Reward. Auto MC-Reward consists of three key LLM-based components: Reward Designer, Reward\nCritic, and Trajectory Analyzer. A suitable dense reward function is iterated through the continuous interaction between the agent and the\nenvironment for reinforcement learning training of specific tasks, so that the model can better complete the task. An example of exploring\ndiamond ore is shown in the figure: i) Trajectory Analyzer finds that the agent dies from lava in the failed trajectory, and then gives\nsuggestion for punishment when encountering lava; ii) Reward Designer adopts the suggestion and updates the reward function; iii) The\nrevised reward function passes the review of Reward Critic, and finally the agent avoids the lava by turning left.\nAbstract\nMany\nreinforcement\nlearning\nenvironments\n(e.g.,\nMinecraft) provide only sparse rewards that indicate task\ncompletion or failure with binary values. The challenge\nin exploration efficiency in such environments makes it\ndifficult for reinforcement-learning-based agents to learn\ncomplex tasks. To address this, this paper introduces an\nadvanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically\ndesign dense reward functions, thereby enhancing the\nlearning efficiency. Auto MC-Reward consists of three im-\nportant components: Reward Designer, Reward Critic, and\nTrajectory Analyzer.\nGiven the environment information\n∗Equal contribution. This work was completed by Hao Li and Zhaokai\nWang during their internship at Shanghai Artificial Intelligence Labora-\ntory. BCorresponding author: Jifeng Dai <daijifeng@tsinghua.edu.cn>.\nand task descriptions, the Reward Designer first design the\nreward function by coding an executable Python function\nwith predefined observation inputs.\nThen, our Reward\nCritic will be responsible for verifying the code, checking\nwhether the code is self-consistent and free of syntax\nand semantic errors.\nFurther, the Trajectory Analyzer\nsummarizes possible failure causes and provides refinement\nsuggestions according to collected trajectories. In the next\nround, Reward Designer will further refine and iterate the\ndense reward function based on feedback.\nExperiments\ndemonstrate a significant improvement in the success rate\nand learning efficiency of our agents in complex tasks in\nMinecraft, such as obtaining diamond with the efficient\nability to avoid lava, and efficiently explore trees and\nanimals that are sparse in the plains biome.\narXiv:2312.09238v2  [cs.AI]  30 Mar 2024\n1. Introduction\nMinecraft, as the world’s best-selling game, offers a range\nof tasks from exploration, survival to creating. It has be-\ncome an important environment for researching efficient\nReinforcement Learning (RL) [16, 22]. In particular, its\nextreme sparsity of rewards and huge complexity of the de-\ncision space pose significant challenges for RL. Currently,\nthe most effective learning strategy involves pre-training\nthrough behavior cloning [3], using learned behavioral pri-\nors to narrow the decision space. Nevertheless, it still re-\nquires billions of environmental interactions for effective\nlearning due to the sparse reward nature of Minecraft.\nOn the other hand, previous researchers have proposed a\nvariety of dense reward signals to enable efficient learning\nfor specific sparse reward tasks [2, 27, 28, 35, 37]. However,\ntheir applicability on the complex and long-horizon tasks\nin Minecraft remains an open question. To deeply reveal\nthe challenges in Minecraft, we examine on several repre-\nsentative challenging tasks, e.g. exploring underground for\ndiamonds. We find that even after behavior cloning, most\nof these methods still fail to make significant progress on\nthese tasks, further highlighting the difficulty of Minecraft\nand the limitations of existing dense reward methods.\nIt is noteworthy that for human players, Minecraft is a\nrelatively simple casual game [12]. The advantage of hu-\nman lies in their ability to summarize based on practice. For\nexample, an accidental burning death from lava can teach\nhuman to avoid getting too close to it. Such summaries,\nbased on life experience and practice, are key to human in-\ntelligence [40, 43]. Most existing RL methods overlook this\nability. On the other hand, Large Language Models (LLMs)\nhave recently demonstrated human-like common sense and\nreasoning capabilities [18, 36, 52]. We find that leverag-\ning LLMs can help RL agents simulate the practice sum-\nmarization abilities of human. Based on the historical ac-\ntion trajectories and success-failure signals of the agents,\nLLMs can automatically design and refine corresponding\nauxiliary rewards, effectively overcoming the sparse reward\nchallenge in Minecraft.\nAccording to above analysis, we propose an automated\nmethod named Auto MC-Reward, to design and improve\nauxiliary reward functions according to task descriptions\nand historical action trajectories. This method utilizes the\ntask understanding and experience summarization abilities\nof LLMs, providing detailed and immediate rewards for\nlearning guidance. Specifically We first use LLMs to de-\nsign task-related dense reward functions based on basic de-\nscriptions of the environment and tasks, named as Reward\nDesigner. These reward functions are used to train agents\nafter self-verification, i.e. Reward Critic. To address poten-\ntial biases or oversights in LLM’s understanding, we also\npropose a LLM-based Trajectory Analyzer to analyze and\nsummarize collected trajectories from the trained agent, and\nhelp Reward Designer to improve the reward functions.\nWe verify the effectiveness of Auto MC-Reward on a se-\nries of representative benchmarks, including horizontal ex-\nploration for diamonds underground and approaching trees\nand animals in the plains biome. Experiments show that\nAuto MC-Reward achieves significantly better results on\nthese tasks compared to original sparse reward and existing\ndense reward methods, showing its advanced ability of em-\npowering efficient learning on sparse reward tasks. By iter-\natively refining the design of rewards functions, Auto MC-\nReward enables the agent to efficiently learn new behaviors\nthat is beneficial to the corresponding tasks, e.g. avoiding\nlava, which greatly improves the success rate. Moreover,\nAuto MC-Reward achieves a high diamond obtaining suc-\ncess rate (36.5%) with only raw information, demonstrating\nits ability of solving long-horizon tasks.\n2. Related Work\nMinecraft Agents are intelligent agents designed to accom-\nplish various tasks while playing the game Minecraft. Most\nof previous works adopt reinforcement learning for agent\ntraining. Due to the extremely sparse rewards and com-\nplex decision space of Minecraft tasks, early attempts have\ntried hierarchical RL [30, 34, 41, 42], curriculum learn-\ning [23], and imitation learning [1] to empower more ef-\nficient RL training. To narrow the decision space, recent\nwork [3] build a foundation model by performing imita-\ntion on YouTube videos. DreamerV3 [17] instead learns\na world model that explores the environment efficiently. As\nthe LLMs demonstrate their general planning ability, a se-\nries of research [19, 46, 47, 51, 53] leverage LLMs as high-\nlevel planners that decompose long-term complex tasks as\nbasic skills and implement the skills with RL agents or\nhandcrafted scripts.\nAuto MC-Reward aims to design dense rewards for\nMinecraft tasks automatically using LLMs, which is orthog-\nonal to previous works on Minecraft agents that mainly fo-\ncus on RL learning algorithms or high-level planning.\nEfficient Learning in Sparse Reward Tasks is a long-\nstanding challenge in RL due to the lack of effective learn-\ning signals [26]. A common solution is to handcraft dense\nreward functions that provide intermediate reward signals\nbased on human expertise, which requires time-consuming\ntrial-and-error for each environment and task. Another line\nof previous research focus on proposing general-purpose\ndense auxiliary reward functions, such as curiosity-driven\nexploration [6, 20, 28, 37], self-imitation learning [35], and\ngoal-conditioned reinforcement learning [2, 24, 27, 45].\nDespite the success on certain specific tasks, the appli-\ncability of these methods in the complex environment of\nMinecraft remains uncertain. Recent works [10, 11, 14, 31,\n32] also propose to use pre-trained models to assign reward\nto intermediate states of completing tasks. However, these\napproaches produce reward values in a black-box manner,\nwhich cannot be interpreted and improved based on the\nexperience of the agents, and the generalizability of these\nmodels on new tasks is not guaranteed.\nIn contrast, Auto MC-Reward automatically produces\nexplainable reward functions according to the task descrip-\ntions. Moreover, the reward functions can be improved to\nbe more precise based on the experience of the agent.\nAutomated Reward Function Design aims to find an op-\ntimal reward function that drives efficient reinforcement\nlearning for interested tasks. Previous works [9, 15] employ\nevolutionary algorithm for searching optimal reward func-\ntions for specific tasks. Most of these attempts have a highly\nconstrained search space that only adjusts parameters of\ntask-specific handcrafted reward templates. Recently, a se-\nries of research [7, 11, 25, 29] employs LLMs for integrat-\ning human preference into open-domain tasks without clear\ncompletion criteria by directly prompting LLMs with envi-\nronment trajectories and natural language task descriptions.\nThe reward values are generated on-the-fly by LLMs, which\nis black-box and has heavy computational cost due to the\nnature of LLMs. In contrast, Auto MC-Reward employs\nLLMs to generate white-box code-form reward functions.\nConcurrent works [33, 49, 50] also propose to use LLMs\nas a coder to generate reward functions for robotics control\ntasks. Specifically, L2R [50] needs to prepare reward func-\ntion templates and cannot cope with unexpected situations\nin open worlds. Text2Reward [49] and EUREKA [33] re-\nquire complete environment code or description and rely on\nhuman feedback, which are not available in open worlds.\nDifferent from these methods, Auto MC-Reward consid-\ners more complex Minecraft environments that has diverse\nscenarios and high uncertainty, requiring more precise and\nthorough reward designing.\n3. Method\nAuto MC-Reward consists of three components: Reward\nDesigner, Reward Critic, and Trajectory Analyzer. Given\nthe environment information and task descriptions, the Re-\nward Designer proposes the reward function by coding an\nexecutable Python function with pre-defined observation in-\nputs.\nThe Reward Critic verifies if the proposed reward\nfunction is self-consistent and if it meets the format require-\nments. The designed reward function which passes the Re-\nward Critic is used to train agents in the environment. To\nimprove the designed reward function according to empiri-\ncal experience, the Trajectory Analyzer is proposed to sum-\nmarize possible failure causes and provide refinement sug-\ngestions on the reward function based on the inference tra-\njectories of the trained agents. Then the Reward Designer\nmodifies the reward function based on the feedback from\nTrajectory Analyzer. Figure 1 shows the overview of the\nAuto MC-Reward.\n3.1. Reward Designer\nWe utilize a Reward Designer to generate the reward func-\ntion code to provide intermediate instructive learning sig-\nnals to the agent. It takes as input task descriptions, game\ninformation, and reward function requirements, generating\nreward functions in executable code form. When updating\nreward function, we also provide analysis of the agent’s per-\nformance when interacting with the game environment. The\ninput prompt is introduced in Section 4.2.\nThe generated reward function uses a pre-defined obser-\nvation format as input. This includes the nearest distance of\neach block type within the visible range in the current and\nprevious steps, changes in inventory between adjacent steps,\nhealth points, and the agent’s location in each past step.\nThese parameters can provide information on the agent’s\ncurrent and historical states, assisting the reward function\nin various situations.\nMulti-Step Memory. Long-term tasks require the transfer\nof information across multiple steps. Thus, we introduce a\nmulti-step memory mechanism. It is provided to Reward\nDesigner as a empty dictionary at the beginning, and the\nreward function can save necessary data into the memory\nto be used in future steps. In the actual reward function of\nthe explore-tree task, we observed that the agent records the\ndistance to a tree at each step, thereby encouraging getting\ncloser with the tree than the previous step.\nChain of Thought. We require the LLM to first describe its\ndesign thoughts, such as considering potential failure rea-\nsons and the details of the reward function design. These\nthoughts are to be written as comments at the beginning of\nthe code. This is a mechanism similar to Chain of Thought\n(CoT) [48], where the thought process precedes the cod-\ning implementation. In the specific code implementation,\nnecessary comments will also be generated every few lines\n(e.g., “Check if lava is in the field of view in the previous\nstep”). This approach not only allows Reward Designer to\nrefer to the text-form thoughts during reward function ini-\ntialization, but also assists subsequent Reward Critic in as-\nsessing the code’s rationality, and helps Reward Designer\nto understand the current reward function’s purpose when\nupdating the reward function.\nScale Constraints. We impose a specific scale constraint\nfor the reward function, where the LLM generates two sub-\nfunctions: dense and sparse. Sparse denotes rewards for\nachieving the final goal or heavy penalties (like death),\nwhile dense represents dense intermediate signals during\nthe task completion process. We preset their numerical val-\nues and only allow the LLM to determine their positivity\nor negativity, limiting sparse to {1, 0, −1}, and dense to\nTrajectory \nAnalyzer\nhistory_location: \n[7,48,9],[6,48,9],[7,48,9],\n[7,48,9],[6,48,9],[7,48,9] \nEncourage exploration \nby whether current \nlocation is never visited \nbefore\nPenalize for \nencountering lava\nAdd a pitch constraint to \nprevent from always \nlooking up to avoid \nseeing lava\nStep 1\ndead:  True\nnearby_blocks:  lava\n{pitch: 0, nearby_block: lava},\n{pitch: -45, nearby_block: none}\n{pitch: -45, nearby_block: none}\nTrajectory \nAnalyzer\nTrajectory \nAnalyzer\nStep 2\nStep 3\nStep 1 update\nStep 2 update\nStep 3 update\nFigure 2. Example of updating the reward function. Trajectory\nAnalyzer provides analysis for three scenarios at different steps,\nand then Reward Designer update the reward function based on\nthe suggestions. We only display part of the trajectory data for\nbrevity. Step 1: rewrite the code of encouraging exploration to\navoid going back and forth. Step 2: add lava penalty to avoid\nfalling into lava. Step 3: add pitch constraint to avoid constantly\nlooking up to avoid lava.\n{0.1, 0, −0.1}. They are then added together for the final\nreward. Therefore, the final reward values can be one value\nof {±1.1, ±1.0, ±0.9, ±0.1, 0}. The final reward is calcu-\nlated as R = sgn(sparse)∗1+sgn(dense)∗0.1, where sgn\ndenotes the sign function. This is to keep the reward within\na reasonable range, allowing the LLM to focus on various\nscenarios that need to be considered in the reward function,\nrather than trivial tasks like adjusting the reward value.\n3.2. Reward Critic\nIn practice, it is difficult for LLM to generate a relatively\ncomplete reward function in the beginning. There may be\nerrors in understanding parameter formats and data types\n(syntax errors), failure to consider game-specific informa-\ntion, or misunderstanding of tasks (semantic errors), etc.\nIn order to eliminate above errors that are not easy to\nfind, we design a LLM based Reward Critic to automati-\ncally review the designed reward function. In addition to\nchecking for syntax errors, Reward Critic is also asked to\ncheck the quality of the reward function to further eliminate\nsemantic errors. Specifically, we require Reward Critic to\ncheck whether the current code implementation matches its\nthoughts, whether it meets the reward function design re-\nquirements, and whether it takes game information into ac-\ncount. If the review fails, the Critic will provide a critique,\nand the Reward Designer will then modify the reward func-\ntion based on the criterion and submit it for review again.\nThe above process is repeated up to 3 times.\nIf an error occurs during the execution of the reward\nfunction in the process of interacting with the environment,\nthe Python traceback of the error message will be fed back\nto Reward Designer for modification. These errors may in-\nclude misunderstandings of input parameters, list index out\nof range, uninitialized keys in dictionaries, and other such\nissues. Some runtime errors only appear during the actual\nexecution of the code.\n3.3. Trajectory Analyzer\nLLMs have the ability to understand environmental infor-\nmation and task instructions through in-context prompts to\ngenerate dense rewards. However, this zero-shot approach\ncompletely relies on LLM’s understanding of the task and\nimagination of the problems it may face, and it is difficult\nto ensure the effectiveness of the designed reward. Take the\nthe yellow highlighted part in Figure 1 as an example, in the\ninitially designed reward function, Reward Designer does\nnot consider the situation where the agent would encounter\nlava and be burned to death. Thus, in order to introduce em-\npirical improvements to the designed dense reward, we pro-\npose to use LLMs, named as Trajectory Analyzer, to sum-\nmarize the historical information of the interaction between\nthe trained agent and the environment and use it to guide\nthe revision of the reward function. The division of labor\nof Reward Designer and Trajectory Analyzer allows for in-\ndependent operations of data analysis and reward function\nupdates. Trajectory Analyzer does not need to know the de-\ntails of the reward function, and Reward Designer does not\nneed to process complex trajectory data.\nSpecifically, the current trained model is used to inter-\nact with the environment and obtain K trajectories. Then,\nwe truncate these trajectories and use a LLM to summarize\nthe observations of the last consecutive L frames of each\nfailed trajectory to automatically infer its possible failure\nreasons. Based on the analysis of the reasons for the fail-\nure, the LLM Trajectory Analyzer is asked to propose key\npoints that Reward Designer needs to consider in the next\nround of reward function revision. For instance, failure sce-\nnarios where punishment is not considered, misalignment of\ndense reward and sparse reward causes the agent’s behavior\nto deviate from the final goal, etc.\nFigure 2 shows an example of multiple rounds of im-\nproving the reward function during the search for diamonds.\nIn the first step, through analysis of the trajectory, Trajec-\ntory Analyzer finds that the agent would opportunistically\nfind a shortcut to increase the reward, that is, move back\nand forth to deceive the reward function into thinking that\nthe agent is moving actively. Therefore, the Reward De-\nsigner modifies the code snippet that encourages the agent\nto move, i.e. encourage the agent to appear in unvisited lo-\ncations as much as possible. Although the initially designed\nreward function has taken into account the penalty for the\nloss of the agent’s health, the agent still cannot effectively\nlearn to avoid lava. When modifying the reward function\nin the second round, Trajectory Analyzer discovers through\nthe failed trajectory that the agent may die from lava, so it is\nsuggested that Reward Designer increase the penalty for en-\ncountering lava, as shown in the step 2 update in Figure 2.\nAccording to the interactive experience, Reward Designer\nexplicitly punishes the continuous appearance of lava in the\nfield of view. However, the excessive punishment of lava\ncaused the agent to choose to turn its perspective upward\nor downward to avoid the appearance of lava in the visible\nrange, making it impossible for the agent to continue ef-\nfective exploration, which deviates from the ultimate goal.\nTo this end, Reward Designer further constrain the agent’s\nperspective in step 3, so that the lava disappeared from the\nagent’s perspective by turning left\/right while continuing to\nsearch for diamonds, which is the desired strategy. Fig-\nure 3(a) shows the successful trajectory of avoiding lava:\nThe agent sees the lava after breaking the stone ahead using\niron pickaxe, and then turn left to avoid the lava through the\nmining tunnel.\n4. Experiment\n4.1. Environment Setup\nWe mainly use the harvest mode in the MineDojo [14] en-\nvironment to verify the model’s ability to play Minecraft.\nThe training pseudo code of Auto MC-Reward is shown in\nAlgorithm 1. We set up the following challenging tasks for\nmodel performance comparison and ablation study:\n• Exploring diamond ore\non the 11-th floor under-\nground: Initially, the agent\nis equipped with an iron\npickaxe on the 11-th floor underground. When the dia-\nmond ore is within the visible range and the distance is\nless than 2 distance units, the task is deemed completed.\nThe difficulty of the task lies in the fact that diamonds\nare very rare, lava frequently appears during exploration,\nleading to death, and the maximum number of steps is\nlimited to 60,000. When steps exceed the limit, the tra-\nAlgorithm 1 Auto MC-Reward Training Pseudo Code\nRequire: Task (T), Inital Agent (A0), Environment (Env), Max number\nof Critic reviews (NCritic)\nEnsure: Final Agent (AN), Final Reward (RN)\nSummary = None\nCritique = None\nR0 = None\nfor i = 1, . . . , N do\nRi = RewardDesigner(Summary, Critique, T, Ri−1)\nfor j = 1, . . . , NCritic do\nCritique, Done = RewardCritic(Ri)\nif Done then\nbreak\nelse\nRi = RewardDesigner(Summary, Critique, T, Ri)\nend if\nend for\nAi = TrainAgent(A0, Ri, Env, T)\nTraji, Stati = Eval(Env, Ai)\nSummary = TrajectoryAnalyzer(Traji, Stati)\nCritique = None\nend for\njectory is considered failed. Long-term exploration can\ndemonstrate the advantages of dense rewards.\n• Approaching tree\nin plains biome\n: The task is\nconsidered successful if the tree is within the agent’s\nvisible range and the distance is less than 1 distance unit.\nThe difficulty of the task lies in the fact that the trees are\nvery sparse on plains, which is extremely detrimental to\nsparse reward functions. The maximum number of steps\nis limited to 2,000 steps.\n• Approaching specific animal (e.g. cow\n, sheep\n)\nin plains biome\n: The task is considered successful\nif the animal is within the agent’s\nvisible range and\nthe distance is less than 2 distance unit. The difficulty of\nthe task is that the animals are constantly moving. The\nmaximum number of steps is limited to 2,000 steps.\n• Obtaining diamond\n: The agent\nneeds to complete\nthe whole process of mining diamonds, including key be-\nhaviors such as finding and obtaining materials on the sur-\nface, crafting, digging down, going back to the ground,\nand mining stone\/iron ore\/diamond ore. The tech tree is\nshown in Figure 4.\n4.2. Implementation Details\nLLM Prompt. The components of the input prompts for\nTrajectory Analyzer include task description, game infor-\nmation, statistical metrics, and information on failed trajec-\ntories. Components of the input prompts for Reward De-\nsigner and Reward Critic includes task description, game\ninformation, input parameters, and reward function require-\nments and format.\nWe use GPT-4 [36] for all the LLM\ncomponents, and set temperature to 0.3. Since the LLMs\nare only used once for each whole agent training instead of\neach action, their computation overhead is negligible.\nFigure 3. The trajectories of the new behaviors. (a) Avoid lava\nwhen exploring for diamond ore\n. (b) Attack cow\nin plains.\n• Instruction: Instructions on initializing, updating and\nhandling execution error of reward function for Reward\nDesigner, reviewing function for Reward Critic, and ana-\nlyzing trajectory for Trajectory Analyzer.\n• Task description: The objective, initial conditions, suc-\ncess criteria, and task flow. For example, for the explore\ndiamond task, the objective is “to find and approach a\ndiamond, achieving a high success rate while avoiding\ndeath.” The initial condition is “agent at y level 11 with\nan iron pickaxe.” The success criteria is “being less than 1\nmeter from the nearest diamond block”, and the task flow\nis “horizontally explore to find a diamond, face it, and\napproach it”. In the task description, we do not provide\nprior game strategy information (task challenges, DFS ex-\nploration strategies, or avoiding lava, etc.) to ensure the\nmethod’s versatility.\n• Game information: Game version, block names, field\nof view, action space, and units of measurement. Game\ninformation provides knowledge about the game’s simu-\nlation environment, not game strategy.\n• Statistical metrics and information on failed trajec-\ntories: success rates, and actions sequences, reward se-\nquences, final inventory and nearby blocks of K = 10\nfailed trajectories. If a trajectory exceeds L = 32 steps, it\nis truncated to the last 32 steps.\n• Input parameters: The nearest distance of each block\ntype within the visible range in the current and previous\nsteps, changes in inventory between adjacent steps, health\npoints, and the agent’s location in each past step. The\nmemory is also provided as an input parameter for storing\ninformation to monitor changes across different steps. We\nprovide explanation and examples of the parameters in the\ninput prompt.\n• Reward function requirements and format: We require\nthe Designer to write a dense function and a sparse func-\ntion, and consider only the sign of the two functions’ re-\nturn values, not the magnitude. The detail of the scale\nconstraints is in Section 3.1.\nImitation Learning Details. When large labeled datasets\ndo not exist, the canonical strategy for training capable\nagents is RL, which is inefficient and expensive to sam-\nple for hard-exploration problems [3, 4, 21], e.g. mining\ndiamond in Minecraft. Therefore, in order to more effi-\nciently explore the effectiveness of the LLM-based reward\nfunction design mechanism proposed in this paper, we pre-\ntrained some foundation models through imitation learning\nas done by VPT [3]. Specifically, we use GITM [53] to\ncontinuously perform Diamond Mining task and record im-\nportant observation data of each frame, such as RGB, ac-\ntion, inventory, GPS, compass, structured actions, etc. In\nthe end, we collect about 11 million image data, totaling\nabout 153 hours (the control frequency is 20 Hz) of game\nvideos. Subsequently, we train these data through fully su-\npervised learning by using Impala CNN [13] and Trans-\nformer [44] as backbone, and obtained several foundation\nmodels. The main differences between the foundation mod-\nels are different biomes (forest and plains), temporal frames\n(16 and 128), and whether goal embedding is used. In sub-\nsequent experiments, these foundation models were used in\ntwo different purposes:\n• Give the RL model preliminary basic Minecraft gameplay\ncapabilities, e.g. forward\/back, turn left\/right, attack, etc.\nFor some tasks that have not been learned (e.g. approach-\ning cows in Figure 3(b)) or not learned well (e.g. avoid\nlava in Figure 3(a), explore tree on plains) in imitation\nlearning, RL algorithms can be studied more efficiently.\n• In the Diamond Mining task, the diamond collection suc-\ncess rate, lava escape rate, death rate, etc. between the RL\nmodel and the imitation learning model are compared to\ndemonstrate the superiority of the proposed method.\nRL Training Details. We use proximal policy optimization\n(PPO) algorithm [39] with generalized advantage estima-\ntion (GAE) [38] to train our RL model. We use γ = 0.99\nand λ = 0.95 for all of our experiments, and the total train-\ning frames is 256,000. To prevent catastrophically forget-\nting or overly aggressive policy update during RL training,\nwe follow VPT [3] to apply an auxiliary Kullback-Leibler\n(KL) divergence loss between the RL model and the frozen\npre-trained policy. We also normalize the reward based on\nthe trajectory returns to constrain the gradient scales of dif-\nferent tasks. See Appendix for details.\n4.3. Main Results\nBaselines. We compare our Auto MC-Reward against the\nfollowing methods:\n• Naive Handcraft: The agent keeps moving (and mining\nfor diamond exploring task) in one direction with a small\nprobability of turning left\/right.\n• Imitation Learning: Our foundation model pre-trained\nwith GITM-generated data, as introduced in Section 4.2.\n• RL with Sparse Reward: Use only the reward from the\noriginal environment, i.e. only receives a reward when\nTable 1. Comparison with other reward methods on three Minecraft tasks. Max steps for exploring tree\nand cow\nare set to 2000.\n†Sparse reward receives a low death rate because it is often stuck in the same place or move in a small area without encountering lava\n.\nMethod\nReward\nExplore Diamond Ore\nUnderground\nApproach Tree\non Plains\nApproach Cow\non Plains\navg. dist. ↑\ndeath (%) ↓\nlava escape (%) ↑\nsucc. (%) ↑\navg. step ↓\nsucc. (%) ↑\navg. step ↓\nsucc. (%) ↑\nNaive Handcraft\n-\n85.7\n74.3\n1.5\n18.6\n1993\n2.1\n1956\n10.8\nImitation Learning\n-\n102.2\n55.6\n46.8\n38.9\n1988\n2.5\n1772\n22.4\nRL\nSparse\n16.8\n1.5†\n0\n0.5\n1936\n4.3\n1854\n12.6\nRL\nDense (Curiosity)\n102.6\n55.1\n46.0\n39.3\n1672\n45.8\n1477\n13.7\nRL\nDense (Self-Imitation)\n104.0\n54.8\n47.2\n39.7\n1532\n42.5\n1280\n23.5\nRL\nDense (MineCLIP)\n105.9\n54.0\n47.8\n40.5\n1022\n65.6\n1206\n44.9\nOurs\nDense (LLM)\n142.8\n45.2\n70.0\n45.2\n972\n73.4\n1134\n56.3\nTable 2. Comparison with previous methods on success rates of\nobtaining diamond\n. We list observations that are used in the\ninference phase. Auto MC-Reward achieves a remarkable success\nrate without exploiting unfair information (i.e. Lidar and Voxel)\nduring inference.\nMethod\nController\nObservation\nDiamond\nSucc. (%)\nHuman [3]\n-\n-\n50.0\nDreamerV3 [17]\nRL\nRGB, Status\n0.01\nDEPS [47]\nIL\nRGB, Status, Voxel\n0.6\nVPT [3]\nIL + RL\nRGB\n20.0\nGITM [53]\nHandcraft\nLidar, Voxel, Status\n55.0\nOurs\nIL (GITM-guided)\nRGB, GPS\n28.8\nOurs\nIL + RL\nRGB, GPS\n36.5\nTable 3. Ablations on Reward Critic and Trajectory Analyzer for\nexplore diamond ore\ntask. The first row corresponds to using\nthe sparse reward from the original environment. †Sparse reward\nreceives a low death rate because it is often stuck in the same place\nor move in a small area without encountering lava\n.\nDesigner\nCritic\nAnalyzer\nAvg. Dist. ↑\nDeath ↓\nLava Esc. ↑\nSucc. ↑\n16.8\n1.5†\n0\n0.5\n✓\n75.8\n58.2\n30.4\n35.1\n✓\n✓\n95.2\n49.3\n40.7\n40.5\n✓\n✓\n130.6\n47.8\n64.8\n43.1\n✓\n✓\n✓\n142.8\n45.2\n70.0\n45.2\nthe success criteria is completed.\n• RL with Curiosity Dense Reward [37]: Encourage the\nagent to discover and learn about parts of the environment\nthat it has not encountered before.\n• RL with Self-Imitation Dense Reward [35]: Encourage\nthe agent to replicate its past actions that led to high re-\nwards.\n• RL\nwith\nMineCLIP\n[14]\nDense\nReward:\nUse\nMineCLIP to calculate the dense reward based on the sim-\nilarity between RGB frames and task objectives.\nResults on Diamond Ore\nExploring Task. For the plain\nimitation learning model, fitting the training data makes it\nlack the awareness of avoiding lava, so it often dies in lava\nduring the search for diamonds, and only has 38.9% suc-\ncess rate under the limit of 60,000 steps, as shown in Ta-\nble 1. In contrast, our Auto MC-Reward makes the agent\nrealize the importance of avoiding lava by continuously im-\nFigure 4. The tech tree of obtaining diamond. The green squares\nare tasks to be optimized with Auto MC-Reward, i.e. obtaining\nlog\n, cobblestone\n, iron ore\nand diamond\n.\nproving the dense reward function, and the final success rate\nhas increased to 45.2% with 70% lava escape success rate.\nFigure 3(a) demonstrates good awareness of avoiding lava.\nBased on the same reinforcement learning algorithm, the\ndisadvantages of sparse reward functions in long-horizon\ntasks are undoubtedly revealed. By watching the videos of\ncollected trajectories, we find that using sparse functions\noften leads to irreversible behavior, such as being unable\nto break the surrounding ores to move due to maintaining\na head-up posture. Although a low death rate of 1.5% is\nachieved, the actual average moving distance is only 16.8,\nand the success rate is only 0.5%. Due to the similar scenes\nunderground, MineCLIP cannot give differentiated rewards,\nso its performance is close to the initial imitation learning\nmodel.\nOther baselines, like curiosity and self-imitation\ndense reward, also have mediocre performance and the suc-\ncess rate has not been significantly improved.\nResults on Tree\nApproaching Task. Since trees are ex-\ntremely sparse on the plain, the imitation learning model\nand the RL model with sparse reward cannot perform well,\nwith only 2.5% and 4.3% success rates respectively, and\ntheir average action steps are close to the maximum limit.\nMineCLIP dense reward receives a success rate of 65.6%\nsince it can provide positive reward when tree is visible.\nCuriosity and self-imitation methods also achieve better re-\nsults than imitation learning. For Auto MC-Reward, Re-\nward Designer uses a strategy of giving positive rewards for\ngetting closer and deducting rewards for going away, so that\nthe agent learns to slowly approach the target, ultimately\nachieving 73.4% success rate with only 972 average steps.\nResults on Cow\nApproaching Task. The task of explor-\ning for cows does not appear in the training data of imitation\nlearning, so the zero-shot ability on this task is not ideal,\nwith about 22.4% success rate and average steps close to\nthe maximum limit. By checking the videos, we find most\nof the successful cases are due to good luck without inten-\ntion to actively approach the target. The same experimental\nconclusion is also obtained in the experiment of sparse re-\nward function. Similar to the Tree Approaching Task, the\nsuperior dense reward function design mechanism makes\nour agent 43.7% (56.3% vs. 12.6%) higher than sparse re-\nward, as listed in Table 1. Another dense reward MineCLIP\nalso shows strong performance in this task, but due to the\nneed to calculate the similarity of images and texts at all\ntimes during training, the efficiency is unacceptable.\nResults on Obtaining Diamond\n. We verify the pro-\nposed method on a more difficult task, that is, the tech tree\nof collecting diamonds, as shown in Figure 4. As mentioned\nbefore, our foundation imitation learning model already has\na certain ability from birth to diamond mining. We use the\nproposed method to optimize several key tasks in the pro-\ncess to increase the success rate of final diamond acquisi-\ntion. The green parts in Figure 4 are the tasks that need\nto be optimized, i.e. obtaining log, cobblestone, iron ore\nand diamond. We conduct experiments in two biomes in\nMinecraft, and the cumulative success rate is shown in Fig-\nure 5. Specifically, the lower death rate allows our agent\nto have a higher success rate in mining iron ore and dia-\nmond, and ultimately achieves 36.5% success rate on for-\nest biome, which is 7.7% higher than the imitation learning\nmodel. As for plains, the difficulty of obtaining log makes\nthe imitation learning model unable to complete any tasks.\nAuto MC-Reward overcomes the difficulty of obtaining log,\nthus achieving a 28.1% success rate in obtaining diamonds.\nTable 2 provides a rough comparison of several different\nmethods on the task of mining diamonds. We achieve a\nhigh success rate without exploiting unfair information (i.e.\nLidar and Voxel) during the inference phase.\n4.4. Ablation studies\nEffectiveness of Reward Designer. The first row of Table 3\nis an RL experiment with a sparse reward function. As men-\ntioned before, it cannot explore diamonds normally. After\nadding Reward Designer, it regained the ability to explore\nunder a dense reward function.\nEffectiveness of Reward Critic.\nAs listed in Table 3,\nthe success rate of exploring diamonds has increased from\n35.1% to 40.5% by adding Reward Critic, because it can\nreduce the syntax and semantic errors in the code, making\nthe training process more effective and sufficient. For exam-\nple, the Trajectory Analyzer concludes that the agent died in\nlava and asks the Reward Designer to add relevant penalties.\nHowever, without being checked by Critic for semantic er-\nrors, it is possible that the added code snippet uses the word\n“magma” instead of the correct one “lava”. This will result\nFigure 5. Cumulative success rates for 4 key items of obtaining\ndiamond on forest\nand plains\n. In terms of diamond, the per-\nformance comparison between imitation learning and Auto MC-\nReward in two biomes are: 28.8% vs. 36.5%, and 0% vs. 28.1%.\nin insufficient learning of lava avoidance, which is reflected\nin a 2.1% (43.1% vs. 45.2%) success rate difference.\nEffectiveness of Trajectory Analyzer. As observed in Ta-\nble 3, Trajectory Analyzer is the key to improve the suc-\ncess rate of completing tasks. It summarizes the reasons for\nfailure to be fed into Reward Designer, allowing it to itera-\ntively modify an appropriate dense reward function to guide\nthe agent to overcome difficulties. In terms of Diamond Ex-\nploring Task, Trajectory Analyzer provides timely feedback\non the potential risks of lava, which greatly improves the\nsurvival rate and moving distance, ultimately improving the\nsuccess rate from 40.5% to 45.2%.\n5. Conclusion\nWe proposed Auto MC-Reward, an automated dense re-\nward design framework for addressing challenges caused by\nsparse reward and complex environment of Minecraft. It ad-\ndresses the issue of sparse rewards by leveraging LLMs to\nautomatically generate dense reward functions, enhancing\nlearning efficiency. The system consists of three key com-\nponents: Reward Designer, Reward Critic, and Trajectory\nAnalyzer, which are used for the design, verification and\nanalysis of the reward function respectively. Its capabilities\nare validated through experiments, demonstrating a remark-\nable improvement in complex tasks in Minecraft. Future\nwork may deal with the limited trajectory length for analysis\n(last 32 frames) due to the context length of LLMs, which\nhinders the analysis of long-term failures (e.g., not explor-\ning new areas, circling around lava).\nAuto MC-Reward\nhumbly contributes to more effective learning in complex\ntasks through its automated dense reward function design.\nWe hope it can pave the way for further research in rein-\nforcement learning and its real-world applications.\nAcknowledgement\nThis work is supported by the National Key R&D Program\nof China (NO. 2022ZD0161300, NO. 2022ZD0160100),\nby the National Natural Science Foundation of China\n(62376134).","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nAuto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft\n```\n#### 2. 论文摘要\n```\nMany reinforcement learning environments (e.g., Minecraft) provide only\nsparse rewards that indicate task completion or failure with binary values. The\nchallenge in exploration efficiency in such environments makes it difficult for\nreinforcement-learning-based agents to learn complex tasks. To address this,\nthis paper introduces an advanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically design dense reward\nfunctions, thereby enhancing the learning efficiency. Auto MC-Reward consists\nof three important components: Reward Designer, Reward Critic, and Trajectory\nAnalyzer. Given the environment information and task descriptions, the Reward\nDesigner first design the reward function by coding an executable Python\nfunction with predefined observation inputs. Then, our Reward Critic will be\nresponsible for verifying the code, checking whether the code is\nself-consistent and free of syntax and semantic errors. Further, the Trajectory\nAnalyzer summarizes possible failure causes and provides refinement suggestions\naccording to collected trajectories. In the next round, Reward Designer will\nfurther refine and iterate the dense reward function based on feedback.\nExperiments demonstrate a significant improvement in the success rate and\nlearning efficiency of our agents in complex tasks in Minecraft, such as\nobtaining diamond with the efficient ability to avoid lava, and efficiently\nexplore trees and animals that are sparse in the plains biome.\n```\n\n#### 3. 论文全文\n```\nAuto MC-Reward: Automated Dense Reward Design with\nLarge Language Models for Minecraft\nHao Li1,2∗, Xue Yang2∗, Zhaokai Wang2,3∗, Xizhou Zhu4,5,\nJie Zhou4, Yu Qiao2, Xiaogang Wang1,5, Hongsheng Li1, Lewei Lu5, Jifeng Dai4,2B\n1CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong\n2OpenGVLab, Shanghai AI Laboratory\n3Shanghai Jiao Tong University\n4Tsinghua University\n5SenseTime Research\nhttps:\/\/yangxue0827.github.io\/auto_mc-reward.html\nFigure 1. Overview of our Auto MC-Reward. Auto MC-Reward consists of three key LLM-based components: Reward Designer, Reward\nCritic, and Trajectory Analyzer. A suitable dense reward function is iterated through the continuous interaction between the agent and the\nenvironment for reinforcement learning training of specific tasks, so that the model can better complete the task. An example of exploring\ndiamond ore is shown in the figure: i) Trajectory Analyzer finds that the agent dies from lava in the failed trajectory, and then gives\nsuggestion for punishment when encountering lava; ii) Reward Designer adopts the suggestion and updates the reward function; iii) The\nrevised reward function passes the review of Reward Critic, and finally the agent avoids the lava by turning left.\nAbstract\nMany\nreinforcement\nlearning\nenvironments\n(e.g.,\nMinecraft) provide only sparse rewards that indicate task\ncompletion or failure with binary values. The challenge\nin exploration efficiency in such environments makes it\ndifficult for reinforcement-learning-based agents to learn\ncomplex tasks. To address this, this paper introduces an\nadvanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically\ndesign dense reward functions, thereby enhancing the\nlearning efficiency. Auto MC-Reward consists of three im-\nportant components: Reward Designer, Reward Critic, and\nTrajectory Analyzer.\nGiven the environment information\n∗Equal contribution. This work was completed by Hao Li and Zhaokai\nWang during their internship at Shanghai Artificial Intelligence Labora-\ntory. BCorresponding author: Jifeng Dai <daijifeng@tsinghua.edu.cn>.\nand task descriptions, the Reward Designer first design the\nreward function by coding an executable Python function\nwith predefined observation inputs.\nThen, our Reward\nCritic will be responsible for verifying the code, checking\nwhether the code is self-consistent and free of syntax\nand semantic errors.\nFurther, the Trajectory Analyzer\nsummarizes possible failure causes and provides refinement\nsuggestions according to collected trajectories. In the next\nround, Reward Designer will further refine and iterate the\ndense reward function based on feedback.\nExperiments\ndemonstrate a significant improvement in the success rate\nand learning efficiency of our agents in complex tasks in\nMinecraft, such as obtaining diamond with the efficient\nability to avoid lava, and efficiently explore trees and\nanimals that are sparse in the plains biome.\narXiv:2312.09238v2  [cs.AI]  30 Mar 2024\n1. Introduction\nMinecraft, as the world’s best-selling game, offers a range\nof tasks from exploration, survival to creating. It has be-\ncome an important environment for researching efficient\nReinforcement Learning (RL) [16, 22]. In particular, its\nextreme sparsity of rewards and huge complexity of the de-\ncision space pose significant challenges for RL. Currently,\nthe most effective learning strategy involves pre-training\nthrough behavior cloning [3], using learned behavioral pri-\nors to narrow the decision space. Nevertheless, it still re-\nquires billions of environmental interactions for effective\nlearning due to the sparse reward nature of Minecraft.\nOn the other hand, previous researchers have proposed a\nvariety of dense reward signals to enable efficient learning\nfor specific sparse reward tasks [2, 27, 28, 35, 37]. However,\ntheir applicability on the complex and long-horizon tasks\nin Minecraft remains an open question. To deeply reveal\nthe challenges in Minecraft, we examine on several repre-\nsentative challenging tasks, e.g. exploring underground for\ndiamonds. We find that even after behavior cloning, most\nof these methods still fail to make significant progress on\nthese tasks, further highlighting the difficulty of Minecraft\nand the limitations of existing dense reward methods.\nIt is noteworthy that for human players, Minecraft is a\nrelatively simple casual game [12]. The advantage of hu-\nman lies in their ability to summarize based on practice. For\nexample, an accidental burning death from lava can teach\nhuman to avoid getting too close to it. Such summaries,\nbased on life experience and practice, are key to human in-\ntelligence [40, 43]. Most existing RL methods overlook this\nability. On the other hand, Large Language Models (LLMs)\nhave recently demonstrated human-like common sense and\nreasoning capabilities [18, 36, 52]. We find that leverag-\ning LLMs can help RL agents simulate the practice sum-\nmarization abilities of human. Based on the historical ac-\ntion trajectories and success-failure signals of the agents,\nLLMs can automatically design and refine corresponding\nauxiliary rewards, effectively overcoming the sparse reward\nchallenge in Minecraft.\nAccording to above analysis, we propose an automated\nmethod named Auto MC-Reward, to design and improve\nauxiliary reward functions according to task descriptions\nand historical action trajectories. This method utilizes the\ntask understanding and experience summarization abilities\nof LLMs, providing detailed and immediate rewards for\nlearning guidance. Specifically We first use LLMs to de-\nsign task-related dense reward functions based on basic de-\nscriptions of the environment and tasks, named as Reward\nDesigner. These reward functions are used to train agents\nafter self-verification, i.e. Reward Critic. To address poten-\ntial biases or oversights in LLM’s understanding, we also\npropose a LLM-based Trajectory Analyzer to analyze and\nsummarize collected trajectories from the trained agent, and\nhelp Reward Designer to improve the reward functions.\nWe verify the effectiveness of Auto MC-Reward on a se-\nries of representative benchmarks, including horizontal ex-\nploration for diamonds underground and approaching trees\nand animals in the plains biome. Experiments show that\nAuto MC-Reward achieves significantly better results on\nthese tasks compared to original sparse reward and existing\ndense reward methods, showing its advanced ability of em-\npowering efficient learning on sparse reward tasks. By iter-\natively refining the design of rewards functions, Auto MC-\nReward enables the agent to efficiently learn new behaviors\nthat is beneficial to the corresponding tasks, e.g. avoiding\nlava, which greatly improves the success rate. Moreover,\nAuto MC-Reward achieves a high diamond obtaining suc-\ncess rate (36.5%) with only raw information, demonstrating\nits ability of solving long-horizon tasks.\n2. Related Work\nMinecraft Agents are intelligent agents designed to accom-\nplish various tasks while playing the game Minecraft. Most\nof previous works adopt reinforcement learning for agent\ntraining. Due to the extremely sparse rewards and com-\nplex decision space of Minecraft tasks, early attempts have\ntried hierarchical RL [30, 34, 41, 42], curriculum learn-\ning [23], and imitation learning [1] to empower more ef-\nficient RL training. To narrow the decision space, recent\nwork [3] build a foundation model by performing imita-\ntion on YouTube videos. DreamerV3 [17] instead learns\na world model that explores the environment efficiently. As\nthe LLMs demonstrate their general planning ability, a se-\nries of research [19, 46, 47, 51, 53] leverage LLMs as high-\nlevel planners that decompose long-term complex tasks as\nbasic skills and implement the skills with RL agents or\nhandcrafted scripts.\nAuto MC-Reward aims to design dense rewards for\nMinecraft tasks automatically using LLMs, which is orthog-\nonal to previous works on Minecraft agents that mainly fo-\ncus on RL learning algorithms or high-level planning.\nEfficient Learning in Sparse Reward Tasks is a long-\nstanding challenge in RL due to the lack of effective learn-\ning signals [26]. A common solution is to handcraft dense\nreward functions that provide intermediate reward signals\nbased on human expertise, which requires time-consuming\ntrial-and-error for each environment and task. Another line\nof previous research focus on proposing general-purpose\ndense auxiliary reward functions, such as curiosity-driven\nexploration [6, 20, 28, 37], self-imitation learning [35], and\ngoal-conditioned reinforcement learning [2, 24, 27, 45].\nDespite the success on certain specific tasks, the appli-\ncability of these methods in the complex environment of\nMinecraft remains uncertain. Recent works [10, 11, 14, 31,\n32] also propose to use pre-trained models to assign reward\nto intermediate states of completing tasks. However, these\napproaches produce reward values in a black-box manner,\nwhich cannot be interpreted and improved based on the\nexperience of the agents, and the generalizability of these\nmodels on new tasks is not guaranteed.\nIn contrast, Auto MC-Reward automatically produces\nexplainable reward functions according to the task descrip-\ntions. Moreover, the reward functions can be improved to\nbe more precise based on the experience of the agent.\nAutomated Reward Function Design aims to find an op-\ntimal reward function that drives efficient reinforcement\nlearning for interested tasks. Previous works [9, 15] employ\nevolutionary algorithm for searching optimal reward func-\ntions for specific tasks. Most of these attempts have a highly\nconstrained search space that only adjusts parameters of\ntask-specific handcrafted reward templates. Recently, a se-\nries of research [7, 11, 25, 29] employs LLMs for integrat-\ning human preference into open-domain tasks without clear\ncompletion criteria by directly prompting LLMs with envi-\nronment trajectories and natural language task descriptions.\nThe reward values are generated on-the-fly by LLMs, which\nis black-box and has heavy computational cost due to the\nnature of LLMs. In contrast, Auto MC-Reward employs\nLLMs to generate white-box code-form reward functions.\nConcurrent works [33, 49, 50] also propose to use LLMs\nas a coder to generate reward functions for robotics control\ntasks. Specifically, L2R [50] needs to prepare reward func-\ntion templates and cannot cope with unexpected situations\nin open worlds. Text2Reward [49] and EUREKA [33] re-\nquire complete environment code or description and rely on\nhuman feedback, which are not available in open worlds.\nDifferent from these methods, Auto MC-Reward consid-\ners more complex Minecraft environments that has diverse\nscenarios and high uncertainty, requiring more precise and\nthorough reward designing.\n3. Method\nAuto MC-Reward consists of three components: Reward\nDesigner, Reward Critic, and Trajectory Analyzer. Given\nthe environment information and task descriptions, the Re-\nward Designer proposes the reward function by coding an\nexecutable Python function with pre-defined observation in-\nputs.\nThe Reward Critic verifies if the proposed reward\nfunction is self-consistent and if it meets the format require-\nments. The designed reward function which passes the Re-\nward Critic is used to train agents in the environment. To\nimprove the designed reward function according to empiri-\ncal experience, the Trajectory Analyzer is proposed to sum-\nmarize possible failure causes and provide refinement sug-\ngestions on the reward function based on the inference tra-\njectories of the trained agents. Then the Reward Designer\nmodifies the reward function based on the feedback from\nTrajectory Analyzer. Figure 1 shows the overview of the\nAuto MC-Reward.\n3.1. Reward Designer\nWe utilize a Reward Designer to generate the reward func-\ntion code to provide intermediate instructive learning sig-\nnals to the agent. It takes as input task descriptions, game\ninformation, and reward function requirements, generating\nreward functions in executable code form. When updating\nreward function, we also provide analysis of the agent’s per-\nformance when interacting with the game environment. The\ninput prompt is introduced in Section 4.2.\nThe generated reward function uses a pre-defined obser-\nvation format as input. This includes the nearest distance of\neach block type within the visible range in the current and\nprevious steps, changes in inventory between adjacent steps,\nhealth points, and the agent’s location in each past step.\nThese parameters can provide information on the agent’s\ncurrent and historical states, assisting the reward function\nin various situations.\nMulti-Step Memory. Long-term tasks require the transfer\nof information across multiple steps. Thus, we introduce a\nmulti-step memory mechanism. It is provided to Reward\nDesigner as a empty dictionary at the beginning, and the\nreward function can save necessary data into the memory\nto be used in future steps. In the actual reward function of\nthe explore-tree task, we observed that the agent records the\ndistance to a tree at each step, thereby encouraging getting\ncloser with the tree than the previous step.\nChain of Thought. We require the LLM to first describe its\ndesign thoughts, such as considering potential failure rea-\nsons and the details of the reward function design. These\nthoughts are to be written as comments at the beginning of\nthe code. This is a mechanism similar to Chain of Thought\n(CoT) [48], where the thought process precedes the cod-\ning implementation. In the specific code implementation,\nnecessary comments will also be generated every few lines\n(e.g., “Check if lava is in the field of view in the previous\nstep”). This approach not only allows Reward Designer to\nrefer to the text-form thoughts during reward function ini-\ntialization, but also assists subsequent Reward Critic in as-\nsessing the code’s rationality, and helps Reward Designer\nto understand the current reward function’s purpose when\nupdating the reward function.\nScale Constraints. We impose a specific scale constraint\nfor the reward function, where the LLM generates two sub-\nfunctions: dense and sparse. Sparse denotes rewards for\nachieving the final goal or heavy penalties (like death),\nwhile dense represents dense intermediate signals during\nthe task completion process. We preset their numerical val-\nues and only allow the LLM to determine their positivity\nor negativity, limiting sparse to {1, 0, −1}, and dense to\nTrajectory \nAnalyzer\nhistory_location: \n[7,48,9],[6,48,9],[7,48,9],\n[7,48,9],[6,48,9],[7,48,9] \nEncourage exploration \nby whether current \nlocation is never visited \nbefore\nPenalize for \nencountering lava\nAdd a pitch constraint to \nprevent from always \nlooking up to avoid \nseeing lava\nStep 1\ndead:  True\nnearby_blocks:  lava\n{pitch: 0, nearby_block: lava},\n{pitch: -45, nearby_block: none}\n{pitch: -45, nearby_block: none}\nTrajectory \nAnalyzer\nTrajectory \nAnalyzer\nStep 2\nStep 3\nStep 1 update\nStep 2 update\nStep 3 update\nFigure 2. Example of updating the reward function. Trajectory\nAnalyzer provides analysis for three scenarios at different steps,\nand then Reward Designer update the reward function based on\nthe suggestions. We only display part of the trajectory data for\nbrevity. Step 1: rewrite the code of encouraging exploration to\navoid going back and forth. Step 2: add lava penalty to avoid\nfalling into lava. Step 3: add pitch constraint to avoid constantly\nlooking up to avoid lava.\n{0.1, 0, −0.1}. They are then added together for the final\nreward. Therefore, the final reward values can be one value\nof {±1.1, ±1.0, ±0.9, ±0.1, 0}. The final reward is calcu-\nlated as R = sgn(sparse)∗1+sgn(dense)∗0.1, where sgn\ndenotes the sign function. This is to keep the reward within\na reasonable range, allowing the LLM to focus on various\nscenarios that need to be considered in the reward function,\nrather than trivial tasks like adjusting the reward value.\n3.2. Reward Critic\nIn practice, it is difficult for LLM to generate a relatively\ncomplete reward function in the beginning. There may be\nerrors in understanding parameter formats and data types\n(syntax errors), failure to consider game-specific informa-\ntion, or misunderstanding of tasks (semantic errors), etc.\nIn order to eliminate above errors that are not easy to\nfind, we design a LLM based Reward Critic to automati-\ncally review the designed reward function. In addition to\nchecking for syntax errors, Reward Critic is also asked to\ncheck the quality of the reward function to further eliminate\nsemantic errors. Specifically, we require Reward Critic to\ncheck whether the current code implementation matches its\nthoughts, whether it meets the reward function design re-\nquirements, and whether it takes game information into ac-\ncount. If the review fails, the Critic will provide a critique,\nand the Reward Designer will then modify the reward func-\ntion based on the criterion and submit it for review again.\nThe above process is repeated up to 3 times.\nIf an error occurs during the execution of the reward\nfunction in the process of interacting with the environment,\nthe Python traceback of the error message will be fed back\nto Reward Designer for modification. These errors may in-\nclude misunderstandings of input parameters, list index out\nof range, uninitialized keys in dictionaries, and other such\nissues. Some runtime errors only appear during the actual\nexecution of the code.\n3.3. Trajectory Analyzer\nLLMs have the ability to understand environmental infor-\nmation and task instructions through in-context prompts to\ngenerate dense rewards. However, this zero-shot approach\ncompletely relies on LLM’s understanding of the task and\nimagination of the problems it may face, and it is difficult\nto ensure the effectiveness of the designed reward. Take the\nthe yellow highlighted part in Figure 1 as an example, in the\ninitially designed reward function, Reward Designer does\nnot consider the situation where the agent would encounter\nlava and be burned to death. Thus, in order to introduce em-\npirical improvements to the designed dense reward, we pro-\npose to use LLMs, named as Trajectory Analyzer, to sum-\nmarize the historical information of the interaction between\nthe trained agent and the environment and use it to guide\nthe revision of the reward function. The division of labor\nof Reward Designer and Trajectory Analyzer allows for in-\ndependent operations of data analysis and reward function\nupdates. Trajectory Analyzer does not need to know the de-\ntails of the reward function, and Reward Designer does not\nneed to process complex trajectory data.\nSpecifically, the current trained model is used to inter-\nact with the environment and obtain K trajectories. Then,\nwe truncate these trajectories and use a LLM to summarize\nthe observations of the last consecutive L frames of each\nfailed trajectory to automatically infer its possible failure\nreasons. Based on the analysis of the reasons for the fail-\nure, the LLM Trajectory Analyzer is asked to propose key\npoints that Reward Designer needs to consider in the next\nround of reward function revision. For instance, failure sce-\nnarios where punishment is not considered, misalignment of\ndense reward and sparse reward causes the agent’s behavior\nto deviate from the final goal, etc.\nFigure 2 shows an example of multiple rounds of im-\nproving the reward function during the search for diamonds.\nIn the first step, through analysis of the trajectory, Trajec-\ntory Analyzer finds that the agent would opportunistically\nfind a shortcut to increase the reward, that is, move back\nand forth to deceive the reward function into thinking that\nthe agent is moving actively. Therefore, the Reward De-\nsigner modifies the code snippet that encourages the agent\nto move, i.e. encourage the agent to appear in unvisited lo-\ncations as much as possible. Although the initially designed\nreward function has taken into account the penalty for the\nloss of the agent’s health, the agent still cannot effectively\nlearn to avoid lava. When modifying the reward function\nin the second round, Trajectory Analyzer discovers through\nthe failed trajectory that the agent may die from lava, so it is\nsuggested that Reward Designer increase the penalty for en-\ncountering lava, as shown in the step 2 update in Figure 2.\nAccording to the interactive experience, Reward Designer\nexplicitly punishes the continuous appearance of lava in the\nfield of view. However, the excessive punishment of lava\ncaused the agent to choose to turn its perspective upward\nor downward to avoid the appearance of lava in the visible\nrange, making it impossible for the agent to continue ef-\nfective exploration, which deviates from the ultimate goal.\nTo this end, Reward Designer further constrain the agent’s\nperspective in step 3, so that the lava disappeared from the\nagent’s perspective by turning left\/right while continuing to\nsearch for diamonds, which is the desired strategy. Fig-\nure 3(a) shows the successful trajectory of avoiding lava:\nThe agent sees the lava after breaking the stone ahead using\niron pickaxe, and then turn left to avoid the lava through the\nmining tunnel.\n4. Experiment\n4.1. Environment Setup\nWe mainly use the harvest mode in the MineDojo [14] en-\nvironment to verify the model’s ability to play Minecraft.\nThe training pseudo code of Auto MC-Reward is shown in\nAlgorithm 1. We set up the following challenging tasks for\nmodel performance comparison and ablation study:\n• Exploring diamond ore\non the 11-th floor under-\nground: Initially, the agent\nis equipped with an iron\npickaxe on the 11-th floor underground. When the dia-\nmond ore is within the visible range and the distance is\nless than 2 distance units, the task is deemed completed.\nThe difficulty of the task lies in the fact that diamonds\nare very rare, lava frequently appears during exploration,\nleading to death, and the maximum number of steps is\nlimited to 60,000. When steps exceed the limit, the tra-\nAlgorithm 1 Auto MC-Reward Training Pseudo Code\nRequire: Task (T), Inital Agent (A0), Environment (Env), Max number\nof Critic reviews (NCritic)\nEnsure: Final Agent (AN), Final Reward (RN)\nSummary = None\nCritique = None\nR0 = None\nfor i = 1, . . . , N do\nRi = RewardDesigner(Summary, Critique, T, Ri−1)\nfor j = 1, . . . , NCritic do\nCritique, Done = RewardCritic(Ri)\nif Done then\nbreak\nelse\nRi = RewardDesigner(Summary, Critique, T, Ri)\nend if\nend for\nAi = TrainAgent(A0, Ri, Env, T)\nTraji, Stati = Eval(Env, Ai)\nSummary = TrajectoryAnalyzer(Traji, Stati)\nCritique = None\nend for\njectory is considered failed. Long-term exploration can\ndemonstrate the advantages of dense rewards.\n• Approaching tree\nin plains biome\n: The task is\nconsidered successful if the tree is within the agent’s\nvisible range and the distance is less than 1 distance unit.\nThe difficulty of the task lies in the fact that the trees are\nvery sparse on plains, which is extremely detrimental to\nsparse reward functions. The maximum number of steps\nis limited to 2,000 steps.\n• Approaching specific animal (e.g. cow\n, sheep\n)\nin plains biome\n: The task is considered successful\nif the animal is within the agent’s\nvisible range and\nthe distance is less than 2 distance unit. The difficulty of\nthe task is that the animals are constantly moving. The\nmaximum number of steps is limited to 2,000 steps.\n• Obtaining diamond\n: The agent\nneeds to complete\nthe whole process of mining diamonds, including key be-\nhaviors such as finding and obtaining materials on the sur-\nface, crafting, digging down, going back to the ground,\nand mining stone\/iron ore\/diamond ore. The tech tree is\nshown in Figure 4.\n4.2. Implementation Details\nLLM Prompt. The components of the input prompts for\nTrajectory Analyzer include task description, game infor-\nmation, statistical metrics, and information on failed trajec-\ntories. Components of the input prompts for Reward De-\nsigner and Reward Critic includes task description, game\ninformation, input parameters, and reward function require-\nments and format.\nWe use GPT-4 [36] for all the LLM\ncomponents, and set temperature to 0.3. Since the LLMs\nare only used once for each whole agent training instead of\neach action, their computation overhead is negligible.\nFigure 3. The trajectories of the new behaviors. (a) Avoid lava\nwhen exploring for diamond ore\n. (b) Attack cow\nin plains.\n• Instruction: Instructions on initializing, updating and\nhandling execution error of reward function for Reward\nDesigner, reviewing function for Reward Critic, and ana-\nlyzing trajectory for Trajectory Analyzer.\n• Task description: The objective, initial conditions, suc-\ncess criteria, and task flow. For example, for the explore\ndiamond task, the objective is “to find and approach a\ndiamond, achieving a high success rate while avoiding\ndeath.” The initial condition is “agent at y level 11 with\nan iron pickaxe.” The success criteria is “being less than 1\nmeter from the nearest diamond block”, and the task flow\nis “horizontally explore to find a diamond, face it, and\napproach it”. In the task description, we do not provide\nprior game strategy information (task challenges, DFS ex-\nploration strategies, or avoiding lava, etc.) to ensure the\nmethod’s versatility.\n• Game information: Game version, block names, field\nof view, action space, and units of measurement. Game\ninformation provides knowledge about the game’s simu-\nlation environment, not game strategy.\n• Statistical metrics and information on failed trajec-\ntories: success rates, and actions sequences, reward se-\nquences, final inventory and nearby blocks of K = 10\nfailed trajectories. If a trajectory exceeds L = 32 steps, it\nis truncated to the last 32 steps.\n• Input parameters: The nearest distance of each block\ntype within the visible range in the current and previous\nsteps, changes in inventory between adjacent steps, health\npoints, and the agent’s location in each past step. The\nmemory is also provided as an input parameter for storing\ninformation to monitor changes across different steps. We\nprovide explanation and examples of the parameters in the\ninput prompt.\n• Reward function requirements and format: We require\nthe Designer to write a dense function and a sparse func-\ntion, and consider only the sign of the two functions’ re-\nturn values, not the magnitude. The detail of the scale\nconstraints is in Section 3.1.\nImitation Learning Details. When large labeled datasets\ndo not exist, the canonical strategy for training capable\nagents is RL, which is inefficient and expensive to sam-\nple for hard-exploration problems [3, 4, 21], e.g. mining\ndiamond in Minecraft. Therefore, in order to more effi-\nciently explore the effectiveness of the LLM-based reward\nfunction design mechanism proposed in this paper, we pre-\ntrained some foundation models through imitation learning\nas done by VPT [3]. Specifically, we use GITM [53] to\ncontinuously perform Diamond Mining task and record im-\nportant observation data of each frame, such as RGB, ac-\ntion, inventory, GPS, compass, structured actions, etc. In\nthe end, we collect about 11 million image data, totaling\nabout 153 hours (the control frequency is 20 Hz) of game\nvideos. Subsequently, we train these data through fully su-\npervised learning by using Impala CNN [13] and Trans-\nformer [44] as backbone, and obtained several foundation\nmodels. The main differences between the foundation mod-\nels are different biomes (forest and plains), temporal frames\n(16 and 128), and whether goal embedding is used. In sub-\nsequent experiments, these foundation models were used in\ntwo different purposes:\n• Give the RL model preliminary basic Minecraft gameplay\ncapabilities, e.g. forward\/back, turn left\/right, attack, etc.\nFor some tasks that have not been learned (e.g. approach-\ning cows in Figure 3(b)) or not learned well (e.g. avoid\nlava in Figure 3(a), explore tree on plains) in imitation\nlearning, RL algorithms can be studied more efficiently.\n• In the Diamond Mining task, the diamond collection suc-\ncess rate, lava escape rate, death rate, etc. between the RL\nmodel and the imitation learning model are compared to\ndemonstrate the superiority of the proposed method.\nRL Training Details. We use proximal policy optimization\n(PPO) algorithm [39] with generalized advantage estima-\ntion (GAE) [38] to train our RL model. We use γ = 0.99\nand λ = 0.95 for all of our experiments, and the total train-\ning frames is 256,000. To prevent catastrophically forget-\nting or overly aggressive policy update during RL training,\nwe follow VPT [3] to apply an auxiliary Kullback-Leibler\n(KL) divergence loss between the RL model and the frozen\npre-trained policy. We also normalize the reward based on\nthe trajectory returns to constrain the gradient scales of dif-\nferent tasks. See Appendix for details.\n4.3. Main Results\nBaselines. We compare our Auto MC-Reward against the\nfollowing methods:\n• Naive Handcraft: The agent keeps moving (and mining\nfor diamond exploring task) in one direction with a small\nprobability of turning left\/right.\n• Imitation Learning: Our foundation model pre-trained\nwith GITM-generated data, as introduced in Section 4.2.\n• RL with Sparse Reward: Use only the reward from the\noriginal environment, i.e. only receives a reward when\nTable 1. Comparison with other reward methods on three Minecraft tasks. Max steps for exploring tree\nand cow\nare set to 2000.\n†Sparse reward receives a low death rate because it is often stuck in the same place or move in a small area without encountering lava\n.\nMethod\nReward\nExplore Diamond Ore\nUnderground\nApproach Tree\non Plains\nApproach Cow\non Plains\navg. dist. ↑\ndeath (%) ↓\nlava escape (%) ↑\nsucc. (%) ↑\navg. step ↓\nsucc. (%) ↑\navg. step ↓\nsucc. (%) ↑\nNaive Handcraft\n-\n85.7\n74.3\n1.5\n18.6\n1993\n2.1\n1956\n10.8\nImitation Learning\n-\n102.2\n55.6\n46.8\n38.9\n1988\n2.5\n1772\n22.4\nRL\nSparse\n16.8\n1.5†\n0\n0.5\n1936\n4.3\n1854\n12.6\nRL\nDense (Curiosity)\n102.6\n55.1\n46.0\n39.3\n1672\n45.8\n1477\n13.7\nRL\nDense (Self-Imitation)\n104.0\n54.8\n47.2\n39.7\n1532\n42.5\n1280\n23.5\nRL\nDense (MineCLIP)\n105.9\n54.0\n47.8\n40.5\n1022\n65.6\n1206\n44.9\nOurs\nDense (LLM)\n142.8\n45.2\n70.0\n45.2\n972\n73.4\n1134\n56.3\nTable 2. Comparison with previous methods on success rates of\nobtaining diamond\n. We list observations that are used in the\ninference phase. Auto MC-Reward achieves a remarkable success\nrate without exploiting unfair information (i.e. Lidar and Voxel)\nduring inference.\nMethod\nController\nObservation\nDiamond\nSucc. (%)\nHuman [3]\n-\n-\n50.0\nDreamerV3 [17]\nRL\nRGB, Status\n0.01\nDEPS [47]\nIL\nRGB, Status, Voxel\n0.6\nVPT [3]\nIL + RL\nRGB\n20.0\nGITM [53]\nHandcraft\nLidar, Voxel, Status\n55.0\nOurs\nIL (GITM-guided)\nRGB, GPS\n28.8\nOurs\nIL + RL\nRGB, GPS\n36.5\nTable 3. Ablations on Reward Critic and Trajectory Analyzer for\nexplore diamond ore\ntask. The first row corresponds to using\nthe sparse reward from the original environment. †Sparse reward\nreceives a low death rate because it is often stuck in the same place\nor move in a small area without encountering lava\n.\nDesigner\nCritic\nAnalyzer\nAvg. Dist. ↑\nDeath ↓\nLava Esc. ↑\nSucc. ↑\n16.8\n1.5†\n0\n0.5\n✓\n75.8\n58.2\n30.4\n35.1\n✓\n✓\n95.2\n49.3\n40.7\n40.5\n✓\n✓\n130.6\n47.8\n64.8\n43.1\n✓\n✓\n✓\n142.8\n45.2\n70.0\n45.2\nthe success criteria is completed.\n• RL with Curiosity Dense Reward [37]: Encourage the\nagent to discover and learn about parts of the environment\nthat it has not encountered before.\n• RL with Self-Imitation Dense Reward [35]: Encourage\nthe agent to replicate its past actions that led to high re-\nwards.\n• RL\nwith\nMineCLIP\n[14]\nDense\nReward:\nUse\nMineCLIP to calculate the dense reward based on the sim-\nilarity between RGB frames and task objectives.\nResults on Diamond Ore\nExploring Task. For the plain\nimitation learning model, fitting the training data makes it\nlack the awareness of avoiding lava, so it often dies in lava\nduring the search for diamonds, and only has 38.9% suc-\ncess rate under the limit of 60,000 steps, as shown in Ta-\nble 1. In contrast, our Auto MC-Reward makes the agent\nrealize the importance of avoiding lava by continuously im-\nFigure 4. The tech tree of obtaining diamond. The green squares\nare tasks to be optimized with Auto MC-Reward, i.e. obtaining\nlog\n, cobblestone\n, iron ore\nand diamond\n.\nproving the dense reward function, and the final success rate\nhas increased to 45.2% with 70% lava escape success rate.\nFigure 3(a) demonstrates good awareness of avoiding lava.\nBased on the same reinforcement learning algorithm, the\ndisadvantages of sparse reward functions in long-horizon\ntasks are undoubtedly revealed. By watching the videos of\ncollected trajectories, we find that using sparse functions\noften leads to irreversible behavior, such as being unable\nto break the surrounding ores to move due to maintaining\na head-up posture. Although a low death rate of 1.5% is\nachieved, the actual average moving distance is only 16.8,\nand the success rate is only 0.5%. Due to the similar scenes\nunderground, MineCLIP cannot give differentiated rewards,\nso its performance is close to the initial imitation learning\nmodel.\nOther baselines, like curiosity and self-imitation\ndense reward, also have mediocre performance and the suc-\ncess rate has not been significantly improved.\nResults on Tree\nApproaching Task. Since trees are ex-\ntremely sparse on the plain, the imitation learning model\nand the RL model with sparse reward cannot perform well,\nwith only 2.5% and 4.3% success rates respectively, and\ntheir average action steps are close to the maximum limit.\nMineCLIP dense reward receives a success rate of 65.6%\nsince it can provide positive reward when tree is visible.\nCuriosity and self-imitation methods also achieve better re-\nsults than imitation learning. For Auto MC-Reward, Re-\nward Designer uses a strategy of giving positive rewards for\ngetting closer and deducting rewards for going away, so that\nthe agent learns to slowly approach the target, ultimately\nachieving 73.4% success rate with only 972 average steps.\nResults on Cow\nApproaching Task. The task of explor-\ning for cows does not appear in the training data of imitation\nlearning, so the zero-shot ability on this task is not ideal,\nwith about 22.4% success rate and average steps close to\nthe maximum limit. By checking the videos, we find most\nof the successful cases are due to good luck without inten-\ntion to actively approach the target. The same experimental\nconclusion is also obtained in the experiment of sparse re-\nward function. Similar to the Tree Approaching Task, the\nsuperior dense reward function design mechanism makes\nour agent 43.7% (56.3% vs. 12.6%) higher than sparse re-\nward, as listed in Table 1. Another dense reward MineCLIP\nalso shows strong performance in this task, but due to the\nneed to calculate the similarity of images and texts at all\ntimes during training, the efficiency is unacceptable.\nResults on Obtaining Diamond\n. We verify the pro-\nposed method on a more difficult task, that is, the tech tree\nof collecting diamonds, as shown in Figure 4. As mentioned\nbefore, our foundation imitation learning model already has\na certain ability from birth to diamond mining. We use the\nproposed method to optimize several key tasks in the pro-\ncess to increase the success rate of final diamond acquisi-\ntion. The green parts in Figure 4 are the tasks that need\nto be optimized, i.e. obtaining log, cobblestone, iron ore\nand diamond. We conduct experiments in two biomes in\nMinecraft, and the cumulative success rate is shown in Fig-\nure 5. Specifically, the lower death rate allows our agent\nto have a higher success rate in mining iron ore and dia-\nmond, and ultimately achieves 36.5% success rate on for-\nest biome, which is 7.7% higher than the imitation learning\nmodel. As for plains, the difficulty of obtaining log makes\nthe imitation learning model unable to complete any tasks.\nAuto MC-Reward overcomes the difficulty of obtaining log,\nthus achieving a 28.1% success rate in obtaining diamonds.\nTable 2 provides a rough comparison of several different\nmethods on the task of mining diamonds. We achieve a\nhigh success rate without exploiting unfair information (i.e.\nLidar and Voxel) during the inference phase.\n4.4. Ablation studies\nEffectiveness of Reward Designer. The first row of Table 3\nis an RL experiment with a sparse reward function. As men-\ntioned before, it cannot explore diamonds normally. After\nadding Reward Designer, it regained the ability to explore\nunder a dense reward function.\nEffectiveness of Reward Critic.\nAs listed in Table 3,\nthe success rate of exploring diamonds has increased from\n35.1% to 40.5% by adding Reward Critic, because it can\nreduce the syntax and semantic errors in the code, making\nthe training process more effective and sufficient. For exam-\nple, the Trajectory Analyzer concludes that the agent died in\nlava and asks the Reward Designer to add relevant penalties.\nHowever, without being checked by Critic for semantic er-\nrors, it is possible that the added code snippet uses the word\n“magma” instead of the correct one “lava”. This will result\nFigure 5. Cumulative success rates for 4 key items of obtaining\ndiamond on forest\nand plains\n. In terms of diamond, the per-\nformance comparison between imitation learning and Auto MC-\nReward in two biomes are: 28.8% vs. 36.5%, and 0% vs. 28.1%.\nin insufficient learning of lava avoidance, which is reflected\nin a 2.1% (43.1% vs. 45.2%) success rate difference.\nEffectiveness of Trajectory Analyzer. As observed in Ta-\nble 3, Trajectory Analyzer is the key to improve the suc-\ncess rate of completing tasks. It summarizes the reasons for\nfailure to be fed into Reward Designer, allowing it to itera-\ntively modify an appropriate dense reward function to guide\nthe agent to overcome difficulties. In terms of Diamond Ex-\nploring Task, Trajectory Analyzer provides timely feedback\non the potential risks of lava, which greatly improves the\nsurvival rate and moving distance, ultimately improving the\nsuccess rate from 40.5% to 45.2%.\n5. Conclusion\nWe proposed Auto MC-Reward, an automated dense re-\nward design framework for addressing challenges caused by\nsparse reward and complex environment of Minecraft. It ad-\ndresses the issue of sparse rewards by leveraging LLMs to\nautomatically generate dense reward functions, enhancing\nlearning efficiency. The system consists of three key com-\nponents: Reward Designer, Reward Critic, and Trajectory\nAnalyzer, which are used for the design, verification and\nanalysis of the reward function respectively. Its capabilities\nare validated through experiments, demonstrating a remark-\nable improvement in complex tasks in Minecraft. Future\nwork may deal with the limited trajectory length for analysis\n(last 32 frames) due to the context length of LLMs, which\nhinders the analysis of long-term failures (e.g., not explor-\ning new areas, circling around lava).\nAuto MC-Reward\nhumbly contributes to more effective learning in complex\ntasks through its automated dense reward function design.\nWe hope it can pave the way for further research in rein-\nforcement learning and its real-world applications.\nAcknowledgement\nThis work is supported by the National Key R&D Program\nof China (NO. 2022ZD0161300, NO. 2022ZD0160100),\nby the National Natural Science Foundation of China\n(62376134).\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Auto MC-Reward：利用大型语言模型自动设计密集奖励函数，提升Minecraft中强化学习的效率\n\n## 📌 背景痛点\/本文动机\nMinecraft 等强化学习环境通常只提供稀疏奖励，即只有任务完成或失败时才会获得奖励。这种奖励机制使得强化学习代理在探索效率方面面临挑战，难以学习复杂任务。为了解决这个问题，本文提出了 Auto MC-Reward，一个利用大型语言模型 (LLM) 自动设计密集奖励函数的先进学习系统，从而提高学习效率。\n\n## 🚀 核心方法\n💡 创新点1：Auto MC-Reward 由三个关键组件组成：奖励设计器、奖励评论家和轨迹分析器。奖励设计器根据环境信息和任务描述，通过编写可执行的 Python 函数来设计奖励函数。奖励评论家负责验证代码，检查代码是否自洽且没有语法和语义错误。轨迹分析器根据收集的轨迹总结可能的失败原因，并提供改进建议。\n\n💡 创新点2：Auto MC-Reward 利用 LLM 的任务理解和经验总结能力，为学习提供详细和即时的奖励指导。奖励设计器首先根据环境和任务的基本描述，使用 LLM 设计与任务相关的密集奖励函数。然后，奖励评论家对设计的奖励函数进行自我验证。为了解决 LLM 理解的潜在偏差或疏忽，还提出了基于 LLM 的轨迹分析器，用于分析和总结训练代理的轨迹，并帮助奖励设计器改进奖励函数。\n\n## 📈 实验结果\nAuto MC-Reward 在一系列代表性基准测试中进行了验证，包括地下水平探索钻石和探索平原生物群落中的树木和动物。实验结果表明，与原始稀疏奖励和现有密集奖励方法相比，Auto MC-Reward 在这些任务上取得了显著更好的结果，显示出其在稀疏奖励任务上高效学习的先进能力。通过迭代改进奖励函数的设计，Auto MC-Reward 使代理能够有效地学习对新任务有益的新行为，例如避免熔岩，从而大大提高了成功率。此外，Auto MC-Reward 仅使用原始信息就实现了高钻石获取成功率（36.5%），证明了其解决长期任务的能力。\n\n## 💬 可借鉴之处\nAuto MC-Reward 为解决稀疏奖励任务中的探索效率问题提供了一种新的思路。其利用 LLM 自动设计密集奖励函数的方法，可以有效地提高强化学习代理的学习效率。此外，Auto MC-Reward 的三个组件（奖励设计器、奖励评论家和轨迹分析器）可以独立运行，使得数据分析和奖励函数更新更加灵活。","llm_summary_res_status":200}
{"title":"Odyssey: Empowering Minecraft Agents with Open-World Skills","authors":"Shunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui, Wenkai Fang, Yuxuan Zheng, Tongya Zheng, Mingli Song","summary":"Recent studies have delved into constructing generalist agents for open-world\nenvironments like Minecraft. Despite the encouraging results, existing efforts\nmainly focus on solving basic programmatic tasks, e.g., material collection and\ntool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set\nof actions available to agents, requiring them to learn effective long-horizon\nstrategies from scratch. Consequently, discovering diverse gameplay\nopportunities in the open world becomes challenging. In this work, we introduce\nOdyssey, a new framework that empowers Large Language Model (LLM)-based agents\nwith open-world skills to explore the vast Minecraft world. Odyssey comprises\nthree key parts: (1) An interactive agent with an open-world skill library that\nconsists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned\nLLaMA-3 model trained on a large question-answering dataset with 390k+\ninstruction entries derived from the Minecraft Wiki. (3) A new agent capability\nbenchmark includes the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task. Extensive experiments demonstrate\nthat the proposed Odyssey framework can effectively evaluate different\ncapabilities of LLM-based agents. All datasets, model weights, and code are\npublicly available to motivate future research on more advanced autonomous\nagent solutions.","url":"http:\/\/arxiv.org\/abs\/2407.15325v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2407.15325v2","published":1721614019000,"comment":null,"pdf_text":"Preprint\nODYSSEY: EMPOWERING MINECRAFT AGENTS WITH\nOPEN-WORLD SKILLS\nShunyu Liu1∗†, Yaoru Li1∗, Kongcheng Zhang1∗, Zhenyu Cui1∗, Wenkai Fang1∗,\nYuxuan Zheng1, Tongya Zheng2, Mingli Song1 #\n1Zhejiang University,\n2Hangzhou City University\n{liushunyu, liyaoru, zhangkc, zhenyucui, wenkfang, zyxuan}@zju.edu.cn,\ndoujiang_zheng@163.com, brooksong@zju.edu.cn\n∗Equal contribution,\n†Project leader,\n#Corresponding author\n§ https:\/\/github.com\/zju-vipa\/Odyssey\nABSTRACT\nRecent studies have delved into constructing generalist agents for open-world\nenvironments like Minecraft. Despite the encouraging results, existing efforts\nmainly focus on solving basic programmatic tasks, e.g., material collection and\ntool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set of\nactions available to agents, requiring them to learn effective long-horizon strate-\ngies from scratch. Consequently, discovering diverse gameplay opportunities in\nthe open world becomes challenging. In this work, we introduce ODYSSEY, a\nnew framework that empowers Large Language Model (LLM)-based agents with\nopen-world skills to explore the vast Minecraft world. ODYSSEY comprises three\nkey parts: (1) An interactive agent with an open-world skill library that consists of\n40 primitive skills and 183 compositional skills. (2) A fine-tuned LLaMA-3 model\ntrained on a large question-answering dataset with 390k+ instruction entries de-\nrived from the Minecraft Wiki. (3) A new agent capability benchmark includes\nthe long-term planning task, the dynamic-immediate planning task, and the au-\ntonomous exploration task. Extensive experiments demonstrate that the proposed\nODYSSEY framework can effectively evaluate different capabilities of LLM-based\nagents. All datasets, model weights, and code are publicly available to motivate\nfuture research on more advanced autonomous agent solutions.\n1\nINTRODUCTION\nDeveloping autonomous agents capable of performing open-world tasks represents a significant\nmilestone towards achieving artificial general intelligence (Savva et al., 2019; Reed et al., 2022;\nDriess et al., 2023). These open-world tasks necessitate that agents interact with complex and dy-\nnamic environments, make decisions based on incomplete information, and adapt to unexpected\nevents. Early reinforcement learning agents (Tessler et al., 2017; Oh et al., 2017; Guss et al., 2019)\nhave demonstrated limited knowledge in such open-world setting. Furthermore, these agents of-\nten struggle with long-term planning, which is crucial for the fulfillment of intricate goals. Recent\nbreakthrough of Large Language Models (LLMs) (Hu et al., 2021; Achiam et al., 2023; Touvron\net al., 2023) have shown the potential to revolutionize various fields such as healthcare (Zhang et al.,\n2023b; Yang et al., 2024b), robotics (Huang et al., 2022; Ahn et al., 2022; Singh et al., 2023), and\nweb services (Nakano et al., 2021; Deng et al., 2023; Iong et al., 2024), attributed to its capability\non endowing agents with expansive knowledge and sophisticated planning akin to human reason-\ning (Wei et al., 2022a; Wang et al., 2024; Liang et al., 2023). However, the development of LLMs in\nopen-world tasks remains challenging due to the need for well-defined environments and measurable\nbenchmarks (Zhu et al., 2023; Wang et al., 2023a; Qin et al., 2023).\nThe popular Minecraft game features a vast and diverse world with various biomes, terrains, and\nresources, making it an ideal testbed for evaluating the capabilities of autonomous agents in the\nopen-world setting (Guss et al., 2019). To facilitate the development of generalist agents in this\n1\narXiv:2407.15325v2  [cs.AI]  7 Oct 2024\nPreprint\nLong\nShort\nLLaMA 3\nMinecraft\nWiki\nFine-tune Minecraft LLM\nAgent Capability Benchmark\nCombat Zombie\nCraft Diamond Sword\nMine Diamond\nOpen-World Skill Library\nInteractive Agent\nPlan\nCritic\nActor\n...\nMineMA\nQ&A Dataset\nGeneration\nFine-tune\nLoRA\nSkill\nRetrieval\n     Combat\nWeapons\nEquipment\nLong-term Planning\n     Farm\nPlanting\nBreeding\nDynamic-immediate Planning\n     Explore\nSurviving\nCreating\nAutonomous Exploration\n...\nFigure 1: An overview of the proposed ODYSSEY framework. Odyssey consists of three key com-\nponents: (1) a fine-tuned LLaMA-3 model trained on a large-scale question-answering dataset; (2)\nan interactive agent equipped with an extensive open-world skill library; (3) a novel agent capability\nbenchmark encompassing a variety of tasks.\nsetting, MineRL (Guss et al., 2019) and MineDojo (Fan et al., 2022) introduced simulation bench-\nmarks built upon the sandbox Minecraft environment. The seminal work, Voyager (Wang et al.,\n2023a), proposed an LLM-based agent to drive exploration in Minecraft. Subsequently, there has\nbeen a surge of efforts to leverage the superior performance of LLMs to extend the capabilities of\nsuch Minecraft agents (Zhu et al., 2023; Wang et al., 2023b; Zhou et al., 2024a; Wang et al., 2023c;\nQin et al., 2023). Despite recent advancements, existing works mainly focus on solving basic pro-\ngrammatic tasks, often considering the ObtainDiamond task as the ultimate challenge. Basic\nprogrammatic tasks refer to those constrained by the explicit dependencies following the Minecraft\ntech-tree, such as collecting materials and crafting tools. Such tasks inherently only assess the abil-\nity of LLMs to prioritize crafting steps within a limited task space, rather than their potential for\ncomplicated and diverse solutions. This limitation arises from the narrowly defined set of actions\navailable to agents (e.g., mouse and keyboard), which necessitates learning skills from scratch. Since\nMinecraft is fundamentally resource-based, an agent must first learn to collect adequate resources\nand tools to engage in creative play, which limits the exploration of diverse gameplay options. More-\nover, methods like Voyager (Wang et al., 2023a) heavily rely on the powerful GPT-4 for high-quality\nsolutions, imposing a substantial cost burden on researchers who prefer open-source models.\nIn this work, we introduce ODYSSEY1, a novel framework that equips LLM-based agents with ad-\nvanced open-world skills, enabling efficient interaction and exploration within the Minecraft envi-\nronment. ODYSSEY allows agents to move beyond basic programmatic tasks and focus more on\ncomplex open-world challenges. As shown in Fig. 1, ODYSSEY comprises three key contributions:\n1. We develop an LLM-based interactive agent with an open-world skill library, encompassing\n40 primitive skills that serve as underlying interfaces and 183 compositional skills tailored for\ncomplex and diverse tasks in an open-world setting. A recursive method improves skill execution\nby checking prerequisites. The ODYSSEY agent consists of a planner for goal decomposition, an\nactor for skill retrieval and subgoal execution, and a critic for feedback and strategy refinement.\n2. We fine-tune the LLaMA-3 model (Touvron et al., 2023) for Minecraft agents using a compre-\nhensive question-answering dataset. This involves generating a large-scale training dataset with\n390k+ instruction entries from Minecraft Wikis, fine-tuning various sizes of the LLaMA-3 mod-\nels using LoRA (Hu et al., 2021), and evaluating them with a custom multiple-choice dataset.\n3. We introduce a new agent capability benchmark to evaluate different aspects of agent perfor-\nmance in Minecraft, including the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task. Extensive experiments demonstrate that the proposed\n1The Odyssey is a great ancient Greek epic poem attributed to Homer, which is now often used metaphori-\ncally to describe a long adventurous journey (Oxford English Dictionary).\n2\nPreprint\nODYSSEY framework provides a robust measure of agent effectiveness, showcasing the practical\nadvantages of our framework using the open-source models.\nIt is worth noting that our focus is not to design a new LLM-based agent architecture. Instead,\nthis work aims to provide a comprehensive framework for developing and evaluating autonomous\nagents in open-world environments, enabling them to explore the vast and diverse Minecraft world.\nWe have open-sourced all parts of ODYSSEY and will continuously update the repository. We hope\nthis will enable other researchers to build upon our work, fostering further innovation and progress\nin the development of autonomous agents.\n2\nOPEN-WORLD SKILL-BASED INTERACTIVE AGENT\nODYSSEY develops an LLM-based interactive agent with an open-world skill library, aiming to en-\nhance the efficiency and adaptability of agents in complex Minecraft environments. The skill library\ncomprises 40 primitive skills and 183 compositional skills, while the LLM-based agent employs a\nplanner-actor-critic architecture to facilitate task decomposition, skill execution, and performance\nfeedback. The architecture of the interactive agent is depicted in Fig. 2. Full skill and prompt details\nused in the LLM-based interactive agent are given in Appendix C.\n2.1\nOPEN-WORLD SKILL LIBRARY\nPrimitive skills encompass a series of underlying interfaces on top of Mineflayer JavaScript\nAPIs (PrismarineJS, 2023), divided into two main categories: 32 operational skills and 8 spatial\nskills. This suite of skills exceeds the 18 primitive skills (all are operational skills) delineated in\nVoyager (Wang et al., 2023a). Operational skills serve as foundational interfaces with parameter-\nized input, such as mine(·) for material collection and craft(·) for tool crafting. Additionally,\nwe pioneer 8 spatial skills that Voyager (Wang et al., 2023a) lacks, allowing for environmental in-\nteractions based on the agent coordinates. Given that our work is conducted within a text-based\nMinecraft environment (Wang et al., 2023a; Fan et al., 2022), spatial skills are crucial for handling\ntasks that require precise positioning and orientation, especially in the absence of visual input.\nCompositional skills encapsulate primitive skills into higher-level ones, functioning to address a va-\nriety of basic programmatic tasks, such as mineDiamond and craftIronPickaxe. ODYSSEY\nclassifies 183 compositional skills into types like mineX, craftX, plantX, breedX, cookX,\netc. We use a recursive method to construct the skill library, simplifying complex task decomposi-\ntion by ensuring prerequisites are met before skill execution. Taking mineDiamond as an example,\nif the agent lacks an iron pickaxe, it will recursively execute craftIronPickaxe. This indicates\nthat our program internally manages the construction and execution order of skills through its recur-\nsive method, thereby avoiding the need for the agent to engage in additional planning.\nTo facilitate efficient retrieval of skills in the skill library, we first generate a description for each skill\nby calling the LLM and using the complete program code as a prompt. We then employ Sentence\nTransformer (Reimers & Gurevych, 2019) to encode the skill description. This method transforms\ntext information into vector representations, facilitating semantic retrieval and enabling the agent to\nfind the most relevant skill description based on the context provided.\n2.2\nLLM PLANNER\nThe LLM Planner is responsible for developing a comprehensive plan, facilitating efficient explo-\nration through long-term goal decomposition. The LLM Planner breaks down high-level goals into\nspecific low-level subgoals, each corresponding to a particular skill outlined in Sec. 2.1. By address-\ning each subgoal in the plan, the ultimate goal can be progressively achieved. The input prompt to\nthe planner consists of several components: (1) Ultimate goals and behavioral constraints. For\nexample, “My ultimate goal is to ... Propose the current task only when you ensure that you have all\nthe necessary dependent items in inventory”. (2) States of the agent. This reflects the interaction\nbetween the agent and environment, such as hunger and health values, position and nearby entities,\netc. (3) Achievements of the agent. This includes the current inventory and unlocked equipment,\nas well as previously successful and failed tasks.\n3\nPreprint\nKnowledge Q&A\n   First, you should ...\nHow to obtain milk?\nSkill Retrieve\nCode Action\nBreed cow\nKill one cow with sword\nCollect milk with bucket\nAchivements\nUltimate Goals\nPlan\nEnvironment\nMineflayer\nPrimitive\nCompositional\nI want to  breed  as\nmany as animals\nlike      ,     or     ,\nand then collect \nitems from them.\nHealth \nPos\nExecute\nValidate\n[Lack of pre-requirements] \nI cannot collect milk without a     .\nExecution\nFeedback\n[Environment feedback]\nI could not find a       to collect milk.\nSelf-validation:\nSelf-reflection:\nCritic\nSince you only have     ,\nyou might need the      to\nattract a      for milk.\nBased on changes of my\ninventory, is my subgoal\nsuccessful? 🤔\nThought\nYou should analysis the\nreason why my subgoal\nis failed based on the\nlogs provided.\nRethink\nObservation\nSkill Library\n[Biome] snowy\n[Time] day\n[Nearby bocks] dirt, grass, \noak_log, oak_leaves, tall_grass, \ncobblestone, crafting_table,\nacacia_log\n[Nearby entities] horse, pig\n[Health]: 18.0\/20\n[Hunger]: 16.0\/20\n[Position]: x=2134.5, y=69.0,\nz=769.5\n[Inventory] oak_log, ...\n[Equipment] helmet,\nleggings, boots, ...\n[Completed] mine ...\nUpdate\nFailed\nSuccessful\nPotential\nMy subgoal is to: \ncollect milk\nlast_inventory (16\/36): ... \ncur_inventory (18\/36): ... \nFigure 2: An illustrative diagram of the interactive agent following a planner-actor-critic architecture\nbased on the open-world skill library. The LLM Planner decomposes ultimate goals into specific\nsubgoals, while the LLM Actor then sequentially executes code actions for each subgoal using the\nskill library. The LLM Critic evaluates these actions through self-validation and reflection, enabling\nthe agent to update its plan based on execution feedback.\n2.3\nLLM ACTOR\nIn the execution phase, the LLM actor is invoked to sequentially execute the subgoals generated\nby the LLM planner within the Minecraft environment. This process utilizes the open-world skill\nlibrary to achieve these subgoals. The mapping from high-level subgoals to executable skill code is\naccomplished through query context encoding and skill similarity retrieval. This process includes:\n(1) Query context. The text-based subgoals generated by the LLM planner are encoded by Sen-\ntence Transformer (Reimers & Gurevych, 2019) to vector representations as the query context. (2)\nSimilarity matching. The vector similarity between the query context and the skill descriptions in\nthe skill library is computed to determine semantic closeness. (3) Skill selection. The top-5 relevant\nskills with the highest scores are identified, and the actor agent selects the most appropriate code for\nexecution within the environment based on their descriptions.\n2.4\nLLM CRITIC\nDuring action execution, it is critical for an agent to document its experiences, especially noting suc-\ncessful outcomes and failure points. This is crucial in open-world planning to establish a feedback-\ninformed system, which corrects initial plan discrepancies that can cause execution errors. For\ninstance, achieving the animal breeding goal requires prerequisite crops for feed. The LLM critic\ncan assess action effectiveness by comparing expected and actual outcomes, providing insights for\nrefining future strategies. We categorize feedback into three types: (1) Execution feedback. This\ncaptures the progress of skill execution. For example, “No hoe in inventory. Craft a hoe first!” not\nonly highlights the reason for failure in hoeing farmland but also provides a guideline to address this\nproblem. (2) Self-validation. By presenting inventory changes post-action to the LLM critic, we\nempower it to validate whether the skill has achieved its subgoal, eliminating the need for manual\nchecks. (3) Self-reflection. Simply confirming the completion of a subgoal is often inadequate for\ncorrecting planning errors. The LLM critic also serves as an analyst, deducing the cause of task\nfailure by evaluating the current state of the agent and its environment. It then offers a critique,\nsuggesting a more efficient strategy for task completion.\n3\nFINE-TUNE MINECRAFT LLM\nTo improve agent performance in Minecraft, we fine-tune the LLaMA-3 model (Touvron et al., 2023)\nusing a large-scale Question-Answering (Q&A) dataset with 390k+ instruction entries sourced from\nthe Minecraft Wiki. ODYSSEY presents an effective procedure for converting a foundation model\ninto a domain-specific model, which involves dataset generation, model fine-tuning, and model eval-\nuation. The detailed descriptions can be found in Appendix D.\n4\nPreprint\nDataset Generation. We develop a GPT-assisted method to generate an instruction dataset for\nMinecraft. First, we crawl relevant content from the Minecraft Wiki, excluding non-essential sec-\ntions like history. The collected data is then categorized and separated into different files based on\ntheir content type. Then we use GPT-3.5-Turbo (OpenAI, 2023) with different customized prompts\nto automatically generate diverse Q&A pairs. Note that both the questions and answers were gener-\nated by GPT. These Q&A pairs are categorized into four types based on the nature of the answers:\nshort, normal, long, and boolean, yielding 390k+ entries. In contrast, the Wiki dataset released\nby MineDojo (Fan et al., 2022) only collects Minecraft Wiki pages, without refining the content\nand generating Q&A pairs for model training. STEVE (Zhao et al., 2023) introduces a non-public\ndataset with 20k+ Q&A pairs, which is smaller than our dataset in terms of scale and diversity.\nModel Fine-tuning. We employ LoRA (Hu et al., 2021) for model fine-tuning, which is a parameter-\nefficient training technique. LoRA introduces small, trainable low-rank matrices to adapt a pre-\ntrained neural network, enabling targeted updates without the need to retrain the entire model. Us-\ning LoRA, we fine-tune the LLaMA-3-8B-Instruct and LLaMA-3-70B-Instruct models with our\nMinecraft dataset, resulting in the new models termed MineMA-8B and MineMA-70B, respectively.\nModel Evaluation. In Minecraft, questions are often open-ended and can yield diverse answers;\ntherefore, conventional evaluation metrics (Papineni et al., 2002; Lin, 2004) may fall short. Mean-\nwhile, common benchmarks (Wang et al., 2018; 2019; Hendrycks et al., 2021) are not suitable for\nassessing the capabilities of expert models. Thus, we employed GPT-4 (Achiam et al., 2023) to gen-\nerate two Multiple-Choice Question (MCQ) datasets based on different themes and keywords related\nto Minecraft. These datasets can quantitatively evaluate the domain-specific expertise of models.\n4\nAGENT CAPABILITY BENCHMARK\nCraftSword\nCraftArmor\nCombatMonster\nImmediate Feedback\nDynamic Plan\nIterative Optimization \nResource\nExplore\nLong-term Planning Task\nDynamic-immediate Planning Task\nAutonomous Exploration Task\nHoeFarmland\nShearSheep\nMilkCow\nSkill Library\nPlan 1\nPlan 2\nGoal\nFigure 3: Agent capability benchmark.\nODYSSEY presents a new benchmark for evaluating\nagent capabilities within Minecraft, offering three task\ntypes: long-term planning, dynamic-immediate plan-\nning, and autonomous exploration. It is notable that\nthese tasks cannot be solved by any single skill but de-\nmand a sophisticated combination of multiple skills.\nThese tasks are set in various Minecraft scenarios, with\ndifferent tasks in the same scenario testing different\nagent capabilities. For example, in the cooking sce-\nnario, long-term planning requires formulating a com-\nplete plan to locate and hunt a specific animal, whereas\ndynamic-immediate planning involves selecting which\nnearby animal to cook based on the immediate en-\nvironment. Our benchmark provides a standardized\nframework for evaluating agents, where the agent ca-\npability requirements for different tasks are shown in\nTable 1. Please refer to Appendix E for more details.\nLong-term Planning Task. We design a suite of combat scenarios to assess the long-term planning\ncapability of agents, requiring them to craft appropriate weapons and equipment to defeat various\nmonsters. These combat scenarios can be divided into single-type and multi-type monster scenarios.\nFor the single-type scenarios, we choose various unique monsters, each with its own attack styles,\nmovement patterns, and hostility levels. For the multi-type scenarios, we focus on typical monster\ngroupings encountered in the game. Agents must generate a comprehensive long-term plan, detail-\ning the sequence of crafting the necessary weapons and equipment for the assigned combat task.\nPerformance is measured by remaining health and time consumed during combat. After each battle,\nagents can iteratively optimize their plan, learning from previous outcomes to improve performance\nin subsequent rounds. To extend the scope of the long-term planning task beyond combat, we also\nadopt animal husbandry and cooking scenarios, where agents are required to formulate detailed\nplans for completing tasks related to specific animals.\nDynamic-immediate Planning Task. The dynamic-immediate planning task requires agents to\ndynamically generate and execute plans based on immediate environmental feedback. Thus, we\ndesign a suite of farming scenarios, where agents engage in activities like planting, cooking, and\n5\nPreprint\nTable 1: Specific agent capability requirements for different benchmark tasks, including Goal-based\nPlanning (GBP), Feedback-based Planning (FBP), Exploratory Planning (EP), Task Decomposi-\ntion (TD), Resource Management (RM), Skill Retrieval (SR), Self-Reflection (Self-R), and Self-\nValidation (Self-V). Please refer to Appendix E.4 for detailed descriptions of each capability.\nTask\nGBP\nFBP\nEP\nTD\nRM\nSR\nSelf-R\nSelf-V\nSingle-Round Long-Term Planning Task\n✓\n×\n×\n✓\n×\n✓\n✓\n✓\nMulti-Round Long-Term Planning Task\n✓\n✓\n×\n✓\n×\n✓\n✓\n✓\nDynamic-Immediate Planning Task\n✓\n✓\n×\n×\n✓\n✓\n✓\n✓\nAutonomous Exploration Task\n×\n✓\n✓\n×\n✓\n✓\n✓\n✓\nTable 2: Average execution time and success rate (SR) on 5 basic programmatic tasks in Minecraft.\nTask\nTime (min)\nSR in 2min\nSR in 5min\nSR in 10min\nSR in 15min\nCrafting Table\n0.59 ± 0.79\n95.8%\n99.2%\n100.0%\n100.0%\nWooden Tool\n0.95 ± 0.80\n92.5%\n99.2%\n100.0%\n100.0%\nStone Tool\n1.48 ± 0.96\n85.0%\n97.5%\n100.0%\n100.0%\nIron Tool\n4.43 ± 1.48\n0.0%\n76.7%\n100.0%\n100.0%\nObtain Diamond\n6.48 ± 2.02\n0.0%\n21.7%\n92.5%\n100.0%\nanimal husbandry. Although some scenarios are similar to the long-term planning task, the dynamic-\nimmediate planning task emphasizes reacting to real-time feedback like available resources and\nnearby animals. Performance is evaluated through task completion time and success rates.\nAutonomous Exploration Task. To test the exploratory capability of agents within open-world\nsettings, we design an autonomous exploration task in Minecraft. In this task, agents are required\nto determine their subsequent objectives and execute the appropriate skills based on the game con-\ntext. The exploration task involves discovering and utilizing resources, while adapting to unexpected\nevents such as encounters with hostile monsters. Agents must adapt to these challenges by devel-\noping strategies for resource management and task prioritization. The performance metrics include\nthe number of distinct items obtained, the total items crafted, the recipes and advancements (R&A)\nunlocked, and the distance traveled.\n5\nEXPERIMENTS\nTo demonstrate the effectiveness of the proposed ODYSSEY framework, we conduct experiments\non basic programmatic tasks and the agent capability benchmark. Our simulation environment is\nbuilt on top of Voyager (Wang et al., 2023a), providing a text-based interface for agents to interact\nwith Minecraft. We only use GPT-3.5 and GPT-4 for initial data generation, but all experiments are\nconducted with the open-source LLaMA-3 model, significantly reducing costs compared to GPT-4-\nbased skill generation methods (Wang et al., 2023a;b). Notably, we do not employ GPT-4 in Voyager\ndue to the high cost, which we estimate would be in the thousands of dollars per experiment. Instead,\nwe reproduce Voyager using GPT-4o-mini and GPT-3.5 for comparison. More details are provided\nin Appendix F. We aim to answer the following questions: (1) Can the open-world skill library\nimprove the efficiency of agents in Minecraft? (Sec. 5.1). (2) How well do agents with different\nLLMs perform on the agent capability benchmark tasks? (Sec. 5.2). (3) What is the contribution of\ndifferent components of the ODYSSEY agent to its overall performance? (Sec. 5.3).\n5.1\nOPEN-WORLD SKILL LIBRARY\nTo demonstrate the superior capability of our open-world skill library in Minecraft, we first tested it\non 5 basic programmatic tasks from previous studies (Zhu et al., 2023). We conducted 120 repeated\nexperiments on each task and recorded the average completion time for each task as well as the\nsuccess rates at different time points. We report the performance of baselines using the results\nreported from their own paper, including DEPS (Wang et al., 2023b), VPT (Baker et al., 2022),\nGITM (Zhu et al., 2023). The experimental results in Tab. 2 and Tab. 3 show that our skill library\nsignificantly improved the success rate and efficiency of the above tasks, surpassing previous studies.\n6\nPreprint\nTable 3: Average success rate of our framework and previous baselines on 5 basic programmatic\ntasks in Minecraft within ten minutes.\nAgent\nCrafting Table\nWooden Tool\nStone Tool\nIron Tool\nObtain Diamond\nDEPS\n90.0%\n80.0%\n73.3%\n10.0%\n0.6%\nVPT\n100.0%\n100.0%\n100.0%\n85.0%\n20.0%\nGITM\n100.0%\n100.0%\n100.0%\n95.0%\n67.5%\nOurs\n100.0%\n100.0%\n100.0%\n100.0%\n92.5%\n5.2\nAGENT CAPABILITY BENCHMARK\nWe evaluate the LLM-based agent on the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task from the ODYSSEY benchmark. These tasks cover a\nvariety of complex gaming scenarios and require diverse solutions.\n5.2.1\nLONG-TERM PLANNING TASK\n1 Zombie\n1 Spider\n1 Skeleton 1 Enderman\nTask\n0\n3\n6\n9\n12\n15\n18\nTime (min)\nRound 1\nRound 2\nRound 3\nFigure 4: Performance on the multi-round long-\nterm planning task. Note that all presented data\nare from successful tasks.\nThe long-term planning task assesses the agent\ncapability to directly formulate and execute\ncomprehensive plans over extended periods. For\nexample, in the combat scenarios, the agent is\nrequired to plan a list of weapons and equip-\nment to craft based on the strength of different\nmonsters, with the goal of defeating the mon-\nster in as short a time as possible. We compared\nthe performance of our agent with both the fine-\ntuned MineMA-8B and the original LLaMA-3-\n8B models, and also the performance of Voy-\nager (Wang et al., 2023a) with GPT-4o-mini\nacross these tasks.\nMoreover, we also evalu-\nate the performance of single-round and multi-\nround planning. The single-round test results in Tab. 4 show that the fine-tuned MineMA-8B model\nsurpasses the original LLaMA-3-8B model in terms of success rate and time efficiency, albeit at\nthe cost of more LLM iterations. Moreover, our agent with the MineMA-8B model can outperform\nVoyager with GPT-4o-mini in most scenarios, indicating the effectiveness of our fine-tuning strat-\negy. The multi-round test results in Fig. 4 show that the multi-round planning strategy significantly\nimproves the time efficiency of the agent, indicating that the agent can iteratively optimize its plan\nbased on the outcomes of previous battles to enhance its performance in subsequent rounds.\n5.2.2\nDYNAMIC-IMMEDIATE PLANNING TASK\nFor the dynamic-immediate planning task, the agent is required to dynamically generate and execute\nplans based on immediate environmental feedback. We compared our MineMA model with differ-\nent open-sourced LLMs, including Qwen2-7B (Yang et al., 2024a) and Baichuan2-7B (Yang et al.,\n2023). Moreover, we evaluate the performance of the MineMA-8B and the MineMA-70B model\nto investigate the impact of model size on task performance. As shown in Tab. 5, the MineMA-\n8B model outperforms the Baichuan2-7B and Qwen2-7B models in terms of success rate and time\nefficiency. Moreover, the MineMA-70B model shows superior performance compared with the\nMineMA-8B model. Across all tasks, MineMA-70B demonstrates higher success rates and gener-\nally lower average execution times and LLM iterations.\n5.2.3\nAUTONOMOUS EXPLORATION TASK\nIn the autonomous exploration task, the agent is required to explore the Minecraft world freely with-\nout any specific goals. We compare our agent with different Minecraft-based agent methods (Voy-\nager (Wang et al., 2023a) and DEPS (Wang et al., 2023b)) and different LLM-based agent techniques\n(ReAct (Yao et al., 2023) and AutoGPT (Significant-Gravitas, 2023)) on this task. Note that we re-\nproduced different LLM-based agent techniques following the same settings as in Voyager (Wang\n7\nPreprint\nTable 4: Performance comparison of different models on the single-round long-term planning task.\n“Health” refers to the remaining health points. “# LLM iters” is the number of LLM iterations (call-\ning LLM) required to complete the task. “Time (min)” refers to the minutes spent in both gathering\nmaterials and crafting equipment to defeat different monsters. All evaluation metrics are calculated\nonly for successful tasks. ± corresponds to one standard deviation of the average evaluation over\nsuccessful tasks. Bold and italics mean the best and the second-best results. “-” indicates that health\nis not a relevant metric in the scenarios. “N\/A” indicates that all tasks fail.\nTask\nModel\nSuccess Rate\nHealth\nTime (min)\n# LLM Iters\n1 zombie\nVoyager\n3 \/ 3\n20.0 ± 0.0\n9.9 ± 6.0\n67.3 ± 41.7\nLLaMA-3-8B\n4 \/ 8\n20.0 ± 0.0\n8.3 ± 4.2\n6.1 ± 4.1\nMineMA-8B\n8 \/ 8\n19.4 ± 2.3\n8.8 ± 5.4\n10.0 ± 5.8\n1 spider\nVoyager\n3 \/ 3\n10.8 ± 8.0\n9.4 ± 8.8\n19.0 ± 1.4\nLLaMA-3-8B\n4 \/ 8\n19.4 ± 1.0\n12.1 ± 3.8\n8.4 ± 3.5\nMineMA-8B\n8 \/ 8\n19.3 ± 1.6\n8.3 ± 6.7\n15.2 ± 6.0\n1 skeleton\nVoyager\n2 \/ 3\n16.5 ± 0.0\n7.4 ± 2.9\n46.0 ± 32.0\nLLaMA-3-8B\n4 \/ 8\n17.6 ± 2.7\n8.1 ± 3.5\n8.9 ± 3.7\nMineMA-8B\n8 \/ 8\n13.6 ± 5.9\n8.6 ± 7.3\n12.1 ± 7.0\n1 zomb-\nified piglin\nVoyager\n3 \/ 3\n19.0 ± 1.4\n14.5 ± 4.7\n50.3 ± 26.2\nLLaMA-3-8B\n4 \/ 8\n19.9 ± 0.4\n9.2 ± 3.9\n10.0 ± 4.2\nMineMA-8B\n8 \/ 8\n18.7 ± 1.9\n8.5 ± 6.1\n11.7 ± 6.2\n1 ender-\nman\nVoyager\n2 \/ 3\n11.0 ± 9.0\n22.8 ± 1.7\n28.0 ± 4.0\nLLaMA-3-8B\n2 \/ 8\n15.1 ± 7.3\n13.0 ± 3.0\n6.8 ± 1.9\nMineMA-8B\n4 \/ 8\n19.8 ± 0.5\n10.4 ± 6.3\n12.5 ± 5.4\n1 zombie\nvillager\nVoyager\n2 \/ 3\n20.0 ± 0.0\n12.6 ± 2.0\n50.0 ± 3.0\nLLaMA-3-8B\n7 \/ 8\n19.6 ± 1.1\n12.7 ± 5.3\n11.0 ± 5.3\nMineMA-8B\n8 \/ 8\n20.0 ± 0.0\n9.0 ± 3.6\n12.8 ± 6.1\n1 cave\nspider\nVoyager\n2 \/ 3\n16.5 ± 3.5\n10.0 ± 1.8\n79.2 ± 29.0\nLLaMA-3-8B\n6 \/ 8\n19.5 ± 1.2\n12.0 ± 6.3\n19.5 ± 1.2\nMineMA-8B\n7 \/ 8\n20.0 ± 0.0\n3.6 ± 2.6\n8.6 ± 8.8\n1 wither\nskeleton\nVoyager\n1 \/ 3\n20.0 ± 0.0\n20.9 ± 0.0\n100.0 ± 0.0\nLLaMA-3-8B\n6 \/ 8\n13.2 ± 6.0\n11.7 ± 3.7\n12.3 ± 2.7\nMineMA-8B\n7 \/ 8\n17.3 ± 3.7\n11.0 ± 6.8\n12.6 ± 6.9\n1 zombie,\n1 spider\nVoyager\n1 \/ 3\n17.5 ± 0.0\n5.9 ± 0.0\n21.0 ± 0.0\nLLaMA-3-8B\n1 \/ 8\n20.0 ± 0.0\n8.5 ± 0.0\n6.0 ± 0.0\nMineMA-8B\n5 \/ 8\n16.4 ± 4.1\n10.6 ± 6.7\n12.0 ± 4.9\n1 zombie,\n1 skeleton\nVoyager\n2 \/ 3\n19.0 ± 1.0\n15.0 ± 8.6\n40.5 ± 20.5\nLLaMA-3-8B\n1 \/ 8\n0.2 ± 0.0\n13.5 ± 0.0\n9.0 ± 0.0\nMineMA-8B\n3 \/ 8\n12.8 ± 2.8\n14.0 ± 1.9\n10.3 ± 2.8\n3 zombies\nVoyager\n2 \/ 3\n7.8 ± 4.2\n8.2 ± 0.4\n61.0 ± 29.0\nLLaMA-3-8B\n1 \/ 8\n3.7 ± 0.0\n14.3 ± 0.0\n8.0 ± 0.0\nMineMA-8B\n1 \/ 8\n5.2 ± 0.0\n11.1 ± 0.0\n14.0 ± 0.0\ncook meat\nVoyager\n0 \/ 3\n-\nN\/A\nN\/A\nLLaMA-3-8B\n1 \/ 8\n-\n20.3 ± 0.0\n19.0 ± 0.0\nMinema-8B\n2 \/ 8\n-\n21.4 ± 1.2\n30.0 ± 10.0\nanimal\nhusbandry\nVoyager\n1 \/ 3\n-\n19.0 ± 0.0\n12.0 ± 0.0\nLLaMA-3-8B\n2 \/ 8\n-\n15.3 ± 7.6\n31.0 ± 4.0\nMinema-8B\n3 \/ 8\n-\n16.8 ± 7.8\n26.7 ± 16.2\net al., 2023a). As shown in Fig. 5, our agent with the MineMA-8B model can achieve superior\nperformance compared with all baselines, indicating that the agent can autonomously explore the\nMinecraft world without specific goals. It is notable that our agent with the MineMA-8B model can\noutperform Voyager (Wang et al., 2023a) with GPT-4o-mini or GPT-3.5.\n8\nPreprint\nTable 5: Performance comparison of different models on the dynamic-immediate planning task. All\nevaluation metrics are calculated only for successful tasks. “N\/A” indicates that all tasks fail. Please\nrefer to Appendix F.4 for easier visual inspection.\nTask\nModel\nSuccess Rate\nTime (min)\n# LLM Iters\nCollect Seeds\nBaichuan2-7B\n2 \/ 5\n1.8 ± 1.4\n3.0 ± 2.8\nQwen2-7B\n2 \/ 5\n3.8 ± 1.5\n4.5 ± 0.7\nMineMA-8B\n5 \/ 5\n1.3 ± 1.4\n1.4 ± 0.9\nMineMA-70B\n5 \/ 5\n1.4 ± 1.6\n1.0 ± 0.0\nHoe Farmland\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n2 \/ 5\n15.7 ± 16.2\n19.5 ± 10.6\nMineMA-8B\n2 \/ 5\n17.2 ± 14.7\n26.5 ± 9.2\nMineMA-70B\n4 \/ 5\n10.2 ± 6.7\n11.8 ± 2.6\nShear Sheep\nBaichuan2-7B\n1 \/ 5\n26.0 ± 0.0\n30.0 ± 0.0\nQwen2-7B\n2 \/ 5\n11.0 ± 2.8\n10.8 ± 1.5\nMineMA-8B\n2 \/ 5\n15.7 ± 10.9\n13.0 ± 9.9\nMineMA-70B\n3 \/ 5\n6.9 ± 7.8\n11.0 ± 7.5\nMilk Cow\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n1 \/ 5\n26.1 ± 0.0\n30.0 ± 0.0\nMineMA-8B\n1 \/ 5\n7.2 ± 0.0\n7.0 ± 0.0\nMineMA-70B\n2 \/ 5\n8.6 ± 10.0\n10.0 ± 11.3\nCook Meat\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n0 \/ 5\nN\/A\nN\/A\nMineMA-8B\n1 \/ 5\n25.6 ± 0.0\n38.0 ± 0.0\nMineMA-70B\n2 \/ 5\n20.2 ± 8.5\n24.0 ± 2.8\nObtain Leather\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n1 \/ 5\n14.9 ± 0.0\n16.0 ± 0.0\nMineMA-8B\n4 \/ 5\n15.0 ± 8.7\n17.8 ± 15.2\nMineMA-70B\n5 \/ 5\n7.4 ± 7.8\n8.8 ± 8.6\nMake Sugar\nBaichuan2-7B\n2 \/ 5\n16.2 ± 15.6\n22.0 ± 18.4\nQwen2-7B\n2 \/ 5\n15.4 ± 7.0\n15.5 ± 9.2\nMineMA-8B\n5 \/ 5\n4.3 ± 1.9\n7.0 ± 1.9\nMineMA-70B\n5 \/ 5\n4.3 ± 4.4\n7.8 ± 4.0\nCollect Water\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n1 \/ 5\n10.0 ± 0.0\n10.0 ± 0.0\nMineMA-8B\n4 \/ 5\n10.4 ± 3.0\n8.8 ± 5.5\nMineMA-70B\n5 \/ 5\n9.3 ± 4.8\n9.4 ± 3.7\n5.3\nABLATION STUDY\nWe conduct ablation studies on two core components of the ODYSSEY agent, including the LLM\nplanner and the open-world skill library. The results are shown in Fig. 5. In the autonomous ex-\nploration task, the LLM planner is responsible for generating a comprehensive plan based on the\nopen-world skill library. The ablation study demonstrates that the planner is indispensable for the\nagent to effectively navigate the complex Minecraft environment. Additionally, our experimental\nresults indicate that the absence of the open-world skill library significantly degrades performance.\nWithout the open-world skill library, the 8B LLM model alone is largely incapable of generating\nexecutable codes for the agent. This underscores the critical role of the open-world skill library in\nenabling the agent to perform complex tasks within the open-world setting of Minecraft.\n6\nRELATED WORKS\nMinecraft agents have been widely studied in recent years to test the capabilities of autonomous\nagents in open-world environments. Previous works focused on training Minecraft agents with\nreinforcement learning (Tessler et al., 2017; Oh et al., 2017; Lin et al., 2022; Mao et al., 2022; Hafner\n9\nPreprint\nDEPS with GPT-4o\nVoyager with GPT-3.5-Turbo\nVoyager with GPT-4o-mini\nReAct with GPT-4o-mini\nAutoGPT with GPT-4o-mini\nOdyssey with LLaMA3-8B\nOdyssey with MineMA3-8B w\/o Planner\nOdyssey with MineMA3-8B w\/o Skill Library\nOdyssey with MineMA3-8B\n0\n20\n40\n60\n80\nIteration Number\n0\n10\n20\n30\n40\n# Distinct Items Obtained\n(a) Exploration Curves\n0\n10\n20\n30\n40\n# Distinct Items Obtained\n0\n1000\n2000\n3000\n4000\nDistance Traveled\n0\n200\n400\n600\n# Items Crafted\n0\n50\n100\n150\n200\n# R&A Unlocked\n(b) Evaluation Metrics\nFigure 5: Performance comparison of different models on autonomous exploration tasks. To make\nthe results in figures clearer for readers, we adopt a 50% confidence interval to plot the error region.\net al., 2023) or imitation learning (Baker et al., 2022; Cai et al., 2023; Lifshitz et al., 2023), which\nare extensively used in the MineRL (Guss et al., 2019) competition to solve the ObtainDiamond\ntask. With the rapid development of LLMs, numerous studies leverage LLMs to enhance agent\ncapabilities (Zhang et al., 2023a; Zhu et al., 2023; Feng et al., 2023; Zhao et al., 2023; Wang et al.,\n2023a;b; Zhou et al., 2024a). Among these, several works (Li et al., 2023; Yuan et al., 2023; Wang\net al., 2023c; Qin et al., 2023; Ding et al., 2023) employ LLMs to guide skill learning in Minecraft,\nenabling agents to act in a human-like way. However, these methods mainly focus on learning\nprimitive skills from scratch, lacking a reusable skill library. Voyager (Wang et al., 2023a) builds\na skill library by allowing the LLM to write its own skills. However, Voyager must rely on GPT-4\nfor high-quality skill generation, incurring substantial costs. This expense can be prohibitive for\nmany researchers. In contrast, ODYSSEY provides an open-world skill library that agents can call\nupon, achieving performance comparable to Voyager with GPT-4, but using only 8B LLMs. This\nmakes ODYSSEY significantly more accessible and cost-effective, enabling LLM-based agents to\nefficiently generate complex policies for broader exploration.\nOpen-world environments have gained considerable attention from research communities (Cao\net al., 2020; Chevalier-Boisvert et al., 2018; Juliani et al., 2019; Shen et al., 2021; Srivastava et al.,\n2022; Du et al., 2023). Minecraft, with its diverse tasks and mature game mechanics, has emerged\nas an ideal test-bed for open-world tasks. Built on Minecraft, MineRL (Guss et al., 2019) imple-\nments a simulation environment for agent learning. MineDojo (Fan et al., 2022) further extends\nMineRL with thousands of diverse tasks. MCU (Lin et al., 2023) collects a variety of atom tasks,\noffering a method to generate infinite tasks by combining the atom tasks. However, existing bench-\nmarks mainly focus on providing basic programmatic tasks to evaluate agents learned from scratch.\nOur ODYSSEY benchmark is built on top of the skill library, enabling the agents to bypass basic\nprogrammatic tasks and focus on complex open-world challenges.\n7\nCONCLUSION\nThis work proposes ODYSSEY to empower agents with open-world skills in the Minecraft environ-\nment. We introduce (1) an interactive agent endowed with an extensive open-world skill library com-\nprising various primitive skills and compositional skills; (2) a fine-tuned LLaMA-3 model, trained on\na large-scale question-answering dataset sourced from the Minecraft Wiki; (3) a new agent capabil-\nity benchmark that encompasses tasks requiring long-term planning, dynamic-immediate planning,\nand autonomous exploration. The public availability of all datasets, model weights, and code will\nfacilitate future research in the development of autonomous agents. We hope that ODYSSEY will\ninspire further innovation and progress in the field of autonomous agent development.\nLimitations and Future Works. The proposed open-world skill library enables the use of open-\nsource LLMs as the foundation for agents to call upon skills, avoiding the high costs associated with\nprevious work using GPT-4 (Wang et al., 2023a; Li et al., 2023; Qin et al., 2023). However, the open-\nsource LLMs are prone to generating hallucinations, leading to a decrease in agent performance.\n10\nPreprint\nThus, our future research will focus on employing retrieval-augmented generation to improve LLMs\nin Minecraft. Additionally, this work focuses on developing and evaluating text-based LLMs in the\ncontext of Minecraft, with visual aspects currently out of scope. Looking ahead, we plan to integrate\nvisual understanding into the skill library to enhance the agent capabilities.\nREFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical\nreport. arXiv preprint arXiv:2303.08774, 2023.\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. In Advances in Neural Information Processing Systems, volume 35, pp.\n24639–24654, 2022.\nShaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control\nthrough goal-aware representation learning and adaptive horizon prediction. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 13734–13744, 2023.\nTianshi Cao, Jingkang Wang, Yining Zhang, and Sivabalan Manivasagam. Babyai++: Towards\ngrounded-language learning beyond memorization. arXiv preprint arXiv:2004.07200, 2020.\nMaxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia,\nThien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of\ngrounded language learning. In International Conference on Learning Representations, 2018.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su.\nMind2web: Towards a generalist agent for the web. In Advances in Neural Information Processing\nSystems, volume 36, pp. 28091–28114, 2023.\nZiluo Ding, Hao Luo, Ke Li, Junpeng Yue, Tiejun Huang, and Zongqing Lu. Clip4mc: An rl-friendly\nvision-language model for minecraft. arXiv preprint arXiv:2303.10571, 2023.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied mul-\ntimodal language model. In Proceedings of the International Conference on Machine Learning,\nvolume 202, pp. 8469–8488, 2023.\nYuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek\nGupta, and Jacob Andreas. Guiding pretraining in reinforcement learning with large language\nmodels. In Proceedings of the International Conference on Machine Learning, volume 202, pp.\n8657–8677, 2023.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In Advances in Neural Information Processing Systems,\nvolume 35, pp. 18343–18362, 2022.\nYicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, and Zongqing Lu. Llama rider: Spurring\nlarge language models to explore the open world. arXiv preprint arXiv:2310.08922, 2023.\nWilliam H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: a large-scale dataset of minecraft demonstrations.\nIn Proceedings of the International Joint Conference on Artificial Intelligence, pp. 2442–2448,\n2019.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\n11\nPreprint\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference\non Learning Representations, 2021.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations, 2021.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda\nLuu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning\nthrough planning with language models. In Proceedings of the Conference on Robot Learning,\nvolume 205, pp. 1769–1782, 2022.\nIat Long Iong, Xiao Liu, Yuxuan Chen, Hanyu Lai, Shuntian Yao, Pengbo Shen, Hao Yu, Yuxiao\nDong, and Jie Tang. OpenWebAgent: An open toolkit to enable web agents on large language\nmodels. In Proceedings of the Annual Meeting of the Association for Computational Linguistics,\npp. 72–81, 2024.\nArthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter Henry,\nAdam Crespi, Julian Togelius, and Danny Lange. Obstacle tower: A generalization challenge in\nvision, control, and planning. In Proceedings of the International Joint Conference on Artificial\nIntelligence, pp. 2684–2691, 2019.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural\nInformation Processing Systems, volume 33, pp. 9459–9474, 2020.\nHao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li,\nLewei Lu, and Jifeng Dai. Auto mc-reward: Automated dense reward design with large language\nmodels for minecraft. arXiv preprint arXiv:2312.09238, 2023.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence,\nand Andy Zeng. Code as policies: Language model programs for embodied control. In IEEE\nInternational Conference on Robotics and Automation, pp. 9493–9500, 2023.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. In Advances in Neural Information Processing Systems,\nvolume 36, pp. 69900–69929, 2023.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74–81, 2004.\nHaowei Lin, Zihao Wang, Jianzhu Ma, and Yitao Liang. Mcu: A task-centric framework for open-\nended agent evaluation in minecraft. arXiv preprint arXiv:2310.08367, 2023.\nZichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, and Wei Yang. Juewu-mc: Playing\nminecraft with sample-efficient hierarchical reinforcement learning. In Proceedings of the Inter-\nnational Joint Conference on Artificial Intelligence, pp. 3257–3263, 2022.\nHangyu Mao, Chao Wang, Xiaotian Hao, Yihuan Mao, Yiming Lu, Chengjie Wu, Jianye Hao, Dong\nLi, and Pingzhong Tang. Seihai: A sample-efficient hierarchical ai for the minerl competition.\nIn Proceedings of the International Conference on Distributed Artificial Intelligence, pp. 38–51,\n2022.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\nJunhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli.\nZero-shot task generalization\nwith multi-task deep reinforcement learning. In Proceedings of the International Conference on\nMachine Learning, volume 70, pp. 2661–2670, 2017.\n12\nPreprint\nOpenAI. Introducing chatgpt. 2023.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the Annual Meeting of the Association for\nComputational Linguistics, pp. 311–318, 2002.\nPrismarineJS. Mineflayer: Create minecraft bots with a powerful, stable, and high level javascript\napi. https:\/\/github.com\/PrismarineJS\/mineflayer, 2023.\nYiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing\nShao. Mp5: A multi-modal open-ended embodied system in minecraft via active perception.\narXiv preprint arXiv:2312.07472, 2023.\nScott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas.\nA generalist agent.\narXiv preprint\narXiv:2205.06175, 2022.\nNils Reimers and Iryna Gurevych.\nSentence-bert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the Conference on Empirical Methods in Natural Language Pro-\ncessing, pp. 3982–3992, 2019.\nManolis Savva, Jitendra Malik, Devi Parikh, Dhruv Batra, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, and Vladlen Koltun. Habitat: A\nplatform for embodied AI research. In Proceedings of the IEEE\/CVF International Conference\non Computer Vision, pp. 9338–9346, 2019.\nDhruv Shah, Michael Robert Equi, Bła˙zej Osi´nski, Fei Xia, Brian Ichter, and Sergey Levine. Naviga-\ntion with large language models: Semantic guesswork as a heuristic for planning. In Proceedings\nof the Conference on Robot Learning, volume 229, pp. 2683–2699, 2023.\nBokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Clau-\ndia Pérez-D’Arpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, Micael Tchapmi, Kent\nVainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. igibson 1.0: A simulation environment for\ninteractive tasks in large realistic scenes. In Proceedings of the IEEE\/RSJ International Confer-\nence on Intelligent Robots and Systems, pp. 7520–7527, 2021.\nSignificant-Gravitas.\nAutogpt:\nBuild & use ai agents.\nhttps:\/\/github.com\/\nSignificant-Gravitas\/AutoGPT, 2023.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans\nusing large language models. In Proceedings of the IEEE International Conference on Robotics\nand Automation, pp. 11523–11530, 2023.\nSanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia, Kent Elliott\nVainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, Silvio Savarese, Hyowon Gweon,\nJiajun Wu, and Li Fei-Fei. Behavior: Benchmark for everyday household activities in virtual,\ninteractive, and ecological environments. In Proceedings of the Conference on Robot Learning,\nvolume 164, pp. 477–490, 2022.\nChen Tessler, Shahar Givony, Tom Zahavy, Daniel Mankowitz, and Shie Mannor. A deep hier-\narchical approach to lifelong learning in minecraft. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 31, pp. 1553–1561, 2017.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding.\nIn\nInternational Conference on Learning Representations, 2018.\n13\nPreprint\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman.\nSuperglue: A stickier benchmark for general-purpose language\nunderstanding systems. In Advances in Neural Information Processing Systems, volume 32, pp.\n3261–3275, 2019.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\nTransactions on Machine Learning Research, 2023a.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on large\nlanguage model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. De-\nscribe, explain, plan and select: interactive planning with llms enables open-world multi-task\nagents. In Advances in Neural Information Processing Systems, volume 36, pp. 34153–34189,\n2023b.\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin,\nZhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with\nmemory-augmented multimodal language models. arXiv preprint arXiv:2311.05997, 2023c.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. In Advances in Neural Information Processing Systems, volume 35, pp. 24824–24837,\n2022a.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances\nin Neural Information Processing Systems, volume 35, pp. 24824–24837, 2022b.\nXinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, and Börje F Karls-\nson. A survey on game playing agents and large models: Methods, applications, and challenges.\narXiv preprint arXiv:2403.10249, 2024.\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan,\nDian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint\narXiv:2309.10305, 2023.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, et al.\nQwen2 technical report.\narXiv preprint\narXiv:2407.10671, 2024a.\nSonghua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, and Hongying\nZan. Zhongjing: Enhancing the chinese medical capabilities of large language model through\nexpert feedback and real-world multi-turn dialogue. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 38, pp. 19368–19376, 2024b.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. In International Conference on\nLearning Representations, 2023.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Skill reinforcement learning and planning for open-world long-horizon tasks. arXiv preprint\narXiv:2303.16563, 2023.\nChi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, and Zongqing Lu. Creative agents: Empowering\nagents with imagination for creative tasks. arXiv preprint arXiv:2312.02519, 2023a.\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming Chen, Jianquan Li, Xi-\nangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, and Haizhou Li. Huatuogpt,\ntowards taming language model to be a doctor. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing, pp. 10859–10885, 2023b.\n14\nPreprint\nZhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian Ye, Jenq-\nNeng Hwang, and Gaoang Wang. See and think: Embodied agent in virtual environment. arXiv\npreprint arXiv:2311.15209, 2023.\nEnshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and\nJing Shao. Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-\nworld control. arXiv preprint arXiv:2403.12037, 2024a.\nGengze Zhou, Yicong Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language naviga-\ntion with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 38, pp. 7641–7649, 2024b.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n15\nPreprint\nAppendix\nTable of Contents\nA Discussion on Societal Impacts\n17\nB\nDiscussion on Migrating Odyssey to Other Domains\n17\nC Open-World Skill-based Interactive Agent\n17\nC.1\nOpen-World Skill Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nC.2\nLLM Planner\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nC.3\nLLM Actor\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nC.4\nLLM Critic\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nD Fine-tune Minecraft LLM\n25\nD.1\nDataset Generation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nD.2\nModel Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nD.3\nModel Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nE\nAgent Capability Benchmark\n33\nE.1\nLong-term Planning Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nE.2\nDynamic-immediate Planning Task . . . . . . . . . . . . . . . . . . . . . . . .\n33\nE.3\nAutonomous Exploration Task\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nE.4\nSpecific Agent Capability Requirements for Different Tasks . . . . . . . . . . .\n34\nF\nExperiments\n34\nF.1\nExperimental Details\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nF.2\nAgent Capability Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\nF.3\nAblation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nF.4\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n16\nPreprint\nA\nDISCUSSION ON SOCIETAL IMPACTS\nWhen developing autonomous embodied agents within Minecraft, the negative impacts are rela-\ntively minimal. Minecraft provides a controlled environment to test these technologies. Concerns\ninclude potential over-reliance by players, reducing their exploratory and creative thinking, minor\ndata privacy issues due to the collection of anonymized player data, and possible impacts on game\nbalance, particularly in multiplayer settings. Overall, Minecraft is an ideal experimental platform\nwhere these mild negative impacts can be effectively managed.\nB\nDISCUSSION ON MIGRATING ODYSSEY TO OTHER DOMAINS\nThe skill library designed for Minecraft is built with modularity and generalizability in mind, al-\nlowing for potential adaptation to other domains such as robot manipulation (Liang et al., 2023;\nSingh et al., 2023), navigation (Zhou et al., 2024b; Shah et al., 2023), and other game-playing\nenvironments (Xu et al., 2024). These skills abstract underlying actions and focus on high-level\ninteractions, allowing them to be adapted to different environments by redefining low-level actions\nwithout changing the overall structure of the skill library. Even without direct API access, basic\naction spaces (e.g., keyboard and mouse operations in games, or movement operations in robotics)\ncan be employed to construct primitive skills. Prior research in robotic manipulation, including\nCaP (Liang et al., 2023) and ProgPrompt (Singh et al., 2023), demonstrates how primitive skills\nsuch as picking and placing objects or opening containers can be built from basic actions. More-\nover, we believe that the concept of \"skills\" should extend beyond code APIs to include knowledge\nfrom various sources. For example, handbooks can provide informational segments treated as skills,\nretrievable by LLMs using techniques like retrieval-augmented generation (Lewis et al., 2020), en-\nhancing decision-making.\nTo fine-tune the LLaMA-3 model for the Minecraft agent, we crawled the Minecraft Wiki and used a\nGPT-assisted approach to generate an instruction dataset. Researchers in other domains can replicate\nthis process to create their own instruction datasets. To facilitate this, we have also open-sourced\nour Minecraft Wiki crawler, which can be easily modified to crawl similar Wiki websites for other\ndomains. Additionally, our benchmark tasks evaluate agent performance from three perspectives:\nlong-term planning, dynamic-immediate planning, and autonomous exploration. These dimensions\neffectively assess the capabilities of open-world autonomous agents. Researchers in other domains\ncan adopt these perspectives to design comprehensive evaluation tasks for their needs.\nC\nOPEN-WORLD SKILL-BASED INTERACTIVE AGENT\nC.1\nOPEN-WORLD SKILL LIBRARY\nC.1.1\nPRIMITIVE SKILLS\nPrimitive skills encompass a series of underlying interfaces on top of Mineflayer JavaScript\nAPIs (PrismarineJS, 2023), divided into two main categories: 32 operational skills and 8 spatial\nskills. In addition to Voyager’s 18 operational skills (Wang et al., 2023a), 14 operational skills\nimplemented by us are presented as follows:\n• plantSeeds(bot, type): Let the agent find the nearest farmland and plant a partic-\nular kind of seed.\n• feedAnimals(bot, type, count=1): Let the agent find the nearest animals of a\nparticular species and numbers and feed them with the appropriate food.\n• killAnimal(bot, type): Let the agent kill a particular kind of animal using the best\nsword in its inventory.\n• killMonsters(bot, type, count=1): Let the agent kill monsters nearby of a\nparticular species and numbers using the best sword in its inventory.\n• cookFood(bot, type, count=1): Let the agent cook food of a particular kind and\nnumbers using coal and furnace.\n• eatFood(bot, type): Let the agent eat a particular kind of food.\n17\nPreprint\n• equipArmor(bot): Let the agent equip the best armor(helmet, chestplate, leggings and\nboots) in its inventory.\n• equipSword\/Pickaxe\/Axe\/Hoe\/Shovel(bot): Let the agent equip the best cor-\nresponding tool in its inventory.\n• getLogs\/PlanksCount(bot): Return the number of logs\/planks (counted in seven\ndifferent categories) in the inventory.\nAdditionally, we pioneer 8 spatial skills that Voyager (Wang et al., 2023a) lacks, allowing for en-\nvironmental interactions based on the agent coordinates. The spatial skills implemented by us are\npresented as follows:\n• findSuitablePosition(bot): Let the agent find the best nearby location for plac-\ning devices such as a crafting table or furnace. The block must be minecraft:air and\nat least one adjacent reference block exists.\n• checkAdjacentBlock(bot, types, x, y, z): Check blocks adjacent to the\nblock at position (x, y, z). Return true if any of the adjacent blocks match the specified\ntypes.\n• checkBlockAbove(bot, type, x, y, z): Check block above the block at po-\nsition (x, y, z). Return true if the above block matches the specified type.\n• checkBlocksAround(bot, type, x, y, z): Check blocks around the block at\nposition (x, y, z).Return true if any of the around blocks match the specified type.\n• checkNearbyBlock(bot, types, x, y, z, r):\nCheck blocks in a radius\naround the block at position (x, y, z). Return true if any block within the radius matches the\nspecified types.\n• checkNoAdjacentBlock(bot, types, x, y, z): Check adjacent blocks of\nblock at position (x, y, z). Return true if not all adjacent blocks are within the speci-\nfied types.\n• goto(bot, x, y, z): Let the agent go to the corresponding position (x, y, z) until it\nreaches the destination.\n• getAnimal(bot, type, x, y, z): Let the agent attract a particular kind of ani-\nmal to a particular position (x, y, z) with the appropriate food.\nC.1.2\nCOMPOSITIONAL SKILLS\nAll compositional skills are encapsulated by the Mineflayer APIs and the aforementioned primitive\nskills, while higher-level compositional skills recursively call lower-level ones. Fig. 6 illustrates the\nnested relationships among the 13 skills required to complete the mineDiamond task. We classify\nall compositional skills into main types as follows:\n• mineX(bot): Equip the agent with the appropriate tools and find the nearest specific\nblock to mine it.\n• craftX(bot): Let the agent collect the necessary materials and check if the crafting\ntable exists in the inventory (if needed), to craft a specific tool or something.\n• smeltX(bot): Let the agent check the furnace and fuel, and to smelt the specified ma-\nterials.\n• collectX(bot): Similar to mineX, used to collect multiple quantities of a certain item.\n• makeX(bot): Similar to craftX, used to make food.\n• cookX(bot): Similar to smeltX, used to cook food.\n• plantX(bot): Let the agent check the inventory for seeds, collect them if not present,\nand plant them in nearby farmland.\n• breedX(bot): Let the agent check the inventory for the required corresponding feed,\nfind the nearest two animals, feed them, and facilitate their breeding.\n• killX(bot): Let the agent equip the best sword in the inventory, find the nearest specific\nanimal or monster, kill it, and collect the dropped items.\n• placeX(bot): Let the agent place an item at its current or a nearby suitable location,\nand if the item is not in inventory, craft it first.\nAdditionally, there are several other compositional skills aimed at executing specific behaviors, such\nas catchFish, hoeFarmland, shearSheep, takeAndMoveMinecart.\n18\nPreprint\ncraftSticks\ncraftWoodenPlanks\nmineWoodLog\ncraftWoodenPickaxe\ncraftCraftingTable\ncraftIronPickaxe\nsmeltRawIron\nmineIronOre\nmineCoalOre\ncraftFurnace\ncraftStonePickaxe\nmineCobblestone\nmineDiamond\nFigure 6: An illustrative diagram of the skill recursive method for the mineDiamond task. The\nfour colors depicted represent four different technological levels (wood, stone, iron, and diamond)\nfollowing the Minecraft tech-tree.\nC.2\nLLM PLANNER\nODYSSEY relies on LLMs to generate language-based plans. In our Minecraft experiment, we pro-\npose three novel tasks (long-term planning task, dynamic-immediate planning task and autonomous\nexploration task) for agents to explore. Therefore we designate three types of prompt messages for\nthem respectively, offering LLM Planner the ability to generate different routines on different tasks.\nThe format of the prompt is presented thus:\n• \"SYSTEM\" role: A high-level instruction that gives directions to the model behavior. It\nsets an overall goal for the interaction and provides external information.\n• \"USER\" role: Detailed information like environment, states and achievements of the agent\nwill be provided to the planner for the next immediate subgoals.\n• \"ASSISTANT\" role: A guideline generated by the planner.\nC.2.1\nLONG-TERM PLANNING\nWe design a suite of combat tasks to assess the long-term planning capabilities of agents, where the\nLLM Planner should plan to craft appropriate weapons and equipment to defeat monsters.\nThe input prompt to LLM consists of several components:\n• Ultimate goals: The monsters that need to be defeated.\n• Directives and Behavior Constraints: These ensure that the proposed tasks are both achiev-\nable and verifiable.\n• Information of last combat: Incorporating information from previous battles enables the\ndevelopment of more efficient plan.\nLong-term Planning System Prompt\n—Overall goals—\nYour goal is to generate the plan that can defeat all monsters while using the shortest\ntime. So, more is not always better when proposing your plan list.\n—External information—\nIn Minecraft, combating with monsters requires weapons and armor.\nThe weapon\noptions are limited to \"sword\", while the armor includes \"helmet\", \"chestplate\", \"leggings\",\n19\nPreprint\nand \"boots\". The materials for swords range from low to high level: wooden swords, stone\nswords, iron swords, and diamond swords; The materials for armor range from low to high\nlevel: iron, diamond. The higher the material level, the greater the attack damage of the\nweapon and the better the protection effect of the armor. However, the higher the material\nlevel, the more time it costs to collect.\nTips: Wooden, stone, iron and diamond are the only levels of sword; iron and diamond\nare the only levels of armors; helmet, chestplate, leggings and boots are the only types of\narmors; do not generate information that doesn’t relate to them.\nAfter each round of combat, I will give you:\nEquipment obtained from last round: ...\nHealth after last combat: ...\nCritique: ...\nMonster: The monsters you need to defeat.\n—Directions—\nThe critique (if any) will tell you the subgoal list from the previous round and whether you\nshould trim or add to it. Remember to refer to the critique to adjust your task list. Next, you\nwill start a new combat task, the last round of equipment and health is only for planning\nreference, not related to the current round. Plan from scratch!\n—Behaviour constraints—\nYou must follow the following criteria:\n1) Return a Python list of subgoals that can be completed in order to complete the specified\ntask.\n2) Each subgoal should only start with \"craft\"! do not propose any other type of skills!\n3) Each subgoal should follow a concise format \"craft [material type] [equipment type]\".\nYou should only respond in JSON format as described below:\n[\"subgoal1\", \"subgoal2\", \"subgoal3\", ...]\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nAfter finish collecting weapons and equipment, we also plan an efficient routine to combat with\nmonsters for higher survival rates. For example, monsters that are more harmful and aggressive\nshould be placed in a higher priority. The full prompt for re-ranking the combat order of monsters\nis shown below.\nComabt Order System Prompt\nYou are a helpful assistant that generates the order of fighting monsters to defeat all monsters\nspecified by me.\nI’ll give you a list of monsters, and you need to rearrange the order of monsters according to\nhow hard it is to beat them.\nYou should give priority to monsters that attack the player and do more damage, while\nmonsters that don’t actively attack the player or do less damage should be left behind.\nMake sure your list includes all the monsters in your task.\nThe output format must be exactly same as the input, including the underline.\nIf your task is to combat a single type of monsters, return a list containing only that monster\nas well.\nYou should only respond in JSON format as described below:\n[\"quantity monster1\", \"quantity monster2\", \"quantity monster3\", ...]\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\n20\nPreprint\nC.2.2\nDYNAMIC-IMMEDIATE PLANNING\nIn this kind of task, agents are expected to adapt their plans based on the real-time feedback like\nnearby resources and animals.\nThe input prompt to LLM consists of the following components:\n• Ultimate goals: A suite of farming tasks, such as planting, harvesting, and animal hus-\nbandry.\n• The current states of agent: hunger and health values, position and nearby entities, etc.\n• Achievements of the agent: the current inventory and unlocked equipment, as well as pre-\nviously successful and failed tasks.\nDynamic-immediate Planning System Prompt\n—Overall goals—\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft.\nMy ultimate goal is to \"goals\".\nMake sure that the proposed task is related to the ultimate goal, and do not propose unrelated\ntasks!\n—Directions—\nYou need to plan step by step towards your ultimate goal, so propose necessary pre-\nrequisite tasks first.\nFor example, \"craft hoe\" before \"hoe farmland\", \"collect [type] seeds\" and \"hoe farmland\"\nbefore \"plant seed\", \"kill [animalType]\" before \"cook meat\", \"craft shears\" before \"shear\nsheep\", \"craft bucket\" before \"collect milk\".\nPropose the current task only when you ensure that you have all the necessary dependent\nitems in inventory.\nDon’t ask for repetitive tasks. If you already have an item in your inventory, try not to\ncollect it repeatedly.\nFor example, when you already have a hoe in your inventory, propose \"hoe farmland\"\ninstead of \"craft hoe\" again.\n—External information—\nI will give you the following information:\nUltimate goal: ...\nReference: ...\nBiome: ...\nNearby blocks: ...\nOther blocks that are recently seen: ...\nNearby entities (nearest to farthest): ...\nHealth: Higher than 15 means I’m healthy.\nHunger: Higher than 15 means I’m not hungry.\nInventory (xx\/36): ...\nLogs: The execution logs in last task, you can refer to it to propose next task.\nCompleted tasks so far: ...\nFailed tasks that are too hard: ...\n—Behaviour constraints—\nYou must follow the following criteria:\n1) Please be very specific about what resources I need to collect, what I need to farm, or\nwhat animals I need to breed\/kill.\n2) The next task should follow a concise format, such as \"craft [item]\", \"breed\/kill [animal\n21\nPreprint\ntype]\", \"cook\/eat [food type]\", \"plant [seed type] seed\" or some specific action \"shear\nsheep\", \"collect milk\". Do not propose multiple tasks at the same time. Do not mention\nanything else.\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"Based on the information I listed above, do reasoning about what the next task\nshould be\",\n\"task\": \"The next task\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nC.2.3\nAUTONOMOUS EXPLORATION\nIn this task, the agent is required to explore the Minecraft world freely without any specific goals.\nThis poses a great challenge to the planner for maximal exploration. It should propose suitable tasks\nbased on the current state and environment, e.g., plan to obtain sand or cactus before wood if it finds\nitself in a desert rather than a forest. The input prompt to LLM consists of several components:\n• Guidelines encouraging diverse tasks.\n• The current states of agent: hunger and health values, position and nearby entities, etc.\n• Achievements of the agent: the current inventory and unlocked equipment, as well as pre-\nviously successful and failed tasks.\nAutonomous Exploration System Prompt\n—Overall goals—\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft.\nMy ultimate goal is to discover as many diverse things as possible, accomplish as many\ndiverse tasks as possible and become the best Minecraft player in the world.\n—External information—\nI will give you the following information:\nBiome: ...\nTime: ...\nNearby blocks: ...\nOther blocks that are recently seen: ...\nNearby entities (nearest to farthest): ...\nHealth: Higher than 15 means I’m healthy.\nHunger: Higher than 15 means I’m not hungry.\nPosition: ...\nEquipment: If I have better armor in my inventory, you should ask me to equip it.\nInventory (xx\/36): ...\nChests: ...\nCompleted tasks so far: ...\nFailed tasks that are too hard: ...\n—Directions—\nYou must follow the following criteria:\n1) You should act as a mentor and guide me to the next task based on my current learning\nprogress.\n2) Please be very specific about what resources I need to collect, what I need to craft, or\nwhat mobs I need to kill.\n22\nPreprint\n3) The next task should follow a concise format, such as \"Mine [block]\", \"Craft [item]\",\n\"Smelt [item]\", \"Kill [mob]\", \"Cook [food]\", \"Equip\" etc. It should be a single phrase. Do\nnot propose multiple tasks at the same time. Do not mention anything else.\n4) The next task should be novel and interesting. I should look for rare resources, upgrade\nmy equipment and tools using better materials, and discover new things. I should not be\ndoing the same thing over and over again.\n5) Don’t propose tasks that have already completed once or failed more than three times!\n6) Do not ask me to build or dig shelter even if it’s at night. I want to explore the world and\ndiscover new things. I don’t want to stay in one place.\n7) Tasks that require information beyond the player’s status to verify should be avoided. For\ninstance, \"Placing 4 torches\" and \"Dig a 2x1x2 hole\" are not ideal since they require visual\nconfirmation from the screen. All the placing, building and trading tasks should be avoided.\nDo not propose task starting with these keywords.\n8) For wood-related tasks, you don’t need to emphasize the type of wood, just propose\n\"mine log\" or \"craft planks\".\n—Behaviour constraints—\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"Based on the information I listed above, do reasoning about what the next\ntask should be.\",\n\"task\": \"The next task.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nC.3\nLLM ACTOR\nIn actor, the mapping from higher language subgoals S to lower executable codes is implemented\nthrough query context encoding and similarity retrieval. We employ the following prompt during\nthe generation of query context (Question-Answer pairs).\nQuery Context Prompt\nSYSTEM:\nYou are a helpful assistant that answer my question about Minecraft.\nI will give you the following information:\nQuestion: ...\nYou will answer the question based on the context (only if available and helpful) and your\nown knowledge of Minecraft.\n1) Start your answer with \"Answer: \".\n2) Answer \"Answer: Unknown\" if you don’t know the answer.\nUSER:\nHow to complete S in Minecraft?\nAfter recalling the top-10 relevant skills with the highest scores, we require LLM to determine the\nmost appropriate code for execution within the environment based on their description. The full\nprompt of code selection is shown in the following.\nSkill Selection System Prompt\nYou are a helpful assistant that decides Mineflayer javascript code to complete any Minecraft\ntask specified by me.\nI will give you\nTask: The task I need to complete in this stage.\n23\nPreprint\nPrograms: The description of relevant programs that may use to complete the task.\nProgram used in the last round: ...\nCritique: ...\nYou will choose only one program based on the program description and critique. You\nshould only respond in the format as described below:\n{\n\"program\": \"your selected program name\",\n\"reason\": \"Reason you choose the program.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nPlease ensure that the program name you output should be exactly the same (case-inclusive)\nas the information provided!\nC.4\nLLM CRITIC\nThe LLM critic should evaluate the success of the executed actions by comparing expected out-\ncomes with actual results, thereby providing valuable critiques for refining strategies in subsequent\niterations. We design a chain-of-thought (Wei et al., 2022b) prompting mechanism: We first require\nLLM to reason about the task’s success or failure, then output a boolean variable representing the\nexecution result, and finally provide a critique to the agent if the task fails.\nCritic System Prompt\nYou are required to evaluate if I have met the task requirements in Minecraft. Exceeding\nthe task requirements is also considered a success while failing to meet them requires you\nto provide critique to help me improve.\nI will give you the following information:\nTask: The objective I need to accomplish.\nNearby blocks:\nEntities:\nEquipment: My tools, weapons and armor could sometimes be here.\nChests: If the task requires me to place items in a chest, you can find chest information\nhere.\nCurrent inventory (xx\/36): My final inventory after carry out the task.\nLast inventory (xx\/36): My inventory before carry out the task.\nChat log: The logs during carrying out the task.\n**Note** that you only need to check the changes of my inventory to judge whether I meet\nthe task.\nFor a `craft [item]`task, all you need to do is checking if the item I need to craft is in my\ncurrent inventory or equipment. If in, you should judge it as a success and vice versa.\nFor a `mine [item]`task, you only need to check whether the item is in my current inventory\nor has an increase over the last inventory.\nFor a `hoe`or `plant`task, you only need to check whether the `farmland`or `seed`is in\nNearby Blocks.\nDo not judge the success of a `craft`task based on other materials I have!\nYou can only judge a task failure via chat log, not as a reason to judge a task’s success.\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"reasoning\",\n\"success\": boolean,\n\"critique\": \"critique\",\n}\n24\nPreprint\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nThe input prompt to LLM consists of the following components:\n1. Task proposed by the LLM Planner;\n2. Environment feedback: We provide the agent with nearby blocks and entities that are re-\ncently seen for high-quality critiques. We also give the log information during the execution\nstage;\n3. Achievements of the agent. We offer achievement of the agent like inventory and equipment\nto assess the task’s completeness.\nCritic User Prompt\nTask: Mine 1 wood log\nNearby blocks: birch_leaves, oak_leaves, birch_log, oak_log\nEquipment: [None, None, None, None, 'oak_sapling', None]\nChests: None\nCurrent Inventory (2\/36): 'oak_sapling': 1, 'oak_log': 1\nLast Inventory (0\/36):\nChat log: Mined 1 wood log.\nD\nFINE-TUNE MINECRAFT LLM\nFor detailed code, datasets, and models used in this section, please visit our code for more informa-\ntion. The overall fine-tuning framework is shown in Fig. 7.\nData\nGeneration\nMinecraft\nWiki\nTXT\nFormat\nMD\nFormat\nData\nCleaning\nGPT-\n3.5-\nturbo\nMinecraft\nQ&A\nDataset\nLong\nShort\nBool\nModel\nFine-tuning\nModel\nEvaluation\n  Llama-3-8B-Instruct   \n Llama-3-70B-Instruct  \nMineMA \nPage Driven\nPrompt\nTheme Driven\nPrompt\nGPT-4\nMCQ Evaluation\nDataset v1\nMCQ Evaluation\nDataset v2\nEvaluation\nFine-tune  Minecraft LLM\nNormal\nFine-tune\nLoRA\nFigure 7: An overview of the fine-tune Minecraft LLM framework.\nD.1\nDATASET GENERATION\nD.1.1\nDATA CLEANING\nFor this study, we select two primary sources of information, the Minecraft Fandom Wiki (https:\n\/\/minecraft.fandom.com\/wiki\/Minecraft_Wiki) and the Minecraft Wiki (https:\n\/\/minecraft.wiki\/).\nFor the Minecraft Fandom Wiki, we first crawl the content of all its pages and perform a preliminary\nfiltering on the resulting pages, removing pages that were obviously useless for our fine-tuning task,\n25\nPreprint\nsuch as game version information, and obtaining a series of JSON files. These files still contain a\nsignificant amount of information that we do not need to create the dataset, so we carry out a data\ncleaning process, extracting the text and table content of the original pages, storing them in a series\nof TXT files, with each page corresponding to a TXT file. Through the above method, we obtain the\ncleaned TXT format page information.\nFor Minecraft Wiki, we exclude a few categories that are useless for our fine-tuning task, such as\nHistory, and crawl the content of all other categories’ pages. Similar to the process for Minecraft\nFandom Wiki, these pages also contain a lot of information that we didn’t need. We remove some\nirrelevant sections such as Achievements, Advancements, and History, and only retained the text\nand table content of other useful sections. After that, we store the processed data in markdown files\nand perform segmentation operations based on the number of words in the file. We ensure that the\nword count of the segmented file did not exceed the set limit without cutting the section block, thus\nobtaining clean MD format page information.\nD.1.2\nQ&A PAIRS\nIn this study, we design four types of Q&A pairs for our training dataset as follows:\n• Short Answer Q&A: Questions that can be directly answered with a word or phrase, such\nas names, statistics, etc.\n• Long Answer Q&A: Questions that require one or more complete sentences as answers.\nThese questions are suitable for explanatory or definitional responses, where a detailed\nexplanation or a clear definition is needed to fully address the question.\n• Bool Answer Q&A: Questions that can be answered with a simple True or False.\n• Normal Answer Q&A: QA pairs with no specific form restrictions, used to supplement the\ndataset and ensure its comprehensiveness.\nSome Q&A pair examples are shown as follows:\nShort Answer Q&A\nQ: What mobs spawn naturally in the desert biome during the day in Minecraft?\nA: During the day in Minecraft, zombies in desert biomes are replaced by husks, which\ninflict the Hunger effect and do not burn in sunlight.\nLong Answer Q&A\nQ: How does the use of light sources like torches and lava help in preventing mob spawning\nin Minecraft?\nA: In Minecraft, placing light sources such as torches, lava, glowstone, jack o’lanterns, sea\nlanterns, and lanterns helps prevent hostile mob spawning. Mobs generally spawn in low\nlight conditions, and these light sources provide enough illumination to raise the light level\nabove the threshold needed for mob spawning. Torches are the most commonly used due to\ntheir easy availability and simplicity in creating a well-lit perimeter around an area. Lava is\nalso effective but must be used cautiously as it can ignite nearby flammable materials.\nBool Answer Q&A\nQ: Can skeletons shoot through bamboo?\nA: F\nNormal Answer Q&A\nQ: What are piglins in Minecraft?\nA: Piglins are neutral mobs found in the Nether. They become hostile towards players unless\nthe player is wearing at least one piece of golden armor. They can be bartered with using\n26\nPreprint\ngold ingots for various items, and they wield either a golden sword or a crossbow for melee\nor ranged attacks, respectively.\nD.1.3\nINSTRUCTION DATASETS GENERATION PROMPTS\nWe design four types of prompts for four types of Q&A pairs to generate instruction datasets by\nusing GPT-3.5-Turbo. Here are our prompts.\nShort Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nYour task is to generate at least 30 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 30 question-and-answer examples, and you should generate ques-\ntions within the provided user text that can be directly answered with a word or phrase, such\nas dates, names, statistics, etc. This involves identifying specific, concise information within\nthe text that can be succinctly responded to, ensuring that the answers are clear and directly\nrelated to the questions asked. And you will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nLong Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\n27\nPreprint\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nYour task is to generate at least 20 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 20 question-and-answer examples. Identify questions within the\nprovided user text that require one or more complete sentences as answers. These questions\nshould be suitable for explanatory or definitional responses, where a detailed explanation or\na clear definition is needed to fully address the question. This involves crafting answers that\nare comprehensive and informative, ensuring they adequately explain or define the subject\nmatter in question. And you will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nBool Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\n28\nPreprint\nYour task is to generate at least 10 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 10 question-and-answer examples. Look for questions within the\nprovided user text that can be answered with a simple True or False. This task involves\npinpointing statements or queries within the text that lend themselves to binary responses,\nensuring that the answers are straightforward and unambiguous, clearly indicating whether\nthe statement is true or false based on the information available. And you will do so in this\nformat:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nNormal Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nYour task is to generate at least 20 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 20 question-and-answer examples. And you will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\n29\nPreprint\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nD.2\nMODEL FINE-TUNING\nIn this study, we use the instruction dataset with 390,317 instruction entries mentioned above to\nfine-tune the Minecraft Q&A expert models, using the LoRA fine-tuning method. We name the\nseries of fine-tuned models MineMA. The resulting models include MineMA-8B-v1, MineMA-8B-\nv2, MineMA-8B-v3, derived from the base model LLama-3-8B-Instrument, and MineMA-70B-v1,\nMineMA-70B-v2, derived from the base model LLama-3-70B-Instrument. MineMA-70B series of\nmodels are fine-tuned on four A6000 GPUs, while the remaining models are fine-tuned on a single\nA6000 GPU each. Among the models, MineMA-8B-v1 and MineMA-70B-v1 only undergo one\nround of training without an evaluation process, while the other models are trained with multiple\nrounds that incorporate an evaluation procedure. We use early stopping to halt the training process\nwhen there is no reduction in the evaluation loss over a series of evaluations, and finally save the\nmodel which has the best performance. Some training parameters are shown in Tab. 6.\nTable 6: Training parameters for different MineMA models.\nModel\nLoRA r\nLoRA alpha\nLoRA dropout\nLearning Rate\nWeight Decay\nSingle Round?\nMineMA-8B-v1\n64\n128\n0.1\n1E-04\n1E-04\nT\nMineMA-8B-v2\n32\n64\n0.1\n1E-04\n1E-04\nF\nMineMA-8B-v3\n64\n128\n0.1\n1E-04\n1E-04\nF\nMineMA-70B-v1\n16\n32\n0.1\n1E-04\n1E-04\nT\nMineMA-70B-v2\n64\n128\n0.1\n1E-04\n1E-04\nF\nD.3\nMODEL EVALUATION\nD.3.1\nEVALUATION DATASETS CREATING PROCESS\nIn this study, we utilize GPT-4 to create two evaluation MCQ datasets: a multi-theme MCQ dataset\nand a Wiki-based MCQ dataset. For the multi-theme MCQ dataset, we first summarize the following\nMinecraft content themes:\nGame Basics\nBlocks and Items: Basic blocks, special blocks, tools, weapons, armor, etc.\nSurvival Mechanics: Health, hunger, experience levels, death and respawn, etc.\nWorld Exploration\nBiomes: Characteristics of different biomes, generated structures, unique resources, etc.\nTerrain and Landforms: Features and resource distribution of different terrains.\nMobs and Interactions\nMobs: Characteristics and behaviors of passive, neutral, and hostile mobs.\nCombat System: Monster types, combat tactics, weapons and equipment, enchantments,\npotions, etc.\nTrading and Villagers: Villager professions, trading mechanics, village structures, etc.\n30\nPreprint\nSurvival Skills\nResource Gathering: Methods of obtaining various resources and their uses.\nCrafting and Production: Usage of crafting tables, furnaces, etc., equipment crafting and\nupgrading.\nFarming and Animal Husbandry: Crop planting, animal breeding, automated farms, etc.\nBuilding and Creativity\nBuilding Styles: Various building styles and key points.\nBuilding Techniques: Symmetry, proportion, detail handling in construction, etc.\nInterior Decoration: Interior design, lighting, item placement, etc.\nRedstone Mechanics: Redstone components, circuit design, automated devices, etc.\nSpecial Dimensions\nThe Nether: Peculiarities of the Nether, unique blocks and mobs, special structures, etc.\nThe End: Characteristics of the End, Ender Dragon, cities, ships, etc.\nAdventure and Exploration: Special generated structures like ocean monuments, woodland\nmansions, ruins, fortresses, etc.\nThen, we list different numbers of keywords for each theme based on the amount of relevant knowl-\nedge content. According to the amount of information related to each keyword, we match a number\nfor each keyword, representing the number of multiple-choice questions to be generated based on\nthat keyword. After preparing the groundwork, we use GPT-4 to generate the multi-theme MCQ\ndataset, totaling 1,050 multiple-choice questions. The relevant prompts are shown below, taking the\ngeneration of multiple-choice questions in the Special Dimensions theme as an example:\nSystem Message\nYou are an expert in generating Minecraft quiz questions. Your task is to create multiple-\nchoice questions about the game Minecraft based on the theme of \"Special Dimensions\"\nand the provided keywords. The introduction of the theme of \"Special Dimensions\" is as\nfollows:\nSpecial Dimensions:\nThe Nether: Peculiarities of the Nether, unique blocks and mobs, special structures, etc.\nThe End: Characteristics of the End, Ender Dragon, cities, ships, etc.\nAdventure and Exploration: Special generated structures like ocean monuments, woodland\nmansions, ruins, fortresses, etc.\nProvide four answer options labeled A, B, C, and D. Only one option should be correct.\nAfter the question and options, state the correct answer. Please format the output as follows:\nDifficulty: Easy\/Medium\/Hard\nTopic: Special Dimensions\nKey Word: text\nQuestion: Question text\nOptions: A.text B.text C.text D.text\nCorrect Answer: A\/B\/C\/D\nEnsure that the difficulty distribution of the questions and options is reasonable, and the\nanswers should be detailed and informative.\nUser Message\nPlease generate some Minecraft multiple-choice questions based on the following 5 key-\nwords, covering three difficulty levels: simple, moderate, and difficult. The number after the\nkeyword represents how many multiple-choice questions to generate based on this keyword.\nKeywords:\n31\nPreprint\n{keywords_go_here}\nEnsure that the Q&A content is rich and accurate, and test the player’s understanding of the\ngame. Provide a balanced combination of simple, medium, and difficult questions. Generate\neach question and answer in the given format. Here is an example:\nExample:\nDifficulty: Hard\nTopic: Special Dimensions\nKey Word: End Ship\nQuestion: What exclusive item can be found in the End Ship in Minecraft?\nOptions: A. Netherite B. Dragon Egg C. Elytra D. Beacon\nCorrect Answer: C\nFor the Wiki-based MCQ dataset, we utilize GPT-4’s knowledge of Minecraft-related Wiki content\nto create a set of multiple-choice questions that closely align with the information on the Wiki pages.\nFirstly, we list 615 Minecraft-related keywords based on the importance of the relevant knowledge.\nAfterwards, we generate a Wiki-based MCQ dataset using GPT-4 with designed prompts based on\nthese keywords, totaling 2,083 pieces of data. The prompts we used are as follows:\nSystem Message\nYou are an expert in generating Minecraft multiple-choice questions. Your task is to create\nmultiple choice questions about the game Minecraft based on the provided keywords and\nthe information on the corresponding page on the Minecraft Wiki. Ensure that the source\nof information for the multiple-choice questions you generate is the Minecraft Wiki, while\nensuring the objectivity and accuracy of the multiple-choice questions and ensuring good\nquality.\nProvide four answer options labeled A, B, C, and D. Only one option should be correct.\nAfter the question and options, state the correct answer. Please format the output as follows:\nDifficulty: Easy\/Medium\/Hard\nKey Word: text\nQuestion: Question text\nOptions: A.text B.text C.text D.text\nCorrect Answer: A\/B\/C\/D\nEnsure that the difficulty distribution of the questions and options is reasonable, and the\nanswers should be detailed and informative.\nUser Message\nPlease generate some Minecraft multiple-choice questions based on the following 5 key-\nwords, covering three difficulty levels: simple, moderate, and difficult. The number after the\nkeyword represents the minimum number of multiple-choice questions generated based on\nthe keyword. For important keyword, you should generate more questions.\nKeywords:\n{keywords_go_here}\nEnsure the source of information for the multiple-choice questions you generate is the\nMinecraft Wiki, while ensuring the objectivity and accuracy of the multiple-choice ques-\ntions and ensuring good quality. Provide a balanced combination of simple, medium, and\ndifficult questions. Generate each question and answer in the given format, do not use '#'or\n''.. Here is an example:\nExample:\nDifficulty: Medium\nKey Word: Dirt\nQuestion: What happens when you right-click on a dirt block with a hoe?\nOptions: A. It turns into farmland B. It turns into grass C. It turns into a path block D.\nNothing happens\nCorrect Answer: A\n32\nPreprint\nD.3.2\nEVALUATION RESULTS\nWe use the above two MCQ datasets to evaluate the MineMA series models and the corresponding\nbase models. Each model is tested 5 times with the two datasets. The results are shown in Tab. 7.\nTable 7: The evaluation results based on the MCQ datasets.\nModel\nAverage Accuracy (Multi-theme)\nAverage Accuracy (Wiki-based)\nLlama-3-8b-Instruct\n61.09%\n54.38%\nMineMA-8B-v1\n62.69%\n61.97%\nMineMA-8B-v2\n62.23%\n62.09%\nMineMA-8B-v3\n62.99%\n62.42%\nLlama-3-70b-Instruct\n77.41%\n72.52%\nMineMA-70B-v1\n78.11%\n73.03%\nMineMA-70B-v2\n75.68%\n72.88%\nE\nAGENT CAPABILITY BENCHMARK\nE.1\nLONG-TERM PLANNING TASK\nIn Minecraft, there are a total of 35 hostile creatures. We conducted experiments on both single-\nmonster combat tasks and combined combat tasks (up to three types of monsters), resulting in thou-\nsands of different tasks that can all be implemented through the interfaces we provided.\n• combatEnv(bot, h, r, y): Generate a hollow rectangular arena with a height of\nh and a square base with side length 2r at altitude y, positioning the agent at the exact\ncenter of this enclosed space. This configuration ensures controlled conditions for evalu-\nating combat scenarios, especially considering not being influenced by naturally spawning\nmonsters.\n• summonMob(bot, n = 1, r, type): Facilitate the spawning of hostile creatures\naround the bot. It randomly positions n monsters within a designated range (r to 2r along\nthe x and z axes) from the bot, allowing for the creation of varied combat tasks and enabling\ncomprehensive testing of bot performance under different tactical challenges.\nE.2\nDYNAMIC-IMMEDIATE PLANNING TASK\nIn Minecraft, many farming tasks require interaction with the environment and dynamic planning.\nWe propose a series of tasks that can be accomplished through our skill library, including hoeing\nfarmland, planting seeds, harvesting crops, making food, slaughtering animals, cooking meat, feed-\ning and breeding animals, among others. For example, in the task cook meat, if the agent is\ninformed that there is a chicken nearby, it should plan to \"kill one chicken\" rather than anything\nelse. Additionally, in the task milk cow, the agent must simultaneously monitor the appearance\nof cows in the vicinity and gather materials to craft a bucket to collect the milk.\nE.3\nAUTONOMOUS EXPLORATION TASK\nIn Minecraft, autonomous exploration is the gameplay approach that most closely mimics how hu-\nman players engage with the game. To evaluate the diversity of discoveries made by the agent\nduring autonomous exploration, we used \"Distinct Items Obtained\" as the primary evaluation met-\nric. The acquisition of a greater variety of items demonstrates more diverse exploratory behavior\nby the agent. Additionally, based on statistical information and progress in-game achievements, we\ncalculated supplementary evaluation metrics including the \"Distance Traveled\" by the agent (sum-\nming walking, sprinting, climbing, swimming, and other forms of movement), the total number of\n\"Items Crafted\" (the sum of all types of items obtained by crafting), and \"Recipes and Achievements\nUnlocked\" (the sum of crafting recipes and game achievements unlocked).\n33\nPreprint\nE.4\nSPECIFIC AGENT CAPABILITY REQUIREMENTS FOR DIFFERENT TASKS\nThis section provides an overview of the specific agent capabilities required for each task, laying the\nfoundation for a deeper understanding of how our benchmark evaluates different aspects of agent\nperformance. Different agent capabilities are detailed as follows:\n• Goal-based Planning: This capability refers to the agent’s ability to formulate and execute\ncomprehensive plans based on predefined goals. It involves understanding the given goals\nand devising a step-by-step plan to achieve them over extended periods. This is critical\nfor tasks such as the long-term planning task, where agents need to craft weapons and\nequipment to defeat specific monsters.\n• Feedback-based Planning: This capability involves the agent’s ability to adapt its plans\ndynamically based on environmental feedback. It is essential for tasks where environmen-\ntal feedback is crucial, such as in the dynamic-immediate planning task and the multi-round\nlong-term planning task, where agents must adjust their strategies in response to the out-\ncomes of previous actions or environmental changes.\n• Exploratory Planning: This capability evaluates the agent’s ability to set its own goals\nand make decisions independently in a complex environment. Agents must navigate, gather\ninformation, and decide on objectives without predefined goals. This is central to the au-\ntonomous exploration task, where agents explore the Minecraft world, discover resources,\nand adapt to unforeseen events.\n• Task Decomposition: This capability refers to the agent’s ability to break down complex\ntasks into specific, manageable sub-tasks. It is vital for the long-term planning task where\nagents need to craft a sequence of items, requiring the breakdown of the end goal into a\nseries of intermediate steps.\n• Resource Management: This capability involves the efficient allocation and utilization\nof available resources. Agents must maintain awareness of their inventory, manage assets\neffectively, and identify which resources need to be gathered. This is particularly important\nin farming tasks and autonomous exploration, where resource availability and management\nare crucial for subsequent behavior.\n• Skill Retrieval: This capability pertains to the agent’s ability to identify and choose the\nmost suitable skill from a set of options. Agents evaluate a list of relevant skills and select\nthe one that best fits the current environmental context and task requirements. All tasks\nrequire agents to retrieve and apply relevant skills based on situational demands.\n• Self-Reflection: This capability involves the agent’s ability to analyze and learn from the\noutcomes of its actions. Simply confirming the completion of a subgoal is often inadequate\nfor correcting planning errors. The agent evaluates its performance, deduces the cause of\ntask failures, and suggests more efficient strategies for future tasks. This is particularly\nimportant in multi-round tasks.\n• Self-Validation: This capability enables the agent to autonomously confirm the success\nof its actions against intended outcomes. By assessing inventory changes after actions, the\nagent ensures that each step contributes towards the overarching objectives without external\nverification. This capability is crucial for all tasks, as agents need to continuously ensure\ntheir actions align with the objectives.\nF\nEXPERIMENTS\nF.1\nEXPERIMENTAL DETAILS\nWe select the 1.19.4 version of Minecraft as the experimental environment. Within this virtual game\nworld, spatial measurements are determined by blocks, while temporal measurements are dictated\nby ticks, each lasting 0.05 seconds. A single day-night cycle in the game is 24,000 ticks, equivalent\nto 20 minutes in the real world, with 10 minutes of daytime, 7 minutes of nighttime, and a 3-minute\ndawn\/dusk transition (when both the sun and moon are visible in the sky). Additionally, the game’s\nweather system randomly transitions between clear, rainy, thunderstorm, and snowy conditions,\nadding dynamic changes to the environment. Players are born into a randomly generated massive\n34\nPreprint\nworld, covering an area of 30,000,000 blocks × 30,000,000 blocks, which can be approximately\nconsidered an infinite world without boundaries. Players start with no resources and must gather\neverything from scratch that is beneficial for survival and completing the ultimate goal. When a\nplayer character dies, it will respawn randomly within a 32-block radius of the death location on the\nground, and any collected items will not be dropped. Agents can connect to the game through local\nnetworks or multiplayer servers. We have tested on Ubuntu 20.04, Windows 10, and macOS. In all\nexperiments of the agent capability benchmark, the \"MineMA-8B\" refers to \"MineMA-8B-v3\", and\nthe \"MineMA-70B\" refers to \"MineMA-70B-v1\".\nWe use the following Minecraft mods in our experiment. It is important to note that the version of\nmods must be consistent with the game version, specifically 1.19.4.\n• Fabric API: Basic Fabric APIs.\n• Mod Menu: Used to manage all the mods that you download.\n• Complete Config: Dependency of server pause.\n• Multi Server Pause: Used to pause the server when waiting for LLM to reply.\n• Better Respawn: Used for random respawning of player characters.\nConsidering the randomness of resource distribution in the Minecraft world, we ensure that the agent\nstarts from different locations in the game world before each round of experiments. We implemented\nthe respawnAndClear interface to perform some initialization settings.\n• respawnAndClear(bot): Transport the agent to a new location and clear its in-\nventory, ensuring that the game mode is switched to survival and the game difficulty is\nswitched to peaceful.\nF.2\nAGENT CAPABILITY BENCHMARK\nIn our multi-round Long-term Planning Task, the agent is required to iteratively improve planning\nbased on combat outcomes, aiming for victory with the highest efficiency, take as little time as\npossible. Specifically, if the agent wins in the previous round, it should streamline its planning in\nthe next round, gathering materials and crafting equipment in less time to enhance time efficiency\n(reflected in the experimental results as a decrease in time and LLM iterations); conversely, if it\nloses, it must refine its planning to upgrade the quality of weapons and equipment in the planning\nlist to ensure ultimate success (reflected in the experimental results as an increase in health, or\ngo from losing to winning). Additionally, when calculating experimental results, we compute the\naverage and standard deviation for time, LLM iters (LLM iterations) and the health metric only for\nvictorious outcomes, since a defeat, indicated by health being zero, is not meaningful.\nExample of multi-round combat task\nCombat Task: 1 Skeleton\nPlan list of 1st round:[craft iron sword, craft iron helmet, craft iron chestplate, craft\niron leggings, craft iron boots]\nEquipment obtained of 1st round: [iron_helmet, iron_chestplate, iron_leggings, iron_boots,\ncrafting_table, None]\nTime spent on crafting equipment: 15,953 ticks; 8 LLM iters\nRemaining Health after the combat: 14.0 \/ 20 (victory)\n—streamlining—\nPlan list of 2nd round:[craft iron sword]\nEquipment obtained of 2nd round:[None, None, None, None, iron_sword, None]\nTime spent on crafting equipment: 3,614 ticks; 4 LLM iters\nRemaining Health after the combat: 9.2 \/ 20 (victory and more efficiently)\n—streamlining—\nPlan list of 3rd round:[craft wooden sword]\n35\nPreprint\nEquipment obtained of 3rd round:[None, None, None, None, wooden_sword, None]\nTime spent on crafting equipment: 416 ticks; 1 LLM iter\nRemaining Health after the combat: 9.0 \/ 20 (victory and even more efficiently)\nIn our Dynamic-immediate Planning Task, the agent is asked to plan step by step based on en-\nvironmental information. We calculate the success rate across various tasks, the average execution\ntime and LLM iters as well as their standard deviation (only if successful) as evaluation metrics. It is\nimportant to note that skills used in these tasks do not utilize the recursive decomposition mechanism\nwe propose but require the agent to plan the necessary preparatory steps by itself. The following\noutlines the specific skill execution pathways for the five tasks in our experiments:\nSkill execution path of the Dynamic-immediate Planning Task\nCollect Seeds: Collect Wheat Seeds \/ Collect Melon Seeds \/ Collect Pumpkin Seeds\nHoe Farmland: Craft Hoe →Hoe Farmland\nShear Sheep: Craft Shears→Shear Sheep Using Shears\nMilk Cow: Craft Bucket→Milk Cow Using Bucket\nCook Meat: Kill Pig→Cook Porkchop \/ Kill Chicken→Cook Chicken \/ Kill Sheep→Cook\nMutton \/ Kill Cow→Cook Beef\nIn our Autonomous Exploration Task, the agent also needs to plan step by step without a given\ngoal. Every time a new plan is proposed, the agent retrieves the ten most semantically similar skills\nfrom our skill library and selects one to execute. We tally the number of distinct item types obtained\nby the agent in each round, as well as the cumulative number of item types. Here are the distinct\nitems obtained by the agent from one round of the experiment:\nDistinct items obtained within 80 LLM iters\n['oak_log', 'stick', 'wooden_sword', 'crafting_table', 'wooden_pickaxe', 'stone_pickaxe',\n'oak_planks', 'wheat_seeds', 'dirt', 'cobblestone', 'raw_iron', 'granite', 'andesite', 'cob-\nbled_deepslate', 'diorite', 'diamond', 'iron_pickaxe', 'furnace', 'cobblestone_wall', 'coal',\n'iron_ingot', 'iron_trapdoor', 'dandelion', 'azure_bluet', 'poppy', 'oxeye_daisy', 'chest', 'cob-\nblestone_stairs', 'raw_copper', 'copper_ingot', 'calcite', 'copper_block', 'birch_planks', 'jun-\ngle_log', 'arrow', 'bone', 'rotten_flesh'], Num: 37\nThis result is comparable to the Voyager (Wang et al., 2023a) framework that employs GPT-4 for\nskill code generation and significantly outperforms Voyager using GPT-3.5.\nF.3\nABLATION STUDY\nWe conduct ablation studies on two core components of the ODYSSEY agent, including the LLM\nplanner and the open-world skill library.\nFor the LLM planner ablation, we remove the current environmental information in the planner\nsystem prompt as follows. Moreover, in each task proposed during each round, if the retrieved\nskills were not relevant to the current task (i.e., if the semantic retrieval score was below a certain\nthreshold), the execution of those skills was not carried out.\nPlanner System Prompt in Ablation\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft. My\nultimate goal is to discover as many diverse things as possible, accomplish as many diverse\ntasks as possible and become the best Minecraft player in the world. You can propose next\nsuitable tasks for me, such as \"Mine [block]\", \"Craft [item]\", \"Smelt [item]\", \"Kill [mob]\",\n\"Cook [food]\", \"Equip\" etc. It’s better to be a single phrase.\nYou should only respond in JSON format as described below:\n36\nPreprint\n{\n\"reasoning\": \"Do reasoning about what the next task should be.\",\n\"task\": \"The next task.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nFor the open-world skill library ablation, we removed the entire skill library and provided the LLM\nonly with the necessary interfaces required for composing new skills. Each round’s skill retrieval\nand execution were changed to code writing and execution, similar to the approach used in Voy-\nager (Wang et al., 2023a). The actor system prompt is shown as follows:\nActor System Prompt in Ablation\nYou are a helpful assistant that writes Mineflayer javascript code to complete any Minecraft\ntask specified by me.\n—External information—\nAt each round of conversation, I will give you\nCode from the last round: ...\nExecution error: ...\nChat log: ...\nBiome: ...\nNearby blocks: ...\nNearby entities (nearest to farthest):\nHealth: ...\nHunger: ...\nPosition: ...\nEquipment: ...\nInventory (xx\/36): ...\nChests: ...\nTask: ...\nContext: ...\nCritique: ...\n—Directions—\nYou should then respond to me with\nExplain (if applicable): Are there any steps missing in your plan? Why does the code not\ncomplete the task? What does the chat log and execution error imply?\nPlan: How to complete the task step by step. You should pay attention to Inventory since it\ntells what you have. The task completeness check is also based on your final inventory.\nCode:\n1) Write an async function taking the bot as the only argument.\n2) Reuse the above useful programs as much as possible.\n3) ...\n—Behaviour constraints—\nYou should only respond in the format as described below:\nExplain: ...\nPlan:\n1) ...\n2) ...\n3) ...\n37\nPreprint\n...\nCode:\n```javascript\n\/\/ helper functions (only if needed, try to avoid them)\n...\n\/\/ main function after the helper functions\nasync function yourMainFunctionName(bot) {\n\/\/ ...\n}\n```\nF.4\nRESULTS\nWe additionally provide Figure 8 and Figure 9 displaying the results of the single-round long-term\nplanning task and the dynamic-immediate planning task for easier visual inspection.\n38\nPreprint\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nx\n0\n5\n10\n15\n20\nHealth\nx\nx\nx\nx\nx\nx\n0\n5\n10\n15\n20\n25\nTime(min)\nx\n1 zombie\n1 spider\n1 skeleton\n1 zombified piglin\n1 enderman\n1 zombie, 1 spider\n1 zombie, 1 skeleton\n3 zombies\n1 zombie villager\n1 cave spider\n1 wither skeleton\ncook meat\nanimal husbandry\nTasks\n0\n25\n50\n75\n100\nLLM iters\nx\nVoyager with GPT-4-o-mini\nLLaMA-3-8B\nMineMA-8B\nFigure 8: Performance comparison of different models on the single-round long-term planning task.\n“Health” refers to the remaining health points. “# LLM iters” is the number of LLM iterations (call-\ning LLM) required to complete the task. “Time (min)” refers to the minutes spent in both gathering\nmaterials and crafting equipment to defeat different monsters. All evaluation metrics are calculated\nonly for successful tasks. ± corresponds to one standard deviation of the average evaluation over\nsuccessful tasks. Bold and italics mean the best and the second-best results. “×” indicates that\nhealth is not a relevant metric in the cook meat and animal husbandry scenarios, or all tasks fail.\n39\nPreprint\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nx\nx\nx\nx\nx\nx\n0\n10\n20\n30\nTime (min)\nx\nx\nx\nx\nx\nx\nCollect Seeds\nHoe Farmland\nShear Sheep\nMilk Cow\nCook Meat\nObtain Leather\nMake Sugar\nCollect Water\nTasks\n0\n20\n40\n60\n80\nLLM Iters\nx\nx\nx\nx\nx\nx\nBaichuan2-7B\nQwen2-7B\nMineMA-8B\nMineMA-70B\nFigure 9: Performance comparison of different models on the dynamic-immediate planning task.\nAll evaluation metrics are calculated only for successful tasks. “×” indicates that all tasks fail.\n40\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Odyssey: Empowering Minecraft Agents with Open-World Skills.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nOdyssey: Empowering Minecraft Agents with Open-World Skills\n```\n#### 2. 论文摘要\n```\nRecent studies have delved into constructing generalist agents for open-world\nenvironments like Minecraft. Despite the encouraging results, existing efforts\nmainly focus on solving basic programmatic tasks, e.g., material collection and\ntool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set\nof actions available to agents, requiring them to learn effective long-horizon\nstrategies from scratch. Consequently, discovering diverse gameplay\nopportunities in the open world becomes challenging. In this work, we introduce\nOdyssey, a new framework that empowers Large Language Model (LLM)-based agents\nwith open-world skills to explore the vast Minecraft world. Odyssey comprises\nthree key parts: (1) An interactive agent with an open-world skill library that\nconsists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned\nLLaMA-3 model trained on a large question-answering dataset with 390k+\ninstruction entries derived from the Minecraft Wiki. (3) A new agent capability\nbenchmark includes the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task. Extensive experiments demonstrate\nthat the proposed Odyssey framework can effectively evaluate different\ncapabilities of LLM-based agents. All datasets, model weights, and code are\npublicly available to motivate future research on more advanced autonomous\nagent solutions.\n```\n\n#### 3. 论文全文\n```\nPreprint\nODYSSEY: EMPOWERING MINECRAFT AGENTS WITH\nOPEN-WORLD SKILLS\nShunyu Liu1∗†, Yaoru Li1∗, Kongcheng Zhang1∗, Zhenyu Cui1∗, Wenkai Fang1∗,\nYuxuan Zheng1, Tongya Zheng2, Mingli Song1 #\n1Zhejiang University,\n2Hangzhou City University\n{liushunyu, liyaoru, zhangkc, zhenyucui, wenkfang, zyxuan}@zju.edu.cn,\ndoujiang_zheng@163.com, brooksong@zju.edu.cn\n∗Equal contribution,\n†Project leader,\n#Corresponding author\n§ https:\/\/github.com\/zju-vipa\/Odyssey\nABSTRACT\nRecent studies have delved into constructing generalist agents for open-world\nenvironments like Minecraft. Despite the encouraging results, existing efforts\nmainly focus on solving basic programmatic tasks, e.g., material collection and\ntool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set of\nactions available to agents, requiring them to learn effective long-horizon strate-\ngies from scratch. Consequently, discovering diverse gameplay opportunities in\nthe open world becomes challenging. In this work, we introduce ODYSSEY, a\nnew framework that empowers Large Language Model (LLM)-based agents with\nopen-world skills to explore the vast Minecraft world. ODYSSEY comprises three\nkey parts: (1) An interactive agent with an open-world skill library that consists of\n40 primitive skills and 183 compositional skills. (2) A fine-tuned LLaMA-3 model\ntrained on a large question-answering dataset with 390k+ instruction entries de-\nrived from the Minecraft Wiki. (3) A new agent capability benchmark includes\nthe long-term planning task, the dynamic-immediate planning task, and the au-\ntonomous exploration task. Extensive experiments demonstrate that the proposed\nODYSSEY framework can effectively evaluate different capabilities of LLM-based\nagents. All datasets, model weights, and code are publicly available to motivate\nfuture research on more advanced autonomous agent solutions.\n1\nINTRODUCTION\nDeveloping autonomous agents capable of performing open-world tasks represents a significant\nmilestone towards achieving artificial general intelligence (Savva et al., 2019; Reed et al., 2022;\nDriess et al., 2023). These open-world tasks necessitate that agents interact with complex and dy-\nnamic environments, make decisions based on incomplete information, and adapt to unexpected\nevents. Early reinforcement learning agents (Tessler et al., 2017; Oh et al., 2017; Guss et al., 2019)\nhave demonstrated limited knowledge in such open-world setting. Furthermore, these agents of-\nten struggle with long-term planning, which is crucial for the fulfillment of intricate goals. Recent\nbreakthrough of Large Language Models (LLMs) (Hu et al., 2021; Achiam et al., 2023; Touvron\net al., 2023) have shown the potential to revolutionize various fields such as healthcare (Zhang et al.,\n2023b; Yang et al., 2024b), robotics (Huang et al., 2022; Ahn et al., 2022; Singh et al., 2023), and\nweb services (Nakano et al., 2021; Deng et al., 2023; Iong et al., 2024), attributed to its capability\non endowing agents with expansive knowledge and sophisticated planning akin to human reason-\ning (Wei et al., 2022a; Wang et al., 2024; Liang et al., 2023). However, the development of LLMs in\nopen-world tasks remains challenging due to the need for well-defined environments and measurable\nbenchmarks (Zhu et al., 2023; Wang et al., 2023a; Qin et al., 2023).\nThe popular Minecraft game features a vast and diverse world with various biomes, terrains, and\nresources, making it an ideal testbed for evaluating the capabilities of autonomous agents in the\nopen-world setting (Guss et al., 2019). To facilitate the development of generalist agents in this\n1\narXiv:2407.15325v2  [cs.AI]  7 Oct 2024\nPreprint\nLong\nShort\nLLaMA 3\nMinecraft\nWiki\nFine-tune Minecraft LLM\nAgent Capability Benchmark\nCombat Zombie\nCraft Diamond Sword\nMine Diamond\nOpen-World Skill Library\nInteractive Agent\nPlan\nCritic\nActor\n...\nMineMA\nQ&A Dataset\nGeneration\nFine-tune\nLoRA\nSkill\nRetrieval\n     Combat\nWeapons\nEquipment\nLong-term Planning\n     Farm\nPlanting\nBreeding\nDynamic-immediate Planning\n     Explore\nSurviving\nCreating\nAutonomous Exploration\n...\nFigure 1: An overview of the proposed ODYSSEY framework. Odyssey consists of three key com-\nponents: (1) a fine-tuned LLaMA-3 model trained on a large-scale question-answering dataset; (2)\nan interactive agent equipped with an extensive open-world skill library; (3) a novel agent capability\nbenchmark encompassing a variety of tasks.\nsetting, MineRL (Guss et al., 2019) and MineDojo (Fan et al., 2022) introduced simulation bench-\nmarks built upon the sandbox Minecraft environment. The seminal work, Voyager (Wang et al.,\n2023a), proposed an LLM-based agent to drive exploration in Minecraft. Subsequently, there has\nbeen a surge of efforts to leverage the superior performance of LLMs to extend the capabilities of\nsuch Minecraft agents (Zhu et al., 2023; Wang et al., 2023b; Zhou et al., 2024a; Wang et al., 2023c;\nQin et al., 2023). Despite recent advancements, existing works mainly focus on solving basic pro-\ngrammatic tasks, often considering the ObtainDiamond task as the ultimate challenge. Basic\nprogrammatic tasks refer to those constrained by the explicit dependencies following the Minecraft\ntech-tree, such as collecting materials and crafting tools. Such tasks inherently only assess the abil-\nity of LLMs to prioritize crafting steps within a limited task space, rather than their potential for\ncomplicated and diverse solutions. This limitation arises from the narrowly defined set of actions\navailable to agents (e.g., mouse and keyboard), which necessitates learning skills from scratch. Since\nMinecraft is fundamentally resource-based, an agent must first learn to collect adequate resources\nand tools to engage in creative play, which limits the exploration of diverse gameplay options. More-\nover, methods like Voyager (Wang et al., 2023a) heavily rely on the powerful GPT-4 for high-quality\nsolutions, imposing a substantial cost burden on researchers who prefer open-source models.\nIn this work, we introduce ODYSSEY1, a novel framework that equips LLM-based agents with ad-\nvanced open-world skills, enabling efficient interaction and exploration within the Minecraft envi-\nronment. ODYSSEY allows agents to move beyond basic programmatic tasks and focus more on\ncomplex open-world challenges. As shown in Fig. 1, ODYSSEY comprises three key contributions:\n1. We develop an LLM-based interactive agent with an open-world skill library, encompassing\n40 primitive skills that serve as underlying interfaces and 183 compositional skills tailored for\ncomplex and diverse tasks in an open-world setting. A recursive method improves skill execution\nby checking prerequisites. The ODYSSEY agent consists of a planner for goal decomposition, an\nactor for skill retrieval and subgoal execution, and a critic for feedback and strategy refinement.\n2. We fine-tune the LLaMA-3 model (Touvron et al., 2023) for Minecraft agents using a compre-\nhensive question-answering dataset. This involves generating a large-scale training dataset with\n390k+ instruction entries from Minecraft Wikis, fine-tuning various sizes of the LLaMA-3 mod-\nels using LoRA (Hu et al., 2021), and evaluating them with a custom multiple-choice dataset.\n3. We introduce a new agent capability benchmark to evaluate different aspects of agent perfor-\nmance in Minecraft, including the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task. Extensive experiments demonstrate that the proposed\n1The Odyssey is a great ancient Greek epic poem attributed to Homer, which is now often used metaphori-\ncally to describe a long adventurous journey (Oxford English Dictionary).\n2\nPreprint\nODYSSEY framework provides a robust measure of agent effectiveness, showcasing the practical\nadvantages of our framework using the open-source models.\nIt is worth noting that our focus is not to design a new LLM-based agent architecture. Instead,\nthis work aims to provide a comprehensive framework for developing and evaluating autonomous\nagents in open-world environments, enabling them to explore the vast and diverse Minecraft world.\nWe have open-sourced all parts of ODYSSEY and will continuously update the repository. We hope\nthis will enable other researchers to build upon our work, fostering further innovation and progress\nin the development of autonomous agents.\n2\nOPEN-WORLD SKILL-BASED INTERACTIVE AGENT\nODYSSEY develops an LLM-based interactive agent with an open-world skill library, aiming to en-\nhance the efficiency and adaptability of agents in complex Minecraft environments. The skill library\ncomprises 40 primitive skills and 183 compositional skills, while the LLM-based agent employs a\nplanner-actor-critic architecture to facilitate task decomposition, skill execution, and performance\nfeedback. The architecture of the interactive agent is depicted in Fig. 2. Full skill and prompt details\nused in the LLM-based interactive agent are given in Appendix C.\n2.1\nOPEN-WORLD SKILL LIBRARY\nPrimitive skills encompass a series of underlying interfaces on top of Mineflayer JavaScript\nAPIs (PrismarineJS, 2023), divided into two main categories: 32 operational skills and 8 spatial\nskills. This suite of skills exceeds the 18 primitive skills (all are operational skills) delineated in\nVoyager (Wang et al., 2023a). Operational skills serve as foundational interfaces with parameter-\nized input, such as mine(·) for material collection and craft(·) for tool crafting. Additionally,\nwe pioneer 8 spatial skills that Voyager (Wang et al., 2023a) lacks, allowing for environmental in-\nteractions based on the agent coordinates. Given that our work is conducted within a text-based\nMinecraft environment (Wang et al., 2023a; Fan et al., 2022), spatial skills are crucial for handling\ntasks that require precise positioning and orientation, especially in the absence of visual input.\nCompositional skills encapsulate primitive skills into higher-level ones, functioning to address a va-\nriety of basic programmatic tasks, such as mineDiamond and craftIronPickaxe. ODYSSEY\nclassifies 183 compositional skills into types like mineX, craftX, plantX, breedX, cookX,\netc. We use a recursive method to construct the skill library, simplifying complex task decomposi-\ntion by ensuring prerequisites are met before skill execution. Taking mineDiamond as an example,\nif the agent lacks an iron pickaxe, it will recursively execute craftIronPickaxe. This indicates\nthat our program internally manages the construction and execution order of skills through its recur-\nsive method, thereby avoiding the need for the agent to engage in additional planning.\nTo facilitate efficient retrieval of skills in the skill library, we first generate a description for each skill\nby calling the LLM and using the complete program code as a prompt. We then employ Sentence\nTransformer (Reimers & Gurevych, 2019) to encode the skill description. This method transforms\ntext information into vector representations, facilitating semantic retrieval and enabling the agent to\nfind the most relevant skill description based on the context provided.\n2.2\nLLM PLANNER\nThe LLM Planner is responsible for developing a comprehensive plan, facilitating efficient explo-\nration through long-term goal decomposition. The LLM Planner breaks down high-level goals into\nspecific low-level subgoals, each corresponding to a particular skill outlined in Sec. 2.1. By address-\ning each subgoal in the plan, the ultimate goal can be progressively achieved. The input prompt to\nthe planner consists of several components: (1) Ultimate goals and behavioral constraints. For\nexample, “My ultimate goal is to ... Propose the current task only when you ensure that you have all\nthe necessary dependent items in inventory”. (2) States of the agent. This reflects the interaction\nbetween the agent and environment, such as hunger and health values, position and nearby entities,\netc. (3) Achievements of the agent. This includes the current inventory and unlocked equipment,\nas well as previously successful and failed tasks.\n3\nPreprint\nKnowledge Q&A\n   First, you should ...\nHow to obtain milk?\nSkill Retrieve\nCode Action\nBreed cow\nKill one cow with sword\nCollect milk with bucket\nAchivements\nUltimate Goals\nPlan\nEnvironment\nMineflayer\nPrimitive\nCompositional\nI want to  breed  as\nmany as animals\nlike      ,     or     ,\nand then collect \nitems from them.\nHealth \nPos\nExecute\nValidate\n[Lack of pre-requirements] \nI cannot collect milk without a     .\nExecution\nFeedback\n[Environment feedback]\nI could not find a       to collect milk.\nSelf-validation:\nSelf-reflection:\nCritic\nSince you only have     ,\nyou might need the      to\nattract a      for milk.\nBased on changes of my\ninventory, is my subgoal\nsuccessful? 🤔\nThought\nYou should analysis the\nreason why my subgoal\nis failed based on the\nlogs provided.\nRethink\nObservation\nSkill Library\n[Biome] snowy\n[Time] day\n[Nearby bocks] dirt, grass, \noak_log, oak_leaves, tall_grass, \ncobblestone, crafting_table,\nacacia_log\n[Nearby entities] horse, pig\n[Health]: 18.0\/20\n[Hunger]: 16.0\/20\n[Position]: x=2134.5, y=69.0,\nz=769.5\n[Inventory] oak_log, ...\n[Equipment] helmet,\nleggings, boots, ...\n[Completed] mine ...\nUpdate\nFailed\nSuccessful\nPotential\nMy subgoal is to: \ncollect milk\nlast_inventory (16\/36): ... \ncur_inventory (18\/36): ... \nFigure 2: An illustrative diagram of the interactive agent following a planner-actor-critic architecture\nbased on the open-world skill library. The LLM Planner decomposes ultimate goals into specific\nsubgoals, while the LLM Actor then sequentially executes code actions for each subgoal using the\nskill library. The LLM Critic evaluates these actions through self-validation and reflection, enabling\nthe agent to update its plan based on execution feedback.\n2.3\nLLM ACTOR\nIn the execution phase, the LLM actor is invoked to sequentially execute the subgoals generated\nby the LLM planner within the Minecraft environment. This process utilizes the open-world skill\nlibrary to achieve these subgoals. The mapping from high-level subgoals to executable skill code is\naccomplished through query context encoding and skill similarity retrieval. This process includes:\n(1) Query context. The text-based subgoals generated by the LLM planner are encoded by Sen-\ntence Transformer (Reimers & Gurevych, 2019) to vector representations as the query context. (2)\nSimilarity matching. The vector similarity between the query context and the skill descriptions in\nthe skill library is computed to determine semantic closeness. (3) Skill selection. The top-5 relevant\nskills with the highest scores are identified, and the actor agent selects the most appropriate code for\nexecution within the environment based on their descriptions.\n2.4\nLLM CRITIC\nDuring action execution, it is critical for an agent to document its experiences, especially noting suc-\ncessful outcomes and failure points. This is crucial in open-world planning to establish a feedback-\ninformed system, which corrects initial plan discrepancies that can cause execution errors. For\ninstance, achieving the animal breeding goal requires prerequisite crops for feed. The LLM critic\ncan assess action effectiveness by comparing expected and actual outcomes, providing insights for\nrefining future strategies. We categorize feedback into three types: (1) Execution feedback. This\ncaptures the progress of skill execution. For example, “No hoe in inventory. Craft a hoe first!” not\nonly highlights the reason for failure in hoeing farmland but also provides a guideline to address this\nproblem. (2) Self-validation. By presenting inventory changes post-action to the LLM critic, we\nempower it to validate whether the skill has achieved its subgoal, eliminating the need for manual\nchecks. (3) Self-reflection. Simply confirming the completion of a subgoal is often inadequate for\ncorrecting planning errors. The LLM critic also serves as an analyst, deducing the cause of task\nfailure by evaluating the current state of the agent and its environment. It then offers a critique,\nsuggesting a more efficient strategy for task completion.\n3\nFINE-TUNE MINECRAFT LLM\nTo improve agent performance in Minecraft, we fine-tune the LLaMA-3 model (Touvron et al., 2023)\nusing a large-scale Question-Answering (Q&A) dataset with 390k+ instruction entries sourced from\nthe Minecraft Wiki. ODYSSEY presents an effective procedure for converting a foundation model\ninto a domain-specific model, which involves dataset generation, model fine-tuning, and model eval-\nuation. The detailed descriptions can be found in Appendix D.\n4\nPreprint\nDataset Generation. We develop a GPT-assisted method to generate an instruction dataset for\nMinecraft. First, we crawl relevant content from the Minecraft Wiki, excluding non-essential sec-\ntions like history. The collected data is then categorized and separated into different files based on\ntheir content type. Then we use GPT-3.5-Turbo (OpenAI, 2023) with different customized prompts\nto automatically generate diverse Q&A pairs. Note that both the questions and answers were gener-\nated by GPT. These Q&A pairs are categorized into four types based on the nature of the answers:\nshort, normal, long, and boolean, yielding 390k+ entries. In contrast, the Wiki dataset released\nby MineDojo (Fan et al., 2022) only collects Minecraft Wiki pages, without refining the content\nand generating Q&A pairs for model training. STEVE (Zhao et al., 2023) introduces a non-public\ndataset with 20k+ Q&A pairs, which is smaller than our dataset in terms of scale and diversity.\nModel Fine-tuning. We employ LoRA (Hu et al., 2021) for model fine-tuning, which is a parameter-\nefficient training technique. LoRA introduces small, trainable low-rank matrices to adapt a pre-\ntrained neural network, enabling targeted updates without the need to retrain the entire model. Us-\ning LoRA, we fine-tune the LLaMA-3-8B-Instruct and LLaMA-3-70B-Instruct models with our\nMinecraft dataset, resulting in the new models termed MineMA-8B and MineMA-70B, respectively.\nModel Evaluation. In Minecraft, questions are often open-ended and can yield diverse answers;\ntherefore, conventional evaluation metrics (Papineni et al., 2002; Lin, 2004) may fall short. Mean-\nwhile, common benchmarks (Wang et al., 2018; 2019; Hendrycks et al., 2021) are not suitable for\nassessing the capabilities of expert models. Thus, we employed GPT-4 (Achiam et al., 2023) to gen-\nerate two Multiple-Choice Question (MCQ) datasets based on different themes and keywords related\nto Minecraft. These datasets can quantitatively evaluate the domain-specific expertise of models.\n4\nAGENT CAPABILITY BENCHMARK\nCraftSword\nCraftArmor\nCombatMonster\nImmediate Feedback\nDynamic Plan\nIterative Optimization \nResource\nExplore\nLong-term Planning Task\nDynamic-immediate Planning Task\nAutonomous Exploration Task\nHoeFarmland\nShearSheep\nMilkCow\nSkill Library\nPlan 1\nPlan 2\nGoal\nFigure 3: Agent capability benchmark.\nODYSSEY presents a new benchmark for evaluating\nagent capabilities within Minecraft, offering three task\ntypes: long-term planning, dynamic-immediate plan-\nning, and autonomous exploration. It is notable that\nthese tasks cannot be solved by any single skill but de-\nmand a sophisticated combination of multiple skills.\nThese tasks are set in various Minecraft scenarios, with\ndifferent tasks in the same scenario testing different\nagent capabilities. For example, in the cooking sce-\nnario, long-term planning requires formulating a com-\nplete plan to locate and hunt a specific animal, whereas\ndynamic-immediate planning involves selecting which\nnearby animal to cook based on the immediate en-\nvironment. Our benchmark provides a standardized\nframework for evaluating agents, where the agent ca-\npability requirements for different tasks are shown in\nTable 1. Please refer to Appendix E for more details.\nLong-term Planning Task. We design a suite of combat scenarios to assess the long-term planning\ncapability of agents, requiring them to craft appropriate weapons and equipment to defeat various\nmonsters. These combat scenarios can be divided into single-type and multi-type monster scenarios.\nFor the single-type scenarios, we choose various unique monsters, each with its own attack styles,\nmovement patterns, and hostility levels. For the multi-type scenarios, we focus on typical monster\ngroupings encountered in the game. Agents must generate a comprehensive long-term plan, detail-\ning the sequence of crafting the necessary weapons and equipment for the assigned combat task.\nPerformance is measured by remaining health and time consumed during combat. After each battle,\nagents can iteratively optimize their plan, learning from previous outcomes to improve performance\nin subsequent rounds. To extend the scope of the long-term planning task beyond combat, we also\nadopt animal husbandry and cooking scenarios, where agents are required to formulate detailed\nplans for completing tasks related to specific animals.\nDynamic-immediate Planning Task. The dynamic-immediate planning task requires agents to\ndynamically generate and execute plans based on immediate environmental feedback. Thus, we\ndesign a suite of farming scenarios, where agents engage in activities like planting, cooking, and\n5\nPreprint\nTable 1: Specific agent capability requirements for different benchmark tasks, including Goal-based\nPlanning (GBP), Feedback-based Planning (FBP), Exploratory Planning (EP), Task Decomposi-\ntion (TD), Resource Management (RM), Skill Retrieval (SR), Self-Reflection (Self-R), and Self-\nValidation (Self-V). Please refer to Appendix E.4 for detailed descriptions of each capability.\nTask\nGBP\nFBP\nEP\nTD\nRM\nSR\nSelf-R\nSelf-V\nSingle-Round Long-Term Planning Task\n✓\n×\n×\n✓\n×\n✓\n✓\n✓\nMulti-Round Long-Term Planning Task\n✓\n✓\n×\n✓\n×\n✓\n✓\n✓\nDynamic-Immediate Planning Task\n✓\n✓\n×\n×\n✓\n✓\n✓\n✓\nAutonomous Exploration Task\n×\n✓\n✓\n×\n✓\n✓\n✓\n✓\nTable 2: Average execution time and success rate (SR) on 5 basic programmatic tasks in Minecraft.\nTask\nTime (min)\nSR in 2min\nSR in 5min\nSR in 10min\nSR in 15min\nCrafting Table\n0.59 ± 0.79\n95.8%\n99.2%\n100.0%\n100.0%\nWooden Tool\n0.95 ± 0.80\n92.5%\n99.2%\n100.0%\n100.0%\nStone Tool\n1.48 ± 0.96\n85.0%\n97.5%\n100.0%\n100.0%\nIron Tool\n4.43 ± 1.48\n0.0%\n76.7%\n100.0%\n100.0%\nObtain Diamond\n6.48 ± 2.02\n0.0%\n21.7%\n92.5%\n100.0%\nanimal husbandry. Although some scenarios are similar to the long-term planning task, the dynamic-\nimmediate planning task emphasizes reacting to real-time feedback like available resources and\nnearby animals. Performance is evaluated through task completion time and success rates.\nAutonomous Exploration Task. To test the exploratory capability of agents within open-world\nsettings, we design an autonomous exploration task in Minecraft. In this task, agents are required\nto determine their subsequent objectives and execute the appropriate skills based on the game con-\ntext. The exploration task involves discovering and utilizing resources, while adapting to unexpected\nevents such as encounters with hostile monsters. Agents must adapt to these challenges by devel-\noping strategies for resource management and task prioritization. The performance metrics include\nthe number of distinct items obtained, the total items crafted, the recipes and advancements (R&A)\nunlocked, and the distance traveled.\n5\nEXPERIMENTS\nTo demonstrate the effectiveness of the proposed ODYSSEY framework, we conduct experiments\non basic programmatic tasks and the agent capability benchmark. Our simulation environment is\nbuilt on top of Voyager (Wang et al., 2023a), providing a text-based interface for agents to interact\nwith Minecraft. We only use GPT-3.5 and GPT-4 for initial data generation, but all experiments are\nconducted with the open-source LLaMA-3 model, significantly reducing costs compared to GPT-4-\nbased skill generation methods (Wang et al., 2023a;b). Notably, we do not employ GPT-4 in Voyager\ndue to the high cost, which we estimate would be in the thousands of dollars per experiment. Instead,\nwe reproduce Voyager using GPT-4o-mini and GPT-3.5 for comparison. More details are provided\nin Appendix F. We aim to answer the following questions: (1) Can the open-world skill library\nimprove the efficiency of agents in Minecraft? (Sec. 5.1). (2) How well do agents with different\nLLMs perform on the agent capability benchmark tasks? (Sec. 5.2). (3) What is the contribution of\ndifferent components of the ODYSSEY agent to its overall performance? (Sec. 5.3).\n5.1\nOPEN-WORLD SKILL LIBRARY\nTo demonstrate the superior capability of our open-world skill library in Minecraft, we first tested it\non 5 basic programmatic tasks from previous studies (Zhu et al., 2023). We conducted 120 repeated\nexperiments on each task and recorded the average completion time for each task as well as the\nsuccess rates at different time points. We report the performance of baselines using the results\nreported from their own paper, including DEPS (Wang et al., 2023b), VPT (Baker et al., 2022),\nGITM (Zhu et al., 2023). The experimental results in Tab. 2 and Tab. 3 show that our skill library\nsignificantly improved the success rate and efficiency of the above tasks, surpassing previous studies.\n6\nPreprint\nTable 3: Average success rate of our framework and previous baselines on 5 basic programmatic\ntasks in Minecraft within ten minutes.\nAgent\nCrafting Table\nWooden Tool\nStone Tool\nIron Tool\nObtain Diamond\nDEPS\n90.0%\n80.0%\n73.3%\n10.0%\n0.6%\nVPT\n100.0%\n100.0%\n100.0%\n85.0%\n20.0%\nGITM\n100.0%\n100.0%\n100.0%\n95.0%\n67.5%\nOurs\n100.0%\n100.0%\n100.0%\n100.0%\n92.5%\n5.2\nAGENT CAPABILITY BENCHMARK\nWe evaluate the LLM-based agent on the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task from the ODYSSEY benchmark. These tasks cover a\nvariety of complex gaming scenarios and require diverse solutions.\n5.2.1\nLONG-TERM PLANNING TASK\n1 Zombie\n1 Spider\n1 Skeleton 1 Enderman\nTask\n0\n3\n6\n9\n12\n15\n18\nTime (min)\nRound 1\nRound 2\nRound 3\nFigure 4: Performance on the multi-round long-\nterm planning task. Note that all presented data\nare from successful tasks.\nThe long-term planning task assesses the agent\ncapability to directly formulate and execute\ncomprehensive plans over extended periods. For\nexample, in the combat scenarios, the agent is\nrequired to plan a list of weapons and equip-\nment to craft based on the strength of different\nmonsters, with the goal of defeating the mon-\nster in as short a time as possible. We compared\nthe performance of our agent with both the fine-\ntuned MineMA-8B and the original LLaMA-3-\n8B models, and also the performance of Voy-\nager (Wang et al., 2023a) with GPT-4o-mini\nacross these tasks.\nMoreover, we also evalu-\nate the performance of single-round and multi-\nround planning. The single-round test results in Tab. 4 show that the fine-tuned MineMA-8B model\nsurpasses the original LLaMA-3-8B model in terms of success rate and time efficiency, albeit at\nthe cost of more LLM iterations. Moreover, our agent with the MineMA-8B model can outperform\nVoyager with GPT-4o-mini in most scenarios, indicating the effectiveness of our fine-tuning strat-\negy. The multi-round test results in Fig. 4 show that the multi-round planning strategy significantly\nimproves the time efficiency of the agent, indicating that the agent can iteratively optimize its plan\nbased on the outcomes of previous battles to enhance its performance in subsequent rounds.\n5.2.2\nDYNAMIC-IMMEDIATE PLANNING TASK\nFor the dynamic-immediate planning task, the agent is required to dynamically generate and execute\nplans based on immediate environmental feedback. We compared our MineMA model with differ-\nent open-sourced LLMs, including Qwen2-7B (Yang et al., 2024a) and Baichuan2-7B (Yang et al.,\n2023). Moreover, we evaluate the performance of the MineMA-8B and the MineMA-70B model\nto investigate the impact of model size on task performance. As shown in Tab. 5, the MineMA-\n8B model outperforms the Baichuan2-7B and Qwen2-7B models in terms of success rate and time\nefficiency. Moreover, the MineMA-70B model shows superior performance compared with the\nMineMA-8B model. Across all tasks, MineMA-70B demonstrates higher success rates and gener-\nally lower average execution times and LLM iterations.\n5.2.3\nAUTONOMOUS EXPLORATION TASK\nIn the autonomous exploration task, the agent is required to explore the Minecraft world freely with-\nout any specific goals. We compare our agent with different Minecraft-based agent methods (Voy-\nager (Wang et al., 2023a) and DEPS (Wang et al., 2023b)) and different LLM-based agent techniques\n(ReAct (Yao et al., 2023) and AutoGPT (Significant-Gravitas, 2023)) on this task. Note that we re-\nproduced different LLM-based agent techniques following the same settings as in Voyager (Wang\n7\nPreprint\nTable 4: Performance comparison of different models on the single-round long-term planning task.\n“Health” refers to the remaining health points. “# LLM iters” is the number of LLM iterations (call-\ning LLM) required to complete the task. “Time (min)” refers to the minutes spent in both gathering\nmaterials and crafting equipment to defeat different monsters. All evaluation metrics are calculated\nonly for successful tasks. ± corresponds to one standard deviation of the average evaluation over\nsuccessful tasks. Bold and italics mean the best and the second-best results. “-” indicates that health\nis not a relevant metric in the scenarios. “N\/A” indicates that all tasks fail.\nTask\nModel\nSuccess Rate\nHealth\nTime (min)\n# LLM Iters\n1 zombie\nVoyager\n3 \/ 3\n20.0 ± 0.0\n9.9 ± 6.0\n67.3 ± 41.7\nLLaMA-3-8B\n4 \/ 8\n20.0 ± 0.0\n8.3 ± 4.2\n6.1 ± 4.1\nMineMA-8B\n8 \/ 8\n19.4 ± 2.3\n8.8 ± 5.4\n10.0 ± 5.8\n1 spider\nVoyager\n3 \/ 3\n10.8 ± 8.0\n9.4 ± 8.8\n19.0 ± 1.4\nLLaMA-3-8B\n4 \/ 8\n19.4 ± 1.0\n12.1 ± 3.8\n8.4 ± 3.5\nMineMA-8B\n8 \/ 8\n19.3 ± 1.6\n8.3 ± 6.7\n15.2 ± 6.0\n1 skeleton\nVoyager\n2 \/ 3\n16.5 ± 0.0\n7.4 ± 2.9\n46.0 ± 32.0\nLLaMA-3-8B\n4 \/ 8\n17.6 ± 2.7\n8.1 ± 3.5\n8.9 ± 3.7\nMineMA-8B\n8 \/ 8\n13.6 ± 5.9\n8.6 ± 7.3\n12.1 ± 7.0\n1 zomb-\nified piglin\nVoyager\n3 \/ 3\n19.0 ± 1.4\n14.5 ± 4.7\n50.3 ± 26.2\nLLaMA-3-8B\n4 \/ 8\n19.9 ± 0.4\n9.2 ± 3.9\n10.0 ± 4.2\nMineMA-8B\n8 \/ 8\n18.7 ± 1.9\n8.5 ± 6.1\n11.7 ± 6.2\n1 ender-\nman\nVoyager\n2 \/ 3\n11.0 ± 9.0\n22.8 ± 1.7\n28.0 ± 4.0\nLLaMA-3-8B\n2 \/ 8\n15.1 ± 7.3\n13.0 ± 3.0\n6.8 ± 1.9\nMineMA-8B\n4 \/ 8\n19.8 ± 0.5\n10.4 ± 6.3\n12.5 ± 5.4\n1 zombie\nvillager\nVoyager\n2 \/ 3\n20.0 ± 0.0\n12.6 ± 2.0\n50.0 ± 3.0\nLLaMA-3-8B\n7 \/ 8\n19.6 ± 1.1\n12.7 ± 5.3\n11.0 ± 5.3\nMineMA-8B\n8 \/ 8\n20.0 ± 0.0\n9.0 ± 3.6\n12.8 ± 6.1\n1 cave\nspider\nVoyager\n2 \/ 3\n16.5 ± 3.5\n10.0 ± 1.8\n79.2 ± 29.0\nLLaMA-3-8B\n6 \/ 8\n19.5 ± 1.2\n12.0 ± 6.3\n19.5 ± 1.2\nMineMA-8B\n7 \/ 8\n20.0 ± 0.0\n3.6 ± 2.6\n8.6 ± 8.8\n1 wither\nskeleton\nVoyager\n1 \/ 3\n20.0 ± 0.0\n20.9 ± 0.0\n100.0 ± 0.0\nLLaMA-3-8B\n6 \/ 8\n13.2 ± 6.0\n11.7 ± 3.7\n12.3 ± 2.7\nMineMA-8B\n7 \/ 8\n17.3 ± 3.7\n11.0 ± 6.8\n12.6 ± 6.9\n1 zombie,\n1 spider\nVoyager\n1 \/ 3\n17.5 ± 0.0\n5.9 ± 0.0\n21.0 ± 0.0\nLLaMA-3-8B\n1 \/ 8\n20.0 ± 0.0\n8.5 ± 0.0\n6.0 ± 0.0\nMineMA-8B\n5 \/ 8\n16.4 ± 4.1\n10.6 ± 6.7\n12.0 ± 4.9\n1 zombie,\n1 skeleton\nVoyager\n2 \/ 3\n19.0 ± 1.0\n15.0 ± 8.6\n40.5 ± 20.5\nLLaMA-3-8B\n1 \/ 8\n0.2 ± 0.0\n13.5 ± 0.0\n9.0 ± 0.0\nMineMA-8B\n3 \/ 8\n12.8 ± 2.8\n14.0 ± 1.9\n10.3 ± 2.8\n3 zombies\nVoyager\n2 \/ 3\n7.8 ± 4.2\n8.2 ± 0.4\n61.0 ± 29.0\nLLaMA-3-8B\n1 \/ 8\n3.7 ± 0.0\n14.3 ± 0.0\n8.0 ± 0.0\nMineMA-8B\n1 \/ 8\n5.2 ± 0.0\n11.1 ± 0.0\n14.0 ± 0.0\ncook meat\nVoyager\n0 \/ 3\n-\nN\/A\nN\/A\nLLaMA-3-8B\n1 \/ 8\n-\n20.3 ± 0.0\n19.0 ± 0.0\nMinema-8B\n2 \/ 8\n-\n21.4 ± 1.2\n30.0 ± 10.0\nanimal\nhusbandry\nVoyager\n1 \/ 3\n-\n19.0 ± 0.0\n12.0 ± 0.0\nLLaMA-3-8B\n2 \/ 8\n-\n15.3 ± 7.6\n31.0 ± 4.0\nMinema-8B\n3 \/ 8\n-\n16.8 ± 7.8\n26.7 ± 16.2\net al., 2023a). As shown in Fig. 5, our agent with the MineMA-8B model can achieve superior\nperformance compared with all baselines, indicating that the agent can autonomously explore the\nMinecraft world without specific goals. It is notable that our agent with the MineMA-8B model can\noutperform Voyager (Wang et al., 2023a) with GPT-4o-mini or GPT-3.5.\n8\nPreprint\nTable 5: Performance comparison of different models on the dynamic-immediate planning task. All\nevaluation metrics are calculated only for successful tasks. “N\/A” indicates that all tasks fail. Please\nrefer to Appendix F.4 for easier visual inspection.\nTask\nModel\nSuccess Rate\nTime (min)\n# LLM Iters\nCollect Seeds\nBaichuan2-7B\n2 \/ 5\n1.8 ± 1.4\n3.0 ± 2.8\nQwen2-7B\n2 \/ 5\n3.8 ± 1.5\n4.5 ± 0.7\nMineMA-8B\n5 \/ 5\n1.3 ± 1.4\n1.4 ± 0.9\nMineMA-70B\n5 \/ 5\n1.4 ± 1.6\n1.0 ± 0.0\nHoe Farmland\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n2 \/ 5\n15.7 ± 16.2\n19.5 ± 10.6\nMineMA-8B\n2 \/ 5\n17.2 ± 14.7\n26.5 ± 9.2\nMineMA-70B\n4 \/ 5\n10.2 ± 6.7\n11.8 ± 2.6\nShear Sheep\nBaichuan2-7B\n1 \/ 5\n26.0 ± 0.0\n30.0 ± 0.0\nQwen2-7B\n2 \/ 5\n11.0 ± 2.8\n10.8 ± 1.5\nMineMA-8B\n2 \/ 5\n15.7 ± 10.9\n13.0 ± 9.9\nMineMA-70B\n3 \/ 5\n6.9 ± 7.8\n11.0 ± 7.5\nMilk Cow\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n1 \/ 5\n26.1 ± 0.0\n30.0 ± 0.0\nMineMA-8B\n1 \/ 5\n7.2 ± 0.0\n7.0 ± 0.0\nMineMA-70B\n2 \/ 5\n8.6 ± 10.0\n10.0 ± 11.3\nCook Meat\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n0 \/ 5\nN\/A\nN\/A\nMineMA-8B\n1 \/ 5\n25.6 ± 0.0\n38.0 ± 0.0\nMineMA-70B\n2 \/ 5\n20.2 ± 8.5\n24.0 ± 2.8\nObtain Leather\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n1 \/ 5\n14.9 ± 0.0\n16.0 ± 0.0\nMineMA-8B\n4 \/ 5\n15.0 ± 8.7\n17.8 ± 15.2\nMineMA-70B\n5 \/ 5\n7.4 ± 7.8\n8.8 ± 8.6\nMake Sugar\nBaichuan2-7B\n2 \/ 5\n16.2 ± 15.6\n22.0 ± 18.4\nQwen2-7B\n2 \/ 5\n15.4 ± 7.0\n15.5 ± 9.2\nMineMA-8B\n5 \/ 5\n4.3 ± 1.9\n7.0 ± 1.9\nMineMA-70B\n5 \/ 5\n4.3 ± 4.4\n7.8 ± 4.0\nCollect Water\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n1 \/ 5\n10.0 ± 0.0\n10.0 ± 0.0\nMineMA-8B\n4 \/ 5\n10.4 ± 3.0\n8.8 ± 5.5\nMineMA-70B\n5 \/ 5\n9.3 ± 4.8\n9.4 ± 3.7\n5.3\nABLATION STUDY\nWe conduct ablation studies on two core components of the ODYSSEY agent, including the LLM\nplanner and the open-world skill library. The results are shown in Fig. 5. In the autonomous ex-\nploration task, the LLM planner is responsible for generating a comprehensive plan based on the\nopen-world skill library. The ablation study demonstrates that the planner is indispensable for the\nagent to effectively navigate the complex Minecraft environment. Additionally, our experimental\nresults indicate that the absence of the open-world skill library significantly degrades performance.\nWithout the open-world skill library, the 8B LLM model alone is largely incapable of generating\nexecutable codes for the agent. This underscores the critical role of the open-world skill library in\nenabling the agent to perform complex tasks within the open-world setting of Minecraft.\n6\nRELATED WORKS\nMinecraft agents have been widely studied in recent years to test the capabilities of autonomous\nagents in open-world environments. Previous works focused on training Minecraft agents with\nreinforcement learning (Tessler et al., 2017; Oh et al., 2017; Lin et al., 2022; Mao et al., 2022; Hafner\n9\nPreprint\nDEPS with GPT-4o\nVoyager with GPT-3.5-Turbo\nVoyager with GPT-4o-mini\nReAct with GPT-4o-mini\nAutoGPT with GPT-4o-mini\nOdyssey with LLaMA3-8B\nOdyssey with MineMA3-8B w\/o Planner\nOdyssey with MineMA3-8B w\/o Skill Library\nOdyssey with MineMA3-8B\n0\n20\n40\n60\n80\nIteration Number\n0\n10\n20\n30\n40\n# Distinct Items Obtained\n(a) Exploration Curves\n0\n10\n20\n30\n40\n# Distinct Items Obtained\n0\n1000\n2000\n3000\n4000\nDistance Traveled\n0\n200\n400\n600\n# Items Crafted\n0\n50\n100\n150\n200\n# R&A Unlocked\n(b) Evaluation Metrics\nFigure 5: Performance comparison of different models on autonomous exploration tasks. To make\nthe results in figures clearer for readers, we adopt a 50% confidence interval to plot the error region.\net al., 2023) or imitation learning (Baker et al., 2022; Cai et al., 2023; Lifshitz et al., 2023), which\nare extensively used in the MineRL (Guss et al., 2019) competition to solve the ObtainDiamond\ntask. With the rapid development of LLMs, numerous studies leverage LLMs to enhance agent\ncapabilities (Zhang et al., 2023a; Zhu et al., 2023; Feng et al., 2023; Zhao et al., 2023; Wang et al.,\n2023a;b; Zhou et al., 2024a). Among these, several works (Li et al., 2023; Yuan et al., 2023; Wang\net al., 2023c; Qin et al., 2023; Ding et al., 2023) employ LLMs to guide skill learning in Minecraft,\nenabling agents to act in a human-like way. However, these methods mainly focus on learning\nprimitive skills from scratch, lacking a reusable skill library. Voyager (Wang et al., 2023a) builds\na skill library by allowing the LLM to write its own skills. However, Voyager must rely on GPT-4\nfor high-quality skill generation, incurring substantial costs. This expense can be prohibitive for\nmany researchers. In contrast, ODYSSEY provides an open-world skill library that agents can call\nupon, achieving performance comparable to Voyager with GPT-4, but using only 8B LLMs. This\nmakes ODYSSEY significantly more accessible and cost-effective, enabling LLM-based agents to\nefficiently generate complex policies for broader exploration.\nOpen-world environments have gained considerable attention from research communities (Cao\net al., 2020; Chevalier-Boisvert et al., 2018; Juliani et al., 2019; Shen et al., 2021; Srivastava et al.,\n2022; Du et al., 2023). Minecraft, with its diverse tasks and mature game mechanics, has emerged\nas an ideal test-bed for open-world tasks. Built on Minecraft, MineRL (Guss et al., 2019) imple-\nments a simulation environment for agent learning. MineDojo (Fan et al., 2022) further extends\nMineRL with thousands of diverse tasks. MCU (Lin et al., 2023) collects a variety of atom tasks,\noffering a method to generate infinite tasks by combining the atom tasks. However, existing bench-\nmarks mainly focus on providing basic programmatic tasks to evaluate agents learned from scratch.\nOur ODYSSEY benchmark is built on top of the skill library, enabling the agents to bypass basic\nprogrammatic tasks and focus on complex open-world challenges.\n7\nCONCLUSION\nThis work proposes ODYSSEY to empower agents with open-world skills in the Minecraft environ-\nment. We introduce (1) an interactive agent endowed with an extensive open-world skill library com-\nprising various primitive skills and compositional skills; (2) a fine-tuned LLaMA-3 model, trained on\na large-scale question-answering dataset sourced from the Minecraft Wiki; (3) a new agent capabil-\nity benchmark that encompasses tasks requiring long-term planning, dynamic-immediate planning,\nand autonomous exploration. The public availability of all datasets, model weights, and code will\nfacilitate future research in the development of autonomous agents. We hope that ODYSSEY will\ninspire further innovation and progress in the field of autonomous agent development.\nLimitations and Future Works. The proposed open-world skill library enables the use of open-\nsource LLMs as the foundation for agents to call upon skills, avoiding the high costs associated with\nprevious work using GPT-4 (Wang et al., 2023a; Li et al., 2023; Qin et al., 2023). However, the open-\nsource LLMs are prone to generating hallucinations, leading to a decrease in agent performance.\n10\nPreprint\nThus, our future research will focus on employing retrieval-augmented generation to improve LLMs\nin Minecraft. Additionally, this work focuses on developing and evaluating text-based LLMs in the\ncontext of Minecraft, with visual aspects currently out of scope. Looking ahead, we plan to integrate\nvisual understanding into the skill library to enhance the agent capabilities.\nREFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical\nreport. arXiv preprint arXiv:2303.08774, 2023.\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. In Advances in Neural Information Processing Systems, volume 35, pp.\n24639–24654, 2022.\nShaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control\nthrough goal-aware representation learning and adaptive horizon prediction. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 13734–13744, 2023.\nTianshi Cao, Jingkang Wang, Yining Zhang, and Sivabalan Manivasagam. Babyai++: Towards\ngrounded-language learning beyond memorization. arXiv preprint arXiv:2004.07200, 2020.\nMaxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia,\nThien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of\ngrounded language learning. In International Conference on Learning Representations, 2018.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su.\nMind2web: Towards a generalist agent for the web. In Advances in Neural Information Processing\nSystems, volume 36, pp. 28091–28114, 2023.\nZiluo Ding, Hao Luo, Ke Li, Junpeng Yue, Tiejun Huang, and Zongqing Lu. Clip4mc: An rl-friendly\nvision-language model for minecraft. arXiv preprint arXiv:2303.10571, 2023.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied mul-\ntimodal language model. In Proceedings of the International Conference on Machine Learning,\nvolume 202, pp. 8469–8488, 2023.\nYuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek\nGupta, and Jacob Andreas. Guiding pretraining in reinforcement learning with large language\nmodels. In Proceedings of the International Conference on Machine Learning, volume 202, pp.\n8657–8677, 2023.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In Advances in Neural Information Processing Systems,\nvolume 35, pp. 18343–18362, 2022.\nYicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, and Zongqing Lu. Llama rider: Spurring\nlarge language models to explore the open world. arXiv preprint arXiv:2310.08922, 2023.\nWilliam H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: a large-scale dataset of minecraft demonstrations.\nIn Proceedings of the International Joint Conference on Artificial Intelligence, pp. 2442–2448,\n2019.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\n11\nPreprint\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference\non Learning Representations, 2021.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations, 2021.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda\nLuu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning\nthrough planning with language models. In Proceedings of the Conference on Robot Learning,\nvolume 205, pp. 1769–1782, 2022.\nIat Long Iong, Xiao Liu, Yuxuan Chen, Hanyu Lai, Shuntian Yao, Pengbo Shen, Hao Yu, Yuxiao\nDong, and Jie Tang. OpenWebAgent: An open toolkit to enable web agents on large language\nmodels. In Proceedings of the Annual Meeting of the Association for Computational Linguistics,\npp. 72–81, 2024.\nArthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter Henry,\nAdam Crespi, Julian Togelius, and Danny Lange. Obstacle tower: A generalization challenge in\nvision, control, and planning. In Proceedings of the International Joint Conference on Artificial\nIntelligence, pp. 2684–2691, 2019.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural\nInformation Processing Systems, volume 33, pp. 9459–9474, 2020.\nHao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li,\nLewei Lu, and Jifeng Dai. Auto mc-reward: Automated dense reward design with large language\nmodels for minecraft. arXiv preprint arXiv:2312.09238, 2023.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence,\nand Andy Zeng. Code as policies: Language model programs for embodied control. In IEEE\nInternational Conference on Robotics and Automation, pp. 9493–9500, 2023.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. In Advances in Neural Information Processing Systems,\nvolume 36, pp. 69900–69929, 2023.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74–81, 2004.\nHaowei Lin, Zihao Wang, Jianzhu Ma, and Yitao Liang. Mcu: A task-centric framework for open-\nended agent evaluation in minecraft. arXiv preprint arXiv:2310.08367, 2023.\nZichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, and Wei Yang. Juewu-mc: Playing\nminecraft with sample-efficient hierarchical reinforcement learning. In Proceedings of the Inter-\nnational Joint Conference on Artificial Intelligence, pp. 3257–3263, 2022.\nHangyu Mao, Chao Wang, Xiaotian Hao, Yihuan Mao, Yiming Lu, Chengjie Wu, Jianye Hao, Dong\nLi, and Pingzhong Tang. Seihai: A sample-efficient hierarchical ai for the minerl competition.\nIn Proceedings of the International Conference on Distributed Artificial Intelligence, pp. 38–51,\n2022.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\nJunhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli.\nZero-shot task generalization\nwith multi-task deep reinforcement learning. In Proceedings of the International Conference on\nMachine Learning, volume 70, pp. 2661–2670, 2017.\n12\nPreprint\nOpenAI. Introducing chatgpt. 2023.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the Annual Meeting of the Association for\nComputational Linguistics, pp. 311–318, 2002.\nPrismarineJS. Mineflayer: Create minecraft bots with a powerful, stable, and high level javascript\napi. https:\/\/github.com\/PrismarineJS\/mineflayer, 2023.\nYiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing\nShao. Mp5: A multi-modal open-ended embodied system in minecraft via active perception.\narXiv preprint arXiv:2312.07472, 2023.\nScott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas.\nA generalist agent.\narXiv preprint\narXiv:2205.06175, 2022.\nNils Reimers and Iryna Gurevych.\nSentence-bert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the Conference on Empirical Methods in Natural Language Pro-\ncessing, pp. 3982–3992, 2019.\nManolis Savva, Jitendra Malik, Devi Parikh, Dhruv Batra, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, and Vladlen Koltun. Habitat: A\nplatform for embodied AI research. In Proceedings of the IEEE\/CVF International Conference\non Computer Vision, pp. 9338–9346, 2019.\nDhruv Shah, Michael Robert Equi, Bła˙zej Osi´nski, Fei Xia, Brian Ichter, and Sergey Levine. Naviga-\ntion with large language models: Semantic guesswork as a heuristic for planning. In Proceedings\nof the Conference on Robot Learning, volume 229, pp. 2683–2699, 2023.\nBokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Clau-\ndia Pérez-D’Arpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, Micael Tchapmi, Kent\nVainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. igibson 1.0: A simulation environment for\ninteractive tasks in large realistic scenes. In Proceedings of the IEEE\/RSJ International Confer-\nence on Intelligent Robots and Systems, pp. 7520–7527, 2021.\nSignificant-Gravitas.\nAutogpt:\nBuild & use ai agents.\nhttps:\/\/github.com\/\nSignificant-Gravitas\/AutoGPT, 2023.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans\nusing large language models. In Proceedings of the IEEE International Conference on Robotics\nand Automation, pp. 11523–11530, 2023.\nSanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia, Kent Elliott\nVainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, Silvio Savarese, Hyowon Gweon,\nJiajun Wu, and Li Fei-Fei. Behavior: Benchmark for everyday household activities in virtual,\ninteractive, and ecological environments. In Proceedings of the Conference on Robot Learning,\nvolume 164, pp. 477–490, 2022.\nChen Tessler, Shahar Givony, Tom Zahavy, Daniel Mankowitz, and Shie Mannor. A deep hier-\narchical approach to lifelong learning in minecraft. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 31, pp. 1553–1561, 2017.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding.\nIn\nInternational Conference on Learning Representations, 2018.\n13\nPreprint\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman.\nSuperglue: A stickier benchmark for general-purpose language\nunderstanding systems. In Advances in Neural Information Processing Systems, volume 32, pp.\n3261–3275, 2019.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\nTransactions on Machine Learning Research, 2023a.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on large\nlanguage model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. De-\nscribe, explain, plan and select: interactive planning with llms enables open-world multi-task\nagents. In Advances in Neural Information Processing Systems, volume 36, pp. 34153–34189,\n2023b.\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin,\nZhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with\nmemory-augmented multimodal language models. arXiv preprint arXiv:2311.05997, 2023c.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. In Advances in Neural Information Processing Systems, volume 35, pp. 24824–24837,\n2022a.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances\nin Neural Information Processing Systems, volume 35, pp. 24824–24837, 2022b.\nXinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, and Börje F Karls-\nson. A survey on game playing agents and large models: Methods, applications, and challenges.\narXiv preprint arXiv:2403.10249, 2024.\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan,\nDian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint\narXiv:2309.10305, 2023.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, et al.\nQwen2 technical report.\narXiv preprint\narXiv:2407.10671, 2024a.\nSonghua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, and Hongying\nZan. Zhongjing: Enhancing the chinese medical capabilities of large language model through\nexpert feedback and real-world multi-turn dialogue. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 38, pp. 19368–19376, 2024b.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. In International Conference on\nLearning Representations, 2023.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Skill reinforcement learning and planning for open-world long-horizon tasks. arXiv preprint\narXiv:2303.16563, 2023.\nChi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, and Zongqing Lu. Creative agents: Empowering\nagents with imagination for creative tasks. arXiv preprint arXiv:2312.02519, 2023a.\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming Chen, Jianquan Li, Xi-\nangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, and Haizhou Li. Huatuogpt,\ntowards taming language model to be a doctor. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing, pp. 10859–10885, 2023b.\n14\nPreprint\nZhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian Ye, Jenq-\nNeng Hwang, and Gaoang Wang. See and think: Embodied agent in virtual environment. arXiv\npreprint arXiv:2311.15209, 2023.\nEnshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and\nJing Shao. Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-\nworld control. arXiv preprint arXiv:2403.12037, 2024a.\nGengze Zhou, Yicong Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language naviga-\ntion with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 38, pp. 7641–7649, 2024b.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n15\nPreprint\nAppendix\nTable of Contents\nA Discussion on Societal Impacts\n17\nB\nDiscussion on Migrating Odyssey to Other Domains\n17\nC Open-World Skill-based Interactive Agent\n17\nC.1\nOpen-World Skill Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nC.2\nLLM Planner\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nC.3\nLLM Actor\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nC.4\nLLM Critic\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nD Fine-tune Minecraft LLM\n25\nD.1\nDataset Generation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nD.2\nModel Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nD.3\nModel Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nE\nAgent Capability Benchmark\n33\nE.1\nLong-term Planning Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nE.2\nDynamic-immediate Planning Task . . . . . . . . . . . . . . . . . . . . . . . .\n33\nE.3\nAutonomous Exploration Task\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nE.4\nSpecific Agent Capability Requirements for Different Tasks . . . . . . . . . . .\n34\nF\nExperiments\n34\nF.1\nExperimental Details\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nF.2\nAgent Capability Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\nF.3\nAblation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nF.4\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n16\nPreprint\nA\nDISCUSSION ON SOCIETAL IMPACTS\nWhen developing autonomous embodied agents within Minecraft, the negative impacts are rela-\ntively minimal. Minecraft provides a controlled environment to test these technologies. Concerns\ninclude potential over-reliance by players, reducing their exploratory and creative thinking, minor\ndata privacy issues due to the collection of anonymized player data, and possible impacts on game\nbalance, particularly in multiplayer settings. Overall, Minecraft is an ideal experimental platform\nwhere these mild negative impacts can be effectively managed.\nB\nDISCUSSION ON MIGRATING ODYSSEY TO OTHER DOMAINS\nThe skill library designed for Minecraft is built with modularity and generalizability in mind, al-\nlowing for potential adaptation to other domains such as robot manipulation (Liang et al., 2023;\nSingh et al., 2023), navigation (Zhou et al., 2024b; Shah et al., 2023), and other game-playing\nenvironments (Xu et al., 2024). These skills abstract underlying actions and focus on high-level\ninteractions, allowing them to be adapted to different environments by redefining low-level actions\nwithout changing the overall structure of the skill library. Even without direct API access, basic\naction spaces (e.g., keyboard and mouse operations in games, or movement operations in robotics)\ncan be employed to construct primitive skills. Prior research in robotic manipulation, including\nCaP (Liang et al., 2023) and ProgPrompt (Singh et al., 2023), demonstrates how primitive skills\nsuch as picking and placing objects or opening containers can be built from basic actions. More-\nover, we believe that the concept of \"skills\" should extend beyond code APIs to include knowledge\nfrom various sources. For example, handbooks can provide informational segments treated as skills,\nretrievable by LLMs using techniques like retrieval-augmented generation (Lewis et al., 2020), en-\nhancing decision-making.\nTo fine-tune the LLaMA-3 model for the Minecraft agent, we crawled the Minecraft Wiki and used a\nGPT-assisted approach to generate an instruction dataset. Researchers in other domains can replicate\nthis process to create their own instruction datasets. To facilitate this, we have also open-sourced\nour Minecraft Wiki crawler, which can be easily modified to crawl similar Wiki websites for other\ndomains. Additionally, our benchmark tasks evaluate agent performance from three perspectives:\nlong-term planning, dynamic-immediate planning, and autonomous exploration. These dimensions\neffectively assess the capabilities of open-world autonomous agents. Researchers in other domains\ncan adopt these perspectives to design comprehensive evaluation tasks for their needs.\nC\nOPEN-WORLD SKILL-BASED INTERACTIVE AGENT\nC.1\nOPEN-WORLD SKILL LIBRARY\nC.1.1\nPRIMITIVE SKILLS\nPrimitive skills encompass a series of underlying interfaces on top of Mineflayer JavaScript\nAPIs (PrismarineJS, 2023), divided into two main categories: 32 operational skills and 8 spatial\nskills. In addition to Voyager’s 18 operational skills (Wang et al., 2023a), 14 operational skills\nimplemented by us are presented as follows:\n• plantSeeds(bot, type): Let the agent find the nearest farmland and plant a partic-\nular kind of seed.\n• feedAnimals(bot, type, count=1): Let the agent find the nearest animals of a\nparticular species and numbers and feed them with the appropriate food.\n• killAnimal(bot, type): Let the agent kill a particular kind of animal using the best\nsword in its inventory.\n• killMonsters(bot, type, count=1): Let the agent kill monsters nearby of a\nparticular species and numbers using the best sword in its inventory.\n• cookFood(bot, type, count=1): Let the agent cook food of a particular kind and\nnumbers using coal and furnace.\n• eatFood(bot, type): Let the agent eat a particular kind of food.\n17\nPreprint\n• equipArmor(bot): Let the agent equip the best armor(helmet, chestplate, leggings and\nboots) in its inventory.\n• equipSword\/Pickaxe\/Axe\/Hoe\/Shovel(bot): Let the agent equip the best cor-\nresponding tool in its inventory.\n• getLogs\/PlanksCount(bot): Return the number of logs\/planks (counted in seven\ndifferent categories) in the inventory.\nAdditionally, we pioneer 8 spatial skills that Voyager (Wang et al., 2023a) lacks, allowing for en-\nvironmental interactions based on the agent coordinates. The spatial skills implemented by us are\npresented as follows:\n• findSuitablePosition(bot): Let the agent find the best nearby location for plac-\ning devices such as a crafting table or furnace. The block must be minecraft:air and\nat least one adjacent reference block exists.\n• checkAdjacentBlock(bot, types, x, y, z): Check blocks adjacent to the\nblock at position (x, y, z). Return true if any of the adjacent blocks match the specified\ntypes.\n• checkBlockAbove(bot, type, x, y, z): Check block above the block at po-\nsition (x, y, z). Return true if the above block matches the specified type.\n• checkBlocksAround(bot, type, x, y, z): Check blocks around the block at\nposition (x, y, z).Return true if any of the around blocks match the specified type.\n• checkNearbyBlock(bot, types, x, y, z, r):\nCheck blocks in a radius\naround the block at position (x, y, z). Return true if any block within the radius matches the\nspecified types.\n• checkNoAdjacentBlock(bot, types, x, y, z): Check adjacent blocks of\nblock at position (x, y, z). Return true if not all adjacent blocks are within the speci-\nfied types.\n• goto(bot, x, y, z): Let the agent go to the corresponding position (x, y, z) until it\nreaches the destination.\n• getAnimal(bot, type, x, y, z): Let the agent attract a particular kind of ani-\nmal to a particular position (x, y, z) with the appropriate food.\nC.1.2\nCOMPOSITIONAL SKILLS\nAll compositional skills are encapsulated by the Mineflayer APIs and the aforementioned primitive\nskills, while higher-level compositional skills recursively call lower-level ones. Fig. 6 illustrates the\nnested relationships among the 13 skills required to complete the mineDiamond task. We classify\nall compositional skills into main types as follows:\n• mineX(bot): Equip the agent with the appropriate tools and find the nearest specific\nblock to mine it.\n• craftX(bot): Let the agent collect the necessary materials and check if the crafting\ntable exists in the inventory (if needed), to craft a specific tool or something.\n• smeltX(bot): Let the agent check the furnace and fuel, and to smelt the specified ma-\nterials.\n• collectX(bot): Similar to mineX, used to collect multiple quantities of a certain item.\n• makeX(bot): Similar to craftX, used to make food.\n• cookX(bot): Similar to smeltX, used to cook food.\n• plantX(bot): Let the agent check the inventory for seeds, collect them if not present,\nand plant them in nearby farmland.\n• breedX(bot): Let the agent check the inventory for the required corresponding feed,\nfind the nearest two animals, feed them, and facilitate their breeding.\n• killX(bot): Let the agent equip the best sword in the inventory, find the nearest specific\nanimal or monster, kill it, and collect the dropped items.\n• placeX(bot): Let the agent place an item at its current or a nearby suitable location,\nand if the item is not in inventory, craft it first.\nAdditionally, there are several other compositional skills aimed at executing specific behaviors, such\nas catchFish, hoeFarmland, shearSheep, takeAndMoveMinecart.\n18\nPreprint\ncraftSticks\ncraftWoodenPlanks\nmineWoodLog\ncraftWoodenPickaxe\ncraftCraftingTable\ncraftIronPickaxe\nsmeltRawIron\nmineIronOre\nmineCoalOre\ncraftFurnace\ncraftStonePickaxe\nmineCobblestone\nmineDiamond\nFigure 6: An illustrative diagram of the skill recursive method for the mineDiamond task. The\nfour colors depicted represent four different technological levels (wood, stone, iron, and diamond)\nfollowing the Minecraft tech-tree.\nC.2\nLLM PLANNER\nODYSSEY relies on LLMs to generate language-based plans. In our Minecraft experiment, we pro-\npose three novel tasks (long-term planning task, dynamic-immediate planning task and autonomous\nexploration task) for agents to explore. Therefore we designate three types of prompt messages for\nthem respectively, offering LLM Planner the ability to generate different routines on different tasks.\nThe format of the prompt is presented thus:\n• \"SYSTEM\" role: A high-level instruction that gives directions to the model behavior. It\nsets an overall goal for the interaction and provides external information.\n• \"USER\" role: Detailed information like environment, states and achievements of the agent\nwill be provided to the planner for the next immediate subgoals.\n• \"ASSISTANT\" role: A guideline generated by the planner.\nC.2.1\nLONG-TERM PLANNING\nWe design a suite of combat tasks to assess the long-term planning capabilities of agents, where the\nLLM Planner should plan to craft appropriate weapons and equipment to defeat monsters.\nThe input prompt to LLM consists of several components:\n• Ultimate goals: The monsters that need to be defeated.\n• Directives and Behavior Constraints: These ensure that the proposed tasks are both achiev-\nable and verifiable.\n• Information of last combat: Incorporating information from previous battles enables the\ndevelopment of more efficient plan.\nLong-term Planning System Prompt\n—Overall goals—\nYour goal is to generate the plan that can defeat all monsters while using the shortest\ntime. So, more is not always better when proposing your plan list.\n—External information—\nIn Minecraft, combating with monsters requires weapons and armor.\nThe weapon\noptions are limited to \"sword\", while the armor includes \"helmet\", \"chestplate\", \"leggings\",\n19\nPreprint\nand \"boots\". The materials for swords range from low to high level: wooden swords, stone\nswords, iron swords, and diamond swords; The materials for armor range from low to high\nlevel: iron, diamond. The higher the material level, the greater the attack damage of the\nweapon and the better the protection effect of the armor. However, the higher the material\nlevel, the more time it costs to collect.\nTips: Wooden, stone, iron and diamond are the only levels of sword; iron and diamond\nare the only levels of armors; helmet, chestplate, leggings and boots are the only types of\narmors; do not generate information that doesn’t relate to them.\nAfter each round of combat, I will give you:\nEquipment obtained from last round: ...\nHealth after last combat: ...\nCritique: ...\nMonster: The monsters you need to defeat.\n—Directions—\nThe critique (if any) will tell you the subgoal list from the previous round and whether you\nshould trim or add to it. Remember to refer to the critique to adjust your task list. Next, you\nwill start a new combat task, the last round of equipment and health is only for planning\nreference, not related to the current round. Plan from scratch!\n—Behaviour constraints—\nYou must follow the following criteria:\n1) Return a Python list of subgoals that can be completed in order to complete the specified\ntask.\n2) Each subgoal should only start with \"craft\"! do not propose any other type of skills!\n3) Each subgoal should follow a concise format \"craft [material type] [equipment type]\".\nYou should only respond in JSON format as described below:\n[\"subgoal1\", \"subgoal2\", \"subgoal3\", ...]\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nAfter finish collecting weapons and equipment, we also plan an efficient routine to combat with\nmonsters for higher survival rates. For example, monsters that are more harmful and aggressive\nshould be placed in a higher priority. The full prompt for re-ranking the combat order of monsters\nis shown below.\nComabt Order System Prompt\nYou are a helpful assistant that generates the order of fighting monsters to defeat all monsters\nspecified by me.\nI’ll give you a list of monsters, and you need to rearrange the order of monsters according to\nhow hard it is to beat them.\nYou should give priority to monsters that attack the player and do more damage, while\nmonsters that don’t actively attack the player or do less damage should be left behind.\nMake sure your list includes all the monsters in your task.\nThe output format must be exactly same as the input, including the underline.\nIf your task is to combat a single type of monsters, return a list containing only that monster\nas well.\nYou should only respond in JSON format as described below:\n[\"quantity monster1\", \"quantity monster2\", \"quantity monster3\", ...]\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\n20\nPreprint\nC.2.2\nDYNAMIC-IMMEDIATE PLANNING\nIn this kind of task, agents are expected to adapt their plans based on the real-time feedback like\nnearby resources and animals.\nThe input prompt to LLM consists of the following components:\n• Ultimate goals: A suite of farming tasks, such as planting, harvesting, and animal hus-\nbandry.\n• The current states of agent: hunger and health values, position and nearby entities, etc.\n• Achievements of the agent: the current inventory and unlocked equipment, as well as pre-\nviously successful and failed tasks.\nDynamic-immediate Planning System Prompt\n—Overall goals—\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft.\nMy ultimate goal is to \"goals\".\nMake sure that the proposed task is related to the ultimate goal, and do not propose unrelated\ntasks!\n—Directions—\nYou need to plan step by step towards your ultimate goal, so propose necessary pre-\nrequisite tasks first.\nFor example, \"craft hoe\" before \"hoe farmland\", \"collect [type] seeds\" and \"hoe farmland\"\nbefore \"plant seed\", \"kill [animalType]\" before \"cook meat\", \"craft shears\" before \"shear\nsheep\", \"craft bucket\" before \"collect milk\".\nPropose the current task only when you ensure that you have all the necessary dependent\nitems in inventory.\nDon’t ask for repetitive tasks. If you already have an item in your inventory, try not to\ncollect it repeatedly.\nFor example, when you already have a hoe in your inventory, propose \"hoe farmland\"\ninstead of \"craft hoe\" again.\n—External information—\nI will give you the following information:\nUltimate goal: ...\nReference: ...\nBiome: ...\nNearby blocks: ...\nOther blocks that are recently seen: ...\nNearby entities (nearest to farthest): ...\nHealth: Higher than 15 means I’m healthy.\nHunger: Higher than 15 means I’m not hungry.\nInventory (xx\/36): ...\nLogs: The execution logs in last task, you can refer to it to propose next task.\nCompleted tasks so far: ...\nFailed tasks that are too hard: ...\n—Behaviour constraints—\nYou must follow the following criteria:\n1) Please be very specific about what resources I need to collect, what I need to farm, or\nwhat animals I need to breed\/kill.\n2) The next task should follow a concise format, such as \"craft [item]\", \"breed\/kill [animal\n21\nPreprint\ntype]\", \"cook\/eat [food type]\", \"plant [seed type] seed\" or some specific action \"shear\nsheep\", \"collect milk\". Do not propose multiple tasks at the same time. Do not mention\nanything else.\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"Based on the information I listed above, do reasoning about what the next task\nshould be\",\n\"task\": \"The next task\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nC.2.3\nAUTONOMOUS EXPLORATION\nIn this task, the agent is required to explore the Minecraft world freely without any specific goals.\nThis poses a great challenge to the planner for maximal exploration. It should propose suitable tasks\nbased on the current state and environment, e.g., plan to obtain sand or cactus before wood if it finds\nitself in a desert rather than a forest. The input prompt to LLM consists of several components:\n• Guidelines encouraging diverse tasks.\n• The current states of agent: hunger and health values, position and nearby entities, etc.\n• Achievements of the agent: the current inventory and unlocked equipment, as well as pre-\nviously successful and failed tasks.\nAutonomous Exploration System Prompt\n—Overall goals—\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft.\nMy ultimate goal is to discover as many diverse things as possible, accomplish as many\ndiverse tasks as possible and become the best Minecraft player in the world.\n—External information—\nI will give you the following information:\nBiome: ...\nTime: ...\nNearby blocks: ...\nOther blocks that are recently seen: ...\nNearby entities (nearest to farthest): ...\nHealth: Higher than 15 means I’m healthy.\nHunger: Higher than 15 means I’m not hungry.\nPosition: ...\nEquipment: If I have better armor in my inventory, you should ask me to equip it.\nInventory (xx\/36): ...\nChests: ...\nCompleted tasks so far: ...\nFailed tasks that are too hard: ...\n—Directions—\nYou must follow the following criteria:\n1) You should act as a mentor and guide me to the next task based on my current learning\nprogress.\n2) Please be very specific about what resources I need to collect, what I need to craft, or\nwhat mobs I need to kill.\n22\nPreprint\n3) The next task should follow a concise format, such as \"Mine [block]\", \"Craft [item]\",\n\"Smelt [item]\", \"Kill [mob]\", \"Cook [food]\", \"Equip\" etc. It should be a single phrase. Do\nnot propose multiple tasks at the same time. Do not mention anything else.\n4) The next task should be novel and interesting. I should look for rare resources, upgrade\nmy equipment and tools using better materials, and discover new things. I should not be\ndoing the same thing over and over again.\n5) Don’t propose tasks that have already completed once or failed more than three times!\n6) Do not ask me to build or dig shelter even if it’s at night. I want to explore the world and\ndiscover new things. I don’t want to stay in one place.\n7) Tasks that require information beyond the player’s status to verify should be avoided. For\ninstance, \"Placing 4 torches\" and \"Dig a 2x1x2 hole\" are not ideal since they require visual\nconfirmation from the screen. All the placing, building and trading tasks should be avoided.\nDo not propose task starting with these keywords.\n8) For wood-related tasks, you don’t need to emphasize the type of wood, just propose\n\"mine log\" or \"craft planks\".\n—Behaviour constraints—\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"Based on the information I listed above, do reasoning about what the next\ntask should be.\",\n\"task\": \"The next task.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nC.3\nLLM ACTOR\nIn actor, the mapping from higher language subgoals S to lower executable codes is implemented\nthrough query context encoding and similarity retrieval. We employ the following prompt during\nthe generation of query context (Question-Answer pairs).\nQuery Context Prompt\nSYSTEM:\nYou are a helpful assistant that answer my question about Minecraft.\nI will give you the following information:\nQuestion: ...\nYou will answer the question based on the context (only if available and helpful) and your\nown knowledge of Minecraft.\n1) Start your answer with \"Answer: \".\n2) Answer \"Answer: Unknown\" if you don’t know the answer.\nUSER:\nHow to complete S in Minecraft?\nAfter recalling the top-10 relevant skills with the highest scores, we require LLM to determine the\nmost appropriate code for execution within the environment based on their description. The full\nprompt of code selection is shown in the following.\nSkill Selection System Prompt\nYou are a helpful assistant that decides Mineflayer javascript code to complete any Minecraft\ntask specified by me.\nI will give you\nTask: The task I need to complete in this stage.\n23\nPreprint\nPrograms: The description of relevant programs that may use to complete the task.\nProgram used in the last round: ...\nCritique: ...\nYou will choose only one program based on the program description and critique. You\nshould only respond in the format as described below:\n{\n\"program\": \"your selected program name\",\n\"reason\": \"Reason you choose the program.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nPlease ensure that the program name you output should be exactly the same (case-inclusive)\nas the information provided!\nC.4\nLLM CRITIC\nThe LLM critic should evaluate the success of the executed actions by comparing expected out-\ncomes with actual results, thereby providing valuable critiques for refining strategies in subsequent\niterations. We design a chain-of-thought (Wei et al., 2022b) prompting mechanism: We first require\nLLM to reason about the task’s success or failure, then output a boolean variable representing the\nexecution result, and finally provide a critique to the agent if the task fails.\nCritic System Prompt\nYou are required to evaluate if I have met the task requirements in Minecraft. Exceeding\nthe task requirements is also considered a success while failing to meet them requires you\nto provide critique to help me improve.\nI will give you the following information:\nTask: The objective I need to accomplish.\nNearby blocks:\nEntities:\nEquipment: My tools, weapons and armor could sometimes be here.\nChests: If the task requires me to place items in a chest, you can find chest information\nhere.\nCurrent inventory (xx\/36): My final inventory after carry out the task.\nLast inventory (xx\/36): My inventory before carry out the task.\nChat log: The logs during carrying out the task.\n**Note** that you only need to check the changes of my inventory to judge whether I meet\nthe task.\nFor a `craft [item]`task, all you need to do is checking if the item I need to craft is in my\ncurrent inventory or equipment. If in, you should judge it as a success and vice versa.\nFor a `mine [item]`task, you only need to check whether the item is in my current inventory\nor has an increase over the last inventory.\nFor a `hoe`or `plant`task, you only need to check whether the `farmland`or `seed`is in\nNearby Blocks.\nDo not judge the success of a `craft`task based on other materials I have!\nYou can only judge a task failure via chat log, not as a reason to judge a task’s success.\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"reasoning\",\n\"success\": boolean,\n\"critique\": \"critique\",\n}\n24\nPreprint\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nThe input prompt to LLM consists of the following components:\n1. Task proposed by the LLM Planner;\n2. Environment feedback: We provide the agent with nearby blocks and entities that are re-\ncently seen for high-quality critiques. We also give the log information during the execution\nstage;\n3. Achievements of the agent. We offer achievement of the agent like inventory and equipment\nto assess the task’s completeness.\nCritic User Prompt\nTask: Mine 1 wood log\nNearby blocks: birch_leaves, oak_leaves, birch_log, oak_log\nEquipment: [None, None, None, None, 'oak_sapling', None]\nChests: None\nCurrent Inventory (2\/36): 'oak_sapling': 1, 'oak_log': 1\nLast Inventory (0\/36):\nChat log: Mined 1 wood log.\nD\nFINE-TUNE MINECRAFT LLM\nFor detailed code, datasets, and models used in this section, please visit our code for more informa-\ntion. The overall fine-tuning framework is shown in Fig. 7.\nData\nGeneration\nMinecraft\nWiki\nTXT\nFormat\nMD\nFormat\nData\nCleaning\nGPT-\n3.5-\nturbo\nMinecraft\nQ&A\nDataset\nLong\nShort\nBool\nModel\nFine-tuning\nModel\nEvaluation\n  Llama-3-8B-Instruct   \n Llama-3-70B-Instruct  \nMineMA \nPage Driven\nPrompt\nTheme Driven\nPrompt\nGPT-4\nMCQ Evaluation\nDataset v1\nMCQ Evaluation\nDataset v2\nEvaluation\nFine-tune  Minecraft LLM\nNormal\nFine-tune\nLoRA\nFigure 7: An overview of the fine-tune Minecraft LLM framework.\nD.1\nDATASET GENERATION\nD.1.1\nDATA CLEANING\nFor this study, we select two primary sources of information, the Minecraft Fandom Wiki (https:\n\/\/minecraft.fandom.com\/wiki\/Minecraft_Wiki) and the Minecraft Wiki (https:\n\/\/minecraft.wiki\/).\nFor the Minecraft Fandom Wiki, we first crawl the content of all its pages and perform a preliminary\nfiltering on the resulting pages, removing pages that were obviously useless for our fine-tuning task,\n25\nPreprint\nsuch as game version information, and obtaining a series of JSON files. These files still contain a\nsignificant amount of information that we do not need to create the dataset, so we carry out a data\ncleaning process, extracting the text and table content of the original pages, storing them in a series\nof TXT files, with each page corresponding to a TXT file. Through the above method, we obtain the\ncleaned TXT format page information.\nFor Minecraft Wiki, we exclude a few categories that are useless for our fine-tuning task, such as\nHistory, and crawl the content of all other categories’ pages. Similar to the process for Minecraft\nFandom Wiki, these pages also contain a lot of information that we didn’t need. We remove some\nirrelevant sections such as Achievements, Advancements, and History, and only retained the text\nand table content of other useful sections. After that, we store the processed data in markdown files\nand perform segmentation operations based on the number of words in the file. We ensure that the\nword count of the segmented file did not exceed the set limit without cutting the section block, thus\nobtaining clean MD format page information.\nD.1.2\nQ&A PAIRS\nIn this study, we design four types of Q&A pairs for our training dataset as follows:\n• Short Answer Q&A: Questions that can be directly answered with a word or phrase, such\nas names, statistics, etc.\n• Long Answer Q&A: Questions that require one or more complete sentences as answers.\nThese questions are suitable for explanatory or definitional responses, where a detailed\nexplanation or a clear definition is needed to fully address the question.\n• Bool Answer Q&A: Questions that can be answered with a simple True or False.\n• Normal Answer Q&A: QA pairs with no specific form restrictions, used to supplement the\ndataset and ensure its comprehensiveness.\nSome Q&A pair examples are shown as follows:\nShort Answer Q&A\nQ: What mobs spawn naturally in the desert biome during the day in Minecraft?\nA: During the day in Minecraft, zombies in desert biomes are replaced by husks, which\ninflict the Hunger effect and do not burn in sunlight.\nLong Answer Q&A\nQ: How does the use of light sources like torches and lava help in preventing mob spawning\nin Minecraft?\nA: In Minecraft, placing light sources such as torches, lava, glowstone, jack o’lanterns, sea\nlanterns, and lanterns helps prevent hostile mob spawning. Mobs generally spawn in low\nlight conditions, and these light sources provide enough illumination to raise the light level\nabove the threshold needed for mob spawning. Torches are the most commonly used due to\ntheir easy availability and simplicity in creating a well-lit perimeter around an area. Lava is\nalso effective but must be used cautiously as it can ignite nearby flammable materials.\nBool Answer Q&A\nQ: Can skeletons shoot through bamboo?\nA: F\nNormal Answer Q&A\nQ: What are piglins in Minecraft?\nA: Piglins are neutral mobs found in the Nether. They become hostile towards players unless\nthe player is wearing at least one piece of golden armor. They can be bartered with using\n26\nPreprint\ngold ingots for various items, and they wield either a golden sword or a crossbow for melee\nor ranged attacks, respectively.\nD.1.3\nINSTRUCTION DATASETS GENERATION PROMPTS\nWe design four types of prompts for four types of Q&A pairs to generate instruction datasets by\nusing GPT-3.5-Turbo. Here are our prompts.\nShort Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nYour task is to generate at least 30 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 30 question-and-answer examples, and you should generate ques-\ntions within the provided user text that can be directly answered with a word or phrase, such\nas dates, names, statistics, etc. This involves identifying specific, concise information within\nthe text that can be succinctly responded to, ensuring that the answers are clear and directly\nrelated to the questions asked. And you will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nLong Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\n27\nPreprint\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nYour task is to generate at least 20 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 20 question-and-answer examples. Identify questions within the\nprovided user text that require one or more complete sentences as answers. These questions\nshould be suitable for explanatory or definitional responses, where a detailed explanation or\na clear definition is needed to fully address the question. This involves crafting answers that\nare comprehensive and informative, ensuring they adequately explain or define the subject\nmatter in question. And you will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nBool Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\n28\nPreprint\nYour task is to generate at least 10 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 10 question-and-answer examples. Look for questions within the\nprovided user text that can be answered with a simple True or False. This task involves\npinpointing statements or queries within the text that lend themselves to binary responses,\nensuring that the answers are straightforward and unambiguous, clearly indicating whether\nthe statement is true or false based on the information available. And you will do so in this\nformat:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nNormal Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nYour task is to generate at least 20 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 20 question-and-answer examples. And you will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\n29\nPreprint\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nD.2\nMODEL FINE-TUNING\nIn this study, we use the instruction dataset with 390,317 instruction entries mentioned above to\nfine-tune the Minecraft Q&A expert models, using the LoRA fine-tuning method. We name the\nseries of fine-tuned models MineMA. The resulting models include MineMA-8B-v1, MineMA-8B-\nv2, MineMA-8B-v3, derived from the base model LLama-3-8B-Instrument, and MineMA-70B-v1,\nMineMA-70B-v2, derived from the base model LLama-3-70B-Instrument. MineMA-70B series of\nmodels are fine-tuned on four A6000 GPUs, while the remaining models are fine-tuned on a single\nA6000 GPU each. Among the models, MineMA-8B-v1 and MineMA-70B-v1 only undergo one\nround of training without an evaluation process, while the other models are trained with multiple\nrounds that incorporate an evaluation procedure. We use early stopping to halt the training process\nwhen there is no reduction in the evaluation loss over a series of evaluations, and finally save the\nmodel which has the best performance. Some training parameters are shown in Tab. 6.\nTable 6: Training parameters for different MineMA models.\nModel\nLoRA r\nLoRA alpha\nLoRA dropout\nLearning Rate\nWeight Decay\nSingle Round?\nMineMA-8B-v1\n64\n128\n0.1\n1E-04\n1E-04\nT\nMineMA-8B-v2\n32\n64\n0.1\n1E-04\n1E-04\nF\nMineMA-8B-v3\n64\n128\n0.1\n1E-04\n1E-04\nF\nMineMA-70B-v1\n16\n32\n0.1\n1E-04\n1E-04\nT\nMineMA-70B-v2\n64\n128\n0.1\n1E-04\n1E-04\nF\nD.3\nMODEL EVALUATION\nD.3.1\nEVALUATION DATASETS CREATING PROCESS\nIn this study, we utilize GPT-4 to create two evaluation MCQ datasets: a multi-theme MCQ dataset\nand a Wiki-based MCQ dataset. For the multi-theme MCQ dataset, we first summarize the following\nMinecraft content themes:\nGame Basics\nBlocks and Items: Basic blocks, special blocks, tools, weapons, armor, etc.\nSurvival Mechanics: Health, hunger, experience levels, death and respawn, etc.\nWorld Exploration\nBiomes: Characteristics of different biomes, generated structures, unique resources, etc.\nTerrain and Landforms: Features and resource distribution of different terrains.\nMobs and Interactions\nMobs: Characteristics and behaviors of passive, neutral, and hostile mobs.\nCombat System: Monster types, combat tactics, weapons and equipment, enchantments,\npotions, etc.\nTrading and Villagers: Villager professions, trading mechanics, village structures, etc.\n30\nPreprint\nSurvival Skills\nResource Gathering: Methods of obtaining various resources and their uses.\nCrafting and Production: Usage of crafting tables, furnaces, etc., equipment crafting and\nupgrading.\nFarming and Animal Husbandry: Crop planting, animal breeding, automated farms, etc.\nBuilding and Creativity\nBuilding Styles: Various building styles and key points.\nBuilding Techniques: Symmetry, proportion, detail handling in construction, etc.\nInterior Decoration: Interior design, lighting, item placement, etc.\nRedstone Mechanics: Redstone components, circuit design, automated devices, etc.\nSpecial Dimensions\nThe Nether: Peculiarities of the Nether, unique blocks and mobs, special structures, etc.\nThe End: Characteristics of the End, Ender Dragon, cities, ships, etc.\nAdventure and Exploration: Special generated structures like ocean monuments, woodland\nmansions, ruins, fortresses, etc.\nThen, we list different numbers of keywords for each theme based on the amount of relevant knowl-\nedge content. According to the amount of information related to each keyword, we match a number\nfor each keyword, representing the number of multiple-choice questions to be generated based on\nthat keyword. After preparing the groundwork, we use GPT-4 to generate the multi-theme MCQ\ndataset, totaling 1,050 multiple-choice questions. The relevant prompts are shown below, taking the\ngeneration of multiple-choice questions in the Special Dimensions theme as an example:\nSystem Message\nYou are an expert in generating Minecraft quiz questions. Your task is to create multiple-\nchoice questions about the game Minecraft based on the theme of \"Special Dimensions\"\nand the provided keywords. The introduction of the theme of \"Special Dimensions\" is as\nfollows:\nSpecial Dimensions:\nThe Nether: Peculiarities of the Nether, unique blocks and mobs, special structures, etc.\nThe End: Characteristics of the End, Ender Dragon, cities, ships, etc.\nAdventure and Exploration: Special generated structures like ocean monuments, woodland\nmansions, ruins, fortresses, etc.\nProvide four answer options labeled A, B, C, and D. Only one option should be correct.\nAfter the question and options, state the correct answer. Please format the output as follows:\nDifficulty: Easy\/Medium\/Hard\nTopic: Special Dimensions\nKey Word: text\nQuestion: Question text\nOptions: A.text B.text C.text D.text\nCorrect Answer: A\/B\/C\/D\nEnsure that the difficulty distribution of the questions and options is reasonable, and the\nanswers should be detailed and informative.\nUser Message\nPlease generate some Minecraft multiple-choice questions based on the following 5 key-\nwords, covering three difficulty levels: simple, moderate, and difficult. The number after the\nkeyword represents how many multiple-choice questions to generate based on this keyword.\nKeywords:\n31\nPreprint\n{keywords_go_here}\nEnsure that the Q&A content is rich and accurate, and test the player’s understanding of the\ngame. Provide a balanced combination of simple, medium, and difficult questions. Generate\neach question and answer in the given format. Here is an example:\nExample:\nDifficulty: Hard\nTopic: Special Dimensions\nKey Word: End Ship\nQuestion: What exclusive item can be found in the End Ship in Minecraft?\nOptions: A. Netherite B. Dragon Egg C. Elytra D. Beacon\nCorrect Answer: C\nFor the Wiki-based MCQ dataset, we utilize GPT-4’s knowledge of Minecraft-related Wiki content\nto create a set of multiple-choice questions that closely align with the information on the Wiki pages.\nFirstly, we list 615 Minecraft-related keywords based on the importance of the relevant knowledge.\nAfterwards, we generate a Wiki-based MCQ dataset using GPT-4 with designed prompts based on\nthese keywords, totaling 2,083 pieces of data. The prompts we used are as follows:\nSystem Message\nYou are an expert in generating Minecraft multiple-choice questions. Your task is to create\nmultiple choice questions about the game Minecraft based on the provided keywords and\nthe information on the corresponding page on the Minecraft Wiki. Ensure that the source\nof information for the multiple-choice questions you generate is the Minecraft Wiki, while\nensuring the objectivity and accuracy of the multiple-choice questions and ensuring good\nquality.\nProvide four answer options labeled A, B, C, and D. Only one option should be correct.\nAfter the question and options, state the correct answer. Please format the output as follows:\nDifficulty: Easy\/Medium\/Hard\nKey Word: text\nQuestion: Question text\nOptions: A.text B.text C.text D.text\nCorrect Answer: A\/B\/C\/D\nEnsure that the difficulty distribution of the questions and options is reasonable, and the\nanswers should be detailed and informative.\nUser Message\nPlease generate some Minecraft multiple-choice questions based on the following 5 key-\nwords, covering three difficulty levels: simple, moderate, and difficult. The number after the\nkeyword represents the minimum number of multiple-choice questions generated based on\nthe keyword. For important keyword, you should generate more questions.\nKeywords:\n{keywords_go_here}\nEnsure the source of information for the multiple-choice questions you generate is the\nMinecraft Wiki, while ensuring the objectivity and accuracy of the multiple-choice ques-\ntions and ensuring good quality. Provide a balanced combination of simple, medium, and\ndifficult questions. Generate each question and answer in the given format, do not use '#'or\n''.. Here is an example:\nExample:\nDifficulty: Medium\nKey Word: Dirt\nQuestion: What happens when you right-click on a dirt block with a hoe?\nOptions: A. It turns into farmland B. It turns into grass C. It turns into a path block D.\nNothing happens\nCorrect Answer: A\n32\nPreprint\nD.3.2\nEVALUATION RESULTS\nWe use the above two MCQ datasets to evaluate the MineMA series models and the corresponding\nbase models. Each model is tested 5 times with the two datasets. The results are shown in Tab. 7.\nTable 7: The evaluation results based on the MCQ datasets.\nModel\nAverage Accuracy (Multi-theme)\nAverage Accuracy (Wiki-based)\nLlama-3-8b-Instruct\n61.09%\n54.38%\nMineMA-8B-v1\n62.69%\n61.97%\nMineMA-8B-v2\n62.23%\n62.09%\nMineMA-8B-v3\n62.99%\n62.42%\nLlama-3-70b-Instruct\n77.41%\n72.52%\nMineMA-70B-v1\n78.11%\n73.03%\nMineMA-70B-v2\n75.68%\n72.88%\nE\nAGENT CAPABILITY BENCHMARK\nE.1\nLONG-TERM PLANNING TASK\nIn Minecraft, there are a total of 35 hostile creatures. We conducted experiments on both single-\nmonster combat tasks and combined combat tasks (up to three types of monsters), resulting in thou-\nsands of different tasks that can all be implemented through the interfaces we provided.\n• combatEnv(bot, h, r, y): Generate a hollow rectangular arena with a height of\nh and a square base with side length 2r at altitude y, positioning the agent at the exact\ncenter of this enclosed space. This configuration ensures controlled conditions for evalu-\nating combat scenarios, especially considering not being influenced by naturally spawning\nmonsters.\n• summonMob(bot, n = 1, r, type): Facilitate the spawning of hostile creatures\naround the bot. It randomly positions n monsters within a designated range (r to 2r along\nthe x and z axes) from the bot, allowing for the creation of varied combat tasks and enabling\ncomprehensive testing of bot performance under different tactical challenges.\nE.2\nDYNAMIC-IMMEDIATE PLANNING TASK\nIn Minecraft, many farming tasks require interaction with the environment and dynamic planning.\nWe propose a series of tasks that can be accomplished through our skill library, including hoeing\nfarmland, planting seeds, harvesting crops, making food, slaughtering animals, cooking meat, feed-\ning and breeding animals, among others. For example, in the task cook meat, if the agent is\ninformed that there is a chicken nearby, it should plan to \"kill one chicken\" rather than anything\nelse. Additionally, in the task milk cow, the agent must simultaneously monitor the appearance\nof cows in the vicinity and gather materials to craft a bucket to collect the milk.\nE.3\nAUTONOMOUS EXPLORATION TASK\nIn Minecraft, autonomous exploration is the gameplay approach that most closely mimics how hu-\nman players engage with the game. To evaluate the diversity of discoveries made by the agent\nduring autonomous exploration, we used \"Distinct Items Obtained\" as the primary evaluation met-\nric. The acquisition of a greater variety of items demonstrates more diverse exploratory behavior\nby the agent. Additionally, based on statistical information and progress in-game achievements, we\ncalculated supplementary evaluation metrics including the \"Distance Traveled\" by the agent (sum-\nming walking, sprinting, climbing, swimming, and other forms of movement), the total number of\n\"Items Crafted\" (the sum of all types of items obtained by crafting), and \"Recipes and Achievements\nUnlocked\" (the sum of crafting recipes and game achievements unlocked).\n33\nPreprint\nE.4\nSPECIFIC AGENT CAPABILITY REQUIREMENTS FOR DIFFERENT TASKS\nThis section provides an overview of the specific agent capabilities required for each task, laying the\nfoundation for a deeper understanding of how our benchmark evaluates different aspects of agent\nperformance. Different agent capabilities are detailed as follows:\n• Goal-based Planning: This capability refers to the agent’s ability to formulate and execute\ncomprehensive plans based on predefined goals. It involves understanding the given goals\nand devising a step-by-step plan to achieve them over extended periods. This is critical\nfor tasks such as the long-term planning task, where agents need to craft weapons and\nequipment to defeat specific monsters.\n• Feedback-based Planning: This capability involves the agent’s ability to adapt its plans\ndynamically based on environmental feedback. It is essential for tasks where environmen-\ntal feedback is crucial, such as in the dynamic-immediate planning task and the multi-round\nlong-term planning task, where agents must adjust their strategies in response to the out-\ncomes of previous actions or environmental changes.\n• Exploratory Planning: This capability evaluates the agent’s ability to set its own goals\nand make decisions independently in a complex environment. Agents must navigate, gather\ninformation, and decide on objectives without predefined goals. This is central to the au-\ntonomous exploration task, where agents explore the Minecraft world, discover resources,\nand adapt to unforeseen events.\n• Task Decomposition: This capability refers to the agent’s ability to break down complex\ntasks into specific, manageable sub-tasks. It is vital for the long-term planning task where\nagents need to craft a sequence of items, requiring the breakdown of the end goal into a\nseries of intermediate steps.\n• Resource Management: This capability involves the efficient allocation and utilization\nof available resources. Agents must maintain awareness of their inventory, manage assets\neffectively, and identify which resources need to be gathered. This is particularly important\nin farming tasks and autonomous exploration, where resource availability and management\nare crucial for subsequent behavior.\n• Skill Retrieval: This capability pertains to the agent’s ability to identify and choose the\nmost suitable skill from a set of options. Agents evaluate a list of relevant skills and select\nthe one that best fits the current environmental context and task requirements. All tasks\nrequire agents to retrieve and apply relevant skills based on situational demands.\n• Self-Reflection: This capability involves the agent’s ability to analyze and learn from the\noutcomes of its actions. Simply confirming the completion of a subgoal is often inadequate\nfor correcting planning errors. The agent evaluates its performance, deduces the cause of\ntask failures, and suggests more efficient strategies for future tasks. This is particularly\nimportant in multi-round tasks.\n• Self-Validation: This capability enables the agent to autonomously confirm the success\nof its actions against intended outcomes. By assessing inventory changes after actions, the\nagent ensures that each step contributes towards the overarching objectives without external\nverification. This capability is crucial for all tasks, as agents need to continuously ensure\ntheir actions align with the objectives.\nF\nEXPERIMENTS\nF.1\nEXPERIMENTAL DETAILS\nWe select the 1.19.4 version of Minecraft as the experimental environment. Within this virtual game\nworld, spatial measurements are determined by blocks, while temporal measurements are dictated\nby ticks, each lasting 0.05 seconds. A single day-night cycle in the game is 24,000 ticks, equivalent\nto 20 minutes in the real world, with 10 minutes of daytime, 7 minutes of nighttime, and a 3-minute\ndawn\/dusk transition (when both the sun and moon are visible in the sky). Additionally, the game’s\nweather system randomly transitions between clear, rainy, thunderstorm, and snowy conditions,\nadding dynamic changes to the environment. Players are born into a randomly generated massive\n34\nPreprint\nworld, covering an area of 30,000,000 blocks × 30,000,000 blocks, which can be approximately\nconsidered an infinite world without boundaries. Players start with no resources and must gather\neverything from scratch that is beneficial for survival and completing the ultimate goal. When a\nplayer character dies, it will respawn randomly within a 32-block radius of the death location on the\nground, and any collected items will not be dropped. Agents can connect to the game through local\nnetworks or multiplayer servers. We have tested on Ubuntu 20.04, Windows 10, and macOS. In all\nexperiments of the agent capability benchmark, the \"MineMA-8B\" refers to \"MineMA-8B-v3\", and\nthe \"MineMA-70B\" refers to \"MineMA-70B-v1\".\nWe use the following Minecraft mods in our experiment. It is important to note that the version of\nmods must be consistent with the game version, specifically 1.19.4.\n• Fabric API: Basic Fabric APIs.\n• Mod Menu: Used to manage all the mods that you download.\n• Complete Config: Dependency of server pause.\n• Multi Server Pause: Used to pause the server when waiting for LLM to reply.\n• Better Respawn: Used for random respawning of player characters.\nConsidering the randomness of resource distribution in the Minecraft world, we ensure that the agent\nstarts from different locations in the game world before each round of experiments. We implemented\nthe respawnAndClear interface to perform some initialization settings.\n• respawnAndClear(bot): Transport the agent to a new location and clear its in-\nventory, ensuring that the game mode is switched to survival and the game difficulty is\nswitched to peaceful.\nF.2\nAGENT CAPABILITY BENCHMARK\nIn our multi-round Long-term Planning Task, the agent is required to iteratively improve planning\nbased on combat outcomes, aiming for victory with the highest efficiency, take as little time as\npossible. Specifically, if the agent wins in the previous round, it should streamline its planning in\nthe next round, gathering materials and crafting equipment in less time to enhance time efficiency\n(reflected in the experimental results as a decrease in time and LLM iterations); conversely, if it\nloses, it must refine its planning to upgrade the quality of weapons and equipment in the planning\nlist to ensure ultimate success (reflected in the experimental results as an increase in health, or\ngo from losing to winning). Additionally, when calculating experimental results, we compute the\naverage and standard deviation for time, LLM iters (LLM iterations) and the health metric only for\nvictorious outcomes, since a defeat, indicated by health being zero, is not meaningful.\nExample of multi-round combat task\nCombat Task: 1 Skeleton\nPlan list of 1st round:[craft iron sword, craft iron helmet, craft iron chestplate, craft\niron leggings, craft iron boots]\nEquipment obtained of 1st round: [iron_helmet, iron_chestplate, iron_leggings, iron_boots,\ncrafting_table, None]\nTime spent on crafting equipment: 15,953 ticks; 8 LLM iters\nRemaining Health after the combat: 14.0 \/ 20 (victory)\n—streamlining—\nPlan list of 2nd round:[craft iron sword]\nEquipment obtained of 2nd round:[None, None, None, None, iron_sword, None]\nTime spent on crafting equipment: 3,614 ticks; 4 LLM iters\nRemaining Health after the combat: 9.2 \/ 20 (victory and more efficiently)\n—streamlining—\nPlan list of 3rd round:[craft wooden sword]\n35\nPreprint\nEquipment obtained of 3rd round:[None, None, None, None, wooden_sword, None]\nTime spent on crafting equipment: 416 ticks; 1 LLM iter\nRemaining Health after the combat: 9.0 \/ 20 (victory and even more efficiently)\nIn our Dynamic-immediate Planning Task, the agent is asked to plan step by step based on en-\nvironmental information. We calculate the success rate across various tasks, the average execution\ntime and LLM iters as well as their standard deviation (only if successful) as evaluation metrics. It is\nimportant to note that skills used in these tasks do not utilize the recursive decomposition mechanism\nwe propose but require the agent to plan the necessary preparatory steps by itself. The following\noutlines the specific skill execution pathways for the five tasks in our experiments:\nSkill execution path of the Dynamic-immediate Planning Task\nCollect Seeds: Collect Wheat Seeds \/ Collect Melon Seeds \/ Collect Pumpkin Seeds\nHoe Farmland: Craft Hoe →Hoe Farmland\nShear Sheep: Craft Shears→Shear Sheep Using Shears\nMilk Cow: Craft Bucket→Milk Cow Using Bucket\nCook Meat: Kill Pig→Cook Porkchop \/ Kill Chicken→Cook Chicken \/ Kill Sheep→Cook\nMutton \/ Kill Cow→Cook Beef\nIn our Autonomous Exploration Task, the agent also needs to plan step by step without a given\ngoal. Every time a new plan is proposed, the agent retrieves the ten most semantically similar skills\nfrom our skill library and selects one to execute. We tally the number of distinct item types obtained\nby the agent in each round, as well as the cumulative number of item types. Here are the distinct\nitems obtained by the agent from one round of the experiment:\nDistinct items obtained within 80 LLM iters\n['oak_log', 'stick', 'wooden_sword', 'crafting_table', 'wooden_pickaxe', 'stone_pickaxe',\n'oak_planks', 'wheat_seeds', 'dirt', 'cobblestone', 'raw_iron', 'granite', 'andesite', 'cob-\nbled_deepslate', 'diorite', 'diamond', 'iron_pickaxe', 'furnace', 'cobblestone_wall', 'coal',\n'iron_ingot', 'iron_trapdoor', 'dandelion', 'azure_bluet', 'poppy', 'oxeye_daisy', 'chest', 'cob-\nblestone_stairs', 'raw_copper', 'copper_ingot', 'calcite', 'copper_block', 'birch_planks', 'jun-\ngle_log', 'arrow', 'bone', 'rotten_flesh'], Num: 37\nThis result is comparable to the Voyager (Wang et al., 2023a) framework that employs GPT-4 for\nskill code generation and significantly outperforms Voyager using GPT-3.5.\nF.3\nABLATION STUDY\nWe conduct ablation studies on two core components of the ODYSSEY agent, including the LLM\nplanner and the open-world skill library.\nFor the LLM planner ablation, we remove the current environmental information in the planner\nsystem prompt as follows. Moreover, in each task proposed during each round, if the retrieved\nskills were not relevant to the current task (i.e., if the semantic retrieval score was below a certain\nthreshold), the execution of those skills was not carried out.\nPlanner System Prompt in Ablation\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft. My\nultimate goal is to discover as many diverse things as possible, accomplish as many diverse\ntasks as possible and become the best Minecraft player in the world. You can propose next\nsuitable tasks for me, such as \"Mine [block]\", \"Craft [item]\", \"Smelt [item]\", \"Kill [mob]\",\n\"Cook [food]\", \"Equip\" etc. It’s better to be a single phrase.\nYou should only respond in JSON format as described below:\n36\nPreprint\n{\n\"reasoning\": \"Do reasoning about what the next task should be.\",\n\"task\": \"The next task.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nFor the open-world skill library ablation, we removed the entire skill library and provided the LLM\nonly with the necessary interfaces required for composing new skills. Each round’s skill retrieval\nand execution were changed to code writing and execution, similar to the approach used in Voy-\nager (Wang et al., 2023a). The actor system prompt is shown as follows:\nActor System Prompt in Ablation\nYou are a helpful assistant that writes Mineflayer javascript code to complete any Minecraft\ntask specified by me.\n—External information—\nAt each round of conversation, I will give you\nCode from the last round: ...\nExecution error: ...\nChat log: ...\nBiome: ...\nNearby blocks: ...\nNearby entities (nearest to farthest):\nHealth: ...\nHunger: ...\nPosition: ...\nEquipment: ...\nInventory (xx\/36): ...\nChests: ...\nTask: ...\nContext: ...\nCritique: ...\n—Directions—\nYou should then respond to me with\nExplain (if applicable): Are there any steps missing in your plan? Why does the code not\ncomplete the task? What does the chat log and execution error imply?\nPlan: How to complete the task step by step. You should pay attention to Inventory since it\ntells what you have. The task completeness check is also based on your final inventory.\nCode:\n1) Write an async function taking the bot as the only argument.\n2) Reuse the above useful programs as much as possible.\n3) ...\n—Behaviour constraints—\nYou should only respond in the format as described below:\nExplain: ...\nPlan:\n1) ...\n2) ...\n3) ...\n37\nPreprint\n...\nCode:\n```javascript\n\/\/ helper functions (only if needed, try to avoid them)\n...\n\/\/ main function after the helper functions\nasync function yourMainFunctionName(bot) {\n\/\/ ...\n}\n```\nF.4\nRESULTS\nWe additionally provide Figure 8 and Figure 9 displaying the results of the single-round long-term\nplanning task and the dynamic-immediate planning task for easier visual inspection.\n38\nPreprint\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nx\n0\n5\n10\n15\n20\nHealth\nx\nx\nx\nx\nx\nx\n0\n5\n10\n15\n20\n25\nTime(min)\nx\n1 zombie\n1 spider\n1 skeleton\n1 zombified piglin\n1 enderman\n1 zombie, 1 spider\n1 zombie, 1 skeleton\n3 zombies\n1 zombie villager\n1 cave spider\n1 wither skeleton\ncook meat\nanimal husbandry\nTasks\n0\n25\n50\n75\n100\nLLM iters\nx\nVoyager with GPT-4-o-mini\nLLaMA-3-8B\nMineMA-8B\nFigure 8: Performance comparison of different models on the single-round long-term planning task.\n“Health” refers to the remaining health points. “# LLM iters” is the number of LLM iterations (call-\ning LLM) required to complete the task. “Time (min)” refers to the minutes spent in both gathering\nmaterials and crafting equipment to defeat different monsters. All evaluation metrics are calculated\nonly for successful tasks. ± corresponds to one standard deviation of the average evaluation over\nsuccessful tasks. Bold and italics mean the best and the second-best results. “×” indicates that\nhealth is not a relevant metric in the cook meat and animal husbandry scenarios, or all tasks fail.\n39\nPreprint\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nx\nx\nx\nx\nx\nx\n0\n10\n20\n30\nTime (min)\nx\nx\nx\nx\nx\nx\nCollect Seeds\nHoe Farmland\nShear Sheep\nMilk Cow\nCook Meat\nObtain Leather\nMake Sugar\nCollect Water\nTasks\n0\n20\n40\n60\n80\nLLM Iters\nx\nx\nx\nx\nx\nx\nBaichuan2-7B\nQwen2-7B\nMineMA-8B\nMineMA-70B\nFigure 9: Performance comparison of different models on the dynamic-immediate planning task.\nAll evaluation metrics are calculated only for successful tasks. “×” indicates that all tasks fail.\n40\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Odyssey：赋予Minecraft智能体开放世界技能\n\n## 📌 背景痛点\/本文动机\n近年来，许多研究致力于构建能够在开放世界环境中（如Minecraft）执行任务的通用智能体。尽管取得了令人鼓舞的成果，但现有工作主要集中在解决基本的编程任务，例如收集材料和制作工具，并将“获得钻石”任务视为最终目标。这种局限性源于智能体可用的动作集过于狭窄，需要它们从头开始学习有效的长期策略。因此，在开放世界中探索多样化的游戏玩法变得具有挑战性。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：开放世界技能库\nOdyssey框架开发了一个基于大型语言模型（LLM）的交互式智能体，该智能体配备了一个开放世界技能库，其中包含40个基本技能和183个组合技能。这些技能涵盖了从资源收集到工具制作，再到战斗和探索的各种任务，为智能体提供了丰富的工具来应对开放世界的挑战。\n\n💡 创新点2：微调LLaMA-3模型\n为了提高智能体在Minecraft中的性能，Odyssey框架使用来自Minecraft维基的大规模问答数据集对LLaMA-3模型进行了微调。通过生成包含390k+指令条目的训练数据集，并使用LoRA技术进行高效训练，Odyssey框架显著提升了LLM模型在Minecraft领域的知识储备和推理能力。\n\n💡 创新点3：智能体能力基准\nOdyssey框架引入了一个新的智能体能力基准，包括长期规划任务、动态即时规划任务和自主探索任务。这些任务涵盖了Minecraft中的各种复杂场景，并要求智能体展现出多样化的解决方案。通过这些基准任务，研究人员可以全面评估智能体的规划能力、资源管理能力、技能检索能力以及自主探索能力。\n\n## 📈 实验结果\n实验结果表明，Odyssey框架在基本编程任务和智能体能力基准任务上都取得了显著的性能提升。与现有方法相比，Odyssey框架的智能体在完成任务的速度、成功率和资源利用率方面都表现出色。此外，消融实验也证明了开放世界技能库和LLM规划器对智能体整体性能的关键作用。\n\n## 💬 可借鉴之处\nOdyssey框架为开发和研究开放世界智能体提供了一个全面的框架，具有以下可借鉴之处：\n\n* **开放世界技能库**：为智能体提供丰富的工具和策略，使其能够应对各种复杂的任务和挑战。\n* **微调LLM模型**：通过领域特定的数据集进行微调，提升LLM模型在特定领域的知识储备和推理能力。\n* **智能体能力基准**：为评估智能体的不同能力提供标准化的框架，促进开放世界智能体研究的进展。\n\n## 🌟 总结\nOdyssey框架为开放世界智能体的发展开辟了新的可能性，并为研究人员提供了一个强大的工具来探索和评估智能体的能力。随着未来研究的不断深入，Odyssey框架有望推动开放世界智能体技术的进一步发展，并为人工智能的通用性研究做出贡献。","llm_summary_res_status":200}
{"title":"See and Think: Embodied Agent in Virtual Environment","authors":"Zhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian Ye, Gaoang Wang","summary":"Large language models (LLMs) have achieved impressive pro-gress on several\nopen-world tasks. Recently, using LLMs to build embodied agents has been a\nhotspot. This paper proposes STEVE, a comprehensive and visionary embodied\nagent in the Minecraft virtual environment. STEVE comprises three key\ncomponents: vision perception, language instruction, and code action. Vision\nperception involves interpreting visual information in the environment, which\nis then integrated into the LLMs component with agent state and task\ninstruction. Language instruction is responsible for iterative reasoning and\ndecomposing complex tasks into manageable guidelines. Code action generates\nexecutable skill actions based on retrieval in skill database, enabling the\nagent to interact effectively within the Minecraft environment. We also collect\nSTEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge\nquestion-answering pairs, and 200+ skill-code pairs. We conduct continuous\nblock search, knowledge question and answering, and tech tree mastery to\nevaluate the performance. Extensive experiments show that STEVE achieves at\nmost 1.5x faster unlocking key tech trees and 2.5x quicker in block search\ntasks.","url":"http:\/\/arxiv.org\/abs\/2311.15209v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2311.15209v3","published":1700980696000,"comment":"ECCV 2024. First three authors contribute equally to this work.\n  Project Website https:\/\/rese1f.github.io\/STEVE\/","pdf_text":"See and Think: Embodied Agent in Virtual\nEnvironment\nZhonghan Zhao1,♠\n, Wenhao Chai2,♠\n, Xuan Wang1,♠\n, Boyi Li1\n,\nShengyu Hao1\n, Shidong Cao1\n, Tian Ye3\n, and Gaoang Wang1,†\n1 Zhejiang University\n2 University of Washington\n3 Hong Kong University of Science and Technology (GZ)\nAbstract. Large language models (LLMs) have achieved impressive pro-\ngress on several open-world tasks. Recently, using LLMs to build embod-\nied agents has been a hotspot. This paper proposes STEVE, a compre-\nhensive and visionary embodied agent in the Minecraft virtual environ-\nment. STEVE comprises three key components: vision perception, lan-\nguage instruction, and code action. Vision perception involves interpret-\ning visual information in the environment, which is then integrated into\nthe LLMs component with agent state and task instruction. Language\ninstruction is responsible for iterative reasoning and decomposing com-\nplex tasks into manageable guidelines. Code action generates executable\nskill actions based on retrieval in skill database, enabling the agent to\ninteract effectively within the Minecraft environment. We also collect\nSTEVE-21K dataset, which includes 600+ vision-environment pairs, 20K\nknowledge question-answering pairs, and 200+ skill-code pairs. We con-\nduct continuous block search, knowledge question and answering, and\ntech tree mastery to evaluate the performance. Extensive experiments\nshow that STEVE achieves at most 1.5× faster unlocking key tech trees\nand 2.5× quicker in block search tasks.\nKeywords: Open-world embodied agent · Multimodal pre-training ·\nLarge language model\n1\nIntroduction\nDesigning agents that demonstrate intelligent behavior and adaptability in open-\nworld settings has been a longstanding and significant challenge in the field of ar-\ntificial intelligence [13,14,26,46,70]. However, recent progress in the development\nof large language models (LLMs) [5, 55] has exhibited their potential as versa-\ntile, general-purpose assistants. Recent innovations in agent design [57,59,69,77]\n♠\nEqual\ncontribution:\nzhaozhonghan@zju.edu.cn,\nwchai@uw.edu,\nxu-\nanw@zju.edu.cn.\n† Corresponding author: gaoangwang@intl.zju.edu.cn.\narXiv:2311.15209v3  [cs.AI]  9 Jul 2024\n2\nZ. Zhao et al.\nVision Perception\nLanguage Instruction\nCode Action\nSTEVE-21K \nDataset\nVision-\nEnvironment\nQuestion-\nAnswering\nSkill-Code\ntrain\nfinetune\nretrieval\nTasks in Minecraft\nContinuous Block Search\nKnowledge QA\nTech Tree Mastery\nWooden\nStone\nIron\nDiamond\nHow to craft a \nstone pickaxe? \nOpen your \ncrafting table…\nobserve\nsearch\nFig. 1: STEVE integrates the three parts: Vision Perception, Language Instruction,\nand Code Action, supported closely by our proposed STEVE-21K. It demonstrates\ncommendable performance on Continuous Block Search, Knowledge QA, and Tech\nTree Mastery.\nhave effectively harnessed these advanced LLMs, tapping into their extensive\nworld knowledge and reasoning abilities. This development has paved the way\nfor agents, that are autonomously driven, to formulate and implement strate-\ngies and actions across a diverse array of skills and tasks in diverse open-world\nenvironments.\nIn many open-world settings, like Minecraft, contemporary agents predomi-\nnantly use LLMs for their textual interactions. However, this reliance on text for\ncommunication poses considerable limitations in their interactions within these\nworlds including low-level regrading cases [24,56,65,66]. Minecraft, with its ex-\npansive and interactive sandbox environment [17,20], demands a variety of skills\nfrom agents, ranging from crafting basic items to executing complex tasks. Yet,\nagents driven by LLMs often generate unpredictable outputs. The effectiveness\nof their interactions is largely contingent on meticulously crafted prompts [23],\ndesigned to align the LLM’s understanding with the environmental context and\nthe intended objectives. This process of prompt engineering is not only labori-\nous but also fails to meet the goal of fostering autonomous, self-directed agents.\nFurthermore, textual communication has its limitations in naturally conveying\ncertain concepts of the world, like crafting recipes, which are often more effec-\ntively communicated through vision.\nPlayers have the distinct capability to assimilate and convey information us-\ning both visual and textual channels, significantly enhancing our interactions\nwith the world around us. Yet, the integration of LLM-based agents with mul-\ntimodal inputs in open-ended environments remains an under-explored area.\nSTEVE in STEVE-Series [71–73], is named after the protagonist of the game\n“Minecraft,”. It is our proposed framework to build an embodied agent based\non the vision model and LLMs within an open world, as illustrated in Fig. 1.\nSTEVE harnesses a vision model to perceive its surroundings visually, coupled\nwith an LLM to strategize and plan actions. This model represents a leap for-\nward in agent design, combining these two input modes, vision, and text, to\noffer a more nuanced and comprehensive understanding of the environment and\npractical and executable skills.\nSTEVE\n3\nOur key contributions are outlined as follows:\n• We propose STEVE, an embodied agent in virtual environment, consists\nof vision perception, language instruction, and code action, achieving 1.5×\nfaster unlocking of key tech trees and is 2.3× quicker in block search tasks\ncompared to previous state-of-the-art methods.\n• We present STEVE-7B\/13B, a series of large language model obtained by\nfine-tuning with Minecraft knowledge question-answering pairs from Llama-\n2-7B\/13B.\n• We collect STEVE-21K dataset, including 600+ vision-environment pairs,\n20K knowledge question-answering pairs, and 200+ skill-code pairs, for jus-\ntifying the effective performance of STEVE.\n2\nRelated Works\n2.1\nIntelligent Agent in Minecraft\nAs an open-ended sandbox game, Minecraft has always been an ideal setting for\ntesting the performance of intelligent agents [21,25]. The agents are required to\nautonomously perform various tasks in Minecraft, such as chopping trees, craft-\ning tools, and mining diamonds. At the beginning, much of the works focus on\nexploring reinforcement learning [32,33,42,50] or imitation learning [2,3], with-\nout satisfactory performance. VPT [3] and MineDojo [17] collect internet-scale\ndatasets for their model pre-training. More specifically, VPT offers the exciting\npossibility of directly learning to act during video pre-training and using these\nlearned behavioral priors as extremely effective exploration priors for reinforce-\nment learning. Yet, recent works found that the pre-trained LLMs could serve\nas a strong “mind” that provides planning ability to the agents. Voyager [57]\nleverages GPT-4 [44] as both a high-level planner and a low-level action code\ngenerator. Plan4MC [69] proposes a skill graph pre-generated by the LLMs.\nDEPS [59], an interactive planning method based on LLMs, addresses multi-step\nreasoning issue in open-world planning. GITM [77] develops a set of structured\nactions and leverages LLMs to generate action plans for the agents to execute,\nachieving impressive results in various tasks.\n2.2\nEmbodied Multimodal Model\nEmbodied agent operates within various environment by synthesizing sensory\nperceptions and physical actions supported by computational intelligence. This\nsynthesis enables the agent to undertake a variety of tasks, achieving specific\nobjectives. Its key areas of application are diverse, including Navigation [6, 16,\n27, 43, 61, 68], Embodied Question Answering [10, 11, 67], Active Visual Track-\ning [37,38,74,75], and Visual Exploration [7,12,35]. The field is evolving rapidly\nwith the development of Large Language Models (LLMs) [51] and Multimodal\nLLMs (MLLMs) [1, 9, 18, 19, 28–30, 34, 39, 41, 54, 58, 64, 76], integrating multiple\nmodalities for more effective processing. A prime example of this innovation is\n4\nZ. Zhao et al.\nVision \nEncoder\n…\nVision Perception\n…\n…\nSTEVE-13B\nLanguage Instruction\n1.\nCrafting Table: \nOpen your \ncrafting table\n2.\nIngredients … \n…\nImage\nor\nVideo\n1.\nGather Wood \nLogs\n2.\nConvert Wood \nLogs into Planks\n…\nDecompose\nHigh-level\nLow-level\n…\nQuery\nEncoding\nRetrieval\nMine Wood Log\nCraft Iron Sword\nCombat Creeper\nCook Mutton\nCraft Shears\nMake Crafting Table        …\nCraft Bucket\nCatch Pufferfish\nNo crafting table nearby\nTokenizer\nCraft a bow\nAgent \nState\nTask\nSkill Database\nconst woodLogBlock = \nawait exploreUntil(bot, \nnew Vec3(1, 0, 1), 60, () => \n{\nreturn bot.findBlock({\nmatching: block => \n...\nCode Action\nFig. 2: STEVE framework. The Vision Perception part takes images or videos, en-\ncodes them into tokens, and combines them with Agent State and Task tokens as input.\nThe STEVE-13B in the Language Instruction part is used for automatic reasoning and\ntask decomposition, and it calls the Skill Database in the form of the Query to output\ncode as action.\nPaLM-E [15], a sophisticated multimodal model with 562B parameters, adept at\na broad spectrum of embodied tasks and demonstrating exceptional capabilities\nin visual reasoning.\n2.3\nLarge Language Model with Equipped Tools\nWhile Large Language Models (LLMs) demonstrate impressive skill in tackling\nnovel tasks via prompt-based instructions, they often face challenges in areas\nwhere simpler models or tools excel, like mathematical calculations or iden-\ntifying palindromes. However, LLMs’ potential is significantly expanded when\nintegrated with other modality-specific models, such as those for vision or audio,\nenabling multi-modal capabilities [4,36]. Innovations like Toolformer [47] demon-\nstrate LLMs’ self-learning to utilize tools through finetuning with extensive API\ncall samples. Visual ChatGPT [62] extends this by integrating various visual\nfoundation models, facilitating interactive user experiences with ChatGPT. Simi-\nlarly, HuggingGPT [48] presents a framework that harnesses LLMs to link diverse\nmodels from Hugging Face for task resolution. AutoGPT [49] is an open-source\napplication that broadens GPT-4’s capabilities with internet access, memory\nmanagement, and plug-ins. The recent introduction of MovieChat [52,53] brings\na memory mechanism to MLLM, enhancing its performance in video under-\nstanding tasks. Furthermore, LLMs can be used for goal planning, analogous to\nlanguage translation [63]. This evolving landscape suggests that tool-equipped\nLLMs could forge a new paradigm in AI solution design.\n3\nMethod: STEVE\n3.1\nOverview\nAs shown in Figure 2, STEVE F is an LLM-based multi-modal autonomous\nsystem for embodied agents in Minecraft, which can use visual status Xv in the\nSTEVE\n5\nform of images or videos and agent state Xs to manage and execute complex\ntask Xt for executive code action ac:\n  \\ mathb f { a}^ c  = \\mathcal {F} (X^v, X^s, X^t) = \\mathcal {A}^c(\\mathcal {I}^l(\\mathcal {P}^v(X^v, X^s, X^t))) \n(1)\nwhere STEVE F integrates Visual Perception Pv into the Language Instruction\nIl with large language model (LLM) and combines them with Code Action Ac,\na skill retrieval method to execute actions, details are demonstrated as follows.\n• Vision Perception Pv (Section 3.2); a vision encoder to interpret vi-\nsual information of the environment, such as blocks and entities and a text\ntokenizer for agent state s and task t.\n• Language Instruction Il (Section 3.3): a powerful language model sys-\ntem fine-tuned specifically for Minecraft content using LLaMA2-13B [18].\nThis model enables adaptive interaction for iterative reasoning and step-by-\nstep decomposition.\n• Code Action Ac (Section 3.4): a skill retrieval method based on our\nskill-code database.\n3.2\nVision Perception Pv\nThe vision perception part includes a vision encoder Ev and a text tokenizer\nT, which converts the visual status Xv, agent state Xs, and task Xt to the\ntext-space tokenizer representation Y = {Y v, Y s, Y t}:\n  \\ mathb\nf  {Y\n} _ i \n= \\ ma t hcal  {P}^v(X^v_i, X^s_i, X^t_i), \\mathcal {P}^v = \\{E^v, T\\}, \n(2)\nwhere the vision encoder E, the visual branch of EfficientFormer [31], encodes\nvisual status Xv\ni at each step i into visual tokens Y v = {yv\n1, yv\n2, ..., yv\nn} ∈Rn×d,\nwhere n denotes the number of visual tokens yv and d is the dimensionality of\neach token. The text tokenizer converts agent state Xs\ni and task Xt\ni at each step\ni in the form of text into textual tokens Y s and Y t. Note that visual tokens are\namalgamated with textual tokens representing the agent’s current state (e.g.,\nhealth, inventory, etc.) and the task description. This is accomplished using a\ntokenizer that maps state variables and task parameters to a tokenized form.\nThe resultant unified token set serves as a comprehensive representation of the\ncurrent situational context.\n3.3\nLanguage Instruction Il\nThe Language Instruction Il = {Pl, Cr, Cu, Ds}, which consists of Planner Pl,\nCritic Cr, Curriculum Cu, and Describer Ds, i.e., four independent LLM-based\nagents with different functions. They formulate high-level task guidelines, re-\nfine strategies through feedback, facilitate continuous learning and adaptation\nthrough a curriculum of complex tasks, and finally decompose strategic guide-\nlines into the executable low-level textual action step as, {as ∼Il(·|Yi)}N\ni=0\ntowards efficient completion of the task.\n6\nZ. Zhao et al.\nNote that the Planner Pl, Critic Cr, Curriculum Cu and Describer Ds are\nbased on STEVE-7B\/13B, a powerful language model derived from LLaMA-2-\n13B [18], fine-tuned specifically on Minecraft-related content from the STEVE-\n21K. This model’s expertise covers a broad spectrum of game-specific knowledge\nareas on worlds, entities, player mechanics, survival, and even game practical\nexperience. Finally, they have the performance of their different functions:\n• Planner Pl: formulating comprehensive guidelines and executive plans that\nalign with the overarching objectives of the task.\n• Critic Cr: evaluating the planner decisions, providing feedback that can\nrefine strategies.\n• Curriculum Cu: facilitating continuous learning and adaptation for action\nagents by engaging with a series of progressively complex tasks.\n• Describer Ds: distilling the extensive data into a concise summary, making\nit more manageable and interpretable.\nIterative reasoning. The STEVE-13B receives a stream of tokens that encode\nthe current visual scene, the agent’s state, and the task’s textual description.\nSTEVE-13B interprets this rich context to undertake complex reasoning. The\nmodel initiates the reasoning process by constructing a series of high-level strate-\ngies that outline the pathway to task completion. Considering all gameplay el-\nements, the reasoning mechanism is akin to an experienced player who can vi-\nsualize the end game and chart a course to victory. This approach ensures the\nplans are reactive and proactive, allowing the agent to anticipate and mitigate\nfuture challenges. However, most strategies are high-level and abstract therefore,\nthey often require step-by-step decomposition to derive executable guidelines.\nDecomposition. The decomposition process makes the complex strategies break\ndown into simple, low-level guidelines that can be directly mapped to actions\nin Minecraft. It is similar to how a high-level command like “build a shelter”\nis divided into actionable instructions like “collect wood”, “craft planks”, and\n“place blocks”. The granular steps are structured to provide explicit instructions\nthat the game engine can readily interpret. This systematic breakdown from\nhigh-level reasoning to low-level actionable steps is the hallmark of the STEVE\nsystem, enabling the embodied agent to interact with the Minecraft environment\nin a meaningful and goal-oriented manner. Through this intricate process of rea-\nsoning and decomposition, STEVE embodies the cognitive capabilities required\nfor sophisticated task management in virtual environments.\nCurriculum learning with memory. We draw inspiration from the lifelong learn-\ning strategy utilized in many reinforcement learning problems [57] in both closed-\nworld and open-world settings. We start by creating a set of tasks that serve as a\ncurriculum for agents to explore the environment. During this process, STEVE\ngenerates plans, interacts with the surroundings, learns from mistakes, and stores\nall these experiences in memory. Next, we evaluate STEVE on various tasks af-\nter this learning stage. Consequently, STEVE can produce better plans with its\nSTEVE\n7\nmemory teaming up with the planning experiences. We use this as the default\nsetting for all tasks in our experiments.\nContinous learning with summarization.\nWe’ve observed that the learning\nprocess, where the memory is being filled, can continue throughout the gameplay.\nThe agent can gradually acquire more skills as the gameplay progresses and more\nexperiences are gained. However, as the memory gets larger, it becomes difficult\nto understand the game’s situation and interact with the game slowly. To tackle\nthese challenges, we have implemented the Chain of Summarization method [40].\nBy finding better references, we can improve our ability to handle challenging\ntasks, such as “Diamond Tool” in the tech tree, including obtaining materials\nand creating diamond tools. This will lead to a higher success rate, as shown in\nSection 5.3. Additionally, curriculum learning with memory allows for in-context\nlifelong learning without needing gradient updates.\n3.4\nCode Action Ac\nThe code action part Ac is the execution phase, where the STEVE system con-\nverts planned, decomposed guidelines into concrete actions within the Minecraft\nenvironment. This process leverages a specialized skill database that pairs code\nsnippets with their descriptions and relevant metadata, encoded as vectors vs for\nefficient retrieval. The transition from language instruction to executable code\nis achieved through the retrieval process R:\n  \\m at h cal {R} ( \\ mathbf {q}, \\mathbf {v}) = \\sigma (\\mathbf {q}, \\mathbf {v}), \\mathbf {q} = E^q(\\mathbf {a^s}), \n(3)\nwhere Eq and σ are query encoding and cosine similarity matching. Each low-\nlevel textual action step as derived from the Language Instruction is encoded\ninto a query q. This encoding captures the essence of the action to be performed\nin a format that the skill database can understand. Once the queries q are\nencoded, the system computes similarities between the query vectors and the\ncode snippets vectors v stored in the database to determine which skill best\nmatches the required action.\n3.5\nTraining\nTo reduce the training overhead to a certain extent, we adopt a two-stage training\nmethod, the first stage being a warm-up on the question-answering pairs of\nSTEVE-21K to ensure a certain degree of instruction capability. The second\nstage is simulated in the environment, and the Expert LLM Ep integrated with\nGPT-4 [44] generates instructions for the same situation to modify our model.\nIn training, we use either a single-round conversation {XQ, XA} or a multi-\nround conversation {XQ\ni , XA\ni }i≤N, where N is the total number of rounds in the\nconversation. We use the negative log-likelihood objective over the prediction\n8\nZ. Zhao et al.\ntokens with training and finetuning:\n  \\m a t\nh\nc\nal \n{L} (\\theta ) =-\\sum _{j=1}^{L} \\log \\mathcal {F}_{\\theta }(Y_j|X^v, \\hat {Y}_{1:j-1}), \n(4)\nwhere Y and ˆY refer to the non-vision input and target token sequences, θ repre-\nsents the model parameters, and L represents the length of the target sequence.\nThe vision input Xv varies depending on the input step. We only consider the\nanswer tokens XA when computing the loss to ensure that the model focuses on\ngenerating coherent responses.\nOffline warm-up. In the first stage, we finetune the STEVE-7B\/13B only on\nthe single-round question-answering pairs of STEVE-21K to ensure a certain\ndegree of instruction capability, as shown in Tab. 3.\nOnline finetuning. In the second stage, we simulate the warmed-up STEVE-\n7B\/13B in Minecraft, and both train the vision encoder Ev and finetune the\nSTEVE-7B\/13B simultaneously.\nIt is necessary to obtain the environment information to train the vision en-\ncoder, denoted as l = RT (Xv), where l is the label of seen blocks and entities,\nRT is the Ray Tracing method [60] to eliminate all content outside the agent’s\nperspective screen. We train the vision encoder Ev once we have obtained the\nenvironment information. To finetune the warmed-up STEVE-7B\/13B in simu-\nlation, we give both the STEVE-7B\/13B and the Expert LLM the same input,\nand we consider the output of the Expert LLM as the ground truth label.\nAfter 5,000 simulations, we collect all the context information from success-\nful runs, including sequences of vision and question-answering chat flow as the\nVision-Environment part of STEVE-21K, as mentioned in Section 4.\n4\nDataset: STEVE-21K\nAs shown in Fig. 3, STEVE-21K has been meticulously compiled, featuring a\ndiverse collection of Vision-Environment pairs, Question-Answering pairs, and a\nSkill-Code database.\nVision-Environment pairs contain 600 pairs of first-person perspective videos\nfrom Minecraft gameplay across six different terrains (including forest, desert,\ncoastal, etc.), along with corresponding environmental block entities in the field\nof vision and context information for each timestamp. Additionally, all pairs\nare oriented around executing the skill-related task supported by Skill-Code\npart mentioned in Section 4. We employ the STEVE-7B\/13B model to enable\nrobots to plan and execute actions autonomously based on tasks defined by\nhuman supervisors. We record the video of the agent operation, the environment\ninformation, and all the corresponding chatflow.\nQuestion-Answering pairs contain 20K question-answering pairs from the\nMinecraft-Wiki and Reddit corpus across six data types partly sourced from [17].\nThe pairs are organized into instruction, input, and output triplets and used to\nSTEVE\n9\nVision-Environment\n20K Single-round QA pairs in\nQuestion-Answering\nSkill-Code\nHuman Player\nModels\nMinecraft World\nPrismarine Viewer\n…\nVision Record\nRay Tracing\nWikipedia\nReddit\nChatGPT\nEnv. Record\n1. World & Entities \n2. Player Mechanics & Survival \n3. Knowledge & Discovery \n4. Resources & Crafting \n5. Tools & Utilities\n6. Miscellaneous\nSingle-round QA pairs\nHuman Player\nChatGPT\nprompt\ncode snippets\noutput\nexecute\ncheck and revise\nBlocks: stone, dead \nbushes, cacti, orange \nterracotta...\nClean\nConstruct\nCheck\nFig. 3: STEVE-21K collection pipeline. In the Vision-Environment section,\nSTEVE-13B plays the game according to specified tasks defined by the human player,\ncollecting visual information through prismarine-viewer and capturing environmental\ninformation from the screen using Ray Tracing [60]. Note that the language instruction\ntask is also performed during the collection phase. We simultaneously record and save\nthe chat flow from the reasoning and decomposition stages. In the Question-Answering\nsection, we obtain information from the Minecraft-Wiki and Reddit forums and use\nGPT-3.5 to clean the data into single-round QA pairs. In the Skill-Code section, we\nuse GPT-3.5 combined with the human player’s code to synthesize code snippets and\nthen check and revise them in the game environment.\ntrain STEVE-13B. The GPT-3.5 [5] is employed to derive meaningful single-\nround question-answer pairs, and LoRA [22] is incorporated during the fine-\ntuning process for efficient resource allocation.\nSkill-Code pairs contain 210 skill execution scripts with descriptions, covering\n8 skill types including collecting, crafting, exploration etc. The code part is\ncollected by manual coding. We use GPT-3.5 [5] to describe all codes and utilize\nlangchain vectordb to give all pairs a database vector.\n5\nExperiments\n5.1\nExperimental Setup\nWe train STEVE-7B\/13B, which is finetuned from LLaMA-2 [18] with the\nQueston-Answering pair in STEVE-21K dataset for warm-up and simulation\ncontext data from successful runs. We use LoRA [22] for finetuning process.\nNote that the process is to adjust STEVE-13B to work on correct simulation\nknowledge while remaining adapted to visual perception. In the text part, we set\nall temperatures to 0 except for the task proposal, which uses 0.9 to encourage\ntask diversity. The vision unit is based on EfficientFormerV2-S0 [31], which is\ntrained on the Vision-Environment part of our STEVE-21K dataset. Our simula-\ntion environment is built on top of MineDojo [17] and leverages Mineflayer [45].\n10\nZ. Zhao et al.\nContext\n[Question] How to \nsurvive, according to \nthe current situation?\n[Answer] Dealing with \nthe Creeper and \nHealth Management...\nChat Log\nA Creeper is \nNearby, please \ncombat Creeper\nwith the iron \nsword first.\nPlan\nUse `combatOneCreeper` \nfunction to combat the \nCreeper until the Creeper is \neliminated. Once the \nCreeper is eliminated, call \nthe `eatCarrot` ...\nEnvironment \n[Blocks] stone, \ndead bushes, \ncacti, orange \nterracotta, ...\n[Equipment] \niron sword\n[Entities]\ncreeper, Rabbit\n[Health] 15\/20\n[Hunger] 18\/20\n[Inventory]\nCarrots, torches\nFig. 4: Example of Vision-Environment pairs. It represents the data format of\nthe Vision-Environment pairs in our STEVE-21K dataset: including visual signals,\nenvironmental information, Chat Log, Context QA pairs, and planning in actual tasks.\nWe use GPT-4-0613 for all GPT-4 models used in Voyager [57] and code gener-\nation tasks.\n5.2\nBaselines\nAs no vision-based LLM-driven agents are immediately operable in Minecraft, we\nselected several algorithms as baselines that extract information from a system’s\nbackend, differing significantly from real-world applications.\nAutoGPT [49] is an NLP automation tool that decomposes a high-level goal\ninto executable subgoals in MineDojo, aligning with our experimental frame-\nwork. Our setup, AutoGPT, powered by GPT-4 [44], processes agent states,\nenvironment feedback, and execution errors to manage subgoal execution.\nVoyager [57] relies solely on textual grounding for perception and features a long-\nterm procedural memory with a hierarchical library of code-based procedures,\nallowing the integration of simple skills into complex behaviors. Proficient in\nenvironment exploration and tech tree mastery, Voyager uses GPT-4 [44] for\nprocessing background text in embodied agents, with a lesser emphasis on visual\nperception.\nSTEVE\n11\nTable 2: Comparison on tech tree mastery task. The values presented are in frac-\ntions, representing successful trials out of three attempts. A score of 0\/3 signifies the\nmethod’s inability to progress within the tech tree after a maximum of 160 prompt-\ning iterations. The reported numbers denote the average iterations across three trials.\nLower iteration values indicate higher efficiency of the respective method.\nMethod\nWooden Tool Stone Tool\nIron Tool\nDiamond Tool\nAutoGPT [49] 92 ± 72 (3\/3) 94 ± 72 (3\/3) 135 ± 103 (3\/3)\nN\/A (0\/3)\nVoyager [57]\n6 ± 2 (3\/3)\n11 ± 2 (3\/3)\n21 ± 7 (3\/3)\n102 (1\/3)\nSTEVE\n4 ± 1 (3⁄3)\n8 ± 1 (3⁄3)\n15 ± 2 (3⁄3)\n106 ± 12 (3⁄3)\nTable 3: Quantitive comparison on knowledge question and answering task.\nQuestions, model-generated responses, and ground truth inputs are evaluated in GPT-\n4 [5], Claude-2 [8] and human blind rating rated on a scale of 0 to 10; The scores above\nare the average of them. Higher scores indicate greater alignment of the generated\nanswers with the ground truth. Wld., Ent., Mech., Surv., Know., Disc., Res., Craft.,\nTools, Util., Miscell. stand for World, Entities, Player Mechanics, Survival, Knowledge,\nDiscovery, Resources, Crafting, Tools, Utilities and Miscellaneous.\nMethod\nWld. & Ent. Mech. & Surv. Know. & Disc. Res. & Craft. Tl. & Util. Miscell. Overall\nLlama2-7B\n6.44\n6.68\n6.58\n6.42\n6.80\n6.96\n6.56\nLlama2-13B\n6.93\n6.95\n6.77\n6.77\n6.98\n6.64\n6.89\nSTEVE-7B\n7.99\n7.88\n7.84\n7.95\n7.93\n7.82\n7.94\nSTEVE-13B\n8.14\n8.13\n8.03\n8.15\n8.12\n7.72\n8.12\nGPT-4\n8.06\n8.07\n8.07\n7.92\n8.09\n8.21\n8.04\n5.3\nEvaluation Results\nTable\n1:\nComparison\non\ncontinues\nblock search task. # Iters stand for av-\nerage iterations to find 10 diamonds (max\n100). # Blocks stand for average diamonds\nfound in 100 iterations.\nMethod\n# Iters (↓) # Blocks (↑)\nAutoGPT [49]\nN\/A\n7\nVoyager [57]\n35\n26\nSTEVE\n14\n67\nContinuous block search.\nAs shown\nin Tab. 1, we experiment with block-\nsearching tasks to assess the agent’s ex-\nploratory capabilities and proficiency\nin locating specified blocks. Diamond\nblocks are placed at every 16-block in-\nterval across the land map. The agent’s\nobjective is to identify as many blocks\nas possible within the fewest iterations,\nwhich indicates the method’s efficiency.\nAs shown in Fig. 5, enriching information through visual perception significantly\nenhances the efficiency of search and exploration tasks, leading to more effective\nworld exploration.\n12\nZ. Zhao et al.\nSTEVE\nVoyager\nAutoGPT\nFig. 5: Schematic of the continuous block search task. We capture an asyn-\nchronous segment with each method 30 iterations from the experiments for illustration.\nThe reason we choose diamond blocks is that they are not naturally occurring in the\ngiven context, making them easily distinguishable from other blocks.\nKnowledge question and answering. Using a validation dataset, we established a\nquestion-answering database to evaluate our model’s performance on Minecraft-\nrelated queries. Responses from each model are rated blindly by GPT-4, Claude-\n2, and human participants based on accuracy, relevance, and detail. Responses\nare initially checked for accuracy, and the comprehensive evaluation yields a\nscore ranging from 0 to 10, with higher scores indicating superior performance.\nAs detailed in Tab. 3, we tested the instructional capabilities of various LLM\nmodels using the STEVE-21k dataset, divided into 18, 622 training and 1, 000\ntesting samples. The STEVE-7B and STEVE-13B models surpassed LLaMA2\nin all metrics, with STEVE-13B scoring the highest at 8.12, demonstrating\nits exceptional understanding of Minecraft-related queries. This indicates that\nSTEVE models, optimized specifically for Minecraft content, perform better in\nknowledge-intensive tasks.\nOur results confirm that larger, domain-specifically tuned models like STEVE-\n13B outperform broader models like GPT-4, underscoring the benefit of special-\nized fine-tuning for domain-specific applications.\nSTEVE\n13\nHow to restore full \nhunger efficiently? \nYou should eat 1 bread and 1 \napple. The bread will restore \n5 hunger points, and the apple \nwill restore 4 hunger points \nadding up to the 6 hunger\npoints you need to have a full \nhunger bar...\nSimply eat one bread from \nyour inventory. It will \nincrease your hunger level by \n5 points, which is precisely \nwhat you need to reach 20...\nYou should eat an Apple or 2 \nBreads. Both Apples and \nBreads restore 5 hunger \npointswhich will bring your \nhunger level back to full.\nShould I combat zombie \nand how to survive?\nYou should combat the \nzombie using your iron \nsword. Zombies are relatively \nweak and should not pose a \nsignificant threat to you...\nYou should combat the \nzombie; remember to strike \nand step back to avoid getting \nhit. You seem to be in an \nabandoned mineshaft. Once \nthe area is safe, continue to \nexplore...\nyou should combat the \nzombie and collect resources \nto craft better equipment. The \ncurrent environment is likely \na village or a farm...\nWhat do you find and how \nto get more resources?\nYou should open the nearby \nChest, or explore the \nsurrounding area to find \nbetter resources...\nBegin by inspecting the \nnearby Chest, it could contain \nvaluable resources. Use your \nIron Pickaxe to mine any \nprecious ores you encounter \nin the mineshaft...\nYou should find more \nCobblestones to craft better \narmor and weapons. The \nnearby blocks and entities \nsuggest that you are in a \nMinecraft world, and the \nresources you need are likely \nto be found in the nearby \narea...\nShould I combat skeleton \nand how to survive?\nGiven your current situation, \nit is not advisable to engage \nin combat with skeletons or \nany other hostile mobs...\nYou should avoid engaging \nthe skeleton at the moment\nsince your health is a bit low\nand you have no armor to \nprotect yourself. You should \ninstead focus on your \nimmediate safety...\nYou should combat the \nskeleton. To survive, it is \nrecommended to prioritize\ncrafting better equipment, \nsuch as armor and weapons, \nto increase your health and \ncombat effectiveness...\nFig. 6: Qualitative comparison on knowledge question and answering tasks.\nGreen ( Red ) highlights the correct or good (wrong or bad) answer. Blue indicates\nthe suboptimal answer. Grey indicates the meaningless answer.\nTech tree mastery. As shown in Tab. 2, we experiment on the Minecraft tech\ntree mastery to test the agent’s ability to craft and use a hierarchy of tools. Pro-\ngressing through this tree (wooden tool →stone tool →iron tool →diamond\ntool) requires the agent to master systematic and compositional skills. As to\nthe wooden, stone, and iron levels of the tech tree, STEVE achieves remarkable\nefficiency: 23×, 11.8×, and 9× faster than AutoGPT [49], and 1.5×, 1.4×, and\n1.3× faster than Voyager [57]. STEVE has achieved the diamond level, as shown\nin Tab. 2. Its performance slightly lags behind Voyager [57], which also uses\nGPT4 [44] for critical inference. However, STEVE is more cost-effective, start-\ning with lower initial performance. It includes a vision unit, prioritizing visual\ndata over background information, offering distinct advantages. Additionally, we\nobserved a decrease in performance when using a basic skill database.\n5.4\nAblation Study\nTo understand the impact of different components on the performance of our\nsystem, we conducted ablation studies focusing on the tech tree mastery task in\nMinecraft. The results, as shown in Tab. 4, provide insights into the effectiveness\nof the vision unit and compare our STEVE model with the STEVE GPT-4\nversion (with the same vision unit as ours). Note that the w\/o vision unit setup\nis that the environmental perception encompasses data on blocks within an 8x8\narea surrounding the agent, including the front, back, left, and right directions.\nThe following observations are made:\n14\nZ. Zhao et al.\nTable 4: Ablation studies for the tech tree mastery. STEVE (Ours) is the\nSTEVE-13B version. The 0\/3 score means the method can’t progress beyond 160 iter-\nations in the tech tree.\nMethod\nWooden Tool Stone Tool\nIron Tool\nDiamond Tool\nw\/o vision unit\n11 ± 5 (3\/3)\n27 ± 5 (3\/3) 46 ± 11 (3\/3)\n158 (1\/3)\nSTEVE (GPT-4)\n6 ± 2 (3\/3)\n10 ± 1 (3\/3) 14 ± 3 (3⁄3)\n89 ± 9 (3⁄3)\nSTEVE (Ours)\n4 ± 1 (3⁄3)\n8 ± 1 (3⁄3) 15 ± 2 (3\/3) 106 ± 12 (3\/3)\nVision unit is critical. The omission of the vision unit markedly affects the\nsystem’s performance, especially in more advanced tasks. While it successfully\ncrafts Wooden, Stone, and Iron Tools, it is challenged with Diamond Tools. This\noutcome underscores the vital importance of visual information in accomplishing\ncomplex tasks.\nComparison with GPT-4. As our vision encoder directly encodes into text space,\nit can be easily replaced with any language model. For instance, the GPT-4 we\ncompared exhibits consistent success across all categories and secures a flawless\nsuccess rate. Interestingly, the STEVE-13B version excels in simpler tasks such\nas crafting wooden and stone tools. Moreover, it requires fewer iterations than\nmethods without the vision part, underscoring its superior efficiency.\n5.5\nCase Study\nAs shown in Fig. 6, we perform an extensive case study comparison of GPT-4,\nLLaMA2-13B, and our method STEVE-13B. Each model maintains the same\ninformation and question inputs to compare feedback under different environ-\nmental information. Our STEVE overall achieves the best results, surpassing\nGPT-4 and showing significant improvement compared to the original LLaMA.\nEspecially in parts involving numerical calculations, such as the leftmost image,\nSTEVE accurately tracks food values to restore hunger levels.\n6\nConclusion\nSTEVE enhances multi-modal learning by combining visual encoder and LLM-\nbased agents. It has three functions: vision perception, language instruction,\nand code action, allowing it to understand, predict, and act in virtual envi-\nronments. We provide a straightforward approach to creating a robust, multi-\nmodal, autonomous, embodied agent using an open-source language model with\na small number of parameters. Additionally, we provide a comprehensive dataset\nSTEVE-21K for sustainable community development that can be verified.\nSTEVE\n15\nAcknowledgement\nThis work is supported by the Zhejiang Provincial Natural Science Foundation\nof China (No. LZ24F030005) and the National Natural Science Foundation of\nChina (No. 62106219).","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/See and Think: Embodied Agent in Virtual Environment.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nSee and Think: Embodied Agent in Virtual Environment\n```\n#### 2. 论文摘要\n```\nLarge language models (LLMs) have achieved impressive pro-gress on several\nopen-world tasks. Recently, using LLMs to build embodied agents has been a\nhotspot. This paper proposes STEVE, a comprehensive and visionary embodied\nagent in the Minecraft virtual environment. STEVE comprises three key\ncomponents: vision perception, language instruction, and code action. Vision\nperception involves interpreting visual information in the environment, which\nis then integrated into the LLMs component with agent state and task\ninstruction. Language instruction is responsible for iterative reasoning and\ndecomposing complex tasks into manageable guidelines. Code action generates\nexecutable skill actions based on retrieval in skill database, enabling the\nagent to interact effectively within the Minecraft environment. We also collect\nSTEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge\nquestion-answering pairs, and 200+ skill-code pairs. We conduct continuous\nblock search, knowledge question and answering, and tech tree mastery to\nevaluate the performance. Extensive experiments show that STEVE achieves at\nmost 1.5x faster unlocking key tech trees and 2.5x quicker in block search\ntasks.\n```\n\n#### 3. 论文全文\n```\nSee and Think: Embodied Agent in Virtual\nEnvironment\nZhonghan Zhao1,♠\n, Wenhao Chai2,♠\n, Xuan Wang1,♠\n, Boyi Li1\n,\nShengyu Hao1\n, Shidong Cao1\n, Tian Ye3\n, and Gaoang Wang1,†\n1 Zhejiang University\n2 University of Washington\n3 Hong Kong University of Science and Technology (GZ)\nAbstract. Large language models (LLMs) have achieved impressive pro-\ngress on several open-world tasks. Recently, using LLMs to build embod-\nied agents has been a hotspot. This paper proposes STEVE, a compre-\nhensive and visionary embodied agent in the Minecraft virtual environ-\nment. STEVE comprises three key components: vision perception, lan-\nguage instruction, and code action. Vision perception involves interpret-\ning visual information in the environment, which is then integrated into\nthe LLMs component with agent state and task instruction. Language\ninstruction is responsible for iterative reasoning and decomposing com-\nplex tasks into manageable guidelines. Code action generates executable\nskill actions based on retrieval in skill database, enabling the agent to\ninteract effectively within the Minecraft environment. We also collect\nSTEVE-21K dataset, which includes 600+ vision-environment pairs, 20K\nknowledge question-answering pairs, and 200+ skill-code pairs. We con-\nduct continuous block search, knowledge question and answering, and\ntech tree mastery to evaluate the performance. Extensive experiments\nshow that STEVE achieves at most 1.5× faster unlocking key tech trees\nand 2.5× quicker in block search tasks.\nKeywords: Open-world embodied agent · Multimodal pre-training ·\nLarge language model\n1\nIntroduction\nDesigning agents that demonstrate intelligent behavior and adaptability in open-\nworld settings has been a longstanding and significant challenge in the field of ar-\ntificial intelligence [13,14,26,46,70]. However, recent progress in the development\nof large language models (LLMs) [5, 55] has exhibited their potential as versa-\ntile, general-purpose assistants. Recent innovations in agent design [57,59,69,77]\n♠\nEqual\ncontribution:\nzhaozhonghan@zju.edu.cn,\nwchai@uw.edu,\nxu-\nanw@zju.edu.cn.\n† Corresponding author: gaoangwang@intl.zju.edu.cn.\narXiv:2311.15209v3  [cs.AI]  9 Jul 2024\n2\nZ. Zhao et al.\nVision Perception\nLanguage Instruction\nCode Action\nSTEVE-21K \nDataset\nVision-\nEnvironment\nQuestion-\nAnswering\nSkill-Code\ntrain\nfinetune\nretrieval\nTasks in Minecraft\nContinuous Block Search\nKnowledge QA\nTech Tree Mastery\nWooden\nStone\nIron\nDiamond\nHow to craft a \nstone pickaxe? \nOpen your \ncrafting table…\nobserve\nsearch\nFig. 1: STEVE integrates the three parts: Vision Perception, Language Instruction,\nand Code Action, supported closely by our proposed STEVE-21K. It demonstrates\ncommendable performance on Continuous Block Search, Knowledge QA, and Tech\nTree Mastery.\nhave effectively harnessed these advanced LLMs, tapping into their extensive\nworld knowledge and reasoning abilities. This development has paved the way\nfor agents, that are autonomously driven, to formulate and implement strate-\ngies and actions across a diverse array of skills and tasks in diverse open-world\nenvironments.\nIn many open-world settings, like Minecraft, contemporary agents predomi-\nnantly use LLMs for their textual interactions. However, this reliance on text for\ncommunication poses considerable limitations in their interactions within these\nworlds including low-level regrading cases [24,56,65,66]. Minecraft, with its ex-\npansive and interactive sandbox environment [17,20], demands a variety of skills\nfrom agents, ranging from crafting basic items to executing complex tasks. Yet,\nagents driven by LLMs often generate unpredictable outputs. The effectiveness\nof their interactions is largely contingent on meticulously crafted prompts [23],\ndesigned to align the LLM’s understanding with the environmental context and\nthe intended objectives. This process of prompt engineering is not only labori-\nous but also fails to meet the goal of fostering autonomous, self-directed agents.\nFurthermore, textual communication has its limitations in naturally conveying\ncertain concepts of the world, like crafting recipes, which are often more effec-\ntively communicated through vision.\nPlayers have the distinct capability to assimilate and convey information us-\ning both visual and textual channels, significantly enhancing our interactions\nwith the world around us. Yet, the integration of LLM-based agents with mul-\ntimodal inputs in open-ended environments remains an under-explored area.\nSTEVE in STEVE-Series [71–73], is named after the protagonist of the game\n“Minecraft,”. It is our proposed framework to build an embodied agent based\non the vision model and LLMs within an open world, as illustrated in Fig. 1.\nSTEVE harnesses a vision model to perceive its surroundings visually, coupled\nwith an LLM to strategize and plan actions. This model represents a leap for-\nward in agent design, combining these two input modes, vision, and text, to\noffer a more nuanced and comprehensive understanding of the environment and\npractical and executable skills.\nSTEVE\n3\nOur key contributions are outlined as follows:\n• We propose STEVE, an embodied agent in virtual environment, consists\nof vision perception, language instruction, and code action, achieving 1.5×\nfaster unlocking of key tech trees and is 2.3× quicker in block search tasks\ncompared to previous state-of-the-art methods.\n• We present STEVE-7B\/13B, a series of large language model obtained by\nfine-tuning with Minecraft knowledge question-answering pairs from Llama-\n2-7B\/13B.\n• We collect STEVE-21K dataset, including 600+ vision-environment pairs,\n20K knowledge question-answering pairs, and 200+ skill-code pairs, for jus-\ntifying the effective performance of STEVE.\n2\nRelated Works\n2.1\nIntelligent Agent in Minecraft\nAs an open-ended sandbox game, Minecraft has always been an ideal setting for\ntesting the performance of intelligent agents [21,25]. The agents are required to\nautonomously perform various tasks in Minecraft, such as chopping trees, craft-\ning tools, and mining diamonds. At the beginning, much of the works focus on\nexploring reinforcement learning [32,33,42,50] or imitation learning [2,3], with-\nout satisfactory performance. VPT [3] and MineDojo [17] collect internet-scale\ndatasets for their model pre-training. More specifically, VPT offers the exciting\npossibility of directly learning to act during video pre-training and using these\nlearned behavioral priors as extremely effective exploration priors for reinforce-\nment learning. Yet, recent works found that the pre-trained LLMs could serve\nas a strong “mind” that provides planning ability to the agents. Voyager [57]\nleverages GPT-4 [44] as both a high-level planner and a low-level action code\ngenerator. Plan4MC [69] proposes a skill graph pre-generated by the LLMs.\nDEPS [59], an interactive planning method based on LLMs, addresses multi-step\nreasoning issue in open-world planning. GITM [77] develops a set of structured\nactions and leverages LLMs to generate action plans for the agents to execute,\nachieving impressive results in various tasks.\n2.2\nEmbodied Multimodal Model\nEmbodied agent operates within various environment by synthesizing sensory\nperceptions and physical actions supported by computational intelligence. This\nsynthesis enables the agent to undertake a variety of tasks, achieving specific\nobjectives. Its key areas of application are diverse, including Navigation [6, 16,\n27, 43, 61, 68], Embodied Question Answering [10, 11, 67], Active Visual Track-\ning [37,38,74,75], and Visual Exploration [7,12,35]. The field is evolving rapidly\nwith the development of Large Language Models (LLMs) [51] and Multimodal\nLLMs (MLLMs) [1, 9, 18, 19, 28–30, 34, 39, 41, 54, 58, 64, 76], integrating multiple\nmodalities for more effective processing. A prime example of this innovation is\n4\nZ. Zhao et al.\nVision \nEncoder\n…\nVision Perception\n…\n…\nSTEVE-13B\nLanguage Instruction\n1.\nCrafting Table: \nOpen your \ncrafting table\n2.\nIngredients … \n…\nImage\nor\nVideo\n1.\nGather Wood \nLogs\n2.\nConvert Wood \nLogs into Planks\n…\nDecompose\nHigh-level\nLow-level\n…\nQuery\nEncoding\nRetrieval\nMine Wood Log\nCraft Iron Sword\nCombat Creeper\nCook Mutton\nCraft Shears\nMake Crafting Table        …\nCraft Bucket\nCatch Pufferfish\nNo crafting table nearby\nTokenizer\nCraft a bow\nAgent \nState\nTask\nSkill Database\nconst woodLogBlock = \nawait exploreUntil(bot, \nnew Vec3(1, 0, 1), 60, () => \n{\nreturn bot.findBlock({\nmatching: block => \n...\nCode Action\nFig. 2: STEVE framework. The Vision Perception part takes images or videos, en-\ncodes them into tokens, and combines them with Agent State and Task tokens as input.\nThe STEVE-13B in the Language Instruction part is used for automatic reasoning and\ntask decomposition, and it calls the Skill Database in the form of the Query to output\ncode as action.\nPaLM-E [15], a sophisticated multimodal model with 562B parameters, adept at\na broad spectrum of embodied tasks and demonstrating exceptional capabilities\nin visual reasoning.\n2.3\nLarge Language Model with Equipped Tools\nWhile Large Language Models (LLMs) demonstrate impressive skill in tackling\nnovel tasks via prompt-based instructions, they often face challenges in areas\nwhere simpler models or tools excel, like mathematical calculations or iden-\ntifying palindromes. However, LLMs’ potential is significantly expanded when\nintegrated with other modality-specific models, such as those for vision or audio,\nenabling multi-modal capabilities [4,36]. Innovations like Toolformer [47] demon-\nstrate LLMs’ self-learning to utilize tools through finetuning with extensive API\ncall samples. Visual ChatGPT [62] extends this by integrating various visual\nfoundation models, facilitating interactive user experiences with ChatGPT. Simi-\nlarly, HuggingGPT [48] presents a framework that harnesses LLMs to link diverse\nmodels from Hugging Face for task resolution. AutoGPT [49] is an open-source\napplication that broadens GPT-4’s capabilities with internet access, memory\nmanagement, and plug-ins. The recent introduction of MovieChat [52,53] brings\na memory mechanism to MLLM, enhancing its performance in video under-\nstanding tasks. Furthermore, LLMs can be used for goal planning, analogous to\nlanguage translation [63]. This evolving landscape suggests that tool-equipped\nLLMs could forge a new paradigm in AI solution design.\n3\nMethod: STEVE\n3.1\nOverview\nAs shown in Figure 2, STEVE F is an LLM-based multi-modal autonomous\nsystem for embodied agents in Minecraft, which can use visual status Xv in the\nSTEVE\n5\nform of images or videos and agent state Xs to manage and execute complex\ntask Xt for executive code action ac:\n  \\ mathb f { a}^ c  = \\mathcal {F} (X^v, X^s, X^t) = \\mathcal {A}^c(\\mathcal {I}^l(\\mathcal {P}^v(X^v, X^s, X^t))) \n(1)\nwhere STEVE F integrates Visual Perception Pv into the Language Instruction\nIl with large language model (LLM) and combines them with Code Action Ac,\na skill retrieval method to execute actions, details are demonstrated as follows.\n• Vision Perception Pv (Section 3.2); a vision encoder to interpret vi-\nsual information of the environment, such as blocks and entities and a text\ntokenizer for agent state s and task t.\n• Language Instruction Il (Section 3.3): a powerful language model sys-\ntem fine-tuned specifically for Minecraft content using LLaMA2-13B [18].\nThis model enables adaptive interaction for iterative reasoning and step-by-\nstep decomposition.\n• Code Action Ac (Section 3.4): a skill retrieval method based on our\nskill-code database.\n3.2\nVision Perception Pv\nThe vision perception part includes a vision encoder Ev and a text tokenizer\nT, which converts the visual status Xv, agent state Xs, and task Xt to the\ntext-space tokenizer representation Y = {Y v, Y s, Y t}:\n  \\ mathb\nf  {Y\n} _ i \n= \\ ma t hcal  {P}^v(X^v_i, X^s_i, X^t_i), \\mathcal {P}^v = \\{E^v, T\\}, \n(2)\nwhere the vision encoder E, the visual branch of EfficientFormer [31], encodes\nvisual status Xv\ni at each step i into visual tokens Y v = {yv\n1, yv\n2, ..., yv\nn} ∈Rn×d,\nwhere n denotes the number of visual tokens yv and d is the dimensionality of\neach token. The text tokenizer converts agent state Xs\ni and task Xt\ni at each step\ni in the form of text into textual tokens Y s and Y t. Note that visual tokens are\namalgamated with textual tokens representing the agent’s current state (e.g.,\nhealth, inventory, etc.) and the task description. This is accomplished using a\ntokenizer that maps state variables and task parameters to a tokenized form.\nThe resultant unified token set serves as a comprehensive representation of the\ncurrent situational context.\n3.3\nLanguage Instruction Il\nThe Language Instruction Il = {Pl, Cr, Cu, Ds}, which consists of Planner Pl,\nCritic Cr, Curriculum Cu, and Describer Ds, i.e., four independent LLM-based\nagents with different functions. They formulate high-level task guidelines, re-\nfine strategies through feedback, facilitate continuous learning and adaptation\nthrough a curriculum of complex tasks, and finally decompose strategic guide-\nlines into the executable low-level textual action step as, {as ∼Il(·|Yi)}N\ni=0\ntowards efficient completion of the task.\n6\nZ. Zhao et al.\nNote that the Planner Pl, Critic Cr, Curriculum Cu and Describer Ds are\nbased on STEVE-7B\/13B, a powerful language model derived from LLaMA-2-\n13B [18], fine-tuned specifically on Minecraft-related content from the STEVE-\n21K. This model’s expertise covers a broad spectrum of game-specific knowledge\nareas on worlds, entities, player mechanics, survival, and even game practical\nexperience. Finally, they have the performance of their different functions:\n• Planner Pl: formulating comprehensive guidelines and executive plans that\nalign with the overarching objectives of the task.\n• Critic Cr: evaluating the planner decisions, providing feedback that can\nrefine strategies.\n• Curriculum Cu: facilitating continuous learning and adaptation for action\nagents by engaging with a series of progressively complex tasks.\n• Describer Ds: distilling the extensive data into a concise summary, making\nit more manageable and interpretable.\nIterative reasoning. The STEVE-13B receives a stream of tokens that encode\nthe current visual scene, the agent’s state, and the task’s textual description.\nSTEVE-13B interprets this rich context to undertake complex reasoning. The\nmodel initiates the reasoning process by constructing a series of high-level strate-\ngies that outline the pathway to task completion. Considering all gameplay el-\nements, the reasoning mechanism is akin to an experienced player who can vi-\nsualize the end game and chart a course to victory. This approach ensures the\nplans are reactive and proactive, allowing the agent to anticipate and mitigate\nfuture challenges. However, most strategies are high-level and abstract therefore,\nthey often require step-by-step decomposition to derive executable guidelines.\nDecomposition. The decomposition process makes the complex strategies break\ndown into simple, low-level guidelines that can be directly mapped to actions\nin Minecraft. It is similar to how a high-level command like “build a shelter”\nis divided into actionable instructions like “collect wood”, “craft planks”, and\n“place blocks”. The granular steps are structured to provide explicit instructions\nthat the game engine can readily interpret. This systematic breakdown from\nhigh-level reasoning to low-level actionable steps is the hallmark of the STEVE\nsystem, enabling the embodied agent to interact with the Minecraft environment\nin a meaningful and goal-oriented manner. Through this intricate process of rea-\nsoning and decomposition, STEVE embodies the cognitive capabilities required\nfor sophisticated task management in virtual environments.\nCurriculum learning with memory. We draw inspiration from the lifelong learn-\ning strategy utilized in many reinforcement learning problems [57] in both closed-\nworld and open-world settings. We start by creating a set of tasks that serve as a\ncurriculum for agents to explore the environment. During this process, STEVE\ngenerates plans, interacts with the surroundings, learns from mistakes, and stores\nall these experiences in memory. Next, we evaluate STEVE on various tasks af-\nter this learning stage. Consequently, STEVE can produce better plans with its\nSTEVE\n7\nmemory teaming up with the planning experiences. We use this as the default\nsetting for all tasks in our experiments.\nContinous learning with summarization.\nWe’ve observed that the learning\nprocess, where the memory is being filled, can continue throughout the gameplay.\nThe agent can gradually acquire more skills as the gameplay progresses and more\nexperiences are gained. However, as the memory gets larger, it becomes difficult\nto understand the game’s situation and interact with the game slowly. To tackle\nthese challenges, we have implemented the Chain of Summarization method [40].\nBy finding better references, we can improve our ability to handle challenging\ntasks, such as “Diamond Tool” in the tech tree, including obtaining materials\nand creating diamond tools. This will lead to a higher success rate, as shown in\nSection 5.3. Additionally, curriculum learning with memory allows for in-context\nlifelong learning without needing gradient updates.\n3.4\nCode Action Ac\nThe code action part Ac is the execution phase, where the STEVE system con-\nverts planned, decomposed guidelines into concrete actions within the Minecraft\nenvironment. This process leverages a specialized skill database that pairs code\nsnippets with their descriptions and relevant metadata, encoded as vectors vs for\nefficient retrieval. The transition from language instruction to executable code\nis achieved through the retrieval process R:\n  \\m at h cal {R} ( \\ mathbf {q}, \\mathbf {v}) = \\sigma (\\mathbf {q}, \\mathbf {v}), \\mathbf {q} = E^q(\\mathbf {a^s}), \n(3)\nwhere Eq and σ are query encoding and cosine similarity matching. Each low-\nlevel textual action step as derived from the Language Instruction is encoded\ninto a query q. This encoding captures the essence of the action to be performed\nin a format that the skill database can understand. Once the queries q are\nencoded, the system computes similarities between the query vectors and the\ncode snippets vectors v stored in the database to determine which skill best\nmatches the required action.\n3.5\nTraining\nTo reduce the training overhead to a certain extent, we adopt a two-stage training\nmethod, the first stage being a warm-up on the question-answering pairs of\nSTEVE-21K to ensure a certain degree of instruction capability. The second\nstage is simulated in the environment, and the Expert LLM Ep integrated with\nGPT-4 [44] generates instructions for the same situation to modify our model.\nIn training, we use either a single-round conversation {XQ, XA} or a multi-\nround conversation {XQ\ni , XA\ni }i≤N, where N is the total number of rounds in the\nconversation. We use the negative log-likelihood objective over the prediction\n8\nZ. Zhao et al.\ntokens with training and finetuning:\n  \\m a t\nh\nc\nal \n{L} (\\theta ) =-\\sum _{j=1}^{L} \\log \\mathcal {F}_{\\theta }(Y_j|X^v, \\hat {Y}_{1:j-1}), \n(4)\nwhere Y and ˆY refer to the non-vision input and target token sequences, θ repre-\nsents the model parameters, and L represents the length of the target sequence.\nThe vision input Xv varies depending on the input step. We only consider the\nanswer tokens XA when computing the loss to ensure that the model focuses on\ngenerating coherent responses.\nOffline warm-up. In the first stage, we finetune the STEVE-7B\/13B only on\nthe single-round question-answering pairs of STEVE-21K to ensure a certain\ndegree of instruction capability, as shown in Tab. 3.\nOnline finetuning. In the second stage, we simulate the warmed-up STEVE-\n7B\/13B in Minecraft, and both train the vision encoder Ev and finetune the\nSTEVE-7B\/13B simultaneously.\nIt is necessary to obtain the environment information to train the vision en-\ncoder, denoted as l = RT (Xv), where l is the label of seen blocks and entities,\nRT is the Ray Tracing method [60] to eliminate all content outside the agent’s\nperspective screen. We train the vision encoder Ev once we have obtained the\nenvironment information. To finetune the warmed-up STEVE-7B\/13B in simu-\nlation, we give both the STEVE-7B\/13B and the Expert LLM the same input,\nand we consider the output of the Expert LLM as the ground truth label.\nAfter 5,000 simulations, we collect all the context information from success-\nful runs, including sequences of vision and question-answering chat flow as the\nVision-Environment part of STEVE-21K, as mentioned in Section 4.\n4\nDataset: STEVE-21K\nAs shown in Fig. 3, STEVE-21K has been meticulously compiled, featuring a\ndiverse collection of Vision-Environment pairs, Question-Answering pairs, and a\nSkill-Code database.\nVision-Environment pairs contain 600 pairs of first-person perspective videos\nfrom Minecraft gameplay across six different terrains (including forest, desert,\ncoastal, etc.), along with corresponding environmental block entities in the field\nof vision and context information for each timestamp. Additionally, all pairs\nare oriented around executing the skill-related task supported by Skill-Code\npart mentioned in Section 4. We employ the STEVE-7B\/13B model to enable\nrobots to plan and execute actions autonomously based on tasks defined by\nhuman supervisors. We record the video of the agent operation, the environment\ninformation, and all the corresponding chatflow.\nQuestion-Answering pairs contain 20K question-answering pairs from the\nMinecraft-Wiki and Reddit corpus across six data types partly sourced from [17].\nThe pairs are organized into instruction, input, and output triplets and used to\nSTEVE\n9\nVision-Environment\n20K Single-round QA pairs in\nQuestion-Answering\nSkill-Code\nHuman Player\nModels\nMinecraft World\nPrismarine Viewer\n…\nVision Record\nRay Tracing\nWikipedia\nReddit\nChatGPT\nEnv. Record\n1. World & Entities \n2. Player Mechanics & Survival \n3. Knowledge & Discovery \n4. Resources & Crafting \n5. Tools & Utilities\n6. Miscellaneous\nSingle-round QA pairs\nHuman Player\nChatGPT\nprompt\ncode snippets\noutput\nexecute\ncheck and revise\nBlocks: stone, dead \nbushes, cacti, orange \nterracotta...\nClean\nConstruct\nCheck\nFig. 3: STEVE-21K collection pipeline. In the Vision-Environment section,\nSTEVE-13B plays the game according to specified tasks defined by the human player,\ncollecting visual information through prismarine-viewer and capturing environmental\ninformation from the screen using Ray Tracing [60]. Note that the language instruction\ntask is also performed during the collection phase. We simultaneously record and save\nthe chat flow from the reasoning and decomposition stages. In the Question-Answering\nsection, we obtain information from the Minecraft-Wiki and Reddit forums and use\nGPT-3.5 to clean the data into single-round QA pairs. In the Skill-Code section, we\nuse GPT-3.5 combined with the human player’s code to synthesize code snippets and\nthen check and revise them in the game environment.\ntrain STEVE-13B. The GPT-3.5 [5] is employed to derive meaningful single-\nround question-answer pairs, and LoRA [22] is incorporated during the fine-\ntuning process for efficient resource allocation.\nSkill-Code pairs contain 210 skill execution scripts with descriptions, covering\n8 skill types including collecting, crafting, exploration etc. The code part is\ncollected by manual coding. We use GPT-3.5 [5] to describe all codes and utilize\nlangchain vectordb to give all pairs a database vector.\n5\nExperiments\n5.1\nExperimental Setup\nWe train STEVE-7B\/13B, which is finetuned from LLaMA-2 [18] with the\nQueston-Answering pair in STEVE-21K dataset for warm-up and simulation\ncontext data from successful runs. We use LoRA [22] for finetuning process.\nNote that the process is to adjust STEVE-13B to work on correct simulation\nknowledge while remaining adapted to visual perception. In the text part, we set\nall temperatures to 0 except for the task proposal, which uses 0.9 to encourage\ntask diversity. The vision unit is based on EfficientFormerV2-S0 [31], which is\ntrained on the Vision-Environment part of our STEVE-21K dataset. Our simula-\ntion environment is built on top of MineDojo [17] and leverages Mineflayer [45].\n10\nZ. Zhao et al.\nContext\n[Question] How to \nsurvive, according to \nthe current situation?\n[Answer] Dealing with \nthe Creeper and \nHealth Management...\nChat Log\nA Creeper is \nNearby, please \ncombat Creeper\nwith the iron \nsword first.\nPlan\nUse `combatOneCreeper` \nfunction to combat the \nCreeper until the Creeper is \neliminated. Once the \nCreeper is eliminated, call \nthe `eatCarrot` ...\nEnvironment \n[Blocks] stone, \ndead bushes, \ncacti, orange \nterracotta, ...\n[Equipment] \niron sword\n[Entities]\ncreeper, Rabbit\n[Health] 15\/20\n[Hunger] 18\/20\n[Inventory]\nCarrots, torches\nFig. 4: Example of Vision-Environment pairs. It represents the data format of\nthe Vision-Environment pairs in our STEVE-21K dataset: including visual signals,\nenvironmental information, Chat Log, Context QA pairs, and planning in actual tasks.\nWe use GPT-4-0613 for all GPT-4 models used in Voyager [57] and code gener-\nation tasks.\n5.2\nBaselines\nAs no vision-based LLM-driven agents are immediately operable in Minecraft, we\nselected several algorithms as baselines that extract information from a system’s\nbackend, differing significantly from real-world applications.\nAutoGPT [49] is an NLP automation tool that decomposes a high-level goal\ninto executable subgoals in MineDojo, aligning with our experimental frame-\nwork. Our setup, AutoGPT, powered by GPT-4 [44], processes agent states,\nenvironment feedback, and execution errors to manage subgoal execution.\nVoyager [57] relies solely on textual grounding for perception and features a long-\nterm procedural memory with a hierarchical library of code-based procedures,\nallowing the integration of simple skills into complex behaviors. Proficient in\nenvironment exploration and tech tree mastery, Voyager uses GPT-4 [44] for\nprocessing background text in embodied agents, with a lesser emphasis on visual\nperception.\nSTEVE\n11\nTable 2: Comparison on tech tree mastery task. The values presented are in frac-\ntions, representing successful trials out of three attempts. A score of 0\/3 signifies the\nmethod’s inability to progress within the tech tree after a maximum of 160 prompt-\ning iterations. The reported numbers denote the average iterations across three trials.\nLower iteration values indicate higher efficiency of the respective method.\nMethod\nWooden Tool Stone Tool\nIron Tool\nDiamond Tool\nAutoGPT [49] 92 ± 72 (3\/3) 94 ± 72 (3\/3) 135 ± 103 (3\/3)\nN\/A (0\/3)\nVoyager [57]\n6 ± 2 (3\/3)\n11 ± 2 (3\/3)\n21 ± 7 (3\/3)\n102 (1\/3)\nSTEVE\n4 ± 1 (3⁄3)\n8 ± 1 (3⁄3)\n15 ± 2 (3⁄3)\n106 ± 12 (3⁄3)\nTable 3: Quantitive comparison on knowledge question and answering task.\nQuestions, model-generated responses, and ground truth inputs are evaluated in GPT-\n4 [5], Claude-2 [8] and human blind rating rated on a scale of 0 to 10; The scores above\nare the average of them. Higher scores indicate greater alignment of the generated\nanswers with the ground truth. Wld., Ent., Mech., Surv., Know., Disc., Res., Craft.,\nTools, Util., Miscell. stand for World, Entities, Player Mechanics, Survival, Knowledge,\nDiscovery, Resources, Crafting, Tools, Utilities and Miscellaneous.\nMethod\nWld. & Ent. Mech. & Surv. Know. & Disc. Res. & Craft. Tl. & Util. Miscell. Overall\nLlama2-7B\n6.44\n6.68\n6.58\n6.42\n6.80\n6.96\n6.56\nLlama2-13B\n6.93\n6.95\n6.77\n6.77\n6.98\n6.64\n6.89\nSTEVE-7B\n7.99\n7.88\n7.84\n7.95\n7.93\n7.82\n7.94\nSTEVE-13B\n8.14\n8.13\n8.03\n8.15\n8.12\n7.72\n8.12\nGPT-4\n8.06\n8.07\n8.07\n7.92\n8.09\n8.21\n8.04\n5.3\nEvaluation Results\nTable\n1:\nComparison\non\ncontinues\nblock search task. # Iters stand for av-\nerage iterations to find 10 diamonds (max\n100). # Blocks stand for average diamonds\nfound in 100 iterations.\nMethod\n# Iters (↓) # Blocks (↑)\nAutoGPT [49]\nN\/A\n7\nVoyager [57]\n35\n26\nSTEVE\n14\n67\nContinuous block search.\nAs shown\nin Tab. 1, we experiment with block-\nsearching tasks to assess the agent’s ex-\nploratory capabilities and proficiency\nin locating specified blocks. Diamond\nblocks are placed at every 16-block in-\nterval across the land map. The agent’s\nobjective is to identify as many blocks\nas possible within the fewest iterations,\nwhich indicates the method’s efficiency.\nAs shown in Fig. 5, enriching information through visual perception significantly\nenhances the efficiency of search and exploration tasks, leading to more effective\nworld exploration.\n12\nZ. Zhao et al.\nSTEVE\nVoyager\nAutoGPT\nFig. 5: Schematic of the continuous block search task. We capture an asyn-\nchronous segment with each method 30 iterations from the experiments for illustration.\nThe reason we choose diamond blocks is that they are not naturally occurring in the\ngiven context, making them easily distinguishable from other blocks.\nKnowledge question and answering. Using a validation dataset, we established a\nquestion-answering database to evaluate our model’s performance on Minecraft-\nrelated queries. Responses from each model are rated blindly by GPT-4, Claude-\n2, and human participants based on accuracy, relevance, and detail. Responses\nare initially checked for accuracy, and the comprehensive evaluation yields a\nscore ranging from 0 to 10, with higher scores indicating superior performance.\nAs detailed in Tab. 3, we tested the instructional capabilities of various LLM\nmodels using the STEVE-21k dataset, divided into 18, 622 training and 1, 000\ntesting samples. The STEVE-7B and STEVE-13B models surpassed LLaMA2\nin all metrics, with STEVE-13B scoring the highest at 8.12, demonstrating\nits exceptional understanding of Minecraft-related queries. This indicates that\nSTEVE models, optimized specifically for Minecraft content, perform better in\nknowledge-intensive tasks.\nOur results confirm that larger, domain-specifically tuned models like STEVE-\n13B outperform broader models like GPT-4, underscoring the benefit of special-\nized fine-tuning for domain-specific applications.\nSTEVE\n13\nHow to restore full \nhunger efficiently? \nYou should eat 1 bread and 1 \napple. The bread will restore \n5 hunger points, and the apple \nwill restore 4 hunger points \nadding up to the 6 hunger\npoints you need to have a full \nhunger bar...\nSimply eat one bread from \nyour inventory. It will \nincrease your hunger level by \n5 points, which is precisely \nwhat you need to reach 20...\nYou should eat an Apple or 2 \nBreads. Both Apples and \nBreads restore 5 hunger \npointswhich will bring your \nhunger level back to full.\nShould I combat zombie \nand how to survive?\nYou should combat the \nzombie using your iron \nsword. Zombies are relatively \nweak and should not pose a \nsignificant threat to you...\nYou should combat the \nzombie; remember to strike \nand step back to avoid getting \nhit. You seem to be in an \nabandoned mineshaft. Once \nthe area is safe, continue to \nexplore...\nyou should combat the \nzombie and collect resources \nto craft better equipment. The \ncurrent environment is likely \na village or a farm...\nWhat do you find and how \nto get more resources?\nYou should open the nearby \nChest, or explore the \nsurrounding area to find \nbetter resources...\nBegin by inspecting the \nnearby Chest, it could contain \nvaluable resources. Use your \nIron Pickaxe to mine any \nprecious ores you encounter \nin the mineshaft...\nYou should find more \nCobblestones to craft better \narmor and weapons. The \nnearby blocks and entities \nsuggest that you are in a \nMinecraft world, and the \nresources you need are likely \nto be found in the nearby \narea...\nShould I combat skeleton \nand how to survive?\nGiven your current situation, \nit is not advisable to engage \nin combat with skeletons or \nany other hostile mobs...\nYou should avoid engaging \nthe skeleton at the moment\nsince your health is a bit low\nand you have no armor to \nprotect yourself. You should \ninstead focus on your \nimmediate safety...\nYou should combat the \nskeleton. To survive, it is \nrecommended to prioritize\ncrafting better equipment, \nsuch as armor and weapons, \nto increase your health and \ncombat effectiveness...\nFig. 6: Qualitative comparison on knowledge question and answering tasks.\nGreen ( Red ) highlights the correct or good (wrong or bad) answer. Blue indicates\nthe suboptimal answer. Grey indicates the meaningless answer.\nTech tree mastery. As shown in Tab. 2, we experiment on the Minecraft tech\ntree mastery to test the agent’s ability to craft and use a hierarchy of tools. Pro-\ngressing through this tree (wooden tool →stone tool →iron tool →diamond\ntool) requires the agent to master systematic and compositional skills. As to\nthe wooden, stone, and iron levels of the tech tree, STEVE achieves remarkable\nefficiency: 23×, 11.8×, and 9× faster than AutoGPT [49], and 1.5×, 1.4×, and\n1.3× faster than Voyager [57]. STEVE has achieved the diamond level, as shown\nin Tab. 2. Its performance slightly lags behind Voyager [57], which also uses\nGPT4 [44] for critical inference. However, STEVE is more cost-effective, start-\ning with lower initial performance. It includes a vision unit, prioritizing visual\ndata over background information, offering distinct advantages. Additionally, we\nobserved a decrease in performance when using a basic skill database.\n5.4\nAblation Study\nTo understand the impact of different components on the performance of our\nsystem, we conducted ablation studies focusing on the tech tree mastery task in\nMinecraft. The results, as shown in Tab. 4, provide insights into the effectiveness\nof the vision unit and compare our STEVE model with the STEVE GPT-4\nversion (with the same vision unit as ours). Note that the w\/o vision unit setup\nis that the environmental perception encompasses data on blocks within an 8x8\narea surrounding the agent, including the front, back, left, and right directions.\nThe following observations are made:\n14\nZ. Zhao et al.\nTable 4: Ablation studies for the tech tree mastery. STEVE (Ours) is the\nSTEVE-13B version. The 0\/3 score means the method can’t progress beyond 160 iter-\nations in the tech tree.\nMethod\nWooden Tool Stone Tool\nIron Tool\nDiamond Tool\nw\/o vision unit\n11 ± 5 (3\/3)\n27 ± 5 (3\/3) 46 ± 11 (3\/3)\n158 (1\/3)\nSTEVE (GPT-4)\n6 ± 2 (3\/3)\n10 ± 1 (3\/3) 14 ± 3 (3⁄3)\n89 ± 9 (3⁄3)\nSTEVE (Ours)\n4 ± 1 (3⁄3)\n8 ± 1 (3⁄3) 15 ± 2 (3\/3) 106 ± 12 (3\/3)\nVision unit is critical. The omission of the vision unit markedly affects the\nsystem’s performance, especially in more advanced tasks. While it successfully\ncrafts Wooden, Stone, and Iron Tools, it is challenged with Diamond Tools. This\noutcome underscores the vital importance of visual information in accomplishing\ncomplex tasks.\nComparison with GPT-4. As our vision encoder directly encodes into text space,\nit can be easily replaced with any language model. For instance, the GPT-4 we\ncompared exhibits consistent success across all categories and secures a flawless\nsuccess rate. Interestingly, the STEVE-13B version excels in simpler tasks such\nas crafting wooden and stone tools. Moreover, it requires fewer iterations than\nmethods without the vision part, underscoring its superior efficiency.\n5.5\nCase Study\nAs shown in Fig. 6, we perform an extensive case study comparison of GPT-4,\nLLaMA2-13B, and our method STEVE-13B. Each model maintains the same\ninformation and question inputs to compare feedback under different environ-\nmental information. Our STEVE overall achieves the best results, surpassing\nGPT-4 and showing significant improvement compared to the original LLaMA.\nEspecially in parts involving numerical calculations, such as the leftmost image,\nSTEVE accurately tracks food values to restore hunger levels.\n6\nConclusion\nSTEVE enhances multi-modal learning by combining visual encoder and LLM-\nbased agents. It has three functions: vision perception, language instruction,\nand code action, allowing it to understand, predict, and act in virtual envi-\nronments. We provide a straightforward approach to creating a robust, multi-\nmodal, autonomous, embodied agent using an open-source language model with\na small number of parameters. Additionally, we provide a comprehensive dataset\nSTEVE-21K for sustainable community development that can be verified.\nSTEVE\n15\nAcknowledgement\nThis work is supported by the Zhejiang Provincial Natural Science Foundation\nof China (No. LZ24F030005) and the National Natural Science Foundation of\nChina (No. 62106219).\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | See and Think: Embodied Agent in Virtual Environment\n\n## 📌 背景痛点\/本文动机\n在开放世界环境中，设计能够表现出智能行为和适应性的智能体一直是人工智能领域的一个长期而重要的挑战。然而，近年来，大型语言模型（LLMs）在开发方面取得了显著进展，展现出其作为多功能、通用型助手的潜力。尽管如此，在许多开放世界环境中，如Minecraft，当代智能体主要使用LLMs进行文本交互。然而，这种对文本通信的依赖限制了它们在这些世界中的交互，包括低级案例。Minecraft要求智能体具备各种技能，从制作基本物品到执行复杂任务。然而，由LLMs驱动的智能体往往产生不可预测的输出。它们交互的有效性在很大程度上取决于精心设计的提示，旨在将LLM的理解与环境的上下文和预期目标相一致。这种提示工程过程不仅费力，而且无法实现培养自主、自我驱动的智能体的目标。此外，文本通信在自然传达某些世界概念方面存在局限性，例如制作配方，这些概念通常通过视觉更有效地传达。\n\n## 🚀 核心方法\n💡 创新点1：提出STEVE，一个在虚拟环境中具有视觉感知、语言指令和代码动作的智能体，与之前最先进的方法相比，在解锁关键技术树方面最多快1.5倍，在块搜索任务中最多快2.3倍。\n💡 创新点2：提出STEVE-7B\/13B，一系列通过使用Llama-2-7B\/13B的Minecraft知识问答对进行微调获得的大型语言模型。\n💡 创新点3：收集STEVE-21K数据集，包括600多个视觉-环境对、20K个知识问答对和200多个技能-代码对，以证明STEVE的有效性能。\n\n## 📈 实验结果\n实验结果表明，STEVE在连续块搜索、知识问答和科技树掌握方面表现出色。与AutoGPT和Voyager等基线方法相比，STEVE在解锁关键技术树方面最多快1.5倍，在块搜索任务中最多快2.3倍。此外，STEVE在知识问答任务中表现出色，其性能优于Llama-2和GPT-4等更广泛的模型。\n\n## 💬 可借鉴之处\n本文提出的STEVE框架为构建具有视觉感知和语言指令能力的智能体提供了一个有价值的参考。此外，STEVE-21K数据集为研究人员提供了进行多模态学习研究的有用资源。","llm_summary_res_status":200}
{"title":"RL-GPT: Integrating Reinforcement Learning and Code-as-policy","authors":"Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia","summary":"Large Language Models (LLMs) have demonstrated proficiency in utilizing\nvarious tools by coding, yet they face limitations in handling intricate logic\nand precise control. In embodied tasks, high-level planning is amenable to\ndirect coding, while low-level actions often necessitate task-specific\nrefinement, such as Reinforcement Learning (RL). To seamlessly integrate both\nmodalities, we introduce a two-level hierarchical framework, RL-GPT, comprising\na slow agent and a fast agent. The slow agent analyzes actions suitable for\ncoding, while the fast agent executes coding tasks. This decomposition\neffectively focuses each agent on specific tasks, proving highly efficient\nwithin our pipeline. Our approach outperforms traditional RL methods and\nexisting GPT agents, demonstrating superior efficiency. In the Minecraft game,\nit rapidly obtains diamonds within a single day on an RTX3090. Additionally, it\nachieves SOTA performance across all designated MineDojo tasks.","url":"http:\/\/arxiv.org\/abs\/2402.19299v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.19299v1","published":1709222842000,"comment":null,"pdf_text":"RL-GPT: Integrating Reinforcement Learning and Code-as-policy\nShaoteng Liu1, Haoqi Yuan3, Minda Hu1, Yanwei Li1,\nYukang Chen1, Shu Liu2, Zongqing Lu3,4, Jiaya Jia1,2\n1The Chinese University of Hong Kong 2SmartMore\n3School of Computer Science, Peking University\n4Beijing Academy of Artificial Intelligence\nhttps:\/\/sites.google.com\/view\/rl-gpt\/\nAbstract\nLarge Language Models (LLMs) have demonstrated pro-\nficiency in utilizing various tools by coding, yet they face\nlimitations in handling intricate logic and precise control.\nIn embodied tasks, high-level planning is amenable to di-\nrect coding, while low-level actions often necessitate task-\nspecific refinement, such as Reinforcement Learning (RL).\nTo seamlessly integrate both modalities, we introduce a two-\nlevel hierarchical framework, RL-GPT, comprising a slow\nagent and a fast agent. The slow agent analyzes actions\nsuitable for coding, while the fast agent executes coding\ntasks. This decomposition effectively focuses each agent on\nspecific tasks, proving highly efficient within our pipeline.\nOur approach outperforms traditional RL methods and ex-\nisting GPT agents, demonstrating superior efficiency. In\nthe Minecraft game, it rapidly obtains diamonds within a\nsingle day on an RTX3090. Additionally, it achieves SOTA\nperformance across all designated MineDojo tasks.\n1. Introduction\nBuilding agents to master tasks in open-world environments\nhas been a long-standing goal in AI research [6, 38, 40]. The\nemergence of Large Language Models (LLMs) has revital-\nized this pursuit, leveraging their expansive world knowledge\nand adept compositional reasoning capabilities [24, 39, 47].\nLLMs agents showcase proficiency in utilizing computer\ntools [10, 14], navigating search engines [11, 15], and even\noperating systems or applications [9, 46]. However, their\nperformance remains constrained in open-world embodied\nenvironments [10, 38]. Despite possessing “world knowl-\nedge” akin to a human professor, LLMs fall short when pitted\nagainst a child in a video game. The inherent limitation lies\nin LLMs’ adeptness at absorbing information but their in-\nability to practice skills within an environment. Proficiency\nGPT\nRL\nRL-GPT (Ours)\nOptimized\nNeural Network\nOptimized\nCoded Actions\nOptimized Actions + \nNeural Network\n1.9X\n6.7X\nFigure 1. The overview of RL-GPT. After the optimization in an\nenvironment, LLMs agents obtain optimized coded actions, RL\nachieves an optimized neural network, and our RL-GPT gets both\noptimized coded actions and neural networks. Our framework\nintegrates the coding parts and the learning parts.\nin activities such as playing a video game demands extensive\npractice, a facet not easily addressed by in-context learning,\nwhich exhibits a relatively low upper bound [10, 24, 47].\nConsequently, existing LLMs necessitate human interven-\ntion to define low-level skills or tools [38, 42].\nReinforcement Learning (RL), proven as an effective\nmethod for learning from interaction, holds promise in fa-\ncilitating LLMs to “practise”. One line of works grounds\nLLMs for open-world control through RL fine-tuning [4, 27,\n33, 44, 45, 53]. Nevertheless, this approach necessitates a\nsubstantial volume of domain-specific data, expert demon-\nstrations, and access to LLMs’ parameters, rendering it slow\nand resource-intensive in most scenarios. Given the mod-\nest learning efficiency, the majority of methods continue\nto operate within the realm of “word games” such as tone\nadjustment rather than tackling intricate embodied tasks.\nAddressing this challenge, we propose to integrate LLMs\n1\narXiv:2402.19299v1  [cs.AI]  29 Feb 2024\nand RL in a novel approach: Empower LLMs agents to use\nan RL training pipeline as a tool. We introduce RL-GPT, a\nframework designed to enhance LLMs with trainable mod-\nules for learning interaction tasks within an environment.\nAs shown in Fig. 3, RL-GPT comprises an agent pipeline\nfeaturing multiple LLMs, wherein the neural network is\nconceptualized as a tool for training the RL pipeline. Illus-\ntrated in Fig. 1, unlike conventional approaches where LLMs\nagents and RL optimize coded actions and networks sepa-\nrately, RL-GPT unifies this optimization process. The line\nchart in Fig. 1 illustrates that RL-GPT outperforms alterna-\ntive approaches on the “harvest a log” task in MineDojo [7].\nWe further point out that the pivotal issue in using RL\nis to decide: Which actions should be learned with RL?\nTo tackle this, RL-GPT is meticulously designed to assign\ndifferent actions to RL and Code-as-policy, respectively. Our\nagent pipeline entails two fundamental steps. Firstly, LLMs\nshould determine “which actions” to code, involving task\ndecomposition into distinct sub-actions and deciding which\nactions can be effectively coded. Actions falling outside\nthis realm will be learned through RL. Secondly, LLMs are\ntasked with writing accurate codes for the “coded actions”\nand test them in the environment.\nWe employ a two-level hierarchical framework to realize\nthe two steps, as depicted in Fig. 3. Allocating these steps\nto two independent agents proves highly effective, as it nar-\nrows down the scope of each LLM’s task. Coded actions\nwith explicit starting conditions are executed sequentially,\nwhile other coded actions are integrated into the RL action\nspace. This strategic insertion into the action space em-\npowers LLMs to make pivotal decisions during the learning\nprocess. Illustrated in Fig. 2, this integration enhances the\nefficiency of learning tasks, exemplified by our ability to\nmore effectively learn how to break a tree.\nFor intricate tasks such as the ObtainDiamond task in the\nMinecraft game, devising a strategy with a single neural net-\nwork proves challenging due to limited computing resources.\nIn response, we incorporate a task planner to facilitate task\ndecomposition. Our RL-GPT framework demonstrates re-\nmarkable efficiency in tackling complex embodied tasks.\nSpecifically, within the MineDojo environment, it attains\nstate-of-the-art performance on the majority of selected tasks\nand adeptly obtains diamonds within a single day, utilizing\nonly an RTX3090 GPU.\nOur contributions are summarized as follows:\n• Introduction of an LLMs agent utilizing an RL training\npipeline as a tool.\n• Development of a two-level hierarchical framework ca-\npable of determining which actions in a task should be\nlearned with RL.\n• Pioneering work as the first to incorporate high-level GPT-\ncoded actions into the RL action space, enhancing the\nsample efficiency for RL.\nObservation\nAction\nenv.action_space\ndef n_attack(env, times = 20):\nfor i in range(times):\nact[5] = 3 # choose attack\nyield act\nAction Space Design\nPolicy Network\nFigure 2. To learn a subtask, the LLM can generate environment\nconfigurations (task, observation, reward, and action space) to\ninstantiate RL. In particular, by reasoning about the agent behavior\nto solve the subtask, the LLM generates code to provide higher-level\nactions in addition to the original environment actions, improving\nthe sample efficiency for RL.\n2. Related Works\n2.1. Agents in Minecraft\nMinecraft, a widely popular open-world sandbox game,\nstands as a formidable benchmark for constructing effi-\ncient and generalized agents. Previous endeavors resort\nto hierarchical reinforcement learning, often relying on hu-\nman demonstrations to facilitate the training of low-level\npolicies [12, 19]. Efforts such as MineAgent [7], Steve-\n1 [23], and VPT [3] leverage large-scale pre-training via\nYouTube videos to enhance policy training efficiency. How-\never, MineAgent and Steve-1 are limited to completing only\na few short-term tasks, and others [3, 50] still require a sub-\nstantial number of steps for finetuning on long-horizon tasks.\nDreamerV3 [13] utilizes a world model to expedite explo-\nration but still demands a substantial number of interactions\nto acquire diamonds. These existing approaches either ne-\ncessitate extensive expert datasets for training or exhibit low\nsample efficiency when addressing long-horizon tasks in the\nMinecraft environment.\nAn alternative research direction employs Large Lan-\nguage Models (LLMs) for task decomposition and high-level\nplanning to address intricate challenges. Certain works [37]\nleverage few-shot prompting with Codex [5] to generate exe-\ncutable policies. DEPS [42] and GITM [55] investigate the\nuse of LLMs as high-level planners in the Minecraft context.\nSome works [38, 41, 51] further explore LLMs for high-level\nplanning, code generation, lifelong exploration, and creative\ntasks. Other studies [8, 54] delve into grounding smaller\nlanguage models for control with domain-specific finetuning.\nNevertheless, these methods often rely on manually designed\n2\ncontrollers or code interfaces, sidestepping the challenge of\nlearning low-level policies.\nPlan4MC [49] integrates LLM-based planning and RL-\nbased policy learning but requires defining and pre-training\nall the policies with manually specified environments. Our\nRL-GPT extends LLMs’ ability in low-level control by equip-\nping it with RL, achieving automatic and efficient task learn-\ning in Minecraft.\n2.2. LLMs Agents\nSeveral works leverage LLMs to generate subgoals for robot\nplanning [2, 16]. Works like Inner Monologue [17] incorpo-\nrate environmental feedback into robot planning with LLMs.\nCode-as-Policies [21] and ProgPrompt [32] directly utilize\nLLMs to formulate executable robot policies. VIMA [18]\nand PaLM-E [6] involve fine-tuning pre-trained LLMs to\nsupport multimodal prompts. Besides, Chameleon [24] ef-\nfectively executes sub-task decomposition and generates\nsequential programs. ReAct [47] utilizes chain-of-thought\nprompting to generate task-specific actions. AutoGPT [10]\nautomates NLP tasks by integrating reasoning and acting\nloops. DERA [26] introduces dialogues between GPT-4 [1]\nagents. Generative Agents [28] simulate human behaviors\nby storing experiences as memories.\nCompared with existing works, RL-GPT equips the LLM\nagent with RL, extending its capability in intricate low-level\ncontrol in open-world tasks.\n2.3. Integrating LLMs and RL\nSince LLMs and RL possess complementary abilities in pro-\nviding prior knowledge and exploring unknown information,\nit is promising to integrate them for efficient task learning.\nMost work studies improve RL with the domain knowl-\nedge in LLMs. SayCan [2] and Plan4MC [49] decompose\nand plan subtasks with LLMs, thereby RL can learn eas-\nier subtasks to solve the whole task. Recent works [20, 25,\n43, 48] studies generating reward functions with LLMs to\nimprove the sample efficiency for RL. Another line of re-\nsearch [31, 33, 34, 44, 45, 52, 53] finetunes LLMs with RL\nto acquire the lacked ability of LLMs in low-level control.\nHowever, these approaches usually require a lot of samples\nand can harm the LLMs’ abilities in other tasks. Our study\nis the first to overcome the inabilities of LLMs in low-level\ncontrol by equipping them with RL as a tool. The acquired\nknowledge is stored in context, thereby continually improv-\ning the LLMs skills and maintaining its capability.\n3. Methods\nRL-GPT incorporates three distinct components, each con-\ntributing to its innovative design: (1) a slow agent tasked\nwith decomposing a given task into several sub-actions and\ndetermining which actions can be directly coded, (2) a fast\nagent responsible for writing code and instantiating RL con-\nfiguration, and (3) an iteration mechanism that facilitates an\niterative process refining both the slow agent and the fast\nagent. This iterative process enhances the overall efficacy\nof the RL-GPT across successive iterations. For complex\nlong-horizon tasks requiring multiple neural networks, we\nemploy a GPT-4 as a planner to initially decompose the task.\nAs discussed in concurrent works [22, 36], segregating\nhigh-level planning and low-level actions into distinct agents\nhas proven to be beneficial. The dual-agent system effec-\ntively narrows down the specific task of each agent, enabling\noptimization for specific targets. Moreover, Liang et al. high-\nlighted the Degeneration-of-Thought (DoT) problem, where\nan LLM becomes overly confident in its responses and lacks\nthe ability for self-correction through self-reflection. Empir-\nical evidence indicates that agents with different roles and\nperspectives can foster divergent thinking, mitigating the\nDoT problem. External feedback from other agents guides\nthe LLM, making it less susceptible to DoT and promoting\naccurate reasoning.\n3.1. RL Interface\nAs previously mentioned, we view the RL training pipeline\nas a tool accessible to LLMs agents, akin to other tools with\ncallable interfaces. Summarizing the interfaces of an RL\ntraining pipeline, we identify the following components: 1)\nLearning task; 2) Environment reset; 3) Observation space;\n4) Action space; 5) Reward function. Specifically, our fo-\ncus lies on studying interfaces 1) and 4) to demonstrate the\npotential for integrating RL and Code-as-policy.\nIn the case of the action space interface, we enable LLMs\nto design high-level actions and integrate them into the ac-\ntion space. A dedicated token is allocated for this purpose,\nallowing the neural network to learn when to utilize this\naction based on observations.\n3.2. Slow Agent: Action Planning\nAssume the task T needs to be learned by a single network\nwithin constrained computing resources. We employ a GPT-\n4 [1] as a slow agent AS. AS is tasked with decomposing\nT into sub-actions αi, where i ∈{0, ..., n}, determining if\neach αi in T can be directly addressed through code imple-\nmentation. This approach optimally allocates computational\nresources to address more challenging sub-tasks using RL.\nImportantly, AS is not required to perform any low-level\ncoding tasks; it solely provides high-level textual instruc-\ntions regarding sub-actions αi. These instructions are then\ntransmitted to the fast agent AF for further processing. The\niterative process of the slow agent involves systematically\nprobing the limits of coding capabilities.\nFor instance, in Fig. 3, consider the specific action of\ncrafting a wooden pickaxe. Although AS is aware that play-\ners need to harvest a log, writing code for this task with a\n3\nPlan\nSub-actions\nRL?\ncode or\nTrain\nNavigate to find\nSub-Action ①\nPolicy Network\nObservation\nAction\ndef craft_w_table(goal):\nfor act in chain(\nplace_down('crafting_table’),\ncraft_wo_table(goal),\nrecycle('crafting_table', 200),\nyield act\nCode Implementation\nEnvironment\nFeedback\nHarvest\nSub-Action ②\nCraft\nwith\nSub-Action ③\ndef n_attack(env, times = 20):\nfor i in range(times):\nact[5] = 3\nyield act\nAction Space Design\nRL Implementation\nDirect Code Implementation\nWhether to implement Sub-action\nEnvironment\nEnvironment\nInteract\nFast\nAgent\nFast\nAgent\nSlow\nAgent\nwith\nTemporal \nAbstraction\nCraft\nObjective\nEnvironment Feedback\nHigh-level Action Feedback\nSub-Action ①: …\nSub-Action ②: …\nSub-Action ③: …\nFigure 3. Overview of RL-GPT. The overall framework consists of a slow agent (orange) and a fast agent (green). The slow agent decomposes\nthe task and determines “which actions” to learn. The fast agent writes code and RL configurations for low-level execution.\nFast\nAgent\nSlow\nAgent\nCritic\nAgent\nEnvironment\nSub-Act ①: …\nSub-Act ②: …\nSub-Act ③: …\nAction Code\nObservation\nbefore \/ after\nthe action\nFor Sub-Act ②\nCritic ②\nCritic ①\nCritic ②\nCritic ③\nFigure 4. The two-loop iteration. We design a method to optimize\nboth slow agent and fast agent with a critic agent.\nhigh success rate can be challenging. The limitation arises\nfrom the insufficient information available through APIs for\nAS to accurately locate and navigate to a tree. To overcome\nthis hurdle, an RL implementation becomes necessary. RL\naids AS in completing tasks by processing complex visual\ninformation and interacting with the environment through\ntrial and error. In contrast, some simple, straightforward\nactions like crafting something with a crafting table can be\ndirectly coded and executed.\nIt is crucial to instruct AS to identify sub-actions\nthat are too challenging for rule-based code imple-\nmentation.\nAs shown in Table 1, the prompt for\nAS\nincorporates role description {role description},\nthe\ngiven\ntask\nT,\nreference\ndocuments,\nenviron-\nment\nknowledge\n{minecraft knowledge},\nplanning\nheuristics {planning tips},\nand programming exam-\nples {programs}. To align AS with our goals, we include\nthe heuristic in the {planning tips}.\nThis heuristic\nencourages AS to further break down an action when coding\nproves challenging. This incremental segmentation aids AS\nin discerning what aspects can be coded. Further details are\navailable in Appendix A.\n3.3. Fast Agent: Code-as-Policy and RL\nThe fast agent AF is also implemented using GPT-4. The pri-\nmary task is to translate the instructions from the slow agent\nAS into Python codes for the sub-actions αi. AF undergoes\na debug iteration where it runs the generated sub-action code\nand endeavors to self-correct through feedback from the en-\nvironment. Sub-actions that can be addressed completely\nwith code implementation are directly executed, as depicted\nin the blue segments of Fig. 3. For challenging sub-actions\nlacking clear starting conditions, the code is integrated into\nthe RL implementation using the temporal abstraction tech-\nnique [29, 35], as illustrated in Fig. 2. This involves inserting\nthe high-level action into the RL action space, akin to the or-\nange segments in Fig. 3. AF iteratively corrects itself based\non the feedback received from the environment.\n3.4. Two-loop Iteration\nIn Fig. 4, we devise the two-loop iteration mechanism to\noptimize the proposed two agents, namely the fast agent AF\nand the slow agent AS. To facilitate it, a critic agent C is\nintroduced, which could be implemented using GPT-3.5 or\nGPT-4.\nThe optimization for the fast agent, as shown in Fig. 4,\naligns with established methods for code-as-policy agents.\nHere, the fast agent receives a sub-action, environment docu-\nments Denv (observation and action space), and examples\nEcode as input, generating Python code. It then iteratively\n4\n{role description}\nIt is difficult to code all actions in this game. We only want to\ncode as many sub-actions as possible. The task of you is to tell\nme which sub-actions can be coded by you with Python.\nAt each round of conversation, I will give you\nTask: T\nContext: ...\nCritique: The results of the generated codes in the last round\nHere are some actions coded by humans:\n{programs}\nYou should then respond to me with\nExplain (if applicable): Why these actions can be coded by\npython? Are there any actions difficult to code?\nActions can be coded: List all actions that can be coded by\nyou.\nImportant Tips:\n{planning tips}\nYou should only respond in the format as described below:\nExplain: ...\nActions can be coded:\n1) Action1: ...\n2) Action2: ...\n3) ...\nTable 1. Slow Agent’s prompt: Decompose a task into sub-actions.\nrefines the code based on environmental feedback. The ob-\njective is to produce error-free Python-coded sub-actions\nthat align with the targets set by the slow agent. Feedback,\nwhich includes execution errors and critiques from C, plays\na crucial role in this process. C evaluates the coded action’s\nsuccess by considering observations before and after the\naction’s execution, offering insights for improvement.\nWithin Fig. 4, the iteration of the slow agent AS en-\ncompasses the aforementioned fast agent AF iteration as a\nstep. In each step of AS, AF must complete an iteration\nloop. Given a task T, Denv, and Ecode, AS decomposes T\ninto sub-actions αi and refines itself based on C’s outputs.\nSpecifically, it receives a sequence of outputs Critici from\nC about each αi to assess the effectiveness of action plan-\nning. If certain actions cannot be coded by the fast agent,\nthe slow agent adjusts the action planning accordingly.\n3.5. Task Planner\nOur primary pipeline is tailored for tasks that can be learned\nusing a neural network within limited computational re-\nsources. However, for intricate tasks such as ObtainDia-\nmond, where it is more effective to train multiple neural\nnetworks like DEPS [42] and Plan4MC [49], we introduce\n{role description}\nHere are some basic actions coded by humans:\n{programs template}\nPlease inherit the class CodeAgent. You are only required to\noverwrite the function main function.\nHere are some reference examples written by me:\n{programs example}\nHere are the attributes of the obs that can be used:\n{obs info}\nHere are the guidelines of the act variable:\n{act info}\nAt each round of conversation, I will give you\nTask: ...\nContext: ...\nCode from the last round: ...\nExecution error: ...\nCritique: ...\nYou should then respond to me with\nExplain (if applicable): Can the code complete the given ac-\ntion? What does the chat log and execution error imply?\nYou should only respond in the format as described below:\n{code format}\nTable 2. Fast Agent’s prompt: Write Python codes.\na task planner reminiscent of DEPS, implemented using\nGPT-4. This task planner iteratively reasons what needs\nto be learned and organizes sub-tasks for our RL-GPT to\naccomplish.\n4. Experiments\n4.1. Environment\nMineDojo\nMineDojo [7] stands out as a pioneering frame-\nwork developed within the renowned Minecraft game, tai-\nlored specifically for research involving embodied agents.\nThis innovative framework comprises a simulation suite fea-\nturing thousands of tasks, blending both open-ended chal-\nlenges and those prompted by language. To validate the ef-\nfectiveness of our approach, we selected certain long-horizon\ntasks from MineDojo, mirroring the strategy employed in\nPlan4MC [49]. These tasks include harvesting and crafting\nactivities. For instance, Crafting one wooden pickaxe re-\nquires the agent to harvest a log, craft planks, craft sticks,\ncraft tables, and craft the pickaxe with the table. Similarly,\ntasks like milking a cow involve the construction of a bucket,\napproaching the cow, and using the bucket to obtain milk.\n5\nTable 3. Comparison of different methods on several tasks in the MineDojo benchmark. Our RL-GPT achieves the highest success rate on\nall tasks.\nTASK\nMINEAGENT\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n-\n-\n-\n-\nMINEAGENT (AUTOCRAFT)\n0.00\n0.03\n0.00\n0.00\n0.00\n0.46\n0.50\n0.33\n0.35\n0.00\nPLAN4MC\n0.30\n0.30\n0.53\n0.37\n0.17\n0.83\n0.53\n0.43\n0.33\n0.17\nRL-GPT\n0.65\n0.65\n0.67\n0.67\n0.64\n0.85\n0.56\n0.46\n0.38\n0.32\nTable 4. Main results in the challenging ObtainDiamond\ntask\nin Minecraft. Existing strong baselines in ObtainDiamond either\nrequire expert data (VPT, DEPS), hand-crafted policies (DEPS-\nOracle) for subtasks, or take huge number of environment steps to\ntrain (DreamerV3, VPT). Our method can automatically decom-\npose and learn subtasks with only a little human prior, achieving\nObtainDiamond with great sample efficiency.\nMETHOD\nTYPE\nSAMPLES\nSUCCESS\nDREAMERV3\nRL\n100M\n2%\nVPT\nIL+RL\n16.8B\n20%\nDEPS-BC\nIL+LLM\n--\n0.6%\nDEPS-ORACLE\nLLM\n--\n60%\nPLAN4MC\nRL+LLM\n7M\n0%\nRL-GPT\nRL+LLM\n3M\n8%\nObtainDiamond Challenge\nIt represents a classic chal-\nlenge for RL agents. The task of obtaining a diamond de-\nmands the agent to complete the comprehensive process of\nharvesting a diamond from the beginning. This constitutes\na long-horizon task, involving actions such as harvesting\nlogs, harvesting stones, crafting items, digging to find iron,\nsmelting iron, locating a diamond, and so on.\n4.2. Implementation Details\nLLM Prompt\nWe choose GPT-4 as our LLMs API. For\nthe slow agents and fast agents, we design special templates,\nresponding formats, and examples. We design some special\nprompts such as “assume you are an experienced RL re-\nsearcher that is designing the RL training job for Minecraft”.\nDetails can be found in the Appendix A. In addition, we\nencourage the slow agent to explore more strategies because\nthe RL task requires more exploring. We encourage the slow\nagent to further decompose the action into sub-actions which\nmay be easier to code.\nPPO Details\nSimilar to MineAgent [7], we employ Prox-\nimal Policy Optimization (PPO) [30] as the RL baseline.\nThis approach alternates between sampling data through in-\nteractions with the environment and optimizing a ”surrogate”\nobjective function using stochastic gradient ascent. PPO is\nconstrained to a limited set of skills. When applying PPO\nwith sparse rewards, specific tasks such as “milk a cow” and\n“shear a sheep” present challenges due to the small size of\nthe target object relative to the scene, and the low probability\nof random encounters. To address this, we introduce basic\ndense rewards to enhance learning efficacy in these tasks. It\nincludes the CLIP Reward, which encourages the agent to\nexhibit behaviors that align with the prompt [7]. Addition-\nally, we incorporate a Distance Reward that provides dense\nreward signals to reach the target items [49]. Further details\ncan be found in the appendix.\n4.3. Main Results\nMineDojo Benchmark\nTable 3 presents a comparative\nanalysis between our RL-GPT and several baselines on se-\nlected MineDojo tasks. Notably, RL-GPT achieves the high-\nest success rate among all baselines. All baselines underwent\ntraining with 10 million samples, and the checkpoint with\nthe highest success rate was chosen for testing.\nMineAgent, as proposed in [7], combines PPO with Clip\nReward. However, naive PPO encounters difficulties in\nlearning long-horizon tasks, such as crafting a bucket and\nobtaining milk from a cow, resulting in an almost 0% suc-\ncess rate for MineAgent across all tasks. Another baseline,\nMineAgent with autocraft, as suggested in Plan4MC [49],\nincorporates crafting actions manually coded by humans.\nThis alternative baseline achieves a 46% success rate on\nthe milking task, demonstrating the importance of code-as-\npolicy. Our approach demonstrates superiority in coding\nactions beyond crafting, enabling us to achieve higher over-\nall performance compared to baselines that focus primarily\non crafting actions.\nPlan4MC [49] breaks down the problem into two es-\nsential components: acquiring fundamental skills and plan-\nning based on these skills. While some skills are acquired\nthrough Reinforcement Learning (RL), Plan4MC outper-\nforms MineAgent due to its reliance on an oracle task de-\ncomposition from the GPT planner. However, it cannot\nmodify the action space of an RL training pipeline or flex-\nibly decompose sub-actions. It is restricted to only three\ntypes of human-designed coded actions. Consequently, our\n6\n+\nà\nCode: Navigate to find\nRL-GPT \n(iter-3)\n58%\nSuccess\nRate\n+\n…\nCode: Navigate to find\nRL-GPT \n(iter-1)\n18%\nSuccess\nRate\n…\n…\nCode: Harvest a\nRL-GPT \n(iter-0)\n0%\nSuccess\nRate\n…\n…\nRL\nMineAgent\n(RL)\n10%\nSuccess\nRate\nRL          : Cut a\nRL          : Cut a\nRL           + Code: Attack 20 times\n+ Code: Aim the tree + Attack 20 times\n✅\nFigure 5. Demonstrations of how different agents learn to harvest a log. While both RL agent and LLM agent learn a single type of solution\n(RL or code-as-policy), our RL-GPT can reasonably decompose the task and correct how to learn each sub-action through the slow iteration\nprocess. RL-GPT decomposes the task into “find a tree” and “cut a log”, solving the former with code generation and the latter with RL.\nAfter a few iterations, it learns to provide RL with a necessary high-level action (attack 20 times) and completes the task with a high success\nrate. Best viewed by zooming in.\nmethod holds a distinct advantage in this context.\nIn tasks involving\nand\n, the agent is tasked with\ncrafting a stick from scratch, necessitating the harvesting of\na log. Our RL-GPT adeptly codes three actions for this: 1)\nNavigate to find a tree; 2) Attack 20 times; 3) Craft items.\nNotably, Action 2) can be seamlessly inserted into the ac-\ntion space. In contrast, Plan4MC is limited to coding craft\nactions only. This key distinction contributes to our method\nachieving higher scores in these tasks.\nTo arrive at the optimal code planning solution, RL-GPT\nundergoes a minimum of three iterations. As illustrated\nin Fig. 5, in the initial iteration, RL-GPT attempts to code\nevery action involved in harvesting a log, yielding a 0%\nsuccess rate. After the first iteration, it decides to code navi-\ngation, aiming at the tree, and attacking 20 times. However,\naiming at the tree proves too challenging for LLMs. As\nmentioned before, the agent will be instructed to further de-\ncompose the actions and give up difficult actions. By the\nthird iteration, the agent correctly converges to the optimal\nsolution—coding navigation and attacking, while leaving\nthe rest to RL, resulting in higher performance.\nIn tasks involving crafting a wooden pickaxe\nand craft-\ning a bed\n, in addition to the previously mentioned actions,\nthe agent needs to utilize the crafting table. While Plan4MC\nmust learn this process, our method can directly code actions\nto place the crafting table on the ground, use it, and recycle\nit. Code-as-policy contributes to our method achieving a\nhigher success rate in these tasks.\nIn tasks involving crafting a furnace\nand a stone pick-\naxe\n, in addition to the previously mentioned actions, the\nagent is further required to harvest stones. Plan4MC needs to\nlearn an RL network to acquire the skill of attacking stones.\nRL-GPT proposes two potential solutions for coding addi-\ntional actions. First, it can code to continuously attack a\nstone and insert this action into the action space. Second,\nsince LLMs understand that stones are underground, the\nagent might choose to dig deep for several levels to obtain\nstones instead of navigating on the ground to find stones.\nIn tasks involving crafting a milk bucket\nand crafting\nwool\n, the primary challenge lies in crafting a bucket or\nshears. Since both RL-GPT and Plan4MC can code actions\nto craft without a crafting table, their performance is similar\nand comparable.\nIn tasks involving obtaining beef\nand obtaining mut-\nton\n, the only actions that can be further coded are navigat-\ning to find the target. Given that both RL-GPT and Plan4MC\ncan code actions to navigate, their performance in these tasks\nis similar.\nObtainDiamond Challenge\nAs shown in Tab. 4, we com-\npare our method with existing competitive methods on the\n7\nTable 5. Ablation study on the necessity of the proposed compo-\nnents in RL-GPT.\nSTRUCTURE\nONE AGENT\n0.34\n0.42\nSLOW + FAST\n0.52\n0.56\nSLOW + FAST + CRITIC\n0.65\n0.67\nTable 6. Ablation study on the effectiveness of our two-loop itera-\ntion strategy. RL-GPT achieves better results when the number of\niterations increases.\nMETHOD\nPURE RL\n0.00\n0.00\n0.00\n0.00\nPURE CODE\n0.13\n0.02\n0.00\n0.00\nOURS (ZERO-SHOT)\n0.26\n0.53\n0.79\n0.32\nOURS (ITER-2 W\/O SP)\n0.26\n0.53\n0.79\n0.30\nOURS (ITER-2)\n0.56\n0.67\n0.88\n0.30\nOURS (ITER-3)\n0.65\n0.67\n0.93\n0.32\nTable 7. Ablation study on the RL interface: reward and action\nspace design.\nRL INTERFACE\nSUCCESS RATE ↑\nDEAD LOOP ↓\nREWARD FUNCTION\n0.418\n≈0.6\nACTION SPACE\n0.585\n≈0.3\nchallenging ObtainDiamond task.\nDreamerV3 [13] leverages a world model to accelerate\nexploration but still requires a significant number of interac-\ntions. Despite the considerable expense of over 100 million\nsamples for learning, it only achieves a 2% success rate on\nthe Diamond task from scratch.\nVPT [3] employs large-scale pre-training using YouTube\nvideos to improve policy training efficiency. This strong\nbaseline is trained on 80 GPUs for 6 days, achieving a 20%\nsuccess rate in obtaining a diamond and a 2.5% success rate\nin crafting a diamond pickaxe.\nDEPS [42] suggests generating training data using a com-\nbination of GPT and human handcrafted code for planning\nand imitation learning. It attains a 0.6% success rate on this\ntask. Moreover, an oracle version, which directly executes\nhuman-written codes, achieves a 60% success rate.\nPlan4MC [49] primarily focuses on crafting the stone\npickaxe. Even with the inclusion of all human-designed\nactions from DEPS, it requires more than 7 million samples\nfor training.\nOur RL-GPT attains an over 8% success rate in the Ob-\ntainDiamond challenge by generating Python code and train-\ning a PPO RL neural network. Despite requiring some\nhuman-written code examples, our approach uses consid-\nerably fewer than DEPS. The final coded actions involve\nnavigating on the ground, crafting items, digging to a spe-\ncific level, and exploring the underground horizontally.\n4.4. Ablation Study\nWe present ablation studies on our core designs in Tab. 5,\nTab. 6, and Tab. 7, covering the framework structure, two-\nloop iteration, and RL interface.\nFramework Structure\nIn Tab. 5, we analyze the impact of\nthe framework structure in RL-GPT, specifically examining\ndifferent task assignments for various agents. Assigning\nall tasks to a single agent results in confusion due to the\nmultitude of requirements, leading to a mere 0.34% success\nrate in crafting a table. Additionally, comparing the 3rd\nand 4th rows emphasizes the crucial role of a critic agent\nin our pipeline. Properly assigning tasks to the fast, slow,\nand critic agents can improve the performance to 0.65%.\nThe slow agent faces difficulty in independently judging the\nsuitability of actions based solely on environmental feedback\nand observation. Incorporating a critic agent facilitates more\ninformed decision-making, especially when dealing with\ncomplex, context-dependent information.\nTwo-loop Iteration\nIn Tab. 6, we ablate the importance\nof our two-loop iteration. Our iteration is to balance RL\nand code-as-policy to explore the bound of GPT’s coding\nability. We can see that pure RL and pure code-as-policy\nonly achieve a low success rate on these chosen tasks. Our\nmethod can improve the results although there is no itera-\ntion (zero-shot). In these three iterations, it shows that the\nsuccessful rate increases. It proves that the two-loop itera-\ntion is a reasonable optimization choice. Qualitative results\ncan be found in Fig. 5. Besides, we also compare the re-\nsults with and without special prompts (SP) to encourage\nthe LLMs to further decompose actions when facing coding\ndifficulty. It shows that suitable prompts are also essential\nfor optimization.\nRL Interface\nRecent works [20, 25] explore the use of\nLLMs for RL reward design, presenting an alternative ap-\nproach to combining RL and code-as-policy. With slight\nmodifications, our fast agent can also generate code to de-\nsign the reward function. However, as previously analyzed,\nreconstructing the action space proves more efficient than\ndesigning the reward function, assuming LLMs understand\nthe necessary actions. Tab. 7 compares our method with the\nreward design approach, revealing that our method achieves\na higher average success rate and lower dead loop ratio on\nour selected MineDojo tasks.\n8\n5. Conclusion\nIn conclusion, we propose RL-GPT, a novel approach that\nintegrates Large Language Models (LLMs) and Reinforce-\nment Learning (RL) to empower LLMs agents on challeng-\ning tasks within complex, embodied environments. Our\ntwo-level hierarchical framework divides the task into high-\nlevel coding and low-level RL-based actions, leveraging the\nstrengths of both approaches. RL-GPT exhibits superior effi-\nciency compared to traditional RL methods and existing GPT\nagents, achieving remarkable performance in challenging\nMinecraft tasks.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/RL-GPT: Integrating Reinforcement Learning and Code-as-policy.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nRL-GPT: Integrating Reinforcement Learning and Code-as-policy\n```\n#### 2. 论文摘要\n```\nLarge Language Models (LLMs) have demonstrated proficiency in utilizing\nvarious tools by coding, yet they face limitations in handling intricate logic\nand precise control. In embodied tasks, high-level planning is amenable to\ndirect coding, while low-level actions often necessitate task-specific\nrefinement, such as Reinforcement Learning (RL). To seamlessly integrate both\nmodalities, we introduce a two-level hierarchical framework, RL-GPT, comprising\na slow agent and a fast agent. The slow agent analyzes actions suitable for\ncoding, while the fast agent executes coding tasks. This decomposition\neffectively focuses each agent on specific tasks, proving highly efficient\nwithin our pipeline. Our approach outperforms traditional RL methods and\nexisting GPT agents, demonstrating superior efficiency. In the Minecraft game,\nit rapidly obtains diamonds within a single day on an RTX3090. Additionally, it\nachieves SOTA performance across all designated MineDojo tasks.\n```\n\n#### 3. 论文全文\n```\nRL-GPT: Integrating Reinforcement Learning and Code-as-policy\nShaoteng Liu1, Haoqi Yuan3, Minda Hu1, Yanwei Li1,\nYukang Chen1, Shu Liu2, Zongqing Lu3,4, Jiaya Jia1,2\n1The Chinese University of Hong Kong 2SmartMore\n3School of Computer Science, Peking University\n4Beijing Academy of Artificial Intelligence\nhttps:\/\/sites.google.com\/view\/rl-gpt\/\nAbstract\nLarge Language Models (LLMs) have demonstrated pro-\nficiency in utilizing various tools by coding, yet they face\nlimitations in handling intricate logic and precise control.\nIn embodied tasks, high-level planning is amenable to di-\nrect coding, while low-level actions often necessitate task-\nspecific refinement, such as Reinforcement Learning (RL).\nTo seamlessly integrate both modalities, we introduce a two-\nlevel hierarchical framework, RL-GPT, comprising a slow\nagent and a fast agent. The slow agent analyzes actions\nsuitable for coding, while the fast agent executes coding\ntasks. This decomposition effectively focuses each agent on\nspecific tasks, proving highly efficient within our pipeline.\nOur approach outperforms traditional RL methods and ex-\nisting GPT agents, demonstrating superior efficiency. In\nthe Minecraft game, it rapidly obtains diamonds within a\nsingle day on an RTX3090. Additionally, it achieves SOTA\nperformance across all designated MineDojo tasks.\n1. Introduction\nBuilding agents to master tasks in open-world environments\nhas been a long-standing goal in AI research [6, 38, 40]. The\nemergence of Large Language Models (LLMs) has revital-\nized this pursuit, leveraging their expansive world knowledge\nand adept compositional reasoning capabilities [24, 39, 47].\nLLMs agents showcase proficiency in utilizing computer\ntools [10, 14], navigating search engines [11, 15], and even\noperating systems or applications [9, 46]. However, their\nperformance remains constrained in open-world embodied\nenvironments [10, 38]. Despite possessing “world knowl-\nedge” akin to a human professor, LLMs fall short when pitted\nagainst a child in a video game. The inherent limitation lies\nin LLMs’ adeptness at absorbing information but their in-\nability to practice skills within an environment. Proficiency\nGPT\nRL\nRL-GPT (Ours)\nOptimized\nNeural Network\nOptimized\nCoded Actions\nOptimized Actions + \nNeural Network\n1.9X\n6.7X\nFigure 1. The overview of RL-GPT. After the optimization in an\nenvironment, LLMs agents obtain optimized coded actions, RL\nachieves an optimized neural network, and our RL-GPT gets both\noptimized coded actions and neural networks. Our framework\nintegrates the coding parts and the learning parts.\nin activities such as playing a video game demands extensive\npractice, a facet not easily addressed by in-context learning,\nwhich exhibits a relatively low upper bound [10, 24, 47].\nConsequently, existing LLMs necessitate human interven-\ntion to define low-level skills or tools [38, 42].\nReinforcement Learning (RL), proven as an effective\nmethod for learning from interaction, holds promise in fa-\ncilitating LLMs to “practise”. One line of works grounds\nLLMs for open-world control through RL fine-tuning [4, 27,\n33, 44, 45, 53]. Nevertheless, this approach necessitates a\nsubstantial volume of domain-specific data, expert demon-\nstrations, and access to LLMs’ parameters, rendering it slow\nand resource-intensive in most scenarios. Given the mod-\nest learning efficiency, the majority of methods continue\nto operate within the realm of “word games” such as tone\nadjustment rather than tackling intricate embodied tasks.\nAddressing this challenge, we propose to integrate LLMs\n1\narXiv:2402.19299v1  [cs.AI]  29 Feb 2024\nand RL in a novel approach: Empower LLMs agents to use\nan RL training pipeline as a tool. We introduce RL-GPT, a\nframework designed to enhance LLMs with trainable mod-\nules for learning interaction tasks within an environment.\nAs shown in Fig. 3, RL-GPT comprises an agent pipeline\nfeaturing multiple LLMs, wherein the neural network is\nconceptualized as a tool for training the RL pipeline. Illus-\ntrated in Fig. 1, unlike conventional approaches where LLMs\nagents and RL optimize coded actions and networks sepa-\nrately, RL-GPT unifies this optimization process. The line\nchart in Fig. 1 illustrates that RL-GPT outperforms alterna-\ntive approaches on the “harvest a log” task in MineDojo [7].\nWe further point out that the pivotal issue in using RL\nis to decide: Which actions should be learned with RL?\nTo tackle this, RL-GPT is meticulously designed to assign\ndifferent actions to RL and Code-as-policy, respectively. Our\nagent pipeline entails two fundamental steps. Firstly, LLMs\nshould determine “which actions” to code, involving task\ndecomposition into distinct sub-actions and deciding which\nactions can be effectively coded. Actions falling outside\nthis realm will be learned through RL. Secondly, LLMs are\ntasked with writing accurate codes for the “coded actions”\nand test them in the environment.\nWe employ a two-level hierarchical framework to realize\nthe two steps, as depicted in Fig. 3. Allocating these steps\nto two independent agents proves highly effective, as it nar-\nrows down the scope of each LLM’s task. Coded actions\nwith explicit starting conditions are executed sequentially,\nwhile other coded actions are integrated into the RL action\nspace. This strategic insertion into the action space em-\npowers LLMs to make pivotal decisions during the learning\nprocess. Illustrated in Fig. 2, this integration enhances the\nefficiency of learning tasks, exemplified by our ability to\nmore effectively learn how to break a tree.\nFor intricate tasks such as the ObtainDiamond task in the\nMinecraft game, devising a strategy with a single neural net-\nwork proves challenging due to limited computing resources.\nIn response, we incorporate a task planner to facilitate task\ndecomposition. Our RL-GPT framework demonstrates re-\nmarkable efficiency in tackling complex embodied tasks.\nSpecifically, within the MineDojo environment, it attains\nstate-of-the-art performance on the majority of selected tasks\nand adeptly obtains diamonds within a single day, utilizing\nonly an RTX3090 GPU.\nOur contributions are summarized as follows:\n• Introduction of an LLMs agent utilizing an RL training\npipeline as a tool.\n• Development of a two-level hierarchical framework ca-\npable of determining which actions in a task should be\nlearned with RL.\n• Pioneering work as the first to incorporate high-level GPT-\ncoded actions into the RL action space, enhancing the\nsample efficiency for RL.\nObservation\nAction\nenv.action_space\ndef n_attack(env, times = 20):\nfor i in range(times):\nact[5] = 3 # choose attack\nyield act\nAction Space Design\nPolicy Network\nFigure 2. To learn a subtask, the LLM can generate environment\nconfigurations (task, observation, reward, and action space) to\ninstantiate RL. In particular, by reasoning about the agent behavior\nto solve the subtask, the LLM generates code to provide higher-level\nactions in addition to the original environment actions, improving\nthe sample efficiency for RL.\n2. Related Works\n2.1. Agents in Minecraft\nMinecraft, a widely popular open-world sandbox game,\nstands as a formidable benchmark for constructing effi-\ncient and generalized agents. Previous endeavors resort\nto hierarchical reinforcement learning, often relying on hu-\nman demonstrations to facilitate the training of low-level\npolicies [12, 19]. Efforts such as MineAgent [7], Steve-\n1 [23], and VPT [3] leverage large-scale pre-training via\nYouTube videos to enhance policy training efficiency. How-\never, MineAgent and Steve-1 are limited to completing only\na few short-term tasks, and others [3, 50] still require a sub-\nstantial number of steps for finetuning on long-horizon tasks.\nDreamerV3 [13] utilizes a world model to expedite explo-\nration but still demands a substantial number of interactions\nto acquire diamonds. These existing approaches either ne-\ncessitate extensive expert datasets for training or exhibit low\nsample efficiency when addressing long-horizon tasks in the\nMinecraft environment.\nAn alternative research direction employs Large Lan-\nguage Models (LLMs) for task decomposition and high-level\nplanning to address intricate challenges. Certain works [37]\nleverage few-shot prompting with Codex [5] to generate exe-\ncutable policies. DEPS [42] and GITM [55] investigate the\nuse of LLMs as high-level planners in the Minecraft context.\nSome works [38, 41, 51] further explore LLMs for high-level\nplanning, code generation, lifelong exploration, and creative\ntasks. Other studies [8, 54] delve into grounding smaller\nlanguage models for control with domain-specific finetuning.\nNevertheless, these methods often rely on manually designed\n2\ncontrollers or code interfaces, sidestepping the challenge of\nlearning low-level policies.\nPlan4MC [49] integrates LLM-based planning and RL-\nbased policy learning but requires defining and pre-training\nall the policies with manually specified environments. Our\nRL-GPT extends LLMs’ ability in low-level control by equip-\nping it with RL, achieving automatic and efficient task learn-\ning in Minecraft.\n2.2. LLMs Agents\nSeveral works leverage LLMs to generate subgoals for robot\nplanning [2, 16]. Works like Inner Monologue [17] incorpo-\nrate environmental feedback into robot planning with LLMs.\nCode-as-Policies [21] and ProgPrompt [32] directly utilize\nLLMs to formulate executable robot policies. VIMA [18]\nand PaLM-E [6] involve fine-tuning pre-trained LLMs to\nsupport multimodal prompts. Besides, Chameleon [24] ef-\nfectively executes sub-task decomposition and generates\nsequential programs. ReAct [47] utilizes chain-of-thought\nprompting to generate task-specific actions. AutoGPT [10]\nautomates NLP tasks by integrating reasoning and acting\nloops. DERA [26] introduces dialogues between GPT-4 [1]\nagents. Generative Agents [28] simulate human behaviors\nby storing experiences as memories.\nCompared with existing works, RL-GPT equips the LLM\nagent with RL, extending its capability in intricate low-level\ncontrol in open-world tasks.\n2.3. Integrating LLMs and RL\nSince LLMs and RL possess complementary abilities in pro-\nviding prior knowledge and exploring unknown information,\nit is promising to integrate them for efficient task learning.\nMost work studies improve RL with the domain knowl-\nedge in LLMs. SayCan [2] and Plan4MC [49] decompose\nand plan subtasks with LLMs, thereby RL can learn eas-\nier subtasks to solve the whole task. Recent works [20, 25,\n43, 48] studies generating reward functions with LLMs to\nimprove the sample efficiency for RL. Another line of re-\nsearch [31, 33, 34, 44, 45, 52, 53] finetunes LLMs with RL\nto acquire the lacked ability of LLMs in low-level control.\nHowever, these approaches usually require a lot of samples\nand can harm the LLMs’ abilities in other tasks. Our study\nis the first to overcome the inabilities of LLMs in low-level\ncontrol by equipping them with RL as a tool. The acquired\nknowledge is stored in context, thereby continually improv-\ning the LLMs skills and maintaining its capability.\n3. Methods\nRL-GPT incorporates three distinct components, each con-\ntributing to its innovative design: (1) a slow agent tasked\nwith decomposing a given task into several sub-actions and\ndetermining which actions can be directly coded, (2) a fast\nagent responsible for writing code and instantiating RL con-\nfiguration, and (3) an iteration mechanism that facilitates an\niterative process refining both the slow agent and the fast\nagent. This iterative process enhances the overall efficacy\nof the RL-GPT across successive iterations. For complex\nlong-horizon tasks requiring multiple neural networks, we\nemploy a GPT-4 as a planner to initially decompose the task.\nAs discussed in concurrent works [22, 36], segregating\nhigh-level planning and low-level actions into distinct agents\nhas proven to be beneficial. The dual-agent system effec-\ntively narrows down the specific task of each agent, enabling\noptimization for specific targets. Moreover, Liang et al. high-\nlighted the Degeneration-of-Thought (DoT) problem, where\nan LLM becomes overly confident in its responses and lacks\nthe ability for self-correction through self-reflection. Empir-\nical evidence indicates that agents with different roles and\nperspectives can foster divergent thinking, mitigating the\nDoT problem. External feedback from other agents guides\nthe LLM, making it less susceptible to DoT and promoting\naccurate reasoning.\n3.1. RL Interface\nAs previously mentioned, we view the RL training pipeline\nas a tool accessible to LLMs agents, akin to other tools with\ncallable interfaces. Summarizing the interfaces of an RL\ntraining pipeline, we identify the following components: 1)\nLearning task; 2) Environment reset; 3) Observation space;\n4) Action space; 5) Reward function. Specifically, our fo-\ncus lies on studying interfaces 1) and 4) to demonstrate the\npotential for integrating RL and Code-as-policy.\nIn the case of the action space interface, we enable LLMs\nto design high-level actions and integrate them into the ac-\ntion space. A dedicated token is allocated for this purpose,\nallowing the neural network to learn when to utilize this\naction based on observations.\n3.2. Slow Agent: Action Planning\nAssume the task T needs to be learned by a single network\nwithin constrained computing resources. We employ a GPT-\n4 [1] as a slow agent AS. AS is tasked with decomposing\nT into sub-actions αi, where i ∈{0, ..., n}, determining if\neach αi in T can be directly addressed through code imple-\nmentation. This approach optimally allocates computational\nresources to address more challenging sub-tasks using RL.\nImportantly, AS is not required to perform any low-level\ncoding tasks; it solely provides high-level textual instruc-\ntions regarding sub-actions αi. These instructions are then\ntransmitted to the fast agent AF for further processing. The\niterative process of the slow agent involves systematically\nprobing the limits of coding capabilities.\nFor instance, in Fig. 3, consider the specific action of\ncrafting a wooden pickaxe. Although AS is aware that play-\ners need to harvest a log, writing code for this task with a\n3\nPlan\nSub-actions\nRL?\ncode or\nTrain\nNavigate to find\nSub-Action ①\nPolicy Network\nObservation\nAction\ndef craft_w_table(goal):\nfor act in chain(\nplace_down('crafting_table’),\ncraft_wo_table(goal),\nrecycle('crafting_table', 200),\nyield act\nCode Implementation\nEnvironment\nFeedback\nHarvest\nSub-Action ②\nCraft\nwith\nSub-Action ③\ndef n_attack(env, times = 20):\nfor i in range(times):\nact[5] = 3\nyield act\nAction Space Design\nRL Implementation\nDirect Code Implementation\nWhether to implement Sub-action\nEnvironment\nEnvironment\nInteract\nFast\nAgent\nFast\nAgent\nSlow\nAgent\nwith\nTemporal \nAbstraction\nCraft\nObjective\nEnvironment Feedback\nHigh-level Action Feedback\nSub-Action ①: …\nSub-Action ②: …\nSub-Action ③: …\nFigure 3. Overview of RL-GPT. The overall framework consists of a slow agent (orange) and a fast agent (green). The slow agent decomposes\nthe task and determines “which actions” to learn. The fast agent writes code and RL configurations for low-level execution.\nFast\nAgent\nSlow\nAgent\nCritic\nAgent\nEnvironment\nSub-Act ①: …\nSub-Act ②: …\nSub-Act ③: …\nAction Code\nObservation\nbefore \/ after\nthe action\nFor Sub-Act ②\nCritic ②\nCritic ①\nCritic ②\nCritic ③\nFigure 4. The two-loop iteration. We design a method to optimize\nboth slow agent and fast agent with a critic agent.\nhigh success rate can be challenging. The limitation arises\nfrom the insufficient information available through APIs for\nAS to accurately locate and navigate to a tree. To overcome\nthis hurdle, an RL implementation becomes necessary. RL\naids AS in completing tasks by processing complex visual\ninformation and interacting with the environment through\ntrial and error. In contrast, some simple, straightforward\nactions like crafting something with a crafting table can be\ndirectly coded and executed.\nIt is crucial to instruct AS to identify sub-actions\nthat are too challenging for rule-based code imple-\nmentation.\nAs shown in Table 1, the prompt for\nAS\nincorporates role description {role description},\nthe\ngiven\ntask\nT,\nreference\ndocuments,\nenviron-\nment\nknowledge\n{minecraft knowledge},\nplanning\nheuristics {planning tips},\nand programming exam-\nples {programs}. To align AS with our goals, we include\nthe heuristic in the {planning tips}.\nThis heuristic\nencourages AS to further break down an action when coding\nproves challenging. This incremental segmentation aids AS\nin discerning what aspects can be coded. Further details are\navailable in Appendix A.\n3.3. Fast Agent: Code-as-Policy and RL\nThe fast agent AF is also implemented using GPT-4. The pri-\nmary task is to translate the instructions from the slow agent\nAS into Python codes for the sub-actions αi. AF undergoes\na debug iteration where it runs the generated sub-action code\nand endeavors to self-correct through feedback from the en-\nvironment. Sub-actions that can be addressed completely\nwith code implementation are directly executed, as depicted\nin the blue segments of Fig. 3. For challenging sub-actions\nlacking clear starting conditions, the code is integrated into\nthe RL implementation using the temporal abstraction tech-\nnique [29, 35], as illustrated in Fig. 2. This involves inserting\nthe high-level action into the RL action space, akin to the or-\nange segments in Fig. 3. AF iteratively corrects itself based\non the feedback received from the environment.\n3.4. Two-loop Iteration\nIn Fig. 4, we devise the two-loop iteration mechanism to\noptimize the proposed two agents, namely the fast agent AF\nand the slow agent AS. To facilitate it, a critic agent C is\nintroduced, which could be implemented using GPT-3.5 or\nGPT-4.\nThe optimization for the fast agent, as shown in Fig. 4,\naligns with established methods for code-as-policy agents.\nHere, the fast agent receives a sub-action, environment docu-\nments Denv (observation and action space), and examples\nEcode as input, generating Python code. It then iteratively\n4\n{role description}\nIt is difficult to code all actions in this game. We only want to\ncode as many sub-actions as possible. The task of you is to tell\nme which sub-actions can be coded by you with Python.\nAt each round of conversation, I will give you\nTask: T\nContext: ...\nCritique: The results of the generated codes in the last round\nHere are some actions coded by humans:\n{programs}\nYou should then respond to me with\nExplain (if applicable): Why these actions can be coded by\npython? Are there any actions difficult to code?\nActions can be coded: List all actions that can be coded by\nyou.\nImportant Tips:\n{planning tips}\nYou should only respond in the format as described below:\nExplain: ...\nActions can be coded:\n1) Action1: ...\n2) Action2: ...\n3) ...\nTable 1. Slow Agent’s prompt: Decompose a task into sub-actions.\nrefines the code based on environmental feedback. The ob-\njective is to produce error-free Python-coded sub-actions\nthat align with the targets set by the slow agent. Feedback,\nwhich includes execution errors and critiques from C, plays\na crucial role in this process. C evaluates the coded action’s\nsuccess by considering observations before and after the\naction’s execution, offering insights for improvement.\nWithin Fig. 4, the iteration of the slow agent AS en-\ncompasses the aforementioned fast agent AF iteration as a\nstep. In each step of AS, AF must complete an iteration\nloop. Given a task T, Denv, and Ecode, AS decomposes T\ninto sub-actions αi and refines itself based on C’s outputs.\nSpecifically, it receives a sequence of outputs Critici from\nC about each αi to assess the effectiveness of action plan-\nning. If certain actions cannot be coded by the fast agent,\nthe slow agent adjusts the action planning accordingly.\n3.5. Task Planner\nOur primary pipeline is tailored for tasks that can be learned\nusing a neural network within limited computational re-\nsources. However, for intricate tasks such as ObtainDia-\nmond, where it is more effective to train multiple neural\nnetworks like DEPS [42] and Plan4MC [49], we introduce\n{role description}\nHere are some basic actions coded by humans:\n{programs template}\nPlease inherit the class CodeAgent. You are only required to\noverwrite the function main function.\nHere are some reference examples written by me:\n{programs example}\nHere are the attributes of the obs that can be used:\n{obs info}\nHere are the guidelines of the act variable:\n{act info}\nAt each round of conversation, I will give you\nTask: ...\nContext: ...\nCode from the last round: ...\nExecution error: ...\nCritique: ...\nYou should then respond to me with\nExplain (if applicable): Can the code complete the given ac-\ntion? What does the chat log and execution error imply?\nYou should only respond in the format as described below:\n{code format}\nTable 2. Fast Agent’s prompt: Write Python codes.\na task planner reminiscent of DEPS, implemented using\nGPT-4. This task planner iteratively reasons what needs\nto be learned and organizes sub-tasks for our RL-GPT to\naccomplish.\n4. Experiments\n4.1. Environment\nMineDojo\nMineDojo [7] stands out as a pioneering frame-\nwork developed within the renowned Minecraft game, tai-\nlored specifically for research involving embodied agents.\nThis innovative framework comprises a simulation suite fea-\nturing thousands of tasks, blending both open-ended chal-\nlenges and those prompted by language. To validate the ef-\nfectiveness of our approach, we selected certain long-horizon\ntasks from MineDojo, mirroring the strategy employed in\nPlan4MC [49]. These tasks include harvesting and crafting\nactivities. For instance, Crafting one wooden pickaxe re-\nquires the agent to harvest a log, craft planks, craft sticks,\ncraft tables, and craft the pickaxe with the table. Similarly,\ntasks like milking a cow involve the construction of a bucket,\napproaching the cow, and using the bucket to obtain milk.\n5\nTable 3. Comparison of different methods on several tasks in the MineDojo benchmark. Our RL-GPT achieves the highest success rate on\nall tasks.\nTASK\nMINEAGENT\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n-\n-\n-\n-\nMINEAGENT (AUTOCRAFT)\n0.00\n0.03\n0.00\n0.00\n0.00\n0.46\n0.50\n0.33\n0.35\n0.00\nPLAN4MC\n0.30\n0.30\n0.53\n0.37\n0.17\n0.83\n0.53\n0.43\n0.33\n0.17\nRL-GPT\n0.65\n0.65\n0.67\n0.67\n0.64\n0.85\n0.56\n0.46\n0.38\n0.32\nTable 4. Main results in the challenging ObtainDiamond\ntask\nin Minecraft. Existing strong baselines in ObtainDiamond either\nrequire expert data (VPT, DEPS), hand-crafted policies (DEPS-\nOracle) for subtasks, or take huge number of environment steps to\ntrain (DreamerV3, VPT). Our method can automatically decom-\npose and learn subtasks with only a little human prior, achieving\nObtainDiamond with great sample efficiency.\nMETHOD\nTYPE\nSAMPLES\nSUCCESS\nDREAMERV3\nRL\n100M\n2%\nVPT\nIL+RL\n16.8B\n20%\nDEPS-BC\nIL+LLM\n--\n0.6%\nDEPS-ORACLE\nLLM\n--\n60%\nPLAN4MC\nRL+LLM\n7M\n0%\nRL-GPT\nRL+LLM\n3M\n8%\nObtainDiamond Challenge\nIt represents a classic chal-\nlenge for RL agents. The task of obtaining a diamond de-\nmands the agent to complete the comprehensive process of\nharvesting a diamond from the beginning. This constitutes\na long-horizon task, involving actions such as harvesting\nlogs, harvesting stones, crafting items, digging to find iron,\nsmelting iron, locating a diamond, and so on.\n4.2. Implementation Details\nLLM Prompt\nWe choose GPT-4 as our LLMs API. For\nthe slow agents and fast agents, we design special templates,\nresponding formats, and examples. We design some special\nprompts such as “assume you are an experienced RL re-\nsearcher that is designing the RL training job for Minecraft”.\nDetails can be found in the Appendix A. In addition, we\nencourage the slow agent to explore more strategies because\nthe RL task requires more exploring. We encourage the slow\nagent to further decompose the action into sub-actions which\nmay be easier to code.\nPPO Details\nSimilar to MineAgent [7], we employ Prox-\nimal Policy Optimization (PPO) [30] as the RL baseline.\nThis approach alternates between sampling data through in-\nteractions with the environment and optimizing a ”surrogate”\nobjective function using stochastic gradient ascent. PPO is\nconstrained to a limited set of skills. When applying PPO\nwith sparse rewards, specific tasks such as “milk a cow” and\n“shear a sheep” present challenges due to the small size of\nthe target object relative to the scene, and the low probability\nof random encounters. To address this, we introduce basic\ndense rewards to enhance learning efficacy in these tasks. It\nincludes the CLIP Reward, which encourages the agent to\nexhibit behaviors that align with the prompt [7]. Addition-\nally, we incorporate a Distance Reward that provides dense\nreward signals to reach the target items [49]. Further details\ncan be found in the appendix.\n4.3. Main Results\nMineDojo Benchmark\nTable 3 presents a comparative\nanalysis between our RL-GPT and several baselines on se-\nlected MineDojo tasks. Notably, RL-GPT achieves the high-\nest success rate among all baselines. All baselines underwent\ntraining with 10 million samples, and the checkpoint with\nthe highest success rate was chosen for testing.\nMineAgent, as proposed in [7], combines PPO with Clip\nReward. However, naive PPO encounters difficulties in\nlearning long-horizon tasks, such as crafting a bucket and\nobtaining milk from a cow, resulting in an almost 0% suc-\ncess rate for MineAgent across all tasks. Another baseline,\nMineAgent with autocraft, as suggested in Plan4MC [49],\nincorporates crafting actions manually coded by humans.\nThis alternative baseline achieves a 46% success rate on\nthe milking task, demonstrating the importance of code-as-\npolicy. Our approach demonstrates superiority in coding\nactions beyond crafting, enabling us to achieve higher over-\nall performance compared to baselines that focus primarily\non crafting actions.\nPlan4MC [49] breaks down the problem into two es-\nsential components: acquiring fundamental skills and plan-\nning based on these skills. While some skills are acquired\nthrough Reinforcement Learning (RL), Plan4MC outper-\nforms MineAgent due to its reliance on an oracle task de-\ncomposition from the GPT planner. However, it cannot\nmodify the action space of an RL training pipeline or flex-\nibly decompose sub-actions. It is restricted to only three\ntypes of human-designed coded actions. Consequently, our\n6\n+\nà\nCode: Navigate to find\nRL-GPT \n(iter-3)\n58%\nSuccess\nRate\n+\n…\nCode: Navigate to find\nRL-GPT \n(iter-1)\n18%\nSuccess\nRate\n…\n…\nCode: Harvest a\nRL-GPT \n(iter-0)\n0%\nSuccess\nRate\n…\n…\nRL\nMineAgent\n(RL)\n10%\nSuccess\nRate\nRL          : Cut a\nRL          : Cut a\nRL           + Code: Attack 20 times\n+ Code: Aim the tree + Attack 20 times\n✅\nFigure 5. Demonstrations of how different agents learn to harvest a log. While both RL agent and LLM agent learn a single type of solution\n(RL or code-as-policy), our RL-GPT can reasonably decompose the task and correct how to learn each sub-action through the slow iteration\nprocess. RL-GPT decomposes the task into “find a tree” and “cut a log”, solving the former with code generation and the latter with RL.\nAfter a few iterations, it learns to provide RL with a necessary high-level action (attack 20 times) and completes the task with a high success\nrate. Best viewed by zooming in.\nmethod holds a distinct advantage in this context.\nIn tasks involving\nand\n, the agent is tasked with\ncrafting a stick from scratch, necessitating the harvesting of\na log. Our RL-GPT adeptly codes three actions for this: 1)\nNavigate to find a tree; 2) Attack 20 times; 3) Craft items.\nNotably, Action 2) can be seamlessly inserted into the ac-\ntion space. In contrast, Plan4MC is limited to coding craft\nactions only. This key distinction contributes to our method\nachieving higher scores in these tasks.\nTo arrive at the optimal code planning solution, RL-GPT\nundergoes a minimum of three iterations. As illustrated\nin Fig. 5, in the initial iteration, RL-GPT attempts to code\nevery action involved in harvesting a log, yielding a 0%\nsuccess rate. After the first iteration, it decides to code navi-\ngation, aiming at the tree, and attacking 20 times. However,\naiming at the tree proves too challenging for LLMs. As\nmentioned before, the agent will be instructed to further de-\ncompose the actions and give up difficult actions. By the\nthird iteration, the agent correctly converges to the optimal\nsolution—coding navigation and attacking, while leaving\nthe rest to RL, resulting in higher performance.\nIn tasks involving crafting a wooden pickaxe\nand craft-\ning a bed\n, in addition to the previously mentioned actions,\nthe agent needs to utilize the crafting table. While Plan4MC\nmust learn this process, our method can directly code actions\nto place the crafting table on the ground, use it, and recycle\nit. Code-as-policy contributes to our method achieving a\nhigher success rate in these tasks.\nIn tasks involving crafting a furnace\nand a stone pick-\naxe\n, in addition to the previously mentioned actions, the\nagent is further required to harvest stones. Plan4MC needs to\nlearn an RL network to acquire the skill of attacking stones.\nRL-GPT proposes two potential solutions for coding addi-\ntional actions. First, it can code to continuously attack a\nstone and insert this action into the action space. Second,\nsince LLMs understand that stones are underground, the\nagent might choose to dig deep for several levels to obtain\nstones instead of navigating on the ground to find stones.\nIn tasks involving crafting a milk bucket\nand crafting\nwool\n, the primary challenge lies in crafting a bucket or\nshears. Since both RL-GPT and Plan4MC can code actions\nto craft without a crafting table, their performance is similar\nand comparable.\nIn tasks involving obtaining beef\nand obtaining mut-\nton\n, the only actions that can be further coded are navigat-\ning to find the target. Given that both RL-GPT and Plan4MC\ncan code actions to navigate, their performance in these tasks\nis similar.\nObtainDiamond Challenge\nAs shown in Tab. 4, we com-\npare our method with existing competitive methods on the\n7\nTable 5. Ablation study on the necessity of the proposed compo-\nnents in RL-GPT.\nSTRUCTURE\nONE AGENT\n0.34\n0.42\nSLOW + FAST\n0.52\n0.56\nSLOW + FAST + CRITIC\n0.65\n0.67\nTable 6. Ablation study on the effectiveness of our two-loop itera-\ntion strategy. RL-GPT achieves better results when the number of\niterations increases.\nMETHOD\nPURE RL\n0.00\n0.00\n0.00\n0.00\nPURE CODE\n0.13\n0.02\n0.00\n0.00\nOURS (ZERO-SHOT)\n0.26\n0.53\n0.79\n0.32\nOURS (ITER-2 W\/O SP)\n0.26\n0.53\n0.79\n0.30\nOURS (ITER-2)\n0.56\n0.67\n0.88\n0.30\nOURS (ITER-3)\n0.65\n0.67\n0.93\n0.32\nTable 7. Ablation study on the RL interface: reward and action\nspace design.\nRL INTERFACE\nSUCCESS RATE ↑\nDEAD LOOP ↓\nREWARD FUNCTION\n0.418\n≈0.6\nACTION SPACE\n0.585\n≈0.3\nchallenging ObtainDiamond task.\nDreamerV3 [13] leverages a world model to accelerate\nexploration but still requires a significant number of interac-\ntions. Despite the considerable expense of over 100 million\nsamples for learning, it only achieves a 2% success rate on\nthe Diamond task from scratch.\nVPT [3] employs large-scale pre-training using YouTube\nvideos to improve policy training efficiency. This strong\nbaseline is trained on 80 GPUs for 6 days, achieving a 20%\nsuccess rate in obtaining a diamond and a 2.5% success rate\nin crafting a diamond pickaxe.\nDEPS [42] suggests generating training data using a com-\nbination of GPT and human handcrafted code for planning\nand imitation learning. It attains a 0.6% success rate on this\ntask. Moreover, an oracle version, which directly executes\nhuman-written codes, achieves a 60% success rate.\nPlan4MC [49] primarily focuses on crafting the stone\npickaxe. Even with the inclusion of all human-designed\nactions from DEPS, it requires more than 7 million samples\nfor training.\nOur RL-GPT attains an over 8% success rate in the Ob-\ntainDiamond challenge by generating Python code and train-\ning a PPO RL neural network. Despite requiring some\nhuman-written code examples, our approach uses consid-\nerably fewer than DEPS. The final coded actions involve\nnavigating on the ground, crafting items, digging to a spe-\ncific level, and exploring the underground horizontally.\n4.4. Ablation Study\nWe present ablation studies on our core designs in Tab. 5,\nTab. 6, and Tab. 7, covering the framework structure, two-\nloop iteration, and RL interface.\nFramework Structure\nIn Tab. 5, we analyze the impact of\nthe framework structure in RL-GPT, specifically examining\ndifferent task assignments for various agents. Assigning\nall tasks to a single agent results in confusion due to the\nmultitude of requirements, leading to a mere 0.34% success\nrate in crafting a table. Additionally, comparing the 3rd\nand 4th rows emphasizes the crucial role of a critic agent\nin our pipeline. Properly assigning tasks to the fast, slow,\nand critic agents can improve the performance to 0.65%.\nThe slow agent faces difficulty in independently judging the\nsuitability of actions based solely on environmental feedback\nand observation. Incorporating a critic agent facilitates more\ninformed decision-making, especially when dealing with\ncomplex, context-dependent information.\nTwo-loop Iteration\nIn Tab. 6, we ablate the importance\nof our two-loop iteration. Our iteration is to balance RL\nand code-as-policy to explore the bound of GPT’s coding\nability. We can see that pure RL and pure code-as-policy\nonly achieve a low success rate on these chosen tasks. Our\nmethod can improve the results although there is no itera-\ntion (zero-shot). In these three iterations, it shows that the\nsuccessful rate increases. It proves that the two-loop itera-\ntion is a reasonable optimization choice. Qualitative results\ncan be found in Fig. 5. Besides, we also compare the re-\nsults with and without special prompts (SP) to encourage\nthe LLMs to further decompose actions when facing coding\ndifficulty. It shows that suitable prompts are also essential\nfor optimization.\nRL Interface\nRecent works [20, 25] explore the use of\nLLMs for RL reward design, presenting an alternative ap-\nproach to combining RL and code-as-policy. With slight\nmodifications, our fast agent can also generate code to de-\nsign the reward function. However, as previously analyzed,\nreconstructing the action space proves more efficient than\ndesigning the reward function, assuming LLMs understand\nthe necessary actions. Tab. 7 compares our method with the\nreward design approach, revealing that our method achieves\na higher average success rate and lower dead loop ratio on\nour selected MineDojo tasks.\n8\n5. Conclusion\nIn conclusion, we propose RL-GPT, a novel approach that\nintegrates Large Language Models (LLMs) and Reinforce-\nment Learning (RL) to empower LLMs agents on challeng-\ning tasks within complex, embodied environments. Our\ntwo-level hierarchical framework divides the task into high-\nlevel coding and low-level RL-based actions, leveraging the\nstrengths of both approaches. RL-GPT exhibits superior effi-\nciency compared to traditional RL methods and existing GPT\nagents, achieving remarkable performance in challenging\nMinecraft tasks.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | RL-GPT：将强化学习与代码策略相结合，提升LLM在复杂环境中的表现\n\n## 📌 背景痛点\/本文动机\n大型语言模型（LLMs）在利用各种工具进行编码方面表现出色，但在处理复杂逻辑和精确控制方面存在局限性。在具身任务中，高级规划可以直接通过编码实现，而低级动作通常需要特定任务的细化，例如强化学习（RL）。为了无缝集成这两种模式，本文提出了一种两级分层框架RL-GPT，包括慢速代理和快速代理。慢速代理分析适合编码的动作，而快速代理执行编码任务。这种分解有效地使每个代理专注于特定任务，在我们的管道中表现出高效率。本文的方法优于传统的RL方法和现有的GPT代理，展示了卓越的效率。在Minecraft游戏中，它在RTX3090上的一天内迅速获得钻石。此外，它在所有指定的MineDojo任务中实现了SOTA性能。\n\n## 🚀 核心方法\n💡 创新点1：引入LLM代理，利用RL训练流程作为工具，以增强LLM在环境中的交互任务学习能力。\n💡 创新点2：开发了一个两级分层框架，能够确定任务中哪些动作应该使用RL学习。\n💡 创新点3：首次将高级GPT编码动作纳入RL动作空间，提高了RL的样本效率。\n\n## 📈 实验结果\n在MineDojo基准测试中，RL-GPT在所有选定的任务中实现了最高的成功率。在Minecraft游戏中，RL-GPT在RTX3090上的一天内迅速获得钻石。此外，它在所有指定的MineDojo任务中实现了SOTA性能。\n\n## 💬 可借鉴之处\n本文提出的RL-GPT框架为LLMs在复杂环境中的任务学习提供了一种新的思路。通过将RL和代码策略相结合，RL-GPT能够有效地处理复杂逻辑和精确控制，从而在具身任务中取得卓越的性能。此外，本文提出的两级分层框架和迭代机制也为LLMs在环境中的任务学习提供了一种新的方法。","llm_summary_res_status":200}
{"title":"LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence","authors":"Zhuoling Li, Xiaogang Xu, Zhenhua Xu, SerNam Lim, Hengshuang Zhao","summary":"Recent embodied agents are primarily built based on reinforcement learning\n(RL) or large language models (LLMs). Among them, RL agents are efficient for\ndeployment but only perform very few tasks. By contrast, giant LLM agents\n(often more than 1000B parameters) present strong generalization while\ndemanding enormous computing resources. In this work, we combine their\nadvantages while avoiding the drawbacks by conducting the proposed referee RL\non our developed large auto-regressive model (LARM). Specifically, LARM is\nbuilt upon a lightweight LLM (fewer than 5B parameters) and directly outputs\nthe next action to execute rather than text. We mathematically reveal that\nclassic RL feedbacks vanish in long-horizon embodied exploration and introduce\na giant LLM based referee to handle this reward vanishment during training\nLARM. In this way, LARM learns to complete diverse open-world tasks without\nhuman intervention. Especially, LARM successfully harvests enchanted diamond\nequipment in Minecraft, which demands significantly longer decision-making\nchains than the highest achievements of prior best methods.","url":"http:\/\/arxiv.org\/abs\/2405.17424v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2405.17424v2","published":1716832772000,"comment":null,"pdf_text":"LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence\nZhuoling Li 1 Xiaogang Xu 2 Zhenhua Xu 1 SerNam Lim 3 Hengshuang Zhao 1\nhttps:\/\/lizhuoling.github.io\/LARM_webpage\/\nAbstract\nRecent embodied agents are primarily built based\non reinforcement learning (RL) or large language\nmodels (LLMs). Among them, RL agents are\nefficient for deployment but only perform very\nfew tasks. By contrast, giant LLM agents (of-\nten more than 1000B parameters) present strong\ngeneralization while demanding enormous com-\nputing resources. In this work, we combine their\nadvantages while avoiding the drawbacks by con-\nducting the proposed referee RL on our developed\nlarge auto-regressive model (LARM). Specifi-\ncally, LARM is built upon a lightweight LLM\n(fewer than 5B parameters) and directly outputs\nthe next action to execute rather than text. We\nmathematically reveal that classic RL feedbacks\nvanish in long-horizon embodied exploration and\nintroduce a giant LLM based referee to handle\nthis reward vanishment during training LARM.\nIn this way, LARM learns to complete diverse\nopen-world tasks without human intervention. Es-\npecially, LARM successfully harvests enchanted\ndiamond equipment in Minecraft, which demands\nsignificantly longer decision-making chains than\nthe highest achievements of prior best methods.\n1. Introduction\nIn recent years, remarkable progress has been achieved\nin various artificial intelligence (AI) topics (LeCun et al.,\n2015) like computer vision (He et al., 2016) and natural\nlanguage processing (Kenton & Toutanova, 2019), but most\nof them lack the capacity to physically interact with the real\nworld. To address this disconnect, the concept of embodied\nAI is introduced (Chrisley, 2003). Early embodied agents\nare predominantly developed on simulation platforms for\nspecific tasks such as object grasping and indoor naviga-\ntion (Savva et al., 2019). While notable advancements are\n1The University of Hong Kong 2The Chinese University of\nHong Kong 3University of Central Florida. Correspondence to:\nHengshuang Zhao <hszhao@cs.hku.hk>.\nachieved, these agents tend to be specialist models confined\nto isolated tasks (Huang et al., 2023). To overcome this limi-\ntation, recent studies, including this work, employ Minecraft\n(Baker et al., 2022; Fan et al., 2022; Guss et al., 2019) as\na benchmark to explore embodied agents with open-ended\nobjectives and long-horizon reasoning chains.\nThe early methods for developing such agents primarily rely\non reinforcement learning (RL) (Fan et al., 2022). Due to the\nlimited exploration efficiency of RL, these methods require\ncareful reward engineering for different tasks, and the de-\nrived RL policies can mostly only complete a single simple\ntask (Yuan et al., 2023). The advantage of RL policies is that\nthey are usually lightweight for real-time deployment. Dif-\nferently, recent embodied works begin to investigate large\nlanguage models (LLMs) (Brown et al., 2020). Owing to\nthe extensive general knowledge and formidable reasoning\ncapabilities of LLMs, these methods demonstrate promis-\ning results with significantly reduced domain-specific engi-\nneering efforts (Wang et al., 2023a). Nevertheless, LLMs\ncontinue to exhibit several limitations. First of all, the out-\nputs of LLMs are usually sentences or code (Zhao et al.,\n2023) generated through iterative token prediction, necessi-\ntating N inference operations for N tokens. Therefore, the\nresponse speeds of LLMs are restricted. Secondly, recent\nresearch suggests that a huge model size is important for\nan LLM to generalize well (Achiam et al., 2023), while the\ncomputing resource for embodied agents is usually very lim-\nited. Our analysis reveals that while giant LLMs with more\nthan 1000B parameters like GPT-4 (Achiam et al., 2023)\ncan answer questions about exploration and crafting issues\nin Minecraft well, the performance of lightweight LLMs\nsuch as LLaVA-7B (Liu et al., 2024a) is limited.\nAs illustrated in Fig. 1, we aim to combine the advantages\nof both RL methods and LLM methods while avoiding\ntheir drawbacks. To this end, we first propose Large Auto-\nRegressive Model (LARM), the main body of which shares\nthe same structure as lightweight LLMs like TinyLLaVA\n(Zhou et al., 2024). This choice enables us to first pre-train\nit utilizing numerous webpage data to provide it with ba-\nsic general knowledge. Taking environmental observation\nas input, LARM predicts the next action to perform in an\nauto-regressive manner. Instead of generating a descriptive\nsentence composed of multiple tokens, LARM directly pro-\n1\narXiv:2405.17424v2  [cs.CV]  5 Feb 2025\nSubmission and Formatting Instructions for ICML 2025\nduces a single token to select the next action, which makes\nLARM respond more swiftly than common LLMs.\nThe following problem is how to train LARM. We find that\nclassic RL algorithms cannot train LARM effectively and\nmathematically reveal this is because the reward feedback\ngradually vanishes in long-horizon embodied exploration.\nThis phenomenon can be empirically understood as even\nthough a policy selects the correct action, it obtains positive\nfeedback only after the target task is completed, meaning\nmany iterations of delay. In addition, any wrong decision-\nmaking in future iterations will cause the policy to get no\npositive reward, which hides the value of the current correct\naction. To handle this problem, we introduce referee RL.\nIts core idea is that we employ a referee (like a giant LLM)\nto provide immediate feedback about whether the just per-\nformed action brings positive contribution to realizing the\nfinal target. In this way, we efficiently distill the concerned\ngeneralizable knowledge of giant LLMs into our lightweight\nend-to-end LARM policy during online exploration with-\nout human supervision. This marks the first attempt that\noptimizes an LLM-style embodied policy through making\nit directly interact with the environment online.\nWe validate our method in both MineDojo (Fan et al., 2022)\nand Mineflayer (PrismarineJS., 2013) environments. The\nexperimental results suggest that our method completes di-\nverse challenging tasks with a single model, indicating its\npromising generalization. LARM achieves higher success\nrates than previous counterparts, although these counter-\nparts may employ a special network for each task. Notably,\nLARM is the first method that harvests enchanted diamond\nequipment in Minecraft. In addition, evaluated with an\nRTX4090 GPU, LARM runs with a speed of 0.58 second\nper inference, which meets the online inference requirement.\n2. Related Work\nMinecraft agents. Compared with other embodied bench-\nmarks, Minecraft is an open-ended platform suitable for\nexploring building agents with long-horizon planning ca-\npabilities (Fan et al., 2022). It simulates diverse weather,\nbiomes, and mobs in an unlimited 3D virtual world. Early\nmethods in Minecraft are mostly based on reinforcement\nlearning (Frazier & Riedl, 2019) or imitation learning (Baker\net al., 2022). Their model outputs are atom actions, e.g., a\nshort movement, mouse click, or keyboard press. However,\ndue to the huge decision space, such atom-based agents are\nquite challenging for optimization. Thus, these works pay\ntheir main attention to devising strategies for alleviating the\noptimization complexity (Scheller et al., 2020). An effective\npractice is devising the policy into a hierarchical architec-\nture, where a complex task is first decomposed into many\nsimple sub-tasks. Different models are trained for various\nsub-tasks and a leader model is built to decide the order of\nperforming these sub-tasks (Liu et al., 2024b).\nDue to its open-world characteristic, Minecraft is suitable\nfor exploring how to develop open-ended embodied intelli-\ngence (Feng et al., 2024). To concentrate on studying this\nproblem, there are plentiful works that take a skill (e.g.,\nchopping down a tree or crafting a table) as the basic model\noutput (Wang et al., 2023a). The skill could be modeled\nas a well-trained policy based on reinforcement learning\nor provided APIs. Among these works, LLM-based meth-\nods achieve the most impressive results thanks to their rich\ngeneral LLM knowledge (Achiam et al., 2023; Wang et al.,\n2023b), especially for giant LLMs with more than 1000B pa-\nrameters like GPT-4. However, due to the huge model sizes,\nthese LLMs can only be deployed in remote computing\nclusters. There are also works that try tuning a lightweight\nLLM like LLaMA using Minecraft relevant text and then\nprompting the tuned LLM to say what skill should be per-\nformed in inference (Feng et al., 2024). However, the text\nused for tuning contains much irrelevant information and\nis not specialized for task execution. As the embodied text\ntuning data volume is also limited, the tuned LLMs often\nfail to describe what skill should be performed correctly.\nLarge language models. LLMs draw broad attention from\nthe research and industrial communities due to their rich\ngeneral knowledge and the ability to generate the answers\nto diverse kinds of questions (Chang et al., 2024). GPT-\n3 emerges as a milestone in the evolution of LLMs, as it\ntakes the next token prediction problem as the pre-training\ntask and showcases remarkable open-world generalization\ncapabilities (Brown et al., 2020). Subsequently, the fine-\ntuning of GPT-3 using reinforcement learning with human\nfeedback leads to the creation of ChatGPT (OpenAI, 2023)\nand GPT-4 (Achiam et al., 2023). However, a significant\nlimitation of LLMs is their inability to interpret information\nin images, which are vital for humans to perceive the world.\nTo overcome this problem, researchers devise strategies that\ninject vision information into LLMs and enable LLMs to\nperceive images. A common method is fine-tuning a small\nnumber of network parameters using numerous language-\nimage data pairs to bridge the representation gap between\ntext and images (Ding et al., 2023). In this way, some large\nvision-language models like LLaVA (Liu et al., 2024a) and\nFlamingo (Alayrac et al., 2022) are derived.\nRL with LLMs in embodied AI. RL can search promis-\ning decision-making policies without human intervention,\nand LLMs are able to provide suitable search start points\nbased on their rich general knowledge (Rashidi & Nili Ah-\nmadabadi, 2024). Therefore, it is natural to design ways to\ncombine them to build advanced embodied intelligence. In\nprevious works, a popular choice is training a network for\neach basic skill based on RL, and then prompting the LLM\nto say what skill should be used according to the task target\n2\nSubmission and Formatting Instructions for ICML 2025\nMore Advanced Achievements \nRL Agent \nEfficient but Task \nspecialized \nLARM Agent (Ours) \nEfficient and \nGeneralizable \nLLM Agent \nGeneralizable but Slow \nCommon RL Agent \nAchievements\nCommon LLM Agent \nAchievements\nLARM Agent \nAchievements\nFigure 1. Comparison among agents based on RL, LLM, and LARM. As shown, RL agents are usually task specialized, and LLM agents\nare computationally expensive to deploy. By contrast, the LARM agent is efficient and generalizable. Besides, LARM presents better\nperformance. As shown, LARM is the first method that achieves enchanted diamond equipment in Minecraft.\nand environment observation (Wang et al., 2023b). Never-\ntheless, this paradigm is not only slow, it requires the LLM\nto have sufficient knowledge about Minecraft. According to\nour analysis, giant LLMs own such an ability but the capa-\nbilities of lightweight LLMs are limited. Another possible\nchoice is utilizing LLM to generate the code for calculating\nRL reward (Xie et al., 2023). Nevertheless, it is not always\nfeasible to define a reward function by writing code. For ex-\nample, in Minecraft, the information is represented as image\nand agent status information, which cannot be mapped as\nreward based on rules. To handle this problem, we propose\nto directly employ GPT-4 to read the agent status before\nand after executing a skill and judge whether the outcome\nbrought by this skill contributes to realizing the given target.\nIn addition, to the best of our knowledge, this is the first\nwork that directly optimizes an LLM-style policy based on\nonline exploration and reinforcement learning. Our results\nsuggest that the rich general knowledge in LLM favors this\nexploration and self-learning process.\n3. Preliminary\n3.1. Problem Formulation\nWhat we study in this work can be conceptualized as an auto-\nregressive prediction problem involving long sequences,\nand is effectively framed as a Markov Decision Process\nsymbolized by a tuple E = (S, A, P, T , R, γ, τ). Specif-\nically, S is the set of all potential states. A is the action\nset, and every action is also called as a skill in this work.\nP : S ×A×S →[0, 1] represents a probability distribution\nthat governs the state transitions given states and actions.\nT is the set of all task targets. R : S →R, γ, and τ de-\nnote the reward function, discount factor, and initial state\ndistribution, respectively. At any discrete time step t, the\nenvironment resides in a state st ∈S, and the correspond-\ning observation ot by a policy π is a function of this state,\nexpressed as ot = f(st). This observation ot is then utilized\nto select the subsequent action according to at ∼π(ot, ι),\nwhere at ∈A and ι ∈T denotes the given target task.\nIn tackling the studied long-horizon embodied task, the ob-\njective is to navigate through a sequence of intermediate\nstates τ, s1, s2, . . . , sT −1 to ultimately reach the target state\nsT at the final time step T. This requires the policy to gener-\nate a series of actions a0, a1, . . . , aT −1 such that each action\nat transitions the environment from state st to the next state\nst+1 correctly, adhering to the dynamics prescribed by the\ntransition probability distribution P. It is crucial that each\nintermediate state st is accurately achieved in sequence to\nensure the policy attains the target state sT .\n3.2. PPO\nProximal Policy Optimization (PPO) (Schulman et al., 2017)\nis a model-free reinforcement learning algorithm widely\nadopted for training policies in complex environments, and\nour referee RL is developed based on this algorithm. A PPO\npolicy π mainly consists of two components, the actor πa\nand critic πc. πc is to estimate the value function Vθc(st),\nthe expected cumulative discounted reward starting from\nstate st and following π. The optimization objective of πc\nis as follows:\nLc\nθc = Et[(Vθc(st) −(rt + γVθc(st+1)))2],\n(1)\n3\nSubmission and Formatting Instructions for ICML 2025\nwhere rt ∈R is the reward received after taking action\nat in state st. To further reduce the variance of the value\nestimate and improve stability, PPO employs the generalized\nadvantage estimation (GAE), which is defined as:\nAt =\nT −1\nX\nk=t\n(γλ)k−tδk,\n(2)\nwhere λ is a factor balancing between temporal differ-\nence (TD) learning and Monte Carlo estimation, and δk\ndenotes the TD-error, which is formulated as δk = rk +\nγVθc(sk+1) −Vθc(sk). With At, the objective function for\nthe critic πa can be given as:\nLa\nθa = Et[ πa(at|st)\nπold\na (at|st)At],\n(3)\nwhere and πold\na\nis the old actor policy before weight update\nand πa(at|st) is the current actor. To improve the training\nstability, PPO further develops a clipped surrogate objective\nbased on Eq. (3), which can be formulated as:\n˜La\nθa = Et[min(ktAt, (kt, 1 −ϵ, 1 + ϵ)At)],\n(4)\nwhere kt =\nπa(at|st)\nπold\na\n(at|st) and ϵ is a small positive parameter.\nBy optimizing πc and πa with respect to Eq. (1) and Eq. (4),\nthe policy gradually learns to perform the target task.\n4. Method\n4.1. Referee Reinforcement Learning\nIn long-horizon embodied task exploration, the policy can\nusually only get positive feedback after the target task is\ncompleted successfully (Fan et al., 2022). Following the\nnotations in Section 3.1, we can assume that there is an\nexploration trajectory {(sk, ak, rk)}T\nk=t, where T is a large\ninteger, and sk, ak, and rk denote the state, action, and\nenvironment reward at the k step, respectively. The agent\ncompletes the target task at the final step T. Therefore, we\ncan get that:\nrk =\n(\n−ε, if k = t, t + 1, · · · , T −1\nR −ε, if k = T\n(5)\nwhere −ε denotes a small negative constant due to time\npenalty and R is the positive reward of completing the target\ntask. As we train the critic πc using the very imbalanced\nreward set {rk}T\nk=t described in Eq. (5) with respect to the\noptimization objective in Eq. (1), we can infer that the output\nof πc gradually converges to Vθc(sk) −γVθc(sk+1) ≈−ε.\nIn this way, the corresponding TD error set {δt}T −1\nt=1 is:\nδk ≈\n(\n0, if k = t, t + 1, · · · , T −2\nR, if k = T −1\n(6)\nAlgorithm 1 Referee RL\nRequire: Target task ι\n1: Initialize the actor πa, critic πc, referee πr\n2: Initialize policy exploration step T, policy update steps\nNπ\n3: for each iteration iter do\n4:\nInitialize data buffer B ←∅\n5:\nfor t = 1 to T do\n6:\nGet the actor observation ot ←f(st)\n7:\nGet state st+1 and environment reward rt by tak-\ning at ∼πθ(ot, ι)\n8:\nGet auxiliary reward brt ←πr(ι, st, at, at+1)\n9:\nAdd transition B ←B∪{(st, ot, at, st+1, rt, brt)}\n10:\nend for\n11:\nfor n = 1 to Nπ do\n12:\nSample\na\nrandom\ntraining\ndata\nbatch\n{(st, ot, at, st+1, rt, brt}B\nj=1 ∼B\n13:\nOptimize πa and with respect to Eq. (1)∼(4)\n14:\nend for\n15: end for\nAccording to the definition of GAE in Eq. (2), we can\nobserve that the first T −1 −t items are close to zero\nas the elements in {σk}T −2\nk=t are nearly zero. For the last\nitem (γλ)T −1−tδT −1, we have limT →∞(γλ)T −1−t →0\nbecause γ ∈(0, 1) and λ ∈(0, 1). Hence, when the task\nneeds long-horizon action chain execution, the obtained\nGAE value At is close to zero, which suggests that the opti-\nmization objective for training πa in Eq. (4) becomes zero.\nIn this way, even though the policy makes the right action\ndecision, it cannot get any positive feedback.\nTo handle this problem, we introduce referee RL. Specif-\nically, we introduce a referee πp to provide an auxiliary\nreward feedback to the trained policy π, and this reward at\nthe step k is represented as brk = πp(ι, sk, ak, sk+1). This\nmeans πp takes the target task information ι, initial state sk,\nselected action ak, and new state sk+1 as input and provides\nfeedback based on whether the selected action is correct\nand outcome caused by this action. In this work, we split\nthe feedback into four categories: (a) The selected action\nis correct and brings a positive outcome to realizing the\ntarget. (b) The selected action is correct but does not bring\na positive outcome. (c) The selected action is incorrect but\ndoes not lead to a negative outcome. (d) The selected action\nis incorrect and results in a negative outcome. For the four\ncategories, πp correspondingly returns the reward value ra,\nrb, rc, and rd, where ra > rb > 0 > rc > rd. By adding\nthis auxiliary reward brk to the original reward described in\nEq. (5), the feedback to π before T does not remain as the\nconstant −ε. In this way, the TD error δk and correspond-\ning GAE value At do not converge to zeros, being able to\nprovide effective optimization guidance to πa.\n4\nSubmission and Formatting Instructions for ICML 2025\nVision Observation\nText Observation\n🚩\nTask Description\nSkill Token\nEnvironment Exploration\nLoRA\nLLM Decoders\nActor Head\nCritic Head\nAction\nValue\nLARM Policy\nAgent\nEnvironment\nEnvironment Reward\nReferee\nAuxiliary \nReward\n👍👍👍\nTotal Reward\nReward Generation\nAgent State\nRL based Policy Optimization\nUpdate\nAgent Status: …\nFigure 2. The overall pipeline of our method. As illustrated, we parametrize the actor πa and critic πc using a single LARM model with\ntwo separate prediction heads, i.e., the action head and critic head. We train LARM based on our proposed referee RL algorithm, which\nutilizes both environment feedback and referee generated auxiliary reward to guide the optimization of LARM.\nIn this work, we model the referee πp based on GPT-4\n(Achiam et al., 2023), which is a giant LLM and owns\nextensive generalizable knowledge. As mentioned before,\nthe information provided to GPT-4 includes ι (target task\ndescription), ak (the executed skill), sk and sk+1 (the inven-\ntory list and environment resource surrounding the agent\nbefore and after executing ak), and then we prompt it to\njudge the situation and response a reward among ra, rb,\nrc, and rd. The full detailed procedure of the referee RL\nalgorithm is elaborated in Algorithm 1.\n4.2. Large Auto-Regressive Model\nIn this part, we explain how to parametrize πa and πc using\nour designed LARM policy. As shown in Fig. 2, the main\nbody of LARM is the decoders of a lightweight decoder-\nonly LLM, TinyLLaVA-3.1B in this work. The parameters\nof these decoders are frozen during training and a trainable\nLoRA (Hu et al., 2021) module is applied to help the model\nlearn new knowledge in the applied task domain. This\ndesign has two key benefits: (i) The model is initialized with\nthe general knowledge and reasoning ability of LLMs while\nmaintaining an acceptable parameter volume. In this way,\nLARM can be deployed based on the restricted computing\nresources in embodied applications and achieve real-time\nresponse. (ii) As LARM adopts a similar model architecture\nas LLMs, we can first pre-train it using numerous question-\nanswer data related to the concerned embodied AI topics.\nThis kind of data is much easier to collect and scale up\nthan embodied data with action execution. We have tried\npre-training LARM using a 34G webpage dataset crawled\nfrom Wiki (Fan et al., 2022), and the results indicate that\nthe training convergence is improved.\nThe input to the LARM model consists of four parts, i.e.,\ntask description, text observation, vision observation, and\na skill token. The task description specifies the target task\nto conduct. The text information primarily includes the in-\nventory list, historical action, and blocks surrounding the\nagent. The vision information is the real-time image per-\nceived by the agent. We encode text and image information\nas tokens based on CLIP (Radford et al., 2021), and these\ntokens with an additional learnable skill token are input to\nthe LARM decoders to conduct feature interaction. After\nthe decoders, the skill token is input to the action head and\ncritic head depicted in Fig. 2 to output the action and state\nvalue. Therefore, LARM parametrizes the actor πa and\ncritic πc in Section 4.1 based on a single model with two\ndifferent trainable prediction heads.\nSimilar to previous literature (Wang et al., 2023a; Liu et al.,\n2024b), the action predicted by LARM is a skill, such as\nchopping down a tree or searching for a cobblestone. LARM\nselects a skill to perform by conducting feature matching\nbetween the action head output and skill description like\nSTEVE (Zhao et al., 2023). In this work, we test LARM\nwith two kinds of skills, the RL-based skills in MineDojo\nand API-based skills based on Mineflayer. The former cat-\negory is easier to be extended to real-world applications\nand the latter class presents higher execution success rates.\n5\nSubmission and Formatting Instructions for ICML 2025\nAccording to the given task requirements, new skills can\nbe dynamically added based on the strategies developed in\nprevious works (Wang et al., 2023a) and how to generate\nthese skills is not the research focus of this work.\nCombining the aforementioned techniques, LARM is op-\ntimized by iterating between environment exploration and\npolicy update described in Algorithm 1. The training de-\ntails like the optimizer choice follow PPO (Schulman et al.,\n2017). For learning to complete the most challenging task\nin this work (craft an enchanted diamond tool), about 42\nhours of exploration is taken using a single RTX4090 GPU.\n5. Experiments\n5.1. Environment\nIn this work, we validate our method using the MineDojo\nand Mineflayer environments. Both these two environments\nare developed based on Minecraft, but there exist some\ndifferences in their testing protocols.\nMineDojo. MineDojo (Fan et al., 2022) is a pioneering\nbenchmark suitable for studying how to develop open-ended\nand generally capable embodied agents. It features a simula-\ntion suite with thousands of tasks specified through natural\nlanguage prompts. The behaviors that can be conducted in\nMineDojo primarily include navigation, harvest, combat,\nand craft. When testing a policy, an agent is randomly ini-\ntialized in a biome with some initial tools. The policy needs\nto control the agent to explore and harvest resources in the\nenvironment and gradually realize the given target. The ac-\ntion space in MineDojo is a compound multi-discrete space\nthat allows the agent to select a movement action or an op-\ntional functional action at each step, encompassing a diverse\nrange of arguments to facilitate complex interactions.\nMineflayer. Compared with MineDojo, the basic actions in\nMineflayer (PrismarineJS., 2013) are provided APIs, such\nas harvesting a log or finding the nearest stone. Compared\nwith MineDojo, these APIs help researchers concentrate\nmore on high-level decision making rather than repetitive\naction details. In this way, several significantly more ad-\nvanced achievements are obtained by previous works based\non Mineflayer, like harvesting diamonds with high success\nrates (Wang et al., 2023a). In addition, the agents are usually\nspawned without initial tools in Mineflayer.\n5.2. Main Results\nWe compare LARM with previous methods in this part. As\nthe basic actions in MineDojo and Mineflayer are different,\nwe conduct the comparison separately.\nComparison on MineDojo. The methods compared on\nMineDojo include MineAgent (Fan et al., 2022), Plan4MC\n(Yuan et al., 2023), LLaMA-Rider (Feng et al., 2024), and\nRL-GPT (Liu et al., 2024b). Among them, MineAgent is the\nbaseline method provided by Minddojo. It first fine-tunes\nCLIP (Radford et al., 2021) based on numerous web data\nand uses the fine-tuned CLIP to guide the training of rein-\nforcement learning algorithms. Plan4MC is a reinforcement\nlearning based method. It splits a task into basic skills and\ntrains an agent to learn them one by one in a hierarchical\nway. LLaMA-Rider is an LLM obtained by fine-tuning\nLLaMA. It first makes the agent explore the environment to\ncollect data. Then, the collected data is adopted to fine-tune\nLLaMA in a supervised manner. RL-GPT builds two LLM\nbased agents (a slow agent and a fast agent) to schedule the\nagent actions. For an action step, this method first queries\nthe agents whether this action step can be completed via\ncode generation. If the code generation is infeasible, an RL\nbased action is performed.\nWe compare LARM with these methods on diverse tasks,\nand the detailed settings of these tasks follow previous works\n(Feng et al., 2024). Notably, we train a single LARM model\nto complete all the tasks, which is different from many pre-\nvious works that employ separate models for various tasks.\nThis characteristic suggests the promising generalization\nability of LARM. To compute success rates, we test LARM\nfor 30 times on every task. The experimental results are re-\nported in Table 1. As shown, LARM presents higher success\nrates than the compared counterparts in all the test tasks, and\nthe promising performance of LARM is mainly attributed\nto our designed referee RL algorithm, which addresses the\nreward vanishment problem in long-horizon embodied ex-\nploration. In addition, we can observe that LARM achieves\nhigher success rates on tasks with shorter action chains,\nindicating the great challenge in developing long-horizon\nembodied intelligence. For example, the Harvest bucket\ntask requires three iron ingots, and the Harvest iron sword\ndemands two iron ingots and one stick. Therefore, Harvest\niron sword needs one more step (Harvest stick) than Harvest\nbucket, which causes that LARM obtains a higher success\nrate in the task of crafting a bucket.\nComparison on Mineflayer. To fully reveal the superiority\nof LARM, we further evaluate LARM using the Mineflayer\nbased environment. The compared methods include Auto-\nGPT (autogpt), Voyager (Wang et al., 2023a), and STEVE\n(Zhao et al., 2023). In this work, Voyager is a training-free\nmethod implemented based on GPT-4. Its main contribution\nis designing a multi-step prompt generation pipeline. When\na target task is given, Voyager prompts GPT-4 to know\nwhich skill should be executed and gradually realize the\ntarget. AutoGPT is an LLM being able to reason which skill\nshould be performed through multi-step question answering.\nSTEVE is a large vision-language model. In STEVE, a\ndataset including both videos and text-image pairs is gath-\nered and utilized to fine-tune LLaMA (Touvron et al., 2023),\nand then the fine-tuned model can invoke pre-defined skills.\n6\nSubmission and Formatting Instructions for ICML 2025\nTable 1. Performance comparison with previous methods based on MineDojo.\nTask\nMineAgent\nPlan4MC\nLLaMA-Rider Base\nLLaMA-Rider\nRL-GPT\nLARM (Ours)\nHarvest stick\n0.00\n0.30\n0.23\n0.43\n0.65\n0.93\nHarvest crafting table\n0.03\n0.30\n0.37\n0.67\n0.65\n0.87\nHarvest bowl\n0.00\n0.47\n0.73\n0.97\n-\n0.97\nHarvest chest\n0.00\n0.23\n0.67\n0.77\n-\n0.83\nHarvest wooden pickaxe\n0.00\n0.03\n0.00\n0.37\n0.67\n0.70\nHarvest wooden sword\n0.00\n0.47\n0.63\n0.10\n-\n0.70\nHarvest furnace\n0.00\n0.37\n0.00\n0.17\n0.67\n0.73\nHarvest stone stairs\n0.00\n0.47\n0.00\n0.57\n-\n0.67\nHarvest stone sword\n0.00\n0.10\n0.00\n0.00\n-\n0.40\nHarvest iron ingot\n0.00\n0.47\n0.03\n0.13\n-\n0.60\nHarvest bucket\n0.00\n0.20\n0.00\n0.00\n0.37\nHarvest iron sword\n0.00\n0.20\n0.00\n0.00\n-\n0.27\nHarvest beef\n0.33\n0.43\n0.03\n0.03\n0.46\n0.60\nHarvest mutton\n0.35\n0.33\n0.00\n0.03\n0.38\n0.63\nTable 2. Performance comparison based on Mineflayer.\nAchievement\nAutoGPT Voyager STEVE LARM (Ours)\nWooden sword\n3\/3\n3\/3\n3\/3\n30\/30\nStone sword\n3\/3\n3\/3\n3\/3\n30\/30\nIron sword\n3\/3\n3\/3\n3\/3\n30\/30\nDiamond sword\n0\/3\n1\/3\n3\/3\n28\/30\nEnchanted sword\n0\/3\n0\/3\n0\/3\n16\/30\nTable 3. Ablation Study on Reward Design.\nReward\nStick\nWooden\nStone\nIron\nER\n0.20\n0.13\n0.10\n0.00\nER+LAR\n0.30\n0.23\n0.13\n0.00\nER+AR2\n0.80\n0.53\n0.20\n0.07\nER+AR4\n0.93\n0.70\n0.40\n0.27\nIn this experiment, the agent is spawned in a random biome\nwithout initial inventory. The test tasks include Harvest\nwooden sword, Harvest stone sword, Harvest iron sword,\nHarvest diamond sword, and Harvest enchanted diamond\nsword. As shown in Fig. 1, Harvest enchanted diamond\nsword is significantly more complex than the other achieve-\nments. The previous methods usually test their models\nthree times in Mineflayer. To reduce randomness, we run\nLARM for 30 times. The experimental results are reported\nin Table 2. We can observe that LARM outperforms the\ncompared methods in obtaining different levels of achieve-\nments. Notably, LARM is the first method that harvests an\nenchanted diamond sword in Minecraft successfully.\n5.3. Ablation Study\nAnalysis on reward design. The key difference of Ref-\neree RL from the classic PPO implementation is the reward\ndesign. Therefore, we ablate different reward settings in\nthis part. We compare four reward choices in MineDojo\nwith four tasks, i.e., Harvest stick, Harvest wooden sword,\nHarvest stone sword, and Harvest iron sword. The four re-\nward choices are ER (environment reward only), ER+LAR\n(environment reward plus auxiliary reward produced by\nLLaVA-7B, a lightweight LLM), ER+AR2 (environment\nTable 4. Ablation Study on LLM base selection.\nLLM Base\nStick\nWooden\nStone\nIron\nTinyLLaVA-0.5B\n0.80\n0.50\n0.27\n0.13\nTinyLLaVA-3.1B\n0.83\n0.57\n0.33\n0.13\nTinyLLaVA-3.1B*\n0.93\n0.70\n0.40\n0.27\nreward plus auxiliary reward produced by GPT-4o, but the\nauxiliary reward is determined based on only whether the ac-\ntion outcome is positive), and ER+AR2 (the standard setting\nof our method described in this paper). The experimental\nresults are presented in Table 3.\nAs shown, when only the environment reward is provided,\nthe agent presents a significantly better success rate on Har-\nvest stick than Harvest stone sword, where the latter task\ndemands a longer action chain. This phenomenon is because\nthat the environment reward gradually vanishes with the in-\ncrease of the action chain length. When the auxiliary reward\nis generated by LLaVA-7B, the result is also poor, which is\nbecause that LLaVA-7B does not understand the Minecraft\nworld well and the provided reward is often incorrect. This\nresult confirms our aforementioned claim that only giant\nLLMs serve as referees well. In addition, the performance\nof ER+AR2 is worse than ER+AR4, which is because se-\nlecting the correct action does not always bring a positive\noutcome. For example, the agent decides to search a tree\nbut does not find it successfully. In this situation, ER+AR2\nreturns a negative reward, which punishes the searching a\ntree decision even though this choice may be correct. By\ncontrast, ER+AR4 will give a small positive reward.\nAnalysis on LLM base selection. As mentioned before,\nwe utilize the weight of a lightweight LLM to initialize\nthe parameters in LARM to provide initial general knowl-\nedge. Then, we fine-tune LARM using Minecraft related\nwebpage data to enhance its understanding. In this experi-\nment, we analyze how the lightweight LLM choice affects\nthe results and whether the webpage data pre-training de-\nscribed in Section 4.2 is beneficial. To this end, we first\ncompare the performances of LARM using TinyLLaVA-\n7\nSubmission and Formatting Instructions for ICML 2025\nEnter the Nether\nSearch a Village \nMulti-agent Combat\nt!\nt\"\nt#\nt$\nt!\nt\"\nt#\nt$\nt!\nt\"\nt#\nt$\nFigure 3. More behavior example illustrations of LARM, which include traveling a long distance to find a village, building a nether portal\nand then entering the nether, multiple agents collaborate with each other to combat zombies.\n0.5B and TinyLLaVA-3.1B as the LLM base, where the\nexperiment results are presented in the 1st and 2nd row of\nTable 4. As shown, adopting an LLM with more parameters\nas the LARM model base favors the embodied performance,\nwhich is mainly attributed to that a larger LLM has stronger\ngeneral knowledge. Then, we compare the performances of\nmodels without and with the webpage data pre-training, and\nthe results are reported in the 2nd and 3rd rows of Table 4. It\ncan be observed that webpage data pre-training improves the\nsuccess rates on embodied task execution. This result sug-\ngests that the numerous text and image data on the Internet\nhas the potential to benefit embodied agents.\nAnalysis on LLM capability. We have mentioned that\ngiant LLMs answer questions about the Minecraft world\nwell while the understanding of lightweight LLMs is poor.\nIn this part, we demonstrate this by providing a question-\nanswer example and highlighting the key content in bold:\nPrompt: In Minecraft, you need to craft a stone pickaxe.\nWhat additional resources do you need to gather if you have\nonly got cobblestones in your inventory?\nGPT-4o: If you already have cobblestone in your inventory,\nyou need to gather wooden planks to craft sticks. You need\ntwo sticks and three cobblestones to craft a stone pickaxe.\nLlama3-8B: You need a stick.\nTinyLLaVA-3.1B: To craft a stone pickaxe in Minecraft, you\nwill need the following additional resources: cobblestone,\nstone, wood, leaves, dirt, grass, pillar, shovel, and sword.\nTinyLLaVA-3.1B after Webpage data pre-training: You ad-\nditionally need two sticks.\nComparing the answers, we can observe that webpage data\npre-training significantly improves the concerned domain\nof knowledge in TinyLLaVA-3.1B.\n5.4. Case Study\nThe previous experiments mainly show the performances\nof LARM on harvesting various categories of materials\nand crafting tools. In Fig. 3, we visualize more examples\nof other behaviors in the Mineflayer based environment,\nsuch as exploring the open world, constructing a building\nwith a specific structure, and multiple agents cooperate to\ncombat dangerous creatures. These behaviors suggest that\nour proposed techniques in this work have the potential to\nbe further generalized to other diverse domains.\n6. Conclusion\nIn this work, we have proposed LARM, which is effi-\ncient and possesses general knowledge. To train it, we\nhave revealed the feedback vanishment problem in apply-\ning classic RLs to long-horizon embodied exploration. To\naddress this feedback vanishment, we have developed the\nreferee RL technique. By optimizing LARM with referee\n8\nSubmission and Formatting Instructions for ICML 2025\nRL, our method can learn to complete diverse embodied\ntasks without human supervision. Especially, LARM is the\nfirst method that obtains the enchanted diamond equipment\nachievement in the Minecraft benchmark successfully.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nLARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence\n```\n#### 2. 论文摘要\n```\nRecent embodied agents are primarily built based on reinforcement learning\n(RL) or large language models (LLMs). Among them, RL agents are efficient for\ndeployment but only perform very few tasks. By contrast, giant LLM agents\n(often more than 1000B parameters) present strong generalization while\ndemanding enormous computing resources. In this work, we combine their\nadvantages while avoiding the drawbacks by conducting the proposed referee RL\non our developed large auto-regressive model (LARM). Specifically, LARM is\nbuilt upon a lightweight LLM (fewer than 5B parameters) and directly outputs\nthe next action to execute rather than text. We mathematically reveal that\nclassic RL feedbacks vanish in long-horizon embodied exploration and introduce\na giant LLM based referee to handle this reward vanishment during training\nLARM. In this way, LARM learns to complete diverse open-world tasks without\nhuman intervention. Especially, LARM successfully harvests enchanted diamond\nequipment in Minecraft, which demands significantly longer decision-making\nchains than the highest achievements of prior best methods.\n```\n\n#### 3. 论文全文\n```\nLARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence\nZhuoling Li 1 Xiaogang Xu 2 Zhenhua Xu 1 SerNam Lim 3 Hengshuang Zhao 1\nhttps:\/\/lizhuoling.github.io\/LARM_webpage\/\nAbstract\nRecent embodied agents are primarily built based\non reinforcement learning (RL) or large language\nmodels (LLMs). Among them, RL agents are\nefficient for deployment but only perform very\nfew tasks. By contrast, giant LLM agents (of-\nten more than 1000B parameters) present strong\ngeneralization while demanding enormous com-\nputing resources. In this work, we combine their\nadvantages while avoiding the drawbacks by con-\nducting the proposed referee RL on our developed\nlarge auto-regressive model (LARM). Specifi-\ncally, LARM is built upon a lightweight LLM\n(fewer than 5B parameters) and directly outputs\nthe next action to execute rather than text. We\nmathematically reveal that classic RL feedbacks\nvanish in long-horizon embodied exploration and\nintroduce a giant LLM based referee to handle\nthis reward vanishment during training LARM.\nIn this way, LARM learns to complete diverse\nopen-world tasks without human intervention. Es-\npecially, LARM successfully harvests enchanted\ndiamond equipment in Minecraft, which demands\nsignificantly longer decision-making chains than\nthe highest achievements of prior best methods.\n1. Introduction\nIn recent years, remarkable progress has been achieved\nin various artificial intelligence (AI) topics (LeCun et al.,\n2015) like computer vision (He et al., 2016) and natural\nlanguage processing (Kenton & Toutanova, 2019), but most\nof them lack the capacity to physically interact with the real\nworld. To address this disconnect, the concept of embodied\nAI is introduced (Chrisley, 2003). Early embodied agents\nare predominantly developed on simulation platforms for\nspecific tasks such as object grasping and indoor naviga-\ntion (Savva et al., 2019). While notable advancements are\n1The University of Hong Kong 2The Chinese University of\nHong Kong 3University of Central Florida. Correspondence to:\nHengshuang Zhao <hszhao@cs.hku.hk>.\nachieved, these agents tend to be specialist models confined\nto isolated tasks (Huang et al., 2023). To overcome this limi-\ntation, recent studies, including this work, employ Minecraft\n(Baker et al., 2022; Fan et al., 2022; Guss et al., 2019) as\na benchmark to explore embodied agents with open-ended\nobjectives and long-horizon reasoning chains.\nThe early methods for developing such agents primarily rely\non reinforcement learning (RL) (Fan et al., 2022). Due to the\nlimited exploration efficiency of RL, these methods require\ncareful reward engineering for different tasks, and the de-\nrived RL policies can mostly only complete a single simple\ntask (Yuan et al., 2023). The advantage of RL policies is that\nthey are usually lightweight for real-time deployment. Dif-\nferently, recent embodied works begin to investigate large\nlanguage models (LLMs) (Brown et al., 2020). Owing to\nthe extensive general knowledge and formidable reasoning\ncapabilities of LLMs, these methods demonstrate promis-\ning results with significantly reduced domain-specific engi-\nneering efforts (Wang et al., 2023a). Nevertheless, LLMs\ncontinue to exhibit several limitations. First of all, the out-\nputs of LLMs are usually sentences or code (Zhao et al.,\n2023) generated through iterative token prediction, necessi-\ntating N inference operations for N tokens. Therefore, the\nresponse speeds of LLMs are restricted. Secondly, recent\nresearch suggests that a huge model size is important for\nan LLM to generalize well (Achiam et al., 2023), while the\ncomputing resource for embodied agents is usually very lim-\nited. Our analysis reveals that while giant LLMs with more\nthan 1000B parameters like GPT-4 (Achiam et al., 2023)\ncan answer questions about exploration and crafting issues\nin Minecraft well, the performance of lightweight LLMs\nsuch as LLaVA-7B (Liu et al., 2024a) is limited.\nAs illustrated in Fig. 1, we aim to combine the advantages\nof both RL methods and LLM methods while avoiding\ntheir drawbacks. To this end, we first propose Large Auto-\nRegressive Model (LARM), the main body of which shares\nthe same structure as lightweight LLMs like TinyLLaVA\n(Zhou et al., 2024). This choice enables us to first pre-train\nit utilizing numerous webpage data to provide it with ba-\nsic general knowledge. Taking environmental observation\nas input, LARM predicts the next action to perform in an\nauto-regressive manner. Instead of generating a descriptive\nsentence composed of multiple tokens, LARM directly pro-\n1\narXiv:2405.17424v2  [cs.CV]  5 Feb 2025\nSubmission and Formatting Instructions for ICML 2025\nduces a single token to select the next action, which makes\nLARM respond more swiftly than common LLMs.\nThe following problem is how to train LARM. We find that\nclassic RL algorithms cannot train LARM effectively and\nmathematically reveal this is because the reward feedback\ngradually vanishes in long-horizon embodied exploration.\nThis phenomenon can be empirically understood as even\nthough a policy selects the correct action, it obtains positive\nfeedback only after the target task is completed, meaning\nmany iterations of delay. In addition, any wrong decision-\nmaking in future iterations will cause the policy to get no\npositive reward, which hides the value of the current correct\naction. To handle this problem, we introduce referee RL.\nIts core idea is that we employ a referee (like a giant LLM)\nto provide immediate feedback about whether the just per-\nformed action brings positive contribution to realizing the\nfinal target. In this way, we efficiently distill the concerned\ngeneralizable knowledge of giant LLMs into our lightweight\nend-to-end LARM policy during online exploration with-\nout human supervision. This marks the first attempt that\noptimizes an LLM-style embodied policy through making\nit directly interact with the environment online.\nWe validate our method in both MineDojo (Fan et al., 2022)\nand Mineflayer (PrismarineJS., 2013) environments. The\nexperimental results suggest that our method completes di-\nverse challenging tasks with a single model, indicating its\npromising generalization. LARM achieves higher success\nrates than previous counterparts, although these counter-\nparts may employ a special network for each task. Notably,\nLARM is the first method that harvests enchanted diamond\nequipment in Minecraft. In addition, evaluated with an\nRTX4090 GPU, LARM runs with a speed of 0.58 second\nper inference, which meets the online inference requirement.\n2. Related Work\nMinecraft agents. Compared with other embodied bench-\nmarks, Minecraft is an open-ended platform suitable for\nexploring building agents with long-horizon planning ca-\npabilities (Fan et al., 2022). It simulates diverse weather,\nbiomes, and mobs in an unlimited 3D virtual world. Early\nmethods in Minecraft are mostly based on reinforcement\nlearning (Frazier & Riedl, 2019) or imitation learning (Baker\net al., 2022). Their model outputs are atom actions, e.g., a\nshort movement, mouse click, or keyboard press. However,\ndue to the huge decision space, such atom-based agents are\nquite challenging for optimization. Thus, these works pay\ntheir main attention to devising strategies for alleviating the\noptimization complexity (Scheller et al., 2020). An effective\npractice is devising the policy into a hierarchical architec-\nture, where a complex task is first decomposed into many\nsimple sub-tasks. Different models are trained for various\nsub-tasks and a leader model is built to decide the order of\nperforming these sub-tasks (Liu et al., 2024b).\nDue to its open-world characteristic, Minecraft is suitable\nfor exploring how to develop open-ended embodied intelli-\ngence (Feng et al., 2024). To concentrate on studying this\nproblem, there are plentiful works that take a skill (e.g.,\nchopping down a tree or crafting a table) as the basic model\noutput (Wang et al., 2023a). The skill could be modeled\nas a well-trained policy based on reinforcement learning\nor provided APIs. Among these works, LLM-based meth-\nods achieve the most impressive results thanks to their rich\ngeneral LLM knowledge (Achiam et al., 2023; Wang et al.,\n2023b), especially for giant LLMs with more than 1000B pa-\nrameters like GPT-4. However, due to the huge model sizes,\nthese LLMs can only be deployed in remote computing\nclusters. There are also works that try tuning a lightweight\nLLM like LLaMA using Minecraft relevant text and then\nprompting the tuned LLM to say what skill should be per-\nformed in inference (Feng et al., 2024). However, the text\nused for tuning contains much irrelevant information and\nis not specialized for task execution. As the embodied text\ntuning data volume is also limited, the tuned LLMs often\nfail to describe what skill should be performed correctly.\nLarge language models. LLMs draw broad attention from\nthe research and industrial communities due to their rich\ngeneral knowledge and the ability to generate the answers\nto diverse kinds of questions (Chang et al., 2024). GPT-\n3 emerges as a milestone in the evolution of LLMs, as it\ntakes the next token prediction problem as the pre-training\ntask and showcases remarkable open-world generalization\ncapabilities (Brown et al., 2020). Subsequently, the fine-\ntuning of GPT-3 using reinforcement learning with human\nfeedback leads to the creation of ChatGPT (OpenAI, 2023)\nand GPT-4 (Achiam et al., 2023). However, a significant\nlimitation of LLMs is their inability to interpret information\nin images, which are vital for humans to perceive the world.\nTo overcome this problem, researchers devise strategies that\ninject vision information into LLMs and enable LLMs to\nperceive images. A common method is fine-tuning a small\nnumber of network parameters using numerous language-\nimage data pairs to bridge the representation gap between\ntext and images (Ding et al., 2023). In this way, some large\nvision-language models like LLaVA (Liu et al., 2024a) and\nFlamingo (Alayrac et al., 2022) are derived.\nRL with LLMs in embodied AI. RL can search promis-\ning decision-making policies without human intervention,\nand LLMs are able to provide suitable search start points\nbased on their rich general knowledge (Rashidi & Nili Ah-\nmadabadi, 2024). Therefore, it is natural to design ways to\ncombine them to build advanced embodied intelligence. In\nprevious works, a popular choice is training a network for\neach basic skill based on RL, and then prompting the LLM\nto say what skill should be used according to the task target\n2\nSubmission and Formatting Instructions for ICML 2025\nMore Advanced Achievements \nRL Agent \nEfficient but Task \nspecialized \nLARM Agent (Ours) \nEfficient and \nGeneralizable \nLLM Agent \nGeneralizable but Slow \nCommon RL Agent \nAchievements\nCommon LLM Agent \nAchievements\nLARM Agent \nAchievements\nFigure 1. Comparison among agents based on RL, LLM, and LARM. As shown, RL agents are usually task specialized, and LLM agents\nare computationally expensive to deploy. By contrast, the LARM agent is efficient and generalizable. Besides, LARM presents better\nperformance. As shown, LARM is the first method that achieves enchanted diamond equipment in Minecraft.\nand environment observation (Wang et al., 2023b). Never-\ntheless, this paradigm is not only slow, it requires the LLM\nto have sufficient knowledge about Minecraft. According to\nour analysis, giant LLMs own such an ability but the capa-\nbilities of lightweight LLMs are limited. Another possible\nchoice is utilizing LLM to generate the code for calculating\nRL reward (Xie et al., 2023). Nevertheless, it is not always\nfeasible to define a reward function by writing code. For ex-\nample, in Minecraft, the information is represented as image\nand agent status information, which cannot be mapped as\nreward based on rules. To handle this problem, we propose\nto directly employ GPT-4 to read the agent status before\nand after executing a skill and judge whether the outcome\nbrought by this skill contributes to realizing the given target.\nIn addition, to the best of our knowledge, this is the first\nwork that directly optimizes an LLM-style policy based on\nonline exploration and reinforcement learning. Our results\nsuggest that the rich general knowledge in LLM favors this\nexploration and self-learning process.\n3. Preliminary\n3.1. Problem Formulation\nWhat we study in this work can be conceptualized as an auto-\nregressive prediction problem involving long sequences,\nand is effectively framed as a Markov Decision Process\nsymbolized by a tuple E = (S, A, P, T , R, γ, τ). Specif-\nically, S is the set of all potential states. A is the action\nset, and every action is also called as a skill in this work.\nP : S ×A×S →[0, 1] represents a probability distribution\nthat governs the state transitions given states and actions.\nT is the set of all task targets. R : S →R, γ, and τ de-\nnote the reward function, discount factor, and initial state\ndistribution, respectively. At any discrete time step t, the\nenvironment resides in a state st ∈S, and the correspond-\ning observation ot by a policy π is a function of this state,\nexpressed as ot = f(st). This observation ot is then utilized\nto select the subsequent action according to at ∼π(ot, ι),\nwhere at ∈A and ι ∈T denotes the given target task.\nIn tackling the studied long-horizon embodied task, the ob-\njective is to navigate through a sequence of intermediate\nstates τ, s1, s2, . . . , sT −1 to ultimately reach the target state\nsT at the final time step T. This requires the policy to gener-\nate a series of actions a0, a1, . . . , aT −1 such that each action\nat transitions the environment from state st to the next state\nst+1 correctly, adhering to the dynamics prescribed by the\ntransition probability distribution P. It is crucial that each\nintermediate state st is accurately achieved in sequence to\nensure the policy attains the target state sT .\n3.2. PPO\nProximal Policy Optimization (PPO) (Schulman et al., 2017)\nis a model-free reinforcement learning algorithm widely\nadopted for training policies in complex environments, and\nour referee RL is developed based on this algorithm. A PPO\npolicy π mainly consists of two components, the actor πa\nand critic πc. πc is to estimate the value function Vθc(st),\nthe expected cumulative discounted reward starting from\nstate st and following π. The optimization objective of πc\nis as follows:\nLc\nθc = Et[(Vθc(st) −(rt + γVθc(st+1)))2],\n(1)\n3\nSubmission and Formatting Instructions for ICML 2025\nwhere rt ∈R is the reward received after taking action\nat in state st. To further reduce the variance of the value\nestimate and improve stability, PPO employs the generalized\nadvantage estimation (GAE), which is defined as:\nAt =\nT −1\nX\nk=t\n(γλ)k−tδk,\n(2)\nwhere λ is a factor balancing between temporal differ-\nence (TD) learning and Monte Carlo estimation, and δk\ndenotes the TD-error, which is formulated as δk = rk +\nγVθc(sk+1) −Vθc(sk). With At, the objective function for\nthe critic πa can be given as:\nLa\nθa = Et[ πa(at|st)\nπold\na (at|st)At],\n(3)\nwhere and πold\na\nis the old actor policy before weight update\nand πa(at|st) is the current actor. To improve the training\nstability, PPO further develops a clipped surrogate objective\nbased on Eq. (3), which can be formulated as:\n˜La\nθa = Et[min(ktAt, (kt, 1 −ϵ, 1 + ϵ)At)],\n(4)\nwhere kt =\nπa(at|st)\nπold\na\n(at|st) and ϵ is a small positive parameter.\nBy optimizing πc and πa with respect to Eq. (1) and Eq. (4),\nthe policy gradually learns to perform the target task.\n4. Method\n4.1. Referee Reinforcement Learning\nIn long-horizon embodied task exploration, the policy can\nusually only get positive feedback after the target task is\ncompleted successfully (Fan et al., 2022). Following the\nnotations in Section 3.1, we can assume that there is an\nexploration trajectory {(sk, ak, rk)}T\nk=t, where T is a large\ninteger, and sk, ak, and rk denote the state, action, and\nenvironment reward at the k step, respectively. The agent\ncompletes the target task at the final step T. Therefore, we\ncan get that:\nrk =\n(\n−ε, if k = t, t + 1, · · · , T −1\nR −ε, if k = T\n(5)\nwhere −ε denotes a small negative constant due to time\npenalty and R is the positive reward of completing the target\ntask. As we train the critic πc using the very imbalanced\nreward set {rk}T\nk=t described in Eq. (5) with respect to the\noptimization objective in Eq. (1), we can infer that the output\nof πc gradually converges to Vθc(sk) −γVθc(sk+1) ≈−ε.\nIn this way, the corresponding TD error set {δt}T −1\nt=1 is:\nδk ≈\n(\n0, if k = t, t + 1, · · · , T −2\nR, if k = T −1\n(6)\nAlgorithm 1 Referee RL\nRequire: Target task ι\n1: Initialize the actor πa, critic πc, referee πr\n2: Initialize policy exploration step T, policy update steps\nNπ\n3: for each iteration iter do\n4:\nInitialize data buffer B ←∅\n5:\nfor t = 1 to T do\n6:\nGet the actor observation ot ←f(st)\n7:\nGet state st+1 and environment reward rt by tak-\ning at ∼πθ(ot, ι)\n8:\nGet auxiliary reward brt ←πr(ι, st, at, at+1)\n9:\nAdd transition B ←B∪{(st, ot, at, st+1, rt, brt)}\n10:\nend for\n11:\nfor n = 1 to Nπ do\n12:\nSample\na\nrandom\ntraining\ndata\nbatch\n{(st, ot, at, st+1, rt, brt}B\nj=1 ∼B\n13:\nOptimize πa and with respect to Eq. (1)∼(4)\n14:\nend for\n15: end for\nAccording to the definition of GAE in Eq. (2), we can\nobserve that the first T −1 −t items are close to zero\nas the elements in {σk}T −2\nk=t are nearly zero. For the last\nitem (γλ)T −1−tδT −1, we have limT →∞(γλ)T −1−t →0\nbecause γ ∈(0, 1) and λ ∈(0, 1). Hence, when the task\nneeds long-horizon action chain execution, the obtained\nGAE value At is close to zero, which suggests that the opti-\nmization objective for training πa in Eq. (4) becomes zero.\nIn this way, even though the policy makes the right action\ndecision, it cannot get any positive feedback.\nTo handle this problem, we introduce referee RL. Specif-\nically, we introduce a referee πp to provide an auxiliary\nreward feedback to the trained policy π, and this reward at\nthe step k is represented as brk = πp(ι, sk, ak, sk+1). This\nmeans πp takes the target task information ι, initial state sk,\nselected action ak, and new state sk+1 as input and provides\nfeedback based on whether the selected action is correct\nand outcome caused by this action. In this work, we split\nthe feedback into four categories: (a) The selected action\nis correct and brings a positive outcome to realizing the\ntarget. (b) The selected action is correct but does not bring\na positive outcome. (c) The selected action is incorrect but\ndoes not lead to a negative outcome. (d) The selected action\nis incorrect and results in a negative outcome. For the four\ncategories, πp correspondingly returns the reward value ra,\nrb, rc, and rd, where ra > rb > 0 > rc > rd. By adding\nthis auxiliary reward brk to the original reward described in\nEq. (5), the feedback to π before T does not remain as the\nconstant −ε. In this way, the TD error δk and correspond-\ning GAE value At do not converge to zeros, being able to\nprovide effective optimization guidance to πa.\n4\nSubmission and Formatting Instructions for ICML 2025\nVision Observation\nText Observation\n🚩\nTask Description\nSkill Token\nEnvironment Exploration\nLoRA\nLLM Decoders\nActor Head\nCritic Head\nAction\nValue\nLARM Policy\nAgent\nEnvironment\nEnvironment Reward\nReferee\nAuxiliary \nReward\n👍👍👍\nTotal Reward\nReward Generation\nAgent State\nRL based Policy Optimization\nUpdate\nAgent Status: …\nFigure 2. The overall pipeline of our method. As illustrated, we parametrize the actor πa and critic πc using a single LARM model with\ntwo separate prediction heads, i.e., the action head and critic head. We train LARM based on our proposed referee RL algorithm, which\nutilizes both environment feedback and referee generated auxiliary reward to guide the optimization of LARM.\nIn this work, we model the referee πp based on GPT-4\n(Achiam et al., 2023), which is a giant LLM and owns\nextensive generalizable knowledge. As mentioned before,\nthe information provided to GPT-4 includes ι (target task\ndescription), ak (the executed skill), sk and sk+1 (the inven-\ntory list and environment resource surrounding the agent\nbefore and after executing ak), and then we prompt it to\njudge the situation and response a reward among ra, rb,\nrc, and rd. The full detailed procedure of the referee RL\nalgorithm is elaborated in Algorithm 1.\n4.2. Large Auto-Regressive Model\nIn this part, we explain how to parametrize πa and πc using\nour designed LARM policy. As shown in Fig. 2, the main\nbody of LARM is the decoders of a lightweight decoder-\nonly LLM, TinyLLaVA-3.1B in this work. The parameters\nof these decoders are frozen during training and a trainable\nLoRA (Hu et al., 2021) module is applied to help the model\nlearn new knowledge in the applied task domain. This\ndesign has two key benefits: (i) The model is initialized with\nthe general knowledge and reasoning ability of LLMs while\nmaintaining an acceptable parameter volume. In this way,\nLARM can be deployed based on the restricted computing\nresources in embodied applications and achieve real-time\nresponse. (ii) As LARM adopts a similar model architecture\nas LLMs, we can first pre-train it using numerous question-\nanswer data related to the concerned embodied AI topics.\nThis kind of data is much easier to collect and scale up\nthan embodied data with action execution. We have tried\npre-training LARM using a 34G webpage dataset crawled\nfrom Wiki (Fan et al., 2022), and the results indicate that\nthe training convergence is improved.\nThe input to the LARM model consists of four parts, i.e.,\ntask description, text observation, vision observation, and\na skill token. The task description specifies the target task\nto conduct. The text information primarily includes the in-\nventory list, historical action, and blocks surrounding the\nagent. The vision information is the real-time image per-\nceived by the agent. We encode text and image information\nas tokens based on CLIP (Radford et al., 2021), and these\ntokens with an additional learnable skill token are input to\nthe LARM decoders to conduct feature interaction. After\nthe decoders, the skill token is input to the action head and\ncritic head depicted in Fig. 2 to output the action and state\nvalue. Therefore, LARM parametrizes the actor πa and\ncritic πc in Section 4.1 based on a single model with two\ndifferent trainable prediction heads.\nSimilar to previous literature (Wang et al., 2023a; Liu et al.,\n2024b), the action predicted by LARM is a skill, such as\nchopping down a tree or searching for a cobblestone. LARM\nselects a skill to perform by conducting feature matching\nbetween the action head output and skill description like\nSTEVE (Zhao et al., 2023). In this work, we test LARM\nwith two kinds of skills, the RL-based skills in MineDojo\nand API-based skills based on Mineflayer. The former cat-\negory is easier to be extended to real-world applications\nand the latter class presents higher execution success rates.\n5\nSubmission and Formatting Instructions for ICML 2025\nAccording to the given task requirements, new skills can\nbe dynamically added based on the strategies developed in\nprevious works (Wang et al., 2023a) and how to generate\nthese skills is not the research focus of this work.\nCombining the aforementioned techniques, LARM is op-\ntimized by iterating between environment exploration and\npolicy update described in Algorithm 1. The training de-\ntails like the optimizer choice follow PPO (Schulman et al.,\n2017). For learning to complete the most challenging task\nin this work (craft an enchanted diamond tool), about 42\nhours of exploration is taken using a single RTX4090 GPU.\n5. Experiments\n5.1. Environment\nIn this work, we validate our method using the MineDojo\nand Mineflayer environments. Both these two environments\nare developed based on Minecraft, but there exist some\ndifferences in their testing protocols.\nMineDojo. MineDojo (Fan et al., 2022) is a pioneering\nbenchmark suitable for studying how to develop open-ended\nand generally capable embodied agents. It features a simula-\ntion suite with thousands of tasks specified through natural\nlanguage prompts. The behaviors that can be conducted in\nMineDojo primarily include navigation, harvest, combat,\nand craft. When testing a policy, an agent is randomly ini-\ntialized in a biome with some initial tools. The policy needs\nto control the agent to explore and harvest resources in the\nenvironment and gradually realize the given target. The ac-\ntion space in MineDojo is a compound multi-discrete space\nthat allows the agent to select a movement action or an op-\ntional functional action at each step, encompassing a diverse\nrange of arguments to facilitate complex interactions.\nMineflayer. Compared with MineDojo, the basic actions in\nMineflayer (PrismarineJS., 2013) are provided APIs, such\nas harvesting a log or finding the nearest stone. Compared\nwith MineDojo, these APIs help researchers concentrate\nmore on high-level decision making rather than repetitive\naction details. In this way, several significantly more ad-\nvanced achievements are obtained by previous works based\non Mineflayer, like harvesting diamonds with high success\nrates (Wang et al., 2023a). In addition, the agents are usually\nspawned without initial tools in Mineflayer.\n5.2. Main Results\nWe compare LARM with previous methods in this part. As\nthe basic actions in MineDojo and Mineflayer are different,\nwe conduct the comparison separately.\nComparison on MineDojo. The methods compared on\nMineDojo include MineAgent (Fan et al., 2022), Plan4MC\n(Yuan et al., 2023), LLaMA-Rider (Feng et al., 2024), and\nRL-GPT (Liu et al., 2024b). Among them, MineAgent is the\nbaseline method provided by Minddojo. It first fine-tunes\nCLIP (Radford et al., 2021) based on numerous web data\nand uses the fine-tuned CLIP to guide the training of rein-\nforcement learning algorithms. Plan4MC is a reinforcement\nlearning based method. It splits a task into basic skills and\ntrains an agent to learn them one by one in a hierarchical\nway. LLaMA-Rider is an LLM obtained by fine-tuning\nLLaMA. It first makes the agent explore the environment to\ncollect data. Then, the collected data is adopted to fine-tune\nLLaMA in a supervised manner. RL-GPT builds two LLM\nbased agents (a slow agent and a fast agent) to schedule the\nagent actions. For an action step, this method first queries\nthe agents whether this action step can be completed via\ncode generation. If the code generation is infeasible, an RL\nbased action is performed.\nWe compare LARM with these methods on diverse tasks,\nand the detailed settings of these tasks follow previous works\n(Feng et al., 2024). Notably, we train a single LARM model\nto complete all the tasks, which is different from many pre-\nvious works that employ separate models for various tasks.\nThis characteristic suggests the promising generalization\nability of LARM. To compute success rates, we test LARM\nfor 30 times on every task. The experimental results are re-\nported in Table 1. As shown, LARM presents higher success\nrates than the compared counterparts in all the test tasks, and\nthe promising performance of LARM is mainly attributed\nto our designed referee RL algorithm, which addresses the\nreward vanishment problem in long-horizon embodied ex-\nploration. In addition, we can observe that LARM achieves\nhigher success rates on tasks with shorter action chains,\nindicating the great challenge in developing long-horizon\nembodied intelligence. For example, the Harvest bucket\ntask requires three iron ingots, and the Harvest iron sword\ndemands two iron ingots and one stick. Therefore, Harvest\niron sword needs one more step (Harvest stick) than Harvest\nbucket, which causes that LARM obtains a higher success\nrate in the task of crafting a bucket.\nComparison on Mineflayer. To fully reveal the superiority\nof LARM, we further evaluate LARM using the Mineflayer\nbased environment. The compared methods include Auto-\nGPT (autogpt), Voyager (Wang et al., 2023a), and STEVE\n(Zhao et al., 2023). In this work, Voyager is a training-free\nmethod implemented based on GPT-4. Its main contribution\nis designing a multi-step prompt generation pipeline. When\na target task is given, Voyager prompts GPT-4 to know\nwhich skill should be executed and gradually realize the\ntarget. AutoGPT is an LLM being able to reason which skill\nshould be performed through multi-step question answering.\nSTEVE is a large vision-language model. In STEVE, a\ndataset including both videos and text-image pairs is gath-\nered and utilized to fine-tune LLaMA (Touvron et al., 2023),\nand then the fine-tuned model can invoke pre-defined skills.\n6\nSubmission and Formatting Instructions for ICML 2025\nTable 1. Performance comparison with previous methods based on MineDojo.\nTask\nMineAgent\nPlan4MC\nLLaMA-Rider Base\nLLaMA-Rider\nRL-GPT\nLARM (Ours)\nHarvest stick\n0.00\n0.30\n0.23\n0.43\n0.65\n0.93\nHarvest crafting table\n0.03\n0.30\n0.37\n0.67\n0.65\n0.87\nHarvest bowl\n0.00\n0.47\n0.73\n0.97\n-\n0.97\nHarvest chest\n0.00\n0.23\n0.67\n0.77\n-\n0.83\nHarvest wooden pickaxe\n0.00\n0.03\n0.00\n0.37\n0.67\n0.70\nHarvest wooden sword\n0.00\n0.47\n0.63\n0.10\n-\n0.70\nHarvest furnace\n0.00\n0.37\n0.00\n0.17\n0.67\n0.73\nHarvest stone stairs\n0.00\n0.47\n0.00\n0.57\n-\n0.67\nHarvest stone sword\n0.00\n0.10\n0.00\n0.00\n-\n0.40\nHarvest iron ingot\n0.00\n0.47\n0.03\n0.13\n-\n0.60\nHarvest bucket\n0.00\n0.20\n0.00\n0.00\n0.37\nHarvest iron sword\n0.00\n0.20\n0.00\n0.00\n-\n0.27\nHarvest beef\n0.33\n0.43\n0.03\n0.03\n0.46\n0.60\nHarvest mutton\n0.35\n0.33\n0.00\n0.03\n0.38\n0.63\nTable 2. Performance comparison based on Mineflayer.\nAchievement\nAutoGPT Voyager STEVE LARM (Ours)\nWooden sword\n3\/3\n3\/3\n3\/3\n30\/30\nStone sword\n3\/3\n3\/3\n3\/3\n30\/30\nIron sword\n3\/3\n3\/3\n3\/3\n30\/30\nDiamond sword\n0\/3\n1\/3\n3\/3\n28\/30\nEnchanted sword\n0\/3\n0\/3\n0\/3\n16\/30\nTable 3. Ablation Study on Reward Design.\nReward\nStick\nWooden\nStone\nIron\nER\n0.20\n0.13\n0.10\n0.00\nER+LAR\n0.30\n0.23\n0.13\n0.00\nER+AR2\n0.80\n0.53\n0.20\n0.07\nER+AR4\n0.93\n0.70\n0.40\n0.27\nIn this experiment, the agent is spawned in a random biome\nwithout initial inventory. The test tasks include Harvest\nwooden sword, Harvest stone sword, Harvest iron sword,\nHarvest diamond sword, and Harvest enchanted diamond\nsword. As shown in Fig. 1, Harvest enchanted diamond\nsword is significantly more complex than the other achieve-\nments. The previous methods usually test their models\nthree times in Mineflayer. To reduce randomness, we run\nLARM for 30 times. The experimental results are reported\nin Table 2. We can observe that LARM outperforms the\ncompared methods in obtaining different levels of achieve-\nments. Notably, LARM is the first method that harvests an\nenchanted diamond sword in Minecraft successfully.\n5.3. Ablation Study\nAnalysis on reward design. The key difference of Ref-\neree RL from the classic PPO implementation is the reward\ndesign. Therefore, we ablate different reward settings in\nthis part. We compare four reward choices in MineDojo\nwith four tasks, i.e., Harvest stick, Harvest wooden sword,\nHarvest stone sword, and Harvest iron sword. The four re-\nward choices are ER (environment reward only), ER+LAR\n(environment reward plus auxiliary reward produced by\nLLaVA-7B, a lightweight LLM), ER+AR2 (environment\nTable 4. Ablation Study on LLM base selection.\nLLM Base\nStick\nWooden\nStone\nIron\nTinyLLaVA-0.5B\n0.80\n0.50\n0.27\n0.13\nTinyLLaVA-3.1B\n0.83\n0.57\n0.33\n0.13\nTinyLLaVA-3.1B*\n0.93\n0.70\n0.40\n0.27\nreward plus auxiliary reward produced by GPT-4o, but the\nauxiliary reward is determined based on only whether the ac-\ntion outcome is positive), and ER+AR2 (the standard setting\nof our method described in this paper). The experimental\nresults are presented in Table 3.\nAs shown, when only the environment reward is provided,\nthe agent presents a significantly better success rate on Har-\nvest stick than Harvest stone sword, where the latter task\ndemands a longer action chain. This phenomenon is because\nthat the environment reward gradually vanishes with the in-\ncrease of the action chain length. When the auxiliary reward\nis generated by LLaVA-7B, the result is also poor, which is\nbecause that LLaVA-7B does not understand the Minecraft\nworld well and the provided reward is often incorrect. This\nresult confirms our aforementioned claim that only giant\nLLMs serve as referees well. In addition, the performance\nof ER+AR2 is worse than ER+AR4, which is because se-\nlecting the correct action does not always bring a positive\noutcome. For example, the agent decides to search a tree\nbut does not find it successfully. In this situation, ER+AR2\nreturns a negative reward, which punishes the searching a\ntree decision even though this choice may be correct. By\ncontrast, ER+AR4 will give a small positive reward.\nAnalysis on LLM base selection. As mentioned before,\nwe utilize the weight of a lightweight LLM to initialize\nthe parameters in LARM to provide initial general knowl-\nedge. Then, we fine-tune LARM using Minecraft related\nwebpage data to enhance its understanding. In this experi-\nment, we analyze how the lightweight LLM choice affects\nthe results and whether the webpage data pre-training de-\nscribed in Section 4.2 is beneficial. To this end, we first\ncompare the performances of LARM using TinyLLaVA-\n7\nSubmission and Formatting Instructions for ICML 2025\nEnter the Nether\nSearch a Village \nMulti-agent Combat\nt!\nt\"\nt#\nt$\nt!\nt\"\nt#\nt$\nt!\nt\"\nt#\nt$\nFigure 3. More behavior example illustrations of LARM, which include traveling a long distance to find a village, building a nether portal\nand then entering the nether, multiple agents collaborate with each other to combat zombies.\n0.5B and TinyLLaVA-3.1B as the LLM base, where the\nexperiment results are presented in the 1st and 2nd row of\nTable 4. As shown, adopting an LLM with more parameters\nas the LARM model base favors the embodied performance,\nwhich is mainly attributed to that a larger LLM has stronger\ngeneral knowledge. Then, we compare the performances of\nmodels without and with the webpage data pre-training, and\nthe results are reported in the 2nd and 3rd rows of Table 4. It\ncan be observed that webpage data pre-training improves the\nsuccess rates on embodied task execution. This result sug-\ngests that the numerous text and image data on the Internet\nhas the potential to benefit embodied agents.\nAnalysis on LLM capability. We have mentioned that\ngiant LLMs answer questions about the Minecraft world\nwell while the understanding of lightweight LLMs is poor.\nIn this part, we demonstrate this by providing a question-\nanswer example and highlighting the key content in bold:\nPrompt: In Minecraft, you need to craft a stone pickaxe.\nWhat additional resources do you need to gather if you have\nonly got cobblestones in your inventory?\nGPT-4o: If you already have cobblestone in your inventory,\nyou need to gather wooden planks to craft sticks. You need\ntwo sticks and three cobblestones to craft a stone pickaxe.\nLlama3-8B: You need a stick.\nTinyLLaVA-3.1B: To craft a stone pickaxe in Minecraft, you\nwill need the following additional resources: cobblestone,\nstone, wood, leaves, dirt, grass, pillar, shovel, and sword.\nTinyLLaVA-3.1B after Webpage data pre-training: You ad-\nditionally need two sticks.\nComparing the answers, we can observe that webpage data\npre-training significantly improves the concerned domain\nof knowledge in TinyLLaVA-3.1B.\n5.4. Case Study\nThe previous experiments mainly show the performances\nof LARM on harvesting various categories of materials\nand crafting tools. In Fig. 3, we visualize more examples\nof other behaviors in the Mineflayer based environment,\nsuch as exploring the open world, constructing a building\nwith a specific structure, and multiple agents cooperate to\ncombat dangerous creatures. These behaviors suggest that\nour proposed techniques in this work have the potential to\nbe further generalized to other diverse domains.\n6. Conclusion\nIn this work, we have proposed LARM, which is effi-\ncient and possesses general knowledge. To train it, we\nhave revealed the feedback vanishment problem in apply-\ning classic RLs to long-horizon embodied exploration. To\naddress this feedback vanishment, we have developed the\nreferee RL technique. By optimizing LARM with referee\n8\nSubmission and Formatting Instructions for ICML 2025\nRL, our method can learn to complete diverse embodied\ntasks without human supervision. Especially, LARM is the\nfirst method that obtains the enchanted diamond equipment\nachievement in the Minecraft benchmark successfully.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | LARM：高效且通用的具身智能模型\n\n## 📌 背景痛点\/本文动机\n近年来，人工智能在计算机视觉和自然语言处理等领域取得了显著进展，但这些技术大多缺乏与真实世界进行物理交互的能力。为了解决这个问题，具身人工智能的概念被提出，旨在开发能够与真实世界进行交互的智能体。然而，现有的具身智能体大多局限于特定任务，缺乏通用性和开放性。本文旨在解决这个问题，提出了一种名为LARM的具身智能模型，该模型结合了强化学习和大型语言模型的优点，能够在开放世界中完成各种任务。\n\n## 🚀 核心方法\n💡 创新点1：LARM模型\nLARM模型基于轻量级的大型语言模型（LLM），直接输出下一步行动，而不是文本。这使得LARM模型能够快速响应环境变化，并具有更强的通用性。\n\n💡 创新点2：裁判强化学习\n传统的强化学习算法在长时序探索中存在反馈消失的问题，即智能体在完成任务之前无法获得有效的奖励信号。为了解决这个问题，本文提出了裁判强化学习技术。该技术利用大型语言模型作为裁判，为智能体提供即时反馈，从而有效地指导智能体的学习过程。\n\n## 📈 实验结果\n本文在MineDojo和Mineflayer环境中对LARM模型进行了评估。实验结果表明，LARM模型在完成各种任务方面取得了优异的性能，包括收集资源、制作工具等。特别是在Mineflayer环境中，LARM模型成功地制作了附魔钻石装备，这是之前方法无法实现的。\n\n## 💬 可借鉴之处\n本文提出的LARM模型和裁判强化学习技术为开发高效且通用的具身智能体提供了新的思路。LARM模型的结构和训练方法可以应用于其他开放世界环境，而裁判强化学习技术可以解决长时序探索中的反馈消失问题。此外，本文还展示了网页数据预训练对提升模型性能的重要性，这为利用互联网数据训练具身智能体提供了启示。","llm_summary_res_status":200}
{"title":"Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification","authors":"Yuxuan Guo, Shaohui Peng, Jiaming Guo, Di Huang, Xishan Zhang, Rui Zhang, Yifan Hao, Ling Li, Zikang Tian, Mingju Gao, Yutai Li, Yiming Gan, Shuai Liang, Zihao Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen","summary":"Building open agents has always been the ultimate goal in AI research, and\ncreative agents are the more enticing. Existing LLM agents excel at\nlong-horizon tasks with well-defined goals (e.g., `mine diamonds' in\nMinecraft). However, they encounter difficulties on creative tasks with open\ngoals and abstract criteria due to the inability to bridge the gap between\nthem, thus lacking feedback for self-improvement in solving the task. In this\nwork, we introduce autonomous embodied verification techniques for agents to\nfill the gap, laying the groundwork for creative tasks. Specifically, we\npropose the Luban agent target creative building tasks in Minecraft, which\nequips with two-level autonomous embodied verification inspired by human design\npractices: (1) visual verification of 3D structural speculates, which comes\nfrom agent synthesized CAD modeling programs; (2) pragmatic verification of the\ncreation by generating and verifying environment-relevant functionality\nprograms based on the abstract criteria. Extensive multi-dimensional human\nstudies and Elo ratings show that the Luban completes diverse creative building\ntasks in our proposed benchmark and outperforms other baselines ($33\\%$ to\n$100\\%$) in both visualization and pragmatism. Additional demos on the\nreal-world robotic arm show the creation potential of the Luban in the physical\nworld.","url":"http:\/\/arxiv.org\/abs\/2405.15414v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2405.15414v1","published":1716546359000,"comment":null,"pdf_text":"Luban: Building Open-Ended Creative\nAgents via Autonomous Embodied Verification\nYuxuan Guo1, 2, 3\nShaohui Peng6\nJiaming Guo2\nDi Huang2\nXishan Zhang2, 3\nRui Zhang2\nYifan Hao2\nLing Li6\nZikang Tian2, 3, 4\nMingju Gao2, 3, 4\nYutai Li2, 3, 4\nYiming Gan7\nShuai Liang7\nZihao Zhang2\nZidong Du2, 5\nQi Guo2\nXing Hu2, 5 ∗\nYunji Chen2, 4 ∗\n1University of Science and Technology of China\n2State Key Lab of Processors, Institute of Computing Technology, CAS\n3Cambricon Technologies\n4University of Chinese Academy of Sciences\n5Shanghai Innovation Center for Processor Technologies\n6Intelligent Software Research Center, Institute of Software, CAS\n7Institute of Computing Technology, CAS\ngyx_20170818@mail.ustc.edu.cn, {huxing, cyj}@ict.ac.cn\nAbstract\nBuilding open agents has always been the ultimate goal in AI research, and creative\nagents are the more enticing. Existing LLM agents excel at long-horizon tasks with\nwell-defined goals (e.g., ‘mine diamonds’ in Minecraft). However, they encounter\ndifficulties on creative tasks with open goals and abstract criteria due to the inability\nto bridge the gap between them, thus lacking feedback for self-improvement in\nsolving the task. In this work, we introduce autonomous embodied verification\ntechniques for agents to fill the gap, laying the groundwork for creative tasks.\nSpecifically, we propose the Luban agent target creative building tasks in Minecraft,\nwhich equips with two-level autonomous embodied verification inspired by human\ndesign practices: (1) visual verification of 3D structural speculates, which comes\nfrom agent synthesized CAD modeling programs; (2) pragmatic verification of the\ncreation by generating and verifying environment-relevant functionality programs\nbased on the abstract criteria. Extensive multi-dimensional human studies and\nElo ratings show that the Luban completes diverse creative building tasks in our\nproposed benchmark and outperforms other baselines (33% to 100%) in both\nvisualization and pragmatism. Additional demos on the real-world robotic arm\nshow the creation potential of the Luban in the physical world.\n1\nIntroduction\nDeveloping open-ended agents capable of autonomously solving complex tasks [1, 11, 12, 18, 26,\n33, 34] directed by high-level abstract instructions is the ultimate goal of artificial intelligence\n(AI) techniques. Within this endeavor, creative tasks stand out as particularly enticing [3, 6, 37].\nUnlike conventional long-horizontal complex tasks, creative tasks do not have well-defined or easily\nautomated success criteria [6], thus stimulating the emergence of more advanced AI techniques with\nhigher intellectual capabilities and the potential to tackle real-world problems.\n∗Corresponding author.\nPreprint. Under review.\narXiv:2405.15414v1  [cs.AI]  24 May 2024\nFigure 1: (a) Agents for Well-defined long-horizontal tasks v.s. (b) Luban agent for creative tasks.\nExisting LLM agent techniques show promising progress in handling conventional long-horizon tasks\nwith well-defined goals related to environment states. For instance, as shown in Figure 1, consider the\nclassic ‘mining diamonds’ task in Minecraft, where it is easy to verify by just checking the diamond\nnumber in the inventory. Empowered by LLM’s rich semantic knowledge and reasoning capabilities\n[25, 31], these agents [33, 34, 39] precisely assess the distance between current states and the goals,\nthen reflect and replan based on the assessment accordingly, eventually solving diamond mining tasks\niteratively.\nHowever, when confronted with creative tasks, existing agents encounter a significant challenge: the\ninability to verify or assess due to the absence of well-defined goals, which is the prerequisite\nof reflection and re-planing. For instance, the creative task of ‘building a house’ in Minecraft lacks\nexplicit goal definitions. It is challenging for LLM agents to verify whether a house has been properly\nconstructed and reflect based on the verification. For the assess criterion of ‘Player can enter the\nhouse through doors’, there is a huge gap between high-level abstract descriptions (‘enter through\ndoors’) and environment-relevant verification actions (move((x1, y1, z1) →(x2, y2, z2))). Such a\ngap impedes agents from accurately assessing their current states and formulating practical plans.\nTo address this issue, we introduce a novel approach termed autonomous embodied verification\ntechniques, aimed at empowering open-ended agents to proficiently confirm high-level abstractions\nof assess criteria in creative building tasks. This lays the groundwork for agents to autonomously\ntackle open-ended creative tasks without well-defined goals. We draw inspiration from human design\npractices that usually progressively design and verify from the visual appearance to functionality.\nBased on such inspiration, we propose a Luban agent, which begins with a building ‘something-like’\nphase, wherein we speculatively construct 3D structural objects based on CAD (Computer-Aided\nDesign) program synthesis and perform visual verification on these objects. After passing the visual\nverification, it subsequently transits to the building ‘something-work’ phase. Luban then generates\nenvironment-relevant functionality programs on these objects for pragmatic verification. With such\nvisual and pragmatic verification, agents can summarize and reflect accordingly and iteratively\ncomplete open-ended creative tasks.\nTo evaluate the performance of Luban on open creative building tasks, we designed a benchmark\ncontaining 5 Minecraft building tasks with diverse visual and functional requirements. Multi-\ndimensional extensive human studies show that the Luban agent successfully completes all open-\nended creative building tasks, and the Elo ratings clearly show that buildings created by Luban\noutperform other baselines (33%~100%) in visualization and pragmatism. Moreover, the pass rate of\nautonomously proposed embodied verification is consistent with human functionality assessment,\ndemonstrating its effectiveness and necessity. Finally, demos on the real-world robotic arm show the\npotential of the Luban agent to perform open-ended creative tasks in the physical world.\n2\nRelated Works\nMinecraft Agents. The openness and authenticity of the Minecraft game make it an important test-\nbed for AI agents. Most existing Minecraft agents focus on tasks with a long horizon and well-defined\ngoals [20], such as collecting and crafting materials. These agents can be further categorized into\ntwo branches: control-centric and planning-centric. The control-centric agents [2, 3, 19, 35] trained\non Minecraft gameplay demos collected from the Internet to build task policies based on low-level\n2\ngame controls (e.g., mouse and keyboard action). The planning-centric agents [33, 34, 39] focus on\naligning high-level instructions with action primitives by utilizing LLM’s reasoning capabilities and\nsemantic knowledge to decompose instructions into plans. These agents often come with carefully\ndesigned memory and reflection mechanisms to ensure they can learn useful skills and take advantage\nof environmental feedback. Unlike the above works, we focus on building planning-centric creative\nagents that aim to autonomously verify the not well-defined goals of the creative tasks to ground\ncreations (ensuring pragmatism) in the environment. Compared with the pioneering attempt [37],\nit did not involve any verification and feedback mechanisms, making it incompetent in grounding\ncreations.\n3D Model Synthesis. Using computers to generate 3D models is a key research topic in computer\ngraphics. Recently, the synthesis of 3D models from given instructions (text or images) has attracted\nmore and more attention from researchers [9, 17, 21]. The methods of 3D model synthesis can be\ndivided into two categories. One category methods synthesize 3D models directly (e.g., meshes [8],\npoint cloud [24], multi-view images [27] and voxels [13]) rely on generative models [7, 10, 15, 29, 32]\nand neural representations [14, 23]. Another category of methods relies on the existing Computer-\nAided-Design (CAD) modeling software (e.g., Blender [4] and FreeCAD [22]) to first synthesize\nthe operations and parameters of the modeling process (i.e., programs) and then execute them to get\nthe 3D model. This line of work includes training-based methods [16, 36, 38] and LLM in-context\nlearning-based method [30] that emerged recently. The models synthesized using the former category\nof methods typically exhibit rich textures and details but lack complete controllability and accurate\ndimensions, whereas those synthesized using the latter demonstrate the opposite. In this work,\ngenerating accurate 3D models is crucial to the Luban agent’s planning and visual verification, so\nwe synthesize 3D models by prompting LLM to synthesize programs based on the CAD modeling\nlibrary we provided. Compared with [30], we consider CAD modeling to rely on a small number of\nlow-level (i.e., sketch-extrude-based) rather than high-level (i.e., pre-defined objects) APIs, which\nallow the creation of diverse 3D models via using API combinations and adding natural language\nannotations. Please refer to Sec. 4.1 and Appendix C for more details.\n3\nProblem Definition\nMinecraft Environment. We formalize the Minecraft environment as a Partially Observable Markov\nDecision Process (POMDP) without the reward function P = (S, A, T, Ω, O), where S is the state\nspace, A is the action space, T is the transition dynamics, Ωis the observation space (i.e., game\nimages), and O is the set of conditional observation probabilities. The action space A consists of\npre-defined action primitives, such as move, place_block, and dig_block, which return a binary\nvalue to reveal action status (success or failure).\nMinecraft Agent for Open-ended Creative Building Tasks. The open-ended creative building\ntasks can be formalized as an Instruction Following (IF) problem, where the instruction I consists of\ntwo parts: (1) Text, including Natural Language (NL) building description, functional requirements,\nand building suggestions; (2) Images, including multi-view images of a general example building\nthat aligns with the building description. The agent takes the instruction I as input and performs\na sequence of actions (a1, a2, . . . ), ai ∈A to build the building in the environment and ensures\nit meets the functional requirements (i.e., grounding creations in the environment by ensuring the\npragmatism). For example, when the instruction involves ‘build a bridge to cross a river’, the agent\nshould build a bridge-like structure in the environment and ensure it is walkable across the river.\n4\nMethod\nIn this section, we introduce the Luban agent, which can complete open-ended creative building tasks\npragmatically in the open world with the help of the two-level autonomous embodied verification: (1)\n3D structural speculation stage with the visual verification (Sec. 4.1); (2) Construction stage with the\npragmatic verification (Sec. 4.2).\n4.1\n3D Structural Speculation stage with Visual Verification\nThe goal of the 3D structural speculation stage is to design the building based on the open-end\ncreative building instruction I. Due to the large goal space of the creative building tasks, it is\n3\nFigure 2: The diagram of Luban agent. (a) The 3D structural speculation stage uses VLM to\nsynthesize Instructions I into a CAD program representing the building 3D objects, which further\nincludes decomposing, subcomponents generation, and assembling. The visual verification evaluates\nthe quality of buildings through the appearance results of the CAD program construction. (b)\nThe construction stage uses VLM to synthesize the building’s 3D object program into executable\nconstruction actions to get the building in the environment. The pragmatic verification evaluates the\nbuilding 3D object’s pragmatism by generating environment-relevant functionality annotations and\naction verify programs.\nnecessary to introduce verifications in the 3D structural speculation stage that filter out the open\nbut inappropriate designs (e.g., designs leading to semantic-less or incomplete building) to reduce\nthe space. We introduce visual verification in the 3D structural speculation stage by exploiting\nVLM’s visual understanding capabilities, thus requiring the generation of visual representations.\nConsider existing deep-learning-based visual representations synthesize techniques are inaccurate and\nuncontrollable (more discussion in Sec 2), we turn to synthesize parametrically modeled 3D models\n(i.e., synthesizing Python CAD programs based on a Python CAD library 2) in the 3D structural\nspeculation stage. Figure 2 (a) shows the 3D structural speculation stage and visual verification.\n3D structural speculation. The 3D structural speculation stage can be formalized as I\nprompt\n−→P B,\nwhere P B is a Python CAD program representing the precise 3D shape of the whole building. To fully\nexploit VLM’s 3D structural speculation and reasoning capabilities and consider the conventions of\nparametric CAD modeling, the 3D structural speculation stage is further divided into three sub-stages\nas shown in Figure 2 (a), including decomposing, subcomponent generation, and assembling. (1)\nDecomposing. The VLM takes I and necessary prompts as input and outputs a subcomponent\ndescription set S = {s1, s2, . . . } that make up the building represented by I, expressed as I\nprompt\n−→S.\nEach subcomponent si in S is represented in natural language and contains semantic, size, and\nposition information. (2) Subcomponent Generation. The sub-stage aims at synthesizing natural\nlanguage subcomponents into 3D subcomponents represented by Python CAD modeling programs\nP S, expressed as S\nprompt\n−→P S. The VLM first plans to determine the precise size and appearance\ninformation of each subcomponent. Then, it in-context learns the documents and few-shot examples\nof the CAD library we provide to synthesize the 3D subcomponents program. (3) Assembling. The\nVLM assembles the 3D subcomponents P S to the building 3D object by reasoning and setting each\nsubcomponent’s position and orientation via synthesizing building 3D object program P B, expressed\nas P S prompt\n−→P B.\n2The Python CAD library used in our work is simplified and encapsulated based on the CadQuery project,\nwhich has a small number of low-level sketch-extrude-based APIs for parametric 3D modeling. Please refer to\nthe Appendix C for more details. The CadQuery project’s link, https:\/\/github.com\/CadQuery\/cadquery.\n4\nFigure 3: The showcases of Luban’s creation on all tasks.\nVisual Verification. Visual verification aims to filter out the best from multiple modeling pro-\ngrams (i.e., 3D subcomponents and object), expressed as I, (P S\n1 , . . . , P S\nk )\nprompt\n−→i, 1 ≤i ≤k\n(or (P B\n1 , . . . , P B\nk )). Specifically, in the subcomponent generation and assembling sub-stages, we\nsample k Python CAD modeling programs (P S or P B) generated by VLM and execute them to get\ncorresponding 3D model multi-view images. Subsequently, the k multi-view images are prompted to\nVLM to evaluate the consistency of the images and the instruction I. The best program returned by\nVLM is selected, with the option to resample if no best program is found.\n4.2\nConstruction stage with the Pragmatic Verification\nConstruction. The construction stage aims to construct the building B in the environment based\non the building 3D object program P B from the 3D structural speculation stage, expressed as\nP B prompt, execute\n−→\nB. Specifically, the building 3D object program P B is first exported to the\nenvironment-level (i.e., Minecraft) coordinates. Then, the VLM is prompted with these coordinates\nand the available action primitives to synthesize the action sequence AC = (aC\n1 , aC\n2 , . . . ) (i.e., the\nJavaScript programs) for constructing the building. Finally, the construction action sequence AC is\nexecuted in the environment to construct the building B.\nPragmatic Verification. The pragmatic verification aims to reason well-defined functionality from\nthe abstract criteria in task instruction I, and further verify corresponding pragmatism of the con-\nstructed building B to get suggestions I+ for improving the next round creation, expressed as\nB\nprompt, interact\n−→\nI+. The Luban’s pragmatic verification can be divided into three sub-stages, as\nshown in Figure 2(b), including verification action generation, verify, and reflection. (1) Verifica-\ntion action generation. Based on the instruction I, the agent generates and attaches the natural\nlanguage verification annotations on the subcomponents of the building 3D object P B to generate\nenvironment-relevant functionality programs. The environment-relevant functionality programs are\nfurther synthesized into embodied verification actions (i.e., the JavaScript programs) with binary\nstatus (action success or not) by the VLM, i.e., AP = (aP\n1 , aP\n2 , . . . ), aP\ni : B →{0, 1}. (2) Verify.\nThe verification actions are further executed in the environment to interact with building B to collect\nthe action status. By analyzing the action status, the agent verifies the building pragmatism and\noutputs verification results. These two sub-stages are expressed as I, P S prompt, execute\n−→\n{0, 1}n.\n(3) Reflection. The verification results, together with instructions I and image observation o, are\nprompted to VLM for further conducting semantic level check and reflection to obtain suggestions\nI+ for the next iteration, expressed as I, o, {0, 1}n prompt\n−→I+.\n5\nExperiments\nIn this section, we first introduce the experimental settings (benchmark, baselines, and metrics) in Sec.\n5.1, then demonstrate Luban’s superiority in creation pragmatism and human preference compared\nto other method baselines and the quality of pragmatic verification in Sec. 5.2, further, show the\neffectiveness and necessity of Luban’s two-level verifications through ablation studies in Sec. 5.3,\nand finally, discuss the real-world application potential of the Luban in Sec. 5.4.\n5\n5.1\nExprimental Settings\nBenchmarking Open-ended Creative Building Tasks. We design a benchmark to test the agent’s\nability to complete open-ended creative building tasks pragmatically. The benchmark contains 5\ntasks (i.e., arrow-tower, bridge, chinese-ancient-house, stair, and two-story-house)\nwith diverse structural and functional requirements. Each task instruction consists of the text and\nmulti-view images 3. as illustrated in Sec. 3. Take the bridge task as an example. The task requires\nthe agent to build a plank bridge similar to the one in multi-view images. The functional requirements\nof the bridge task are two-fold: Environmental level, the bridge needs to cross the river; Building\nlevel, the bridge should be walkable for players, and the bridge’s handrails should prevent players\nfrom falling off. Please refer to Appendix D for more details about the benchmark and the comparison\nwith other benchmarks.\nBaselines. We implement the Luban agent with gpt-4-vision-preview. For simplicity, we\nassume that the action primitives used for construction (i.e., place_block and dig_block) and\nthe building’s position are oracles. All action primitives are implemented with Mineflayer [28]\nJavascript APIs and also adopted by the following baselines for the sake of fairness. To demonstrate\nthe superiority of the Luban agent, we compare it with the following plan-centric Minecraft agent\nbaselines: (a) Voyager agent [33], an LLM-based agent target on exploring the Minecraft world\nto follow instructions, which has 2 variants, gpt-3.5-turbo based Voyager35 and gpt-4 based\nVoyager4; (b) Creative agent [37], a gpt-4-vision-preview based agent targets creative building\ntasks without any feedback from the environment. To demonstrate the effectiveness and necessity\nof the Luban agent’s two-level verification, we consider the following baseline of ablation settings:\n(a) Luban w\/o pv, Luban agent without pragmatic verification, equivalent to plan and construction\nwithout any environmental feedback; (b) Luban w\/o vv, Luban agent without visual verification\nby replacing the VLM visual verification with a random choice; (c) Luban w\/o vvpv, Luban agent\nwithout both verification.\nMetrics. We run all the above baselines to obtain 3 seed building results (multi-view video) for\neach task. The 3 metrics are listed as follows: (1) Quality rating, similar to [37], each result is\nrated (ranging from 1 to 5) from 5 dimensions: Appearance (AP), Complexity (CO), Aesthetics (AE),\nBuilding-level Functional (FB), and Environmental-level Functional (FE). The action verification\nof pragmatic verification (in Sec. 4.2) mainly covers the FB, and the VLM semantic check followed\nby the action verification mainly covers the FE. (2) One-to-one comparison, the result pairs from\nthe same tasks and different baselines are evaluated by selecting the winner. The winning rates of\nbaselines are further used to compute the Elo [5] rating for a comprehensive comparison. (3) Pass\nrate of Luban’s pragmatic verification, we migrate the pragmatic verification actions autonomously\nproposed by the Luban agent to other baselines (by modifying some parameters of the action\nprimitives, e.g., start and end position of move) and execute the actions to calculate the pass rates for\nevaluation. Considering the openness of the task results, we launch human studies on metrics (1) and\n(2). Please refer to Appendix F for more details.\n5.2\nMain Results\nIn this section, we comprehensively compare Luban and other method baselines on the 3 metrics:\nquality rating, one-to-one comparison, and pragmatic verification pass rate. We draw 3 corresponding\nconclusions as follows:\nLuban’s creations outperform other method baselines and are pragmatic in the environment.\nAs the quality ratings shown in the polar chart of Figure 4, on all 5 tasks, Luban’s quality ratings on 3\nnon-functional dimensions significantly exceeded the baselines: AE rating increased by 1.42 to 2.93,\nCO rating increased by 1.44 to 3.22, and AP rating increased by 1.84 to 3.18. Further, Luban also\nreceives the highest ratings in the two functional dimensions, FB and FE, on all 5 tasks (average rating\n4.44 and 4.50 correspondingly, near full rating 5) and significantly outperforms the baselines of other\nmethods (rating increased by 0.80 to 3.42 and 1.67 to 3.76 correspondingly). The quality rating\nresults directly demonstrate this conclusion. Other method baselines cannot generate complicated\ncreations and get feedback from the environment, thus resulting in low-level ratings. We showcase\n3Given the building descriptions, we use the text-to-3D service of https:\/\/meshy.ai to generate general\n3D models and capture multi-view images.\n6\nFigure 4: The polar chart of multi-dimensional quality rating of creations from Luban and other\nmethod baselines. The results are grouped by tasks and averaged across all seeds and human\nevaluators with a 1-sigma bar\nTable 1: The winning rate (%) of one-to-one comparison between Luban and other method baselines\nand Elo ratings across tasks.\nTask ID\nLuban\nCreative\nVoyager35\nVoyager4\narrow-tower\n100.00\n20.00\n44.44\n35.56\nbridge\n100.00\n55.56\n1.39\n43.06\nchinese-ancient-house\n99.26\n15.56\n30.37\n54.81\nstair\n97.78\n19.26\n38.52\n44.44\ntwo-story-house\n98.52\n62.22\n6.67\n32.59\nElo rating across tasks\n2095.83\n1572.22\n1053.55\n1278.40\nLuban’s creation in Figure 3 to facilitate a more intuitive understanding. More showcases of other\nmethod baselines are shown in Figure 12 in Appendix E.1.\nLuban’s creation is more consistent with human preferences than other method baselines. As\nlisted in Table 1, on all 5 tasks, Luban achieves ∼100% one-to-one winning rate compared with\nother method baselines. The Elo rating across tasks provides a more comprehensive perspective\nto reflect the gap between baselines, where Luban outperforms the second baseline ∼500 scores,\ndirectly supporting the conclusion.\nThe pass rate of Luban’s pragmatic verification reveals the degree of creation pragmatism. As\nthe pass rates listed in Table 2 left, Luban achieves 100% verification pass rate on all 5 tasks after\nrounds of iteration and autonomous verification. In contrast, the pass rate of other method baselines\nremains low. We observe the pass rates exhibit similar trending to the quality rating FB, and we\nstatistically reveal it by computing the Spearman correlations, as listed in Table 2 right. The strong\npositive correlation (all ρ > 0.6 and p-value < 0.05) indicates that the pragmatic verification pass\nrate aligns with the human evaluator. Thus, the pass rate reveals the degree of creation pragmatism\nand can measure creative building tasks.\nTable 2: (Left) The average pragmatic verification pass rate (%) across seeds of Luban and other\nmethod baselines. (Right) The Spearman correlation (ρ and p-value) between the pass rate and quality\nratings FB.\nTask ID\nLuban\nCreative\nVoyager35\nVoyager4\nρ (p)\narrow-tower\n100.00\n33.33\n66.67\n100.00\n0.76 (0.00)\nbridge\n100.00\n22.22\n22.22\n33.33\n0.89 (0.00)\nchinese-ancient-house\n100.00\n0.00\n0.00\n0.00\n0.75 (0.00)\nstair\n100.00\n33.33\n0.00\n33.33\n0.63 (0.03)\ntwo-story-house\n100.00\n33.33\n0.00\n0.00\n0.67 (0.02)\n7\nFigure 5: The polar chart of multi-dimensional quality rating of creations from Luban and ablation\nbaselines. The results are grouped by tasks and averaged across all seeds and human evaluators with\na 1-sigma bar.\n5.3\nAblation Study\nIn this section, we ablate Luban’s visual and pragmatic verifications and draw 3 conclusions as\nfollows:\nVisual verification improves the basic quality of the creations. As the quality ratings shown in\nthe polar chart of Figure 5, on all 5 tasks, the quality ratings of baselines with visual verification\n(‘Luban’ and ‘Luban w\/o pv’) significantly outperform those without (‘Luban w\/o vv’ and ‘Luban\nw\/o vvpv’) on both functional and non-functional dimensions (rating increasing from 0.69 to 3.20).\nThe one-to-one winning rates and Elo ratings in Table 3 also exhibit similar trendings, in which\n‘Luban’ and ‘Luban w\/o pv’ are also significantly higher than ‘Luban w\/o vv’ and ‘Luban w\/o vvpv’.\nThese results directly support the conclusion, and the creation quality degraded to GPT4 levels\nwithout visual verification. The visual verification works because it filters out inappropriate Python\nCAD modeling programs to reduce the errors in subcomponent generation (e.g., missed or wrongly\ndesigned subcomponents) and assembling (e.g., incorrect subcomponent’s position and orientation)\nsub-stages by reviewing multiple programs and selecting the best.\nPragmatic verification is effective and necessary for the creation’s pragmatism. As the two\nfunctional dimension ratings (FB and FE) shown in the polar chart of Figure 5, on all 5 tasks, Luban\noutperforms ‘Luban w\/o pv’ baseline, ranging from 0.42 to 1.43 and 1.16 to 2.91 correspondingly.\nMoreover, as the pragmatic verification pass rates listed in Table 4, ‘Luban w\/o pv’ does not\nreach 100% pass on all tasks. The differences between the above functional dimension ratings and\nverification pass rate directly demonstrate this conclusion. The pragmatic verification works because\nit generates purposefully embodied actions to collect the information of creations for feedback to\nimprove the creation’s pragmatism stably. In contrast, those without pragmatic verification can rely\nsolely on VLM output’s randomness to make pragmatic creations occasionally. Additionally, we\nnotice that the verification pass rates of ‘Luban w\/o pv’ on tasks bridge and stair are 100%, which\nmay be attributed to the agreement of these Minecraft buildings and the VLM’s semantic knowledge.\nVisual verification is the prerequisite for pragmatic verification. We access the pragmatic verifica-\ntion gains on baselines with and without visual verification by computing the two functional ratings\n(FB and FE) differences in Figure 5, i.e., gain(Luban w\/o vvpv →Luban w\/o vv) = [−0.29, 1.04]\nand gain(Luban w\/o pv →Luban) = [0.42, 2.91]. The results show that larger pragmatic verifica-\ntion gains occurred in the baseline group with visual verification, which supports the conclusion. This\nis because pragmatic verification means little when the building quality is extremely low, e.g., there\nis no point in verifying that the door is passable when the house is assembled incorrectly. The results\nlisted in Table 4 support the reason, in which the two ablation baselines without visual verification\n(i.e., Luban w\/o vv’ and Luban w\/o vvpv’)’s pragmatic verification pass rate is no longer significantly\ncorrelated to the human functionality ratings (the p-values > 0.05 on all 5 tasks). More intuitive\nshowcases of the ablation baselines can be found in Figure 13 of Appendix E.1.\n8\nTable 3: The winning rate (%) of one-to-one comparison between Luban and ablation baselines and\nElo ratings across tasks.\nTask ID\nLuban\nLuban w\/o pv\nLuban w\/o vv\nLuban w\/o vvpv\narrow-tower\n98.41\n53.97\n42.86\n4.76\nbridge\n98.41\n65.08\n35.71\n0.79\nchinese-ancient-house\n83.33\n80.95\n11.11\n24.60\nstair\n82.54\n84.13\n29.37\n3.97\ntwo-story-house\n86.51\n76.98\n17.46\n19.05\nElo across tasks\n1979.48\n1753.42\n1128.74\n1138.36\nTable 4: (Left) The average pragmatic verification pass rate (%) across seeds of Luban and ablation\nbaselines. (Right) The Spearman correlation (ρ and p-value) between the pass rate of (‘Luban w\/o\nvv’, ‘Luban w\/o vvpv’) and quality ratings FB. The correlation item of arrow-tower task is ‘n\/a’\ndue to the constant pass rate.\nTask ID\nLuban w\/o pv\nLuban w\/o vv\nLuban w\/o vvpv\nρ (p)\narrow-tower\n66.67\n0.00\n0.00\nn\/a (n\/a)\nbridge\n100.00\n55.56\n55.56\n0.53 (0.28)\nchinese-ancient-house\n33.33\n33.33\n66.67\n0.60 (0.21)\nstair\n100.00\n0.00\n25.00\n0.77 (0.07)\ntwo-story-house\n33.33\n0.00\n33.33\n-0.42 (0.41)\n5.4\nPotential in Real-World Embodied Creative Tasks\nLuban has potential in real-world open-ended creative tasks rather than being limited to Minecraft-like\nvirtual worlds, which owes to the general of Luban’s planning framework (i.e., CAD modeling and\nvisual verification) and feedback mechanism (i.e., propose actions to verify not well-defined goals).\nWe demonstrate this by providing demos on two tasks (chinese-ancient-house and bridge) of\nLuban on real-world embodied robotic arm environment, as shown in Figure 6. Specifically, we\nfirst 3D print the model of subcomponents and then use the subcomponent coordinates given by the\nassembling sub-stage to drive the pick-place API-based embodied robotic arm to perform creative\nbuilding tasks. The final assembly result demonstrates the conclusion.\n6\nConclusion\nIn this work, we propose Luban, an agent capable of open-ended creative building tasks in Minecraft,\npowered by the two-level autonomous embodied visual and pragmatic verifications. Extensive human\nstudies demonstrate that Luban’s creations have higher quality (especially functional pragmatism) in\nFigure 6: The robotic demos of task chinese-ancient-house and bridge.\n9\nmultiple dimensions and are more preferred by humans than the other method baselines. Furthermore,\nLuban also shows the potential of Luban in real-world creative tasks through demos we provided on\nembodied robotic arms environment. Our work may inspire the following directions: (1) Develop\nlibraries that represent the 3D physical world to bridge VLM and the physical world, thereby\nfacilitating the emergence of embodied agents with spatial intelligence. (2) Extend Luban’s pragmatic\nverification to obtain feedback in the real world, thereby building a closed-loop, open creative agent\ngrounding in the real world.\n7\nLimitations\nWe summarize our limitations as follows: (1) Due to the lack of a memory mechanism, Luban cannot\nutilize shared knowledge between multiple tasks (e.g., universal design guidelines) and thus cannot\nlearn from the environment continuously; (2) The expensive access costs and limited capabilities of\nadvanced VLM (i.e., GPT-4V) prevent Luban from generating more refined 3D structure inference.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nLuban: Building Open-Ended Creative Agents via Autonomous Embodied Verification\n```\n#### 2. 论文摘要\n```\nBuilding open agents has always been the ultimate goal in AI research, and\ncreative agents are the more enticing. Existing LLM agents excel at\nlong-horizon tasks with well-defined goals (e.g., `mine diamonds' in\nMinecraft). However, they encounter difficulties on creative tasks with open\ngoals and abstract criteria due to the inability to bridge the gap between\nthem, thus lacking feedback for self-improvement in solving the task. In this\nwork, we introduce autonomous embodied verification techniques for agents to\nfill the gap, laying the groundwork for creative tasks. Specifically, we\npropose the Luban agent target creative building tasks in Minecraft, which\nequips with two-level autonomous embodied verification inspired by human design\npractices: (1) visual verification of 3D structural speculates, which comes\nfrom agent synthesized CAD modeling programs; (2) pragmatic verification of the\ncreation by generating and verifying environment-relevant functionality\nprograms based on the abstract criteria. Extensive multi-dimensional human\nstudies and Elo ratings show that the Luban completes diverse creative building\ntasks in our proposed benchmark and outperforms other baselines ($33\\%$ to\n$100\\%$) in both visualization and pragmatism. Additional demos on the\nreal-world robotic arm show the creation potential of the Luban in the physical\nworld.\n```\n\n#### 3. 论文全文\n```\nLuban: Building Open-Ended Creative\nAgents via Autonomous Embodied Verification\nYuxuan Guo1, 2, 3\nShaohui Peng6\nJiaming Guo2\nDi Huang2\nXishan Zhang2, 3\nRui Zhang2\nYifan Hao2\nLing Li6\nZikang Tian2, 3, 4\nMingju Gao2, 3, 4\nYutai Li2, 3, 4\nYiming Gan7\nShuai Liang7\nZihao Zhang2\nZidong Du2, 5\nQi Guo2\nXing Hu2, 5 ∗\nYunji Chen2, 4 ∗\n1University of Science and Technology of China\n2State Key Lab of Processors, Institute of Computing Technology, CAS\n3Cambricon Technologies\n4University of Chinese Academy of Sciences\n5Shanghai Innovation Center for Processor Technologies\n6Intelligent Software Research Center, Institute of Software, CAS\n7Institute of Computing Technology, CAS\ngyx_20170818@mail.ustc.edu.cn, {huxing, cyj}@ict.ac.cn\nAbstract\nBuilding open agents has always been the ultimate goal in AI research, and creative\nagents are the more enticing. Existing LLM agents excel at long-horizon tasks with\nwell-defined goals (e.g., ‘mine diamonds’ in Minecraft). However, they encounter\ndifficulties on creative tasks with open goals and abstract criteria due to the inability\nto bridge the gap between them, thus lacking feedback for self-improvement in\nsolving the task. In this work, we introduce autonomous embodied verification\ntechniques for agents to fill the gap, laying the groundwork for creative tasks.\nSpecifically, we propose the Luban agent target creative building tasks in Minecraft,\nwhich equips with two-level autonomous embodied verification inspired by human\ndesign practices: (1) visual verification of 3D structural speculates, which comes\nfrom agent synthesized CAD modeling programs; (2) pragmatic verification of the\ncreation by generating and verifying environment-relevant functionality programs\nbased on the abstract criteria. Extensive multi-dimensional human studies and\nElo ratings show that the Luban completes diverse creative building tasks in our\nproposed benchmark and outperforms other baselines (33% to 100%) in both\nvisualization and pragmatism. Additional demos on the real-world robotic arm\nshow the creation potential of the Luban in the physical world.\n1\nIntroduction\nDeveloping open-ended agents capable of autonomously solving complex tasks [1, 11, 12, 18, 26,\n33, 34] directed by high-level abstract instructions is the ultimate goal of artificial intelligence\n(AI) techniques. Within this endeavor, creative tasks stand out as particularly enticing [3, 6, 37].\nUnlike conventional long-horizontal complex tasks, creative tasks do not have well-defined or easily\nautomated success criteria [6], thus stimulating the emergence of more advanced AI techniques with\nhigher intellectual capabilities and the potential to tackle real-world problems.\n∗Corresponding author.\nPreprint. Under review.\narXiv:2405.15414v1  [cs.AI]  24 May 2024\nFigure 1: (a) Agents for Well-defined long-horizontal tasks v.s. (b) Luban agent for creative tasks.\nExisting LLM agent techniques show promising progress in handling conventional long-horizon tasks\nwith well-defined goals related to environment states. For instance, as shown in Figure 1, consider the\nclassic ‘mining diamonds’ task in Minecraft, where it is easy to verify by just checking the diamond\nnumber in the inventory. Empowered by LLM’s rich semantic knowledge and reasoning capabilities\n[25, 31], these agents [33, 34, 39] precisely assess the distance between current states and the goals,\nthen reflect and replan based on the assessment accordingly, eventually solving diamond mining tasks\niteratively.\nHowever, when confronted with creative tasks, existing agents encounter a significant challenge: the\ninability to verify or assess due to the absence of well-defined goals, which is the prerequisite\nof reflection and re-planing. For instance, the creative task of ‘building a house’ in Minecraft lacks\nexplicit goal definitions. It is challenging for LLM agents to verify whether a house has been properly\nconstructed and reflect based on the verification. For the assess criterion of ‘Player can enter the\nhouse through doors’, there is a huge gap between high-level abstract descriptions (‘enter through\ndoors’) and environment-relevant verification actions (move((x1, y1, z1) →(x2, y2, z2))). Such a\ngap impedes agents from accurately assessing their current states and formulating practical plans.\nTo address this issue, we introduce a novel approach termed autonomous embodied verification\ntechniques, aimed at empowering open-ended agents to proficiently confirm high-level abstractions\nof assess criteria in creative building tasks. This lays the groundwork for agents to autonomously\ntackle open-ended creative tasks without well-defined goals. We draw inspiration from human design\npractices that usually progressively design and verify from the visual appearance to functionality.\nBased on such inspiration, we propose a Luban agent, which begins with a building ‘something-like’\nphase, wherein we speculatively construct 3D structural objects based on CAD (Computer-Aided\nDesign) program synthesis and perform visual verification on these objects. After passing the visual\nverification, it subsequently transits to the building ‘something-work’ phase. Luban then generates\nenvironment-relevant functionality programs on these objects for pragmatic verification. With such\nvisual and pragmatic verification, agents can summarize and reflect accordingly and iteratively\ncomplete open-ended creative tasks.\nTo evaluate the performance of Luban on open creative building tasks, we designed a benchmark\ncontaining 5 Minecraft building tasks with diverse visual and functional requirements. Multi-\ndimensional extensive human studies show that the Luban agent successfully completes all open-\nended creative building tasks, and the Elo ratings clearly show that buildings created by Luban\noutperform other baselines (33%~100%) in visualization and pragmatism. Moreover, the pass rate of\nautonomously proposed embodied verification is consistent with human functionality assessment,\ndemonstrating its effectiveness and necessity. Finally, demos on the real-world robotic arm show the\npotential of the Luban agent to perform open-ended creative tasks in the physical world.\n2\nRelated Works\nMinecraft Agents. The openness and authenticity of the Minecraft game make it an important test-\nbed for AI agents. Most existing Minecraft agents focus on tasks with a long horizon and well-defined\ngoals [20], such as collecting and crafting materials. These agents can be further categorized into\ntwo branches: control-centric and planning-centric. The control-centric agents [2, 3, 19, 35] trained\non Minecraft gameplay demos collected from the Internet to build task policies based on low-level\n2\ngame controls (e.g., mouse and keyboard action). The planning-centric agents [33, 34, 39] focus on\naligning high-level instructions with action primitives by utilizing LLM’s reasoning capabilities and\nsemantic knowledge to decompose instructions into plans. These agents often come with carefully\ndesigned memory and reflection mechanisms to ensure they can learn useful skills and take advantage\nof environmental feedback. Unlike the above works, we focus on building planning-centric creative\nagents that aim to autonomously verify the not well-defined goals of the creative tasks to ground\ncreations (ensuring pragmatism) in the environment. Compared with the pioneering attempt [37],\nit did not involve any verification and feedback mechanisms, making it incompetent in grounding\ncreations.\n3D Model Synthesis. Using computers to generate 3D models is a key research topic in computer\ngraphics. Recently, the synthesis of 3D models from given instructions (text or images) has attracted\nmore and more attention from researchers [9, 17, 21]. The methods of 3D model synthesis can be\ndivided into two categories. One category methods synthesize 3D models directly (e.g., meshes [8],\npoint cloud [24], multi-view images [27] and voxels [13]) rely on generative models [7, 10, 15, 29, 32]\nand neural representations [14, 23]. Another category of methods relies on the existing Computer-\nAided-Design (CAD) modeling software (e.g., Blender [4] and FreeCAD [22]) to first synthesize\nthe operations and parameters of the modeling process (i.e., programs) and then execute them to get\nthe 3D model. This line of work includes training-based methods [16, 36, 38] and LLM in-context\nlearning-based method [30] that emerged recently. The models synthesized using the former category\nof methods typically exhibit rich textures and details but lack complete controllability and accurate\ndimensions, whereas those synthesized using the latter demonstrate the opposite. In this work,\ngenerating accurate 3D models is crucial to the Luban agent’s planning and visual verification, so\nwe synthesize 3D models by prompting LLM to synthesize programs based on the CAD modeling\nlibrary we provided. Compared with [30], we consider CAD modeling to rely on a small number of\nlow-level (i.e., sketch-extrude-based) rather than high-level (i.e., pre-defined objects) APIs, which\nallow the creation of diverse 3D models via using API combinations and adding natural language\nannotations. Please refer to Sec. 4.1 and Appendix C for more details.\n3\nProblem Definition\nMinecraft Environment. We formalize the Minecraft environment as a Partially Observable Markov\nDecision Process (POMDP) without the reward function P = (S, A, T, Ω, O), where S is the state\nspace, A is the action space, T is the transition dynamics, Ωis the observation space (i.e., game\nimages), and O is the set of conditional observation probabilities. The action space A consists of\npre-defined action primitives, such as move, place_block, and dig_block, which return a binary\nvalue to reveal action status (success or failure).\nMinecraft Agent for Open-ended Creative Building Tasks. The open-ended creative building\ntasks can be formalized as an Instruction Following (IF) problem, where the instruction I consists of\ntwo parts: (1) Text, including Natural Language (NL) building description, functional requirements,\nand building suggestions; (2) Images, including multi-view images of a general example building\nthat aligns with the building description. The agent takes the instruction I as input and performs\na sequence of actions (a1, a2, . . . ), ai ∈A to build the building in the environment and ensures\nit meets the functional requirements (i.e., grounding creations in the environment by ensuring the\npragmatism). For example, when the instruction involves ‘build a bridge to cross a river’, the agent\nshould build a bridge-like structure in the environment and ensure it is walkable across the river.\n4\nMethod\nIn this section, we introduce the Luban agent, which can complete open-ended creative building tasks\npragmatically in the open world with the help of the two-level autonomous embodied verification: (1)\n3D structural speculation stage with the visual verification (Sec. 4.1); (2) Construction stage with the\npragmatic verification (Sec. 4.2).\n4.1\n3D Structural Speculation stage with Visual Verification\nThe goal of the 3D structural speculation stage is to design the building based on the open-end\ncreative building instruction I. Due to the large goal space of the creative building tasks, it is\n3\nFigure 2: The diagram of Luban agent. (a) The 3D structural speculation stage uses VLM to\nsynthesize Instructions I into a CAD program representing the building 3D objects, which further\nincludes decomposing, subcomponents generation, and assembling. The visual verification evaluates\nthe quality of buildings through the appearance results of the CAD program construction. (b)\nThe construction stage uses VLM to synthesize the building’s 3D object program into executable\nconstruction actions to get the building in the environment. The pragmatic verification evaluates the\nbuilding 3D object’s pragmatism by generating environment-relevant functionality annotations and\naction verify programs.\nnecessary to introduce verifications in the 3D structural speculation stage that filter out the open\nbut inappropriate designs (e.g., designs leading to semantic-less or incomplete building) to reduce\nthe space. We introduce visual verification in the 3D structural speculation stage by exploiting\nVLM’s visual understanding capabilities, thus requiring the generation of visual representations.\nConsider existing deep-learning-based visual representations synthesize techniques are inaccurate and\nuncontrollable (more discussion in Sec 2), we turn to synthesize parametrically modeled 3D models\n(i.e., synthesizing Python CAD programs based on a Python CAD library 2) in the 3D structural\nspeculation stage. Figure 2 (a) shows the 3D structural speculation stage and visual verification.\n3D structural speculation. The 3D structural speculation stage can be formalized as I\nprompt\n−→P B,\nwhere P B is a Python CAD program representing the precise 3D shape of the whole building. To fully\nexploit VLM’s 3D structural speculation and reasoning capabilities and consider the conventions of\nparametric CAD modeling, the 3D structural speculation stage is further divided into three sub-stages\nas shown in Figure 2 (a), including decomposing, subcomponent generation, and assembling. (1)\nDecomposing. The VLM takes I and necessary prompts as input and outputs a subcomponent\ndescription set S = {s1, s2, . . . } that make up the building represented by I, expressed as I\nprompt\n−→S.\nEach subcomponent si in S is represented in natural language and contains semantic, size, and\nposition information. (2) Subcomponent Generation. The sub-stage aims at synthesizing natural\nlanguage subcomponents into 3D subcomponents represented by Python CAD modeling programs\nP S, expressed as S\nprompt\n−→P S. The VLM first plans to determine the precise size and appearance\ninformation of each subcomponent. Then, it in-context learns the documents and few-shot examples\nof the CAD library we provide to synthesize the 3D subcomponents program. (3) Assembling. The\nVLM assembles the 3D subcomponents P S to the building 3D object by reasoning and setting each\nsubcomponent’s position and orientation via synthesizing building 3D object program P B, expressed\nas P S prompt\n−→P B.\n2The Python CAD library used in our work is simplified and encapsulated based on the CadQuery project,\nwhich has a small number of low-level sketch-extrude-based APIs for parametric 3D modeling. Please refer to\nthe Appendix C for more details. The CadQuery project’s link, https:\/\/github.com\/CadQuery\/cadquery.\n4\nFigure 3: The showcases of Luban’s creation on all tasks.\nVisual Verification. Visual verification aims to filter out the best from multiple modeling pro-\ngrams (i.e., 3D subcomponents and object), expressed as I, (P S\n1 , . . . , P S\nk )\nprompt\n−→i, 1 ≤i ≤k\n(or (P B\n1 , . . . , P B\nk )). Specifically, in the subcomponent generation and assembling sub-stages, we\nsample k Python CAD modeling programs (P S or P B) generated by VLM and execute them to get\ncorresponding 3D model multi-view images. Subsequently, the k multi-view images are prompted to\nVLM to evaluate the consistency of the images and the instruction I. The best program returned by\nVLM is selected, with the option to resample if no best program is found.\n4.2\nConstruction stage with the Pragmatic Verification\nConstruction. The construction stage aims to construct the building B in the environment based\non the building 3D object program P B from the 3D structural speculation stage, expressed as\nP B prompt, execute\n−→\nB. Specifically, the building 3D object program P B is first exported to the\nenvironment-level (i.e., Minecraft) coordinates. Then, the VLM is prompted with these coordinates\nand the available action primitives to synthesize the action sequence AC = (aC\n1 , aC\n2 , . . . ) (i.e., the\nJavaScript programs) for constructing the building. Finally, the construction action sequence AC is\nexecuted in the environment to construct the building B.\nPragmatic Verification. The pragmatic verification aims to reason well-defined functionality from\nthe abstract criteria in task instruction I, and further verify corresponding pragmatism of the con-\nstructed building B to get suggestions I+ for improving the next round creation, expressed as\nB\nprompt, interact\n−→\nI+. The Luban’s pragmatic verification can be divided into three sub-stages, as\nshown in Figure 2(b), including verification action generation, verify, and reflection. (1) Verifica-\ntion action generation. Based on the instruction I, the agent generates and attaches the natural\nlanguage verification annotations on the subcomponents of the building 3D object P B to generate\nenvironment-relevant functionality programs. The environment-relevant functionality programs are\nfurther synthesized into embodied verification actions (i.e., the JavaScript programs) with binary\nstatus (action success or not) by the VLM, i.e., AP = (aP\n1 , aP\n2 , . . . ), aP\ni : B →{0, 1}. (2) Verify.\nThe verification actions are further executed in the environment to interact with building B to collect\nthe action status. By analyzing the action status, the agent verifies the building pragmatism and\noutputs verification results. These two sub-stages are expressed as I, P S prompt, execute\n−→\n{0, 1}n.\n(3) Reflection. The verification results, together with instructions I and image observation o, are\nprompted to VLM for further conducting semantic level check and reflection to obtain suggestions\nI+ for the next iteration, expressed as I, o, {0, 1}n prompt\n−→I+.\n5\nExperiments\nIn this section, we first introduce the experimental settings (benchmark, baselines, and metrics) in Sec.\n5.1, then demonstrate Luban’s superiority in creation pragmatism and human preference compared\nto other method baselines and the quality of pragmatic verification in Sec. 5.2, further, show the\neffectiveness and necessity of Luban’s two-level verifications through ablation studies in Sec. 5.3,\nand finally, discuss the real-world application potential of the Luban in Sec. 5.4.\n5\n5.1\nExprimental Settings\nBenchmarking Open-ended Creative Building Tasks. We design a benchmark to test the agent’s\nability to complete open-ended creative building tasks pragmatically. The benchmark contains 5\ntasks (i.e., arrow-tower, bridge, chinese-ancient-house, stair, and two-story-house)\nwith diverse structural and functional requirements. Each task instruction consists of the text and\nmulti-view images 3. as illustrated in Sec. 3. Take the bridge task as an example. The task requires\nthe agent to build a plank bridge similar to the one in multi-view images. The functional requirements\nof the bridge task are two-fold: Environmental level, the bridge needs to cross the river; Building\nlevel, the bridge should be walkable for players, and the bridge’s handrails should prevent players\nfrom falling off. Please refer to Appendix D for more details about the benchmark and the comparison\nwith other benchmarks.\nBaselines. We implement the Luban agent with gpt-4-vision-preview. For simplicity, we\nassume that the action primitives used for construction (i.e., place_block and dig_block) and\nthe building’s position are oracles. All action primitives are implemented with Mineflayer [28]\nJavascript APIs and also adopted by the following baselines for the sake of fairness. To demonstrate\nthe superiority of the Luban agent, we compare it with the following plan-centric Minecraft agent\nbaselines: (a) Voyager agent [33], an LLM-based agent target on exploring the Minecraft world\nto follow instructions, which has 2 variants, gpt-3.5-turbo based Voyager35 and gpt-4 based\nVoyager4; (b) Creative agent [37], a gpt-4-vision-preview based agent targets creative building\ntasks without any feedback from the environment. To demonstrate the effectiveness and necessity\nof the Luban agent’s two-level verification, we consider the following baseline of ablation settings:\n(a) Luban w\/o pv, Luban agent without pragmatic verification, equivalent to plan and construction\nwithout any environmental feedback; (b) Luban w\/o vv, Luban agent without visual verification\nby replacing the VLM visual verification with a random choice; (c) Luban w\/o vvpv, Luban agent\nwithout both verification.\nMetrics. We run all the above baselines to obtain 3 seed building results (multi-view video) for\neach task. The 3 metrics are listed as follows: (1) Quality rating, similar to [37], each result is\nrated (ranging from 1 to 5) from 5 dimensions: Appearance (AP), Complexity (CO), Aesthetics (AE),\nBuilding-level Functional (FB), and Environmental-level Functional (FE). The action verification\nof pragmatic verification (in Sec. 4.2) mainly covers the FB, and the VLM semantic check followed\nby the action verification mainly covers the FE. (2) One-to-one comparison, the result pairs from\nthe same tasks and different baselines are evaluated by selecting the winner. The winning rates of\nbaselines are further used to compute the Elo [5] rating for a comprehensive comparison. (3) Pass\nrate of Luban’s pragmatic verification, we migrate the pragmatic verification actions autonomously\nproposed by the Luban agent to other baselines (by modifying some parameters of the action\nprimitives, e.g., start and end position of move) and execute the actions to calculate the pass rates for\nevaluation. Considering the openness of the task results, we launch human studies on metrics (1) and\n(2). Please refer to Appendix F for more details.\n5.2\nMain Results\nIn this section, we comprehensively compare Luban and other method baselines on the 3 metrics:\nquality rating, one-to-one comparison, and pragmatic verification pass rate. We draw 3 corresponding\nconclusions as follows:\nLuban’s creations outperform other method baselines and are pragmatic in the environment.\nAs the quality ratings shown in the polar chart of Figure 4, on all 5 tasks, Luban’s quality ratings on 3\nnon-functional dimensions significantly exceeded the baselines: AE rating increased by 1.42 to 2.93,\nCO rating increased by 1.44 to 3.22, and AP rating increased by 1.84 to 3.18. Further, Luban also\nreceives the highest ratings in the two functional dimensions, FB and FE, on all 5 tasks (average rating\n4.44 and 4.50 correspondingly, near full rating 5) and significantly outperforms the baselines of other\nmethods (rating increased by 0.80 to 3.42 and 1.67 to 3.76 correspondingly). The quality rating\nresults directly demonstrate this conclusion. Other method baselines cannot generate complicated\ncreations and get feedback from the environment, thus resulting in low-level ratings. We showcase\n3Given the building descriptions, we use the text-to-3D service of https:\/\/meshy.ai to generate general\n3D models and capture multi-view images.\n6\nFigure 4: The polar chart of multi-dimensional quality rating of creations from Luban and other\nmethod baselines. The results are grouped by tasks and averaged across all seeds and human\nevaluators with a 1-sigma bar\nTable 1: The winning rate (%) of one-to-one comparison between Luban and other method baselines\nand Elo ratings across tasks.\nTask ID\nLuban\nCreative\nVoyager35\nVoyager4\narrow-tower\n100.00\n20.00\n44.44\n35.56\nbridge\n100.00\n55.56\n1.39\n43.06\nchinese-ancient-house\n99.26\n15.56\n30.37\n54.81\nstair\n97.78\n19.26\n38.52\n44.44\ntwo-story-house\n98.52\n62.22\n6.67\n32.59\nElo rating across tasks\n2095.83\n1572.22\n1053.55\n1278.40\nLuban’s creation in Figure 3 to facilitate a more intuitive understanding. More showcases of other\nmethod baselines are shown in Figure 12 in Appendix E.1.\nLuban’s creation is more consistent with human preferences than other method baselines. As\nlisted in Table 1, on all 5 tasks, Luban achieves ∼100% one-to-one winning rate compared with\nother method baselines. The Elo rating across tasks provides a more comprehensive perspective\nto reflect the gap between baselines, where Luban outperforms the second baseline ∼500 scores,\ndirectly supporting the conclusion.\nThe pass rate of Luban’s pragmatic verification reveals the degree of creation pragmatism. As\nthe pass rates listed in Table 2 left, Luban achieves 100% verification pass rate on all 5 tasks after\nrounds of iteration and autonomous verification. In contrast, the pass rate of other method baselines\nremains low. We observe the pass rates exhibit similar trending to the quality rating FB, and we\nstatistically reveal it by computing the Spearman correlations, as listed in Table 2 right. The strong\npositive correlation (all ρ > 0.6 and p-value < 0.05) indicates that the pragmatic verification pass\nrate aligns with the human evaluator. Thus, the pass rate reveals the degree of creation pragmatism\nand can measure creative building tasks.\nTable 2: (Left) The average pragmatic verification pass rate (%) across seeds of Luban and other\nmethod baselines. (Right) The Spearman correlation (ρ and p-value) between the pass rate and quality\nratings FB.\nTask ID\nLuban\nCreative\nVoyager35\nVoyager4\nρ (p)\narrow-tower\n100.00\n33.33\n66.67\n100.00\n0.76 (0.00)\nbridge\n100.00\n22.22\n22.22\n33.33\n0.89 (0.00)\nchinese-ancient-house\n100.00\n0.00\n0.00\n0.00\n0.75 (0.00)\nstair\n100.00\n33.33\n0.00\n33.33\n0.63 (0.03)\ntwo-story-house\n100.00\n33.33\n0.00\n0.00\n0.67 (0.02)\n7\nFigure 5: The polar chart of multi-dimensional quality rating of creations from Luban and ablation\nbaselines. The results are grouped by tasks and averaged across all seeds and human evaluators with\na 1-sigma bar.\n5.3\nAblation Study\nIn this section, we ablate Luban’s visual and pragmatic verifications and draw 3 conclusions as\nfollows:\nVisual verification improves the basic quality of the creations. As the quality ratings shown in\nthe polar chart of Figure 5, on all 5 tasks, the quality ratings of baselines with visual verification\n(‘Luban’ and ‘Luban w\/o pv’) significantly outperform those without (‘Luban w\/o vv’ and ‘Luban\nw\/o vvpv’) on both functional and non-functional dimensions (rating increasing from 0.69 to 3.20).\nThe one-to-one winning rates and Elo ratings in Table 3 also exhibit similar trendings, in which\n‘Luban’ and ‘Luban w\/o pv’ are also significantly higher than ‘Luban w\/o vv’ and ‘Luban w\/o vvpv’.\nThese results directly support the conclusion, and the creation quality degraded to GPT4 levels\nwithout visual verification. The visual verification works because it filters out inappropriate Python\nCAD modeling programs to reduce the errors in subcomponent generation (e.g., missed or wrongly\ndesigned subcomponents) and assembling (e.g., incorrect subcomponent’s position and orientation)\nsub-stages by reviewing multiple programs and selecting the best.\nPragmatic verification is effective and necessary for the creation’s pragmatism. As the two\nfunctional dimension ratings (FB and FE) shown in the polar chart of Figure 5, on all 5 tasks, Luban\noutperforms ‘Luban w\/o pv’ baseline, ranging from 0.42 to 1.43 and 1.16 to 2.91 correspondingly.\nMoreover, as the pragmatic verification pass rates listed in Table 4, ‘Luban w\/o pv’ does not\nreach 100% pass on all tasks. The differences between the above functional dimension ratings and\nverification pass rate directly demonstrate this conclusion. The pragmatic verification works because\nit generates purposefully embodied actions to collect the information of creations for feedback to\nimprove the creation’s pragmatism stably. In contrast, those without pragmatic verification can rely\nsolely on VLM output’s randomness to make pragmatic creations occasionally. Additionally, we\nnotice that the verification pass rates of ‘Luban w\/o pv’ on tasks bridge and stair are 100%, which\nmay be attributed to the agreement of these Minecraft buildings and the VLM’s semantic knowledge.\nVisual verification is the prerequisite for pragmatic verification. We access the pragmatic verifica-\ntion gains on baselines with and without visual verification by computing the two functional ratings\n(FB and FE) differences in Figure 5, i.e., gain(Luban w\/o vvpv →Luban w\/o vv) = [−0.29, 1.04]\nand gain(Luban w\/o pv →Luban) = [0.42, 2.91]. The results show that larger pragmatic verifica-\ntion gains occurred in the baseline group with visual verification, which supports the conclusion. This\nis because pragmatic verification means little when the building quality is extremely low, e.g., there\nis no point in verifying that the door is passable when the house is assembled incorrectly. The results\nlisted in Table 4 support the reason, in which the two ablation baselines without visual verification\n(i.e., Luban w\/o vv’ and Luban w\/o vvpv’)’s pragmatic verification pass rate is no longer significantly\ncorrelated to the human functionality ratings (the p-values > 0.05 on all 5 tasks). More intuitive\nshowcases of the ablation baselines can be found in Figure 13 of Appendix E.1.\n8\nTable 3: The winning rate (%) of one-to-one comparison between Luban and ablation baselines and\nElo ratings across tasks.\nTask ID\nLuban\nLuban w\/o pv\nLuban w\/o vv\nLuban w\/o vvpv\narrow-tower\n98.41\n53.97\n42.86\n4.76\nbridge\n98.41\n65.08\n35.71\n0.79\nchinese-ancient-house\n83.33\n80.95\n11.11\n24.60\nstair\n82.54\n84.13\n29.37\n3.97\ntwo-story-house\n86.51\n76.98\n17.46\n19.05\nElo across tasks\n1979.48\n1753.42\n1128.74\n1138.36\nTable 4: (Left) The average pragmatic verification pass rate (%) across seeds of Luban and ablation\nbaselines. (Right) The Spearman correlation (ρ and p-value) between the pass rate of (‘Luban w\/o\nvv’, ‘Luban w\/o vvpv’) and quality ratings FB. The correlation item of arrow-tower task is ‘n\/a’\ndue to the constant pass rate.\nTask ID\nLuban w\/o pv\nLuban w\/o vv\nLuban w\/o vvpv\nρ (p)\narrow-tower\n66.67\n0.00\n0.00\nn\/a (n\/a)\nbridge\n100.00\n55.56\n55.56\n0.53 (0.28)\nchinese-ancient-house\n33.33\n33.33\n66.67\n0.60 (0.21)\nstair\n100.00\n0.00\n25.00\n0.77 (0.07)\ntwo-story-house\n33.33\n0.00\n33.33\n-0.42 (0.41)\n5.4\nPotential in Real-World Embodied Creative Tasks\nLuban has potential in real-world open-ended creative tasks rather than being limited to Minecraft-like\nvirtual worlds, which owes to the general of Luban’s planning framework (i.e., CAD modeling and\nvisual verification) and feedback mechanism (i.e., propose actions to verify not well-defined goals).\nWe demonstrate this by providing demos on two tasks (chinese-ancient-house and bridge) of\nLuban on real-world embodied robotic arm environment, as shown in Figure 6. Specifically, we\nfirst 3D print the model of subcomponents and then use the subcomponent coordinates given by the\nassembling sub-stage to drive the pick-place API-based embodied robotic arm to perform creative\nbuilding tasks. The final assembly result demonstrates the conclusion.\n6\nConclusion\nIn this work, we propose Luban, an agent capable of open-ended creative building tasks in Minecraft,\npowered by the two-level autonomous embodied visual and pragmatic verifications. Extensive human\nstudies demonstrate that Luban’s creations have higher quality (especially functional pragmatism) in\nFigure 6: The robotic demos of task chinese-ancient-house and bridge.\n9\nmultiple dimensions and are more preferred by humans than the other method baselines. Furthermore,\nLuban also shows the potential of Luban in real-world creative tasks through demos we provided on\nembodied robotic arms environment. Our work may inspire the following directions: (1) Develop\nlibraries that represent the 3D physical world to bridge VLM and the physical world, thereby\nfacilitating the emergence of embodied agents with spatial intelligence. (2) Extend Luban’s pragmatic\nverification to obtain feedback in the real world, thereby building a closed-loop, open creative agent\ngrounding in the real world.\n7\nLimitations\nWe summarize our limitations as follows: (1) Due to the lack of a memory mechanism, Luban cannot\nutilize shared knowledge between multiple tasks (e.g., universal design guidelines) and thus cannot\nlearn from the environment continuously; (2) The expensive access costs and limited capabilities of\nadvanced VLM (i.e., GPT-4V) prevent Luban from generating more refined 3D structure inference.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Luban：构建开放式的创造性智能体\n\n## 📌 背景痛点\/本文动机\n在人工智能领域，构建能够自主解决复杂任务的开放式智能体一直是研究的目标。特别是创造性任务，由于其开放的目标和抽象的评估标准，对智能体提出了更高的要求。现有的语言模型（LLM）智能体在处理具有明确目标的长期任务方面表现出色，但在面对创造性任务时，由于缺乏明确的评估标准，难以进行自我改进。\n\n## 🚀 核心方法\n本文提出了Luban智能体，旨在通过自主的身体验证技术解决创造性任务中的评估难题。Luban智能体采用了两级自主身体验证机制，灵感来源于人类的设计实践：\n\n1. **视觉验证**：Luban首先通过CAD程序合成3D结构模型，并进行视觉验证，确保模型的外观符合设计要求。\n2. **实用验证**：在通过视觉验证后，Luban会生成与抽象标准相关的环境相关功能程序，并进行实用验证，确保模型的功能性。\n\n## 📈 实验结果\n通过在Minecraft中设计包含5个具有不同视觉和功能要求的建筑任务的基准测试，Luban智能体在所有任务中均成功完成，并且在可视化方面和实用性方面均优于其他基线方法（33%至100%）。此外，Luban在真实世界的机械臂上的演示也展示了其在物理世界中进行开放式创造性任务的潜力。\n\n## 💬 可借鉴之处\nLuban智能体的设计为构建开放式创造性智能体提供了新的思路，其两级自主身体验证机制为解决创造性任务中的评估难题提供了有效的方法。此外，Luban的设计也展示了将虚拟世界中的智能体技术应用于真实世界的潜力。","llm_summary_res_status":200}
