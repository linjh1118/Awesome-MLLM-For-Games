
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks</td>
      <td>Building a general-purpose agent is a long-standing vision in the field of<br>artificial intelligence. Existing agents have made remarkable progress in many<br>domains, yet they still struggle to complete long-horizon tasks in an open<br>world. We attribute this to the lack of necessary world knowledge and<br>multimodal experience that can guide agents through a variety of long-horizon<br>tasks. In this paper, we propose a Hybrid Multimodal Memory module to address<br>the above challenges. It 1) transforms knowledge into Hierarchical Directed<br>Knowledge Graph that allows agents to explicitly represent and learn world<br>knowledge, and 2) summarises historical information into Abstracted Multimodal<br>Experience Pool that provide agents with rich references for in-context<br>learning. On top of the Hybrid Multimodal Memory module, a multimodal agent,<br>Optimus-1, is constructed with dedicated Knowledge-guided Planner and<br>Experience-Driven Reflector, contributing to a better planning and reflection<br>in the face of long-horizon tasks in Minecraft. Extensive experimental results<br>show that Optimus-1 significantly outperforms all existing agents on<br>challenging long-horizon task benchmarks, and exhibits near human-level<br>performance on many tasks. In addition, we introduce various Multimodal Large<br>Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show<br>that Optimus-1 exhibits strong generalization with the help of the Hybrid<br>Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.</td>
      <td>## 🌟 论文解读 | Optimus-1：混合多模态记忆赋能代理在长时任务中表现出色<br><br>## 📌 背景痛点/本文动机<br>构建一个通用代理是人工智能领域长期以来的愿景。尽管现有的代理在许多领域取得了显著进展，但它们仍然难以在开放世界中完成长时任务。本文认为，这主要是因为缺乏必要的世界知识和多模态经验，这些知识和经验可以指导代理完成各种长时任务。<br><br>## 🚀 核心方法<br>💡 创新点1：混合多模态记忆模块<br>本文提出了一个混合多模态记忆模块，该模块由分层有向知识图（HDKG）和抽象多模态经验池（AMEP）组成。HDKG将知识转化为分层有向图结构，使代理能够显式地表示和学习世界知识。AMEP将历史信息总结为抽象多模态经验池，为代理提供丰富的参考，以便进行上下文学习。<br><br>💡 创新点2：Optimus-1代理<br>在混合多模态记忆模块的基础上，本文构建了一个多模态代理Optimus-1，该代理由知识引导规划器、经验驱动反思器和动作控制器组成。知识引导规划器利用HDKG来捕获完成任务所需的知识，并将其转化为可执行的子目标序列。动作控制器根据子目标和当前观察生成低级动作，与游戏环境交互以更新代理的状态。经验驱动反思器定期激活，从AMEP中检索相关的多模态经验，以评估当前子目标是否可以成功执行。如果不行，它会指示知识引导规划器修改其计划。<br><br>## 📈 实验结果<br>在Minecraft中进行的广泛实验结果表明，Optimus-1在具有挑战性的长时任务基准测试中显著优于所有现有代理，并在许多任务上表现出接近人类水平的性能。此外，本文引入了各种多模态大型语言模型（MLLMs）作为Optimus-1的骨干。实验结果表明，在混合多模态记忆模块的帮助下，Optimus-1在各种任务上表现出强大的泛化能力，优于GPT-4V基线。<br><br>## 💬 可借鉴之处<br>本文提出的混合多模态记忆模块和Optimus-1代理为构建能够完成长时任务的通用代理提供了新的思路和方法。此外，本文提出的非参数学习方法也为代理的学习和进化提供了新的思路。</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy</td>
      <td>Building an agent that can mimic human behavior patterns to accomplish<br>various open-world tasks is a long-term goal. To enable agents to effectively<br>learn behavioral patterns across diverse tasks, a key challenge lies in<br>modeling the intricate relationships among observations, actions, and language.<br>To this end, we propose Optimus-2, a novel Minecraft agent that incorporates a<br>Multimodal Large Language Model (MLLM) for high-level planning, alongside a<br>Goal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP<br>contains (1) an Action-guided Behavior Encoder that models causal relationships<br>between observations and actions at each timestep, then dynamically interacts<br>with the historical observation-action sequence, consolidating it into<br>fixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with<br>open-ended language instructions to predict actions auto-regressively.<br>Moreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}<br>dataset, which contains 25,000 videos across 8 atomic tasks, providing about<br>30M goal-observation-action pairs. The automated construction method, along<br>with the MGOA dataset, can contribute to the community's efforts to train<br>Minecraft agents. Extensive experimental results demonstrate that Optimus-2<br>exhibits superior performance across atomic tasks, long-horizon tasks, and<br>open-ended instruction tasks in Minecraft. Please see the project page at<br>https://cybertronagent.github.io/Optimus-2.github.io/.</td>
      <td>## 🌟 论文解读 | Optimus-2：基于多模态大语言模型的Minecraft智能体<br><br>## 📌 背景痛点/本文动机<br>在开放世界环境中，构建能够模仿人类行为模式并完成各种任务的智能体一直是人工智能领域的长期目标。然而，要使智能体有效地学习跨任务的行为模式，关键挑战在于建模观察、动作和语言之间的复杂关系。现有的智能体在处理开放世界环境中的多样化任务时，通常采用任务规划器和目标条件策略的框架。尽管现有的智能体在利用多模态大语言模型（MLLM）作为规划器方面取得了进展，但目标条件策略的性能瓶颈仍然存在。现有的策略通常忽略了观察和动作之间的关系，并且难以建模开放式的子目标和观察-动作序列之间的关系。<br><br>## 🚀 核心方法<br>为了解决上述挑战，本文提出了Optimus-2，一个新颖的Minecraft智能体，它结合了MLLM进行高级规划，并采用目标-观察-动作条件策略（GOAP）进行低级控制。GOAP包含两个关键组件：<br><br>💡 创新点1：动作引导的行为编码器<br>动作引导的行为编码器用于建模观察-动作序列。它首先使用因果感知器将动作嵌入到观察特征中，利用任务相关的动作信息作为指导来调整观察特征，从而为动作预测提供细粒度的观察-动作信息。此外，为了在不超出输入长度限制的情况下对长期观察-动作序列进行建模，引入了历史聚合器，将当前观察-动作信息与历史序列动态地整合成固定长度的行为标记。行为标记可以以固定且适当的长度捕获观察-动作序列的长期依赖关系，使智能体能够预测与观察-动作序列逻辑一致的动作，而不是仅基于当前观察进行孤立的动作预测。<br><br>💡 创新点2：多模态大语言模型<br>为了明确编码子目标的语义，引入了MLLM作为GOAP的骨干网络。它将子目标与行为标记对齐，以自回归方式预测后续动作。利用MLLM的语言理解和多模态感知能力，它可以更好地整合开放式子目标和观察-动作序列的特征，从而增强策略的动作预测能力。<br><br>## 📈 实验结果<br>在Minecraft的开放世界环境中进行了广泛的评估，实验结果表明Optimus-2在原子任务、长期任务和开放式指令任务中表现出优异的性能。与之前的SOTA相比，Optimus-2在原子任务、长期任务和开放式子目标任务上分别实现了平均27%、10%和18%的提升。<br><br>## 💬 可借鉴之处<br>本文提出的Optimus-2智能体及其GOAP策略为开放世界环境中的智能体设计提供了新的思路。动作引导的行为编码器和MLLM的引入有效地解决了观察、动作和语言之间的复杂关系建模问题，使得智能体能够更好地理解和执行开放式指令。此外，本文提出的MGOA数据集为训练Minecraft智能体提供了高质量的数据资源，有助于推动相关研究的发展。</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos</td>
      <td>Pretraining on noisy, internet-scale datasets has been heavily studied as a<br>technique for training models with broad, general capabilities for text,<br>images, and other modalities. However, for many sequential decision domains<br>such as robotics, video games, and computer use, publicly available data does<br>not contain the labels required to train behavioral priors in the same way. We<br>extend the internet-scale pretraining paradigm to sequential decision domains<br>through semi-supervised imitation learning wherein agents learn to act by<br>watching online unlabeled videos. Specifically, we show that with a small<br>amount of labeled data we can train an inverse dynamics model accurate enough<br>to label a huge unlabeled source of online data -- here, online videos of<br>people playing Minecraft -- from which we can then train a general behavioral<br>prior. Despite using the native human interface (mouse and keyboard at 20Hz),<br>we show that this behavioral prior has nontrivial zero-shot capabilities and<br>that it can be fine-tuned, with both imitation learning and reinforcement<br>learning, to hard-exploration tasks that are impossible to learn from scratch<br>via reinforcement learning. For many tasks our models exhibit human-level<br>performance, and we are the first to report computer agents that can craft<br>diamond tools, which can take proficient humans upwards of 20 minutes (24,000<br>environment actions) of gameplay to accomplish.</td>
      <td>## 🌟 论文解读 | 视频预训练（VPT）：通过观看无标签在线视频学习行动<br><br>## 📌 背景痛点/本文动机<br>近年来，在自然语言处理和计算机视觉等领域，通过在大型互联网数据集上进行预训练，已经证明了训练大型通用基础模型的有效性。然而，对于许多序列决策领域，如机器人、视频游戏和计算机使用，公开可用的数据并不包含训练行为先验所需的标签。本文旨在通过利用互联网上大量未标记的视频数据，将这些预训练范式扩展到序列决策领域。<br><br>## 🚀 核心方法<br>💡 创新点1：半监督模仿学习<br>本文提出了一种半监督模仿学习方法，通过观看在线未标记的视频，使智能体学会行动。具体来说，使用少量标记数据训练一个逆动力学模型，该模型足够准确，可以标记大量未标记的在线数据（例如，人们玩Minecraft的视频），然后从中训练一个通用的行为先验。<br><br>💡 创新点2：逆动力学模型<br>与行为克隆相比，逆动力学建模任务更简单，因为它是非因果的，这意味着它可以查看过去和未来的帧来推断动作。在大多数情况下，环境机制比环境中可能发生的人类行为的广度要简单得多，这表明非因果逆动力学模型可能需要比因果行为克隆模型少得多的数据来训练。<br><br>## 📈 实验结果<br>实验结果表明，尽管使用了原生人类界面（20Hz的鼠标和键盘），但这种方法的行为先验具有非平凡的零样本能力，并且可以通过模仿学习和强化学习进行微调，以解决强化学习无法从头学习的困难探索任务。对于许多任务，模型表现出人类水平的性能，并且是第一个报告能够制作钻石工具的计算机代理，这需要熟练的人类玩家超过20分钟（24,000个环境动作）的游戏时间才能完成。<br><br>## 💬 可借鉴之处<br>本文提出的视频预训练（VPT）方法为利用互联网上大量未标记的数据进行序列决策领域的预训练提供了一种新的思路。该方法不仅能够有效地利用少量标记数据，而且能够通过模仿学习和强化学习进行微调，以解决强化学习无法从头学习的困难探索任务。此外，该方法还可以应用于任何具有大量未标记数据的领域，具有广泛的应用前景。</td>
    </tr>
    <tr>
      <th>3</th>
      <td>GROOT: Learning to Follow Instructions by Watching Gameplay Videos</td>
      <td>We study the problem of building a controller that can follow open-ended<br>instructions in open-world environments. We propose to follow reference videos<br>as instructions, which offer expressive goal specifications while eliminating<br>the need for expensive text-gameplay annotations. A new learning framework is<br>derived to allow learning such instruction-following controllers from gameplay<br>videos while producing a video instruction encoder that induces a structured<br>goal space. We implement our agent GROOT in a simple yet effective<br>encoder-decoder architecture based on causal transformers. We evaluate GROOT<br>against open-world counterparts and human players on a proposed Minecraft<br>SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the<br>human-machine gap as well as exhibiting a 70% winning rate over the best<br>generalist agent baseline. Qualitative analysis of the induced goal space<br>further demonstrates some interesting emergent properties, including the goal<br>composition and complex gameplay behavior synthesis. The project page is<br>available at https://craftjarvis-groot.github.io.</td>
      <td>## 🌟 论文解读 | GROOT：通过观看游戏视频学习指令遵循<br><br>## 📌 背景痛点/本文动机<br>在开放世界环境中，构建能够遵循开放指令的控制器一直是人工智能领域的长期目标。然而，现有的控制器通常只能完成预定义的、有限的程序性任务，这限制了它们在开放世界环境中的应用。本文旨在解决这个问题，提出了一种新的学习框架，通过观看游戏视频来学习指令遵循控制器。<br><br>## 🚀 核心方法<br>💡 创新点1：将目标指定为参考游戏视频片段，从而提供丰富的目标规范，同时消除对昂贵的文本-游戏注释的需求。<br>💡 创新点2：引入了一种新的学习框架，该框架同时产生一个目标空间和一个视频指令遵循控制器，从而实现从游戏视频中学习指令遵循控制器。<br><br>## 📈 实验结果<br>在Minecraft SkillForge基准测试中，GROOT在整体Elo评分比较中超过了最先进的基线，并且在解决具有挑战性的获取钻石任务中表现出色。<br><br>## 💬 可借鉴之处<br>本文提出的学习框架和GROOT代理的架构设计为构建能够遵循开放指令的控制器提供了新的思路和方法。此外，本文还展示了目标空间和控制器策略的潜在应用，为解决开放世界环境中的复杂任务提供了新的可能性。</td>
    </tr>
    <tr>
      <th>4</th>
      <td>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</td>
      <td>Autonomous agents have made great strides in specialist domains like Atari<br>games and Go. However, they typically learn tabula rasa in isolated<br>environments with limited and manually conceived objectives, thus failing to<br>generalize across a wide spectrum of tasks and capabilities. Inspired by how<br>humans continually learn and adapt in the open world, we advocate a trinity of<br>ingredients for building generalist agents: 1) an environment that supports a<br>multitude of tasks and goals, 2) a large-scale database of multimodal<br>knowledge, and 3) a flexible and scalable agent architecture. We introduce<br>MineDojo, a new framework built on the popular Minecraft game that features a<br>simulation suite with thousands of diverse open-ended tasks and an<br>internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and<br>forum discussions. Using MineDojo's data, we propose a novel agent learning<br>algorithm that leverages large pre-trained video-language models as a learned<br>reward function. Our agent is able to solve a variety of open-ended tasks<br>specified in free-form language without any manually designed dense shaping<br>reward. We open-source the simulation suite, knowledge bases, algorithm<br>implementation, and pretrained models (https://minedojo.org) to promote<br>research towards the goal of generally capable embodied agents.</td>
      <td>## 🌟 论文解读 | MineDojo：构建具有互联网规模知识的开放式具身智能体<br><br>## 📌 背景痛点/本文动机<br>传统的自主智能体在特定领域（如Atari游戏和围棋）取得了巨大进步，但它们通常在孤立的环境中学习，目标有限且手动设计，因此无法在广泛的任务和能力之间进行泛化。相比之下，人类在开放世界中不断学习和适应，能够利用大量来自自身经验和他人的先验知识。本文旨在构建能够像人类一样在开放世界中学习和适应的通用智能体。<br><br>## 🚀 核心方法<br>💡 创新点1：开放式环境<br>MineDojo基于流行的Minecraft游戏，提供了一个具有数千个多样化开放式任务的模拟套件。这些任务包括生存、采集、技术树和战斗等类别，涵盖了从简单到复杂的各种难度级别。<br><br>💡 创新点2：互联网规模的多模态知识库<br>MineDojo收集了大量的Minecraft相关数据，包括YouTube视频、Wiki页面和Reddit讨论。这些数据涵盖了游戏的所有方面，为智能体提供了丰富的先验知识。<br><br>💡 创新点3：灵活可扩展的智能体架构<br>MineDojo提出了一个基于Transformer预训练范式的智能体学习算法。该算法利用大规模预训练的视频语言模型作为学习奖励函数，能够解决各种开放式任务，而无需手动设计密集的奖励函数。<br><br>## 📈 实验结果<br>MineDojo的实验结果表明，其提出的MINECLIP奖励模型在程序性任务和创造性任务上都取得了良好的性能。与手动设计的密集奖励函数相比，MINECLIP在大多数任务上取得了竞争性的性能，并且在某些情况下甚至超过了它们。此外，MINECLIP还能够有效地评估创造性任务，其评估结果与人类判断高度一致。<br><br>## 💬 可借鉴之处<br>MineDojo为开发通用智能体提供了一个有价值的框架。其开放式环境、互联网规模的知识库和灵活可扩展的智能体架构为研究人员提供了探索开放式智能体学习的强大工具。此外，MineDojo的开源代码和数据集将促进社区对通用智能体研究的进一步发展。</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Mastering Diverse Domains through World Models</td>
      <td>Developing a general algorithm that learns to solve tasks across a wide range<br>of applications has been a fundamental challenge in artificial intelligence.<br>Although current reinforcement learning algorithms can be readily applied to<br>tasks similar to what they have been developed for, configuring them for new<br>application domains requires significant human expertise and experimentation.<br>We present DreamerV3, a general algorithm that outperforms specialized methods<br>across over 150 diverse tasks, with a single configuration. Dreamer learns a<br>model of the environment and improves its behavior by imagining future<br>scenarios. Robustness techniques based on normalization, balancing, and<br>transformations enable stable learning across domains. Applied out of the box,<br>Dreamer is the first algorithm to collect diamonds in Minecraft from scratch<br>without human data or curricula. This achievement has been posed as a<br>significant challenge in artificial intelligence that requires exploring<br>farsighted strategies from pixels and sparse rewards in an open world. Our work<br>allows solving challenging control problems without extensive experimentation,<br>making reinforcement learning broadly applicable.</td>
      <td>## 🌟 论文解读 | DreamerV3：通过世界模型掌握多样化领域<br><br>## 📌 背景痛点/本文动机<br>强化学习在解决特定任务方面取得了显著进展，但将算法应用于新领域通常需要大量的人工调整和实验。这限制了强化学习的通用性和实用性。本文提出了 DreamerV3，一个通用的强化学习算法，能够在多种不同的任务中表现出色，而无需针对每个任务进行重新配置。<br><br>## 🚀 核心方法<br>💡 创新点1：世界模型学习<br>DreamerV3 通过学习一个世界模型来预测潜在行动的结果，从而让智能体能够想象未来的场景并做出更好的决策。世界模型使用循环状态空间模型（RSSM）来预测未来的状态和奖励，并通过重建输入来确保表示信息丰富。<br><br>💡 创新点2：鲁棒性技术<br>DreamerV3 采用了一系列鲁棒性技术，包括归一化、平衡和转换，以确保算法能够在不同的领域稳定学习。这些技术帮助 DreamerV3 在各种环境中表现出色，包括连续控制、离散动作、稀疏奖励、图像输入、空间环境和棋盘游戏等。<br><br>## 📈 实验结果<br>DreamerV3 在超过 150 个多样化的任务中表现出色，超过了专门为特定领域设计的算法。在 Atari、ProcGen、DMLab、Atari100k、Proprio Control、Visual Control 和 BSuite 等基准测试中，DreamerV3 都取得了最佳性能。此外，DreamerV3 还成功地在 Minecraft 游戏中收集钻石，这是人工智能领域的一个重大挑战。<br><br>## 💬 可借鉴之处<br>DreamerV3 的成功表明，通过学习世界模型和采用鲁棒性技术，可以开发出通用的强化学习算法，从而解决各种不同的任务。这项工作为未来研究提供了新的方向，包括从互联网视频中教授智能体世界知识，以及学习跨领域的单一世界模型，以使人工智能体能够积累更广泛的知识和能力。</td>
    </tr>
    <tr>
      <th>6</th>
      <td>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft</td>
      <td>Constructing AI models that respond to text instructions is challenging,<br>especially for sequential decision-making tasks. This work introduces a<br>methodology, inspired by unCLIP, for instruction-tuning generative models of<br>behavior without relying on a large dataset of instruction-labeled<br>trajectories. Using this methodology, we create an instruction-tuned Video<br>Pretraining (VPT) model called STEVE-1, which can follow short-horizon<br>open-ended text and visual instructions in Minecraft. STEVE-1 is trained in two<br>steps: adapting the pretrained VPT model to follow commands in MineCLIP's<br>latent space, then training a prior to predict latent codes from text. This<br>allows us to finetune VPT through self-supervised behavioral cloning and<br>hindsight relabeling, reducing the need for costly human text annotations, and<br>all for only $60 of compute. By leveraging pretrained models like VPT and<br>MineCLIP and employing best practices from text-conditioned image generation,<br>STEVE-1 sets a new bar for open-ended instruction-following in Minecraft with<br>low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming<br>previous baselines and robustly completing 12 of 13 tasks in our early-game<br>evaluation suite. We provide experimental evidence highlighting key factors for<br>downstream performance, including pretraining, classifier-free guidance, and<br>data scaling. All resources, including our model weights, training scripts, and<br>evaluation tools are made available for further research.</td>
      <td>## 🌟 论文解读 | Minecraft 中的文本到行为生成模型：STEVE-1<br><br>## 📌 背景痛点/本文动机<br>构建能够响应文本指令的 AI 模型是一项挑战，尤其是在需要顺序决策的任务中。现有的模型往往需要大量带有指令标签的轨迹数据集，这既昂贵又难以获取。本文旨在解决这一问题，提出了一种无需依赖大量指令标签轨迹数据集的方法，用于构建能够响应文本指令的行为生成模型。<br><br>## 🚀 核心方法<br>💡 创新点1：受 unCLIP 启发的指令微调方法<br>本文提出了一种受 unCLIP 启发的指令微调方法，用于构建行为生成模型。该方法将问题分解为两个模型：一个用于生成行为轨迹的策略模型，另一个用于将文本指令转换为视觉嵌入的先验模型。通过这种方式，可以避免使用昂贵的文本指令标签，而是利用视觉嵌入进行训练。<br><br>💡 创新点2：基于 VPT 和 MineCLIP 的模型构建<br>本文利用了两个预训练模型：VPT 和 MineCLIP。VPT 是一个基于 Minecraft 游戏视频预训练的行为模型，而 MineCLIP 是一个将文本和视频片段对齐的模型。通过将这两个模型结合起来，可以构建一个能够响应文本指令的行为生成模型。<br><br>💡 创新点3：基于自监督学习和回溯重标记的微调<br>本文使用自监督学习和回溯重标记技术对 VPT 模型进行微调。通过这种方式，可以减少对昂贵的人类文本注释的需求，并利用现有的数据集进行训练。<br><br>## 📈 实验结果<br>实验结果表明，STEVE-1 在 Minecraft 中能够有效地响应文本指令，并完成各种任务。与之前的基线模型相比，STEVE-1 在低级控制（鼠标和键盘）和原始像素输入方面取得了显著的性能提升。此外，实验还表明，预训练、分类器无关引导和数据缩放等因素对下游性能至关重要。<br><br>## 💬 可借鉴之处<br>本文提出的指令微调方法可以应用于其他领域和任务，例如机器人控制、虚拟现实等。此外，本文还强调了预训练、分类器无关引导和数据缩放等因素对下游性能的重要性，为构建更强大的 AI 模型提供了参考。</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction</td>
      <td>We study the problem of learning goal-conditioned policies in Minecraft, a<br>popular, widely accessible yet challenging open-ended environment for<br>developing human-level multi-task agents. We first identify two main challenges<br>of learning such policies: 1) the indistinguishability of tasks from the state<br>distribution, due to the vast scene diversity, and 2) the non-stationary nature<br>of environment dynamics caused by partial observability. To tackle the first<br>challenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage<br>the emergence of goal-relevant visual state representations. To tackle the<br>second challenge, the policy is further fueled by an adaptive horizon<br>prediction module that helps alleviate the learning uncertainty brought by the<br>non-stationary dynamics. Experiments on 20 Minecraft tasks show that our method<br>significantly outperforms the best baseline so far; in many of them, we double<br>the performance. Our ablation and exploratory studies then explain how our<br>approach beat the counterparts and also unveil the surprising bonus of<br>zero-shot generalization to new scenes (biomes). We hope our agent could help<br>shed some light on learning goal-conditioned, multi-task agents in challenging,<br>open-ended environments like Minecraft.</td>
      <td>## 🌟 论文解读 | 在开放世界中实现多任务控制：目标感知表示学习和自适应预测<br><br>## 📌 背景痛点/本文动机<br>开放世界环境，如Minecraft，为开发能够执行各种任务的智能体提供了丰富的平台。然而，这些环境也带来了独特的挑战，包括：<br>1. **状态分布的多样性**：由于场景的多样性，不同任务的状态难以区分，这使得学习目标条件策略变得困难。<br>2. **环境动态的非平稳性**：由于部分可观察性，环境动态具有非平稳性，导致学习的不确定性增加。<br><br>## 🚀 核心方法<br>为了解决这些挑战，本文提出了以下创新方法：<br>💡 创新点1：**目标感知骨干网络（GSB**）<br>   - GSB通过在多个层次上融合目标信息，鼓励出现与目标相关的视觉状态表示，从而解决状态分布多样性的问题。<br>   - GSB由多个目标卷积块（g-conv block）组成，这些块通过通道调制将目标信息与视觉特征融合。<br><br>💡 创新点2：**自适应预测模块**<br>   - 为了应对环境动态的非平稳性，本文引入了自适应预测模块，该模块预测从当前状态到目标的剩余时间步数（即距离到目标的距离）。<br>   - 自适应预测模块通过预测剩余时间步数，帮助智能体更好地理解目标的完成程度，从而提高决策的准确性。<br><br>## 📈 实验结果<br>在Minecraft的20个任务上进行的实验表明，本文提出的方法显著优于现有基线，在许多任务中性能翻倍。消融研究和探索性研究解释了本文方法如何优于现有方法，并揭示了令人惊讶的零样本泛化到新场景（生物群落）的额外优势。<br><br>## 💬 可借鉴之处<br>本文提出的GSB和自适应预测模块为在开放世界环境中学习目标条件策略提供了新的思路，并为开发能够在复杂环境中执行多任务的智能体提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>8</th>
      <td>MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control</td>
      <td>It is a long-lasting goal to design a generalist-embodied agent that can<br>follow diverse instructions in human-like ways. However, existing approaches<br>often fail to steadily follow instructions due to difficulties in understanding<br>abstract and sequential natural language instructions. To this end, we<br>introduce MineDreamer, an open-ended embodied agent built upon the challenging<br>Minecraft simulator with an innovative paradigm that enhances<br>instruction-following ability in low-level control signal generation.<br>Specifically, MineDreamer is developed on top of recent advances in Multimodal<br>Large Language Models (MLLMs) and diffusion models, and we employ a<br>Chain-of-Imagination (CoI) mechanism to envision the step-by-step process of<br>executing instructions and translating imaginations into more precise visual<br>prompts tailored to the current state; subsequently, the agent generates<br>keyboard-and-mouse actions to efficiently achieve these imaginations, steadily<br>following the instructions at each step. Extensive experiments demonstrate that<br>MineDreamer follows single and multi-step instructions steadily, significantly<br>outperforming the best generalist agent baseline and nearly doubling its<br>performance. Moreover, qualitative analysis of the agent's imaginative ability<br>reveals its generalization and comprehension of the open world.</td>
      <td>## 🌟 论文解读 | MineDreamer：基于想象链的模拟世界控制指令跟随<br><br>## 📌 背景痛点/本文动机<br>在人工智能领域，设计一个能够以人类方式理解和执行多样化指令的通用型具身智能体一直是长期目标。然而，现有的方法往往难以稳定地遵循指令，尤其是在理解和执行抽象和顺序的自然语言指令方面存在困难。<br><br>## 🚀 核心方法<br>💡 创新点1：引入“想象链”（Chain-of-Imagination, CoI）机制<br>MineDreamer 通过 CoI 机制，使智能体能够根据指令和当前状态逐步想象并执行指令。这种方法模拟了人类在解决问题时，根据当前状态逐步想象下一步目标的过程。<br><br>💡 创新点2：多模态大型语言模型（MLLM）增强的扩散模型<br>MineDreamer 使用 MLLM 增强的扩散模型来生成包含物理规则和环境理解的想象图像，这些图像作为更精确的视觉提示，引导智能体生成低级控制信号。<br><br>💡 创新点3：目标漂移收集方法<br>为了训练 Imaginator，MineDreamer 使用了目标漂移收集方法来收集大量具身数据，帮助 Imaginator 理解如何逐步完成指令以及如何重复完成指令。<br><br>## 📈 实验结果<br>MineDreamer 在执行单步和多步指令方面表现出色，显著优于最佳通用型智能体基线，性能几乎翻倍。此外，对智能体想象能力的定性分析表明，它能够理解和适应开放世界的环境。<br><br>## 💬 可借鉴之处<br>MineDreamer 的 CoI 机制为解决指令跟随问题提供了一种新颖的方法，其 MLLM 增强的扩散模型和目标漂移收集方法也为具身智能体的发展提供了新的思路。此外，MineDreamer 的成功也表明，具身智能体在开放世界环境中具有巨大的潜力。</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations</td>
      <td>Recently, both Contrastive Learning (CL) and Mask Image Modeling (MIM)<br>demonstrate that self-supervision is powerful to learn good representations.<br>However, naively combining them is far from success. In this paper, we start by<br>making the empirical observation that a naive joint optimization of CL and MIM<br>losses leads to conflicting gradient directions - more severe as the layers go<br>deeper. This motivates us to shift the paradigm from combining loss at the end,<br>to choosing the proper learning method per network layer. Inspired by<br>experimental observations, we find that MIM and CL are suitable to lower and<br>higher layers, respectively. We hence propose to combine them in a surprisingly<br>simple, "sequential cascade" fashion: early layers are first trained under one<br>MIM loss, on top of which latter layers continue to be trained under another CL<br>loss. The proposed Layer Grafted Pre-training learns good visual<br>representations that demonstrate superior label efficiency in downstream<br>applications, in particular yielding strong few-shot performance besides linear<br>evaluation. For instance, on ImageNet-1k, Layer Grafted Pre-training yields<br>65.5% Top-1 accuracy in terms of 1% few-shot learning with ViT-B/16, which<br>improves MIM and CL baselines by 14.4% and 2.1% with no bells and whistles. The<br>code is available at<br>https://github.com/VITA-Group/layerGraftedPretraining_ICLR23.git.</td>
      <td>## 🌟 论文解读 | Layer Grafted Pre-training：结合对比学习和掩码图像建模，提升标签效率<br><br>## 📌 背景痛点/本文动机<br>近年来，对比学习（CL）和掩码图像建模（MIM）都证明了自监督学习在视觉表征学习中的强大能力。然而，简单地结合这两种方法并没有取得理想的效果。本文作者通过实验发现，将CL和MIM的损失函数同时优化会导致梯度方向冲突，并且随着网络层数的加深，冲突变得更加严重。这促使作者重新思考如何更好地结合这两种方法。<br><br>## 🚀 核心方法<br>💡 创新点1：层级嫁接预训练<br>本文提出了层级嫁接预训练（Layer Grafted Pre-training）框架，该框架将MIM和CL分别应用于网络的不同层级。具体来说，首先使用MIM损失函数训练网络的低层，然后在这些层的基础上，使用CL损失函数继续训练网络的高层。这种“顺序级联”的方式有效地避免了MIM和CL损失函数之间的冲突，并使每个损失函数都能在其最合适的层级上进行预训练。<br><br>💡 创新点2：平滑嫁接<br>为了进一步优化层级嫁接预训练，本文提出了“平滑嫁接”的策略。具体来说，在第二步中，不仅训练高层，还允许低层以较小的学习率进行微调。这种策略可以避免特征空间中突然的变化，并使低层能够更好地保留MIM学习到的特征。<br><br>## 📈 实验结果<br>在ImageNet-1k数据集上，层级嫁接预训练在1%少样本学习任务中取得了65.5%的Top-1准确率，比MIM和CL基线分别提高了14.4%和2.1%。此外，层级嫁接预训练在10%少样本学习和线性评估任务中也取得了显著的性能提升。<br><br>## 💬 可借鉴之处<br>层级嫁接预训练框架为结合对比学习和掩码图像建模提供了一种简单而有效的方法。该方法可以应用于各种视觉表征学习任务，并有望提升标签效率。此外，本文提出的“平滑嫁接”策略也为其他多阶段预训练任务提供了新的思路。</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Vision-Language Models Provide Promptable Representations for Reinforcement Learning</td>
      <td>Humans can quickly learn new behaviors by leveraging background world<br>knowledge. In contrast, agents trained with reinforcement learning (RL)<br>typically learn behaviors from scratch. We thus propose a novel approach that<br>uses the vast amounts of general and indexable world knowledge encoded in<br>vision-language models (VLMs) pre-trained on Internet-scale data for embodied<br>RL. We initialize policies with VLMs by using them as promptable<br>representations: embeddings that encode semantic features of visual<br>observations based on the VLM's internal knowledge and reasoning capabilities,<br>as elicited through prompts that provide task context and auxiliary<br>information. We evaluate our approach on visually-complex, long horizon RL<br>tasks in Minecraft and robot navigation in Habitat. We find that our policies<br>trained on embeddings from off-the-shelf, general-purpose VLMs outperform<br>equivalent policies trained on generic, non-promptable image embeddings. We<br>also find our approach outperforms instruction-following methods and performs<br>comparably to domain-specific embeddings. Finally, we show that our approach<br>can use chain-of-thought prompting to produce representations of common-sense<br>semantic reasoning, improving policy performance in novel scenes by 1.5 times.</td>
      <td>## 🌟 论文解读 | 利用视觉语言模型为强化学习提供可提示的表示<br><br>## 📌 背景痛点/本文动机<br>人类能够快速学习新行为，这得益于他们丰富的背景知识和推理能力。然而，传统的强化学习（RL）训练的智能体通常需要从头开始学习，缺乏利用背景知识的能力。为了解决这个问题，本文提出了一种新的方法，利用在互联网规模数据上预训练的视觉语言模型（VLM）来为具身RL提供丰富的世界知识。<br><br>## 🚀 核心方法<br>本文提出了一个名为“可提示表示的强化学习”（PR2L）的框架，通过向VLM提供任务相关的提示，使其生成包含语义信息的表示，并将其用于训练RL策略。具体来说，PR2L包含以下几个关键步骤：<br><br>1. **提示设计**：设计任务相关的提示，引导VLM关注并编码图像中与任务相关的语义特征。<br>2. **表示提取**：使用VLM对提示和图像进行编码，并提取中间层的表示作为状态表示。<br>3. **策略训练**：使用RL算法训练策略，将提取的表示转换为低级动作。<br><br>## 📈 实验结果<br>本文在Minecraft和Habitat两个领域进行了实验，结果表明PR2L在以下方面取得了显著优势：<br><br>* **性能提升**：与使用非提示图像嵌入或指令条件的方法相比，PR2L训练的策略在Minecraft和Habitat任务中取得了更好的性能。<br>* **样本效率**：PR2L能够利用VLM的先验知识，从而减少训练所需的样本数量。<br>* **泛化能力**：PR2L能够利用VLM的推理能力，使策略在新的场景中表现出更好的泛化能力。<br><br>## 💬 可借鉴之处<br>PR2L为利用VLM进行强化学习提供了一种新的思路，具有以下可借鉴之处：<br><br>* **提示设计**：设计有效的提示是PR2L成功的关键，需要根据任务特点进行精心设计。<br>* **表示选择**：选择合适的VLM层和表示方法，以提取对任务有用的语义信息。<br>* **策略训练**：使用合适的RL算法和策略网络结构，以充分利用VLM提供的表示。<br><br>## 🌟 未来展望<br>随着VLM能力的不断提升，PR2L有望在更多领域得到应用，并为强化学习带来新的突破。</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Reinforcement Learning Friendly Vision-Language Model for Minecraft</td>
      <td>One of the essential missions in the AI research community is to build an<br>autonomous embodied agent that can achieve high-level performance across a wide<br>spectrum of tasks. However, acquiring or manually designing rewards for all<br>open-ended tasks is unrealistic. In this paper, we propose a novel cross-modal<br>contrastive learning framework architecture, CLIP4MC, aiming to learn a<br>reinforcement learning (RL) friendly vision-language model (VLM) that serves as<br>an intrinsic reward function for open-ended tasks. Simply utilizing the<br>similarity between the video snippet and the language prompt is not RL-friendly<br>since standard VLMs may only capture the similarity at a coarse level. To<br>achieve RL-friendliness, we incorporate the task completion degree into the VLM<br>training objective, as this information can assist agents in distinguishing the<br>importance between different states. Moreover, we provide neat YouTube datasets<br>based on the large-scale YouTube database provided by MineDojo. Specifically,<br>two rounds of filtering operations guarantee that the dataset covers enough<br>essential information and that the video-text pair is highly correlated.<br>Empirically, we demonstrate that the proposed method achieves better<br>performance on RL tasks compared with baselines. The code and datasets are<br>available at https://github.com/PKU-RL/CLIP4MC.</td>
      <td>## 🌟 论文解读 | 基于CLIP4MC的强化学习友好型视觉语言模型在Minecraft中的应用<br><br>## 📌 背景痛点/本文动机<br>在人工智能研究领域，构建一个能够在各种任务中实现高级性能的自主具身智能体是核心目标之一。然而，为所有开放性任务手动获取或设计奖励函数是不切实际的。为了解决这个问题，本文提出了一种新的跨模态对比学习框架架构CLIP4MC，旨在学习一个强化学习（RL）友好的视觉语言模型（VLM），该模型可以作为开放性任务的内在奖励函数。<br><br>## 🚀 核心方法<br>💡 创新点1：构建了两个高质量的数据集<br>- 第一个数据集经过数据清洗和全局级相关性过滤，训练出的VLM性能与官方发布的MineCLIP相当。<br>- 第二个数据集进一步结合了局部级相关性过滤，更适合RL任务，训练出的VLM性能优于第一个数据集。<br><br>💡 创新点2：提出CLIP4MC模型<br>- CLIP4MC模型通过将任务完成程度纳入VLM训练目标，使模型能够更好地反映每个视频片段与任务完成程度之间的关系，从而为RL训练过程提供更友好的奖励信号。<br>- CLIP4MC模型在MineDojo的编程任务上取得了更好的性能，证明了其在RL任务中的有效性。<br><br>## 📈 实验结果<br>- 在MineDojo的编程任务中，CLIP4MC模型在狩猎任务上取得了显著更高的成功率，证明了其在RL任务中的有效性。<br>- 通过相关性分析，CLIP4MC模型的奖励信号与目标实体的尺寸具有更高的相关性，这对于RL训练过程至关重要。<br><br>## 💬 可借鉴之处<br>- 本文提出的CLIP4MC模型和构建的数据集为RL任务中的奖励函数设计提供了新的思路和方法。<br>- 本文的研究结果表明，将任务完成程度纳入VLM训练目标可以有效地提高RL训练性能。<br>- 本文的研究成果可以应用于其他开放性任务中的RL训练，例如自动驾驶、机器人控制等。</td>
    </tr>
    <tr>
      <th>12</th>
      <td>ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting</td>
      <td>Vision-language models (VLMs) have excelled in multimodal tasks, but adapting<br>them to embodied decision-making in open-world environments presents<br>challenges. One critical issue is bridging the gap between discrete entities in<br>low-level observations and the abstract concepts required for effective<br>planning. A common solution is building hierarchical agents, where VLMs serve<br>as high-level reasoners that break down tasks into executable sub-tasks,<br>typically specified using language. However, language suffers from the<br>inability to communicate detailed spatial information. We propose<br>visual-temporal context prompting, a novel communication protocol between VLMs<br>and policy models. This protocol leverages object segmentation from past<br>observations to guide policy-environment interactions. Using this approach, we<br>train ROCKET-1, a low-level policy that predicts actions based on concatenated<br>visual observations and segmentation masks, supported by real-time object<br>tracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to<br>tackle complex tasks that demand spatial reasoning. Experiments in Minecraft<br>show that our approach enables agents to achieve previously unattainable tasks,<br>with a \( \mathbf{76}\% \) absolute improvement in open-world interaction<br>performance. Codes and demos are now available on the project page:<br>https://craftjarvis.github.io/ROCKET-1.</td>
      <td>## 🌟 论文解读 | ROCKET-1：掌握开放世界交互的视觉-时间上下文提示<br><br>## 📌 背景痛点/本文动机<br>视觉-语言模型（VLMs）在多模态任务中表现出色，但在开放世界环境中进行具身决策时面临挑战。一个关键问题是弥合低级观察中离散实体与有效规划所需的抽象概念之间的差距。一种常见的解决方案是构建分层代理，其中VLMs作为高级推理器，将任务分解为可执行的子任务，通常使用语言指定。然而，语言在传达详细的空间信息方面存在局限性。<br><br>## 🚀 核心方法<br>💡 创新点1：视觉-时间上下文提示<br>本文提出了一种新颖的通信协议，称为视觉-时间上下文提示，用于VLMs和政策模型之间的通信。该协议利用过去观察到的对象分割来指导策略-环境交互。<br><br>💡 创新点2：ROCKET-1<br>使用这种方法，我们训练了ROCKET-1，这是一个低级策略，它根据连接的视觉观察和分割掩码预测动作，并由SAM-2的实时对象跟踪支持。我们的方法释放了VLMs的潜力，使它们能够处理需要空间推理的复杂任务。<br><br>## 📈 实验结果<br>在Minecraft中的实验表明，我们的方法使代理能够完成以前无法完成的任务，开放世界交互性能提高了76%。<br><br>## 💬 可借鉴之处<br>本文提出的视觉-时间上下文提示协议为VLMs和政策模型之间的通信提供了一种新颖的方法，有助于解决开放世界环境中的具身决策问题。ROCKET-1作为一种低级策略，能够有效地处理需要空间推理的复杂任务，并具有实时对象跟踪能力。此外，本文提出的向后轨迹重新标记方法可以自动检测和分割收集的轨迹中的所需对象，为ROCKET-1的训练提供了便利。</td>
    </tr>
    <tr>
      <th>13</th>
      <td>GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents</td>
      <td>Developing agents that can follow multimodal instructions remains a<br>fundamental challenge in robotics and AI. Although large-scale pre-training on<br>unlabeled datasets (no language instruction) has enabled agents to learn<br>diverse behaviors, these agents often struggle with following instructions.<br>While augmenting the dataset with instruction labels can mitigate this issue,<br>acquiring such high-quality annotations at scale is impractical. To address<br>this issue, we frame the problem as a semi-supervised learning task and<br>introduce GROOT-2, a multimodal instructable agent trained using a novel<br>approach that combines weak supervision with latent variable models. Our method<br>consists of two key components: constrained self-imitating, which utilizes<br>large amounts of unlabeled demonstrations to enable the policy to learn diverse<br>behaviors, and human intention alignment, which uses a smaller set of labeled<br>demonstrations to ensure the latent space reflects human intentions. GROOT-2's<br>effectiveness is validated across four diverse environments, ranging from video<br>games to robotic manipulation, demonstrating its robust multimodal<br>instruction-following capabilities.</td>
      <td>## 🌟 论文解读 | GROOT-2：弱监督多模态指令跟随代理<br><br>## 📌 背景痛点/本文动机<br>在机器人学和人工智能领域，开发能够遵循多模态指令的代理仍然是一个基本挑战。尽管在大规模无标签数据集上进行预训练（没有语言指令）已经使代理能够学习多样化的行为，但这些代理在遵循指令时往往遇到困难。虽然通过增加数据集的指令标签可以缓解这个问题，但在大规模上获取高质量注释是不切实际的。为了解决这个问题，我们将问题框架化为半监督学习任务，并引入了GROOT-2，这是一个多模态可指令代理，使用一种结合弱监督和潜在变量模型的新方法进行训练。<br><br>## 🚀 核心方法<br>💡 创新点1：约束自我模仿<br>利用大量未标记的演示来使策略能够学习多样化的行为。<br><br>💡 创新点2：人类意图对齐<br>使用较小的一组标记演示来确保潜在空间反映人类意图。<br><br>## 📈 实验结果<br>GROOT-2的有效性在四个不同的环境中得到验证，从视频游戏到机器人操作，展示了其强大的多模态指令跟随能力。<br><br>## 💬 可借鉴之处<br>GROOT-2在多模态指令跟随方面具有强大的能力，可以应用于各种环境，包括视频游戏和机器人操作。此外，GROOT-2的训练方法可以应用于其他需要弱监督学习的任务。</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents</td>
      <td>We investigate the challenge of task planning for multi-task embodied agents<br>in open-world environments. Two main difficulties are identified: 1) executing<br>plans in an open-world environment (e.g., Minecraft) necessitates accurate and<br>multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla<br>planners do not consider how easy the current agent can achieve a given<br>sub-task when ordering parallel sub-goals within a complicated plan, the<br>resulting plan could be inefficient or even infeasible. To this end, we propose<br>"\( \underline{D} \)escribe, \( \underline{E} \)xplain, \( \underline{P} \)lan and<br>\( \underline{S} \)elect" (\( \textbf{DEPS} \)), an interactive planning approach based<br>on Large Language Models (LLMs). DEPS facilitates better error correction on<br>initial LLM-generated \( \textit{plan} \) by integrating \( \textit{description} \) of<br>the plan execution process and providing self-\( \textit{explanation} \) of<br>feedback when encountering failures during the extended planning phases.<br>Furthermore, it includes a goal \( \textit{selector} \), which is a trainable<br>module that ranks parallel candidate sub-goals based on the estimated steps of<br>completion, consequently refining the initial plan. Our experiments mark the<br>milestone of the first zero-shot multi-task agent that can robustly accomplish<br>70+ Minecraft tasks and nearly double the overall performances. Further testing<br>reveals our method's general effectiveness in popularly adopted non-open-ended<br>domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and<br>exploratory studies detail how our design beats the counterparts and provide a<br>promising update on the \( \texttt{ObtainDiamond} \) grand challenge with our<br>approach. The code is released at https://github.com/CraftJarvis/MC-Planner.</td>
      <td>## 🌟 论文解读 | 基于大型语言模型的交互式规划，助力开放世界多任务智能体<br><br>## 📌 背景痛点/本文动机<br>在开放世界环境中，多任务智能体面临着两大挑战：1）执行计划需要精确的多步推理，因为任务具有长期性；2）传统的规划器在排序复杂的计划中的并行子目标时，没有考虑当前智能体完成给定子任务的难易程度，导致生成的计划可能效率低下甚至不可行。<br><br>## 🚀 核心方法<br>本文提出了“描述、解释、规划和选择”（DEPS）的交互式规划方法，基于大型语言模型（LLMs）来解决上述挑战。<br><br>💡 创新点1：描述、解释和规划<br>DEPS 通过集成计划执行过程的描述和提供自我解释的反馈，更好地纠正初始 LLM 生成的计划中的错误。当遇到失败时，描述器会总结当前情况并发送给 LLM，LLM 作为解释器定位错误，然后根据描述器和解释器的信息更新计划。<br><br>💡 创新点2：目标选择器<br>DEPS 包含一个可训练的目标选择器模块，该模块根据完成每个并行候选子目标的估计步骤对它们进行排序，从而细化初始计划。选择器使用预测剩余时间步数来完成每个目标，并根据当前状态选择最接近的目标。<br><br>## 📈 实验结果<br>实验结果表明，DEPS 在开放世界环境（如 Minecraft）中取得了显著的成果，能够稳健地完成 70 多个任务，并且整体性能几乎翻倍。此外，DEPS 在非开放世界环境（如 ALFWorld 和桌面操作）中也表现出良好的效果。<br><br>## 💬 可借鉴之处<br>DEPS 的交互式规划方法为开放世界多任务智能体的开发提供了新的思路。通过集成描述、解释和规划，以及使用目标选择器，DEPS 能够生成更可靠和高效的计划，从而提高智能体在开放世界环境中的任务完成能力。</td>
    </tr>
    <tr>
      <th>15</th>
      <td>LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation</td>
      <td>To assist with everyday human activities, robots must solve complex<br>long-horizon tasks and generalize to new settings. Recent deep reinforcement<br>learning (RL) methods show promise in fully autonomous learning, but they<br>struggle to reach long-term goals in large environments. On the other hand,<br>Task and Motion Planning (TAMP) approaches excel at solving and generalizing<br>across long-horizon tasks, thanks to their powerful state and action<br>abstractions. But they assume predefined skill sets, which limits their<br>real-world applications. In this work, we combine the benefits of these two<br>paradigms and propose an integrated task planning and skill learning framework<br>named LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages the<br>symbolic interface of a task planner to guide RL-based skill learning and<br>creates abstract state space to enable skill reuse. More importantly, LEAGUE<br>learns manipulation skills in-situ of the task planning system, continuously<br>growing its capability and the set of tasks that it can solve. We evaluate<br>LEAGUE on four challenging simulated task domains and show that LEAGUE<br>outperforms baselines by large margins. We also show that the learned skills<br>can be reused to accelerate learning in new tasks domains and transfer to a<br>physical robot platform.</td>
      <td>## 🌟 论文解读 | LEAGUE：基于引导的技能学习和抽象，助力机器人解决长期操作任务<br><br>## 📌 背景痛点/本文动机<br>随着人工智能技术的不断发展，机器人已经逐渐走进我们的日常生活，并在各种场景中发挥着重要作用。然而，要让机器人真正实现自主学习和操作，仍然面临着许多挑战。其中，长期操作任务（long-horizon tasks）的解决和泛化能力是机器人领域的一大难题。现有的深度强化学习（DRL）方法在自主学习方面表现出色，但在大型环境中实现长期目标仍然存在困难。另一方面，任务和运动规划（TAMP）方法擅长解决和泛化长期任务，但由于其依赖于预定义的技能集，限制了其在现实世界中的应用。<br><br>## 🚀 核心方法<br>为了克服上述挑战，本文提出了LEAGUE（Learning and Abstraction with Guidance）框架，该框架结合了DRL和TAMP的优势，实现了长期操作任务的解决和泛化。LEAGUE的核心创新点包括：<br><br>💡 创新点1：利用任务规划器的符号接口指导基于强化学习的技能学习，并创建抽象状态空间以实现技能复用。<br>💡 创新点2：在任务规划系统中学习操作技能，不断扩展其能力和可解决的任务集。<br><br>## 📈 实验结果<br>本文在四个具有挑战性的模拟任务领域对LEAGUE进行了评估，结果表明LEAGUE在性能上显著优于基线方法。此外，本文还展示了学习到的技能可以复用于加速新任务领域的学习，并迁移到物理机器人平台。<br><br>## 💬 可借鉴之处<br>LEAGUE框架为机器人解决长期操作任务提供了一种新的思路，其核心思想可以应用于其他领域，例如自动驾驶、游戏AI等。此外，LEAGUE框架中的技能学习和抽象方法也可以为其他强化学习算法提供借鉴。</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Learning from Visual Observation via Offline Pretrained State-to-Go Transformer</td>
      <td>Learning from visual observation (LfVO), aiming at recovering policies from<br>only visual observation data, is promising yet a challenging problem. Existing<br>LfVO approaches either only adopt inefficient online learning schemes or<br>require additional task-specific information like goal states, making them not<br>suited for open-ended tasks. To address these issues, we propose a two-stage<br>framework for learning from visual observation. In the first stage, we<br>introduce and pretrain State-to-Go (STG) Transformer offline to predict and<br>differentiate latent transitions of demonstrations. Subsequently, in the second<br>stage, the STG Transformer provides intrinsic rewards for downstream<br>reinforcement learning tasks where an agent learns merely from intrinsic<br>rewards. Empirical results on Atari and Minecraft show that our proposed method<br>outperforms baselines and in some tasks even achieves performance comparable to<br>the policy learned from environmental rewards. These results shed light on the<br>potential of utilizing video-only data to solve difficult visual reinforcement<br>learning tasks rather than relying on complete offline datasets containing<br>states, actions, and rewards. The project's website and code can be found at<br>https://sites.google.com/view/stgtransformer.</td>
      <td>## 🌟 论文解读 | 利用离线预训练的State-to-Go Transformer从视觉观察中学习<br><br>## 📌 背景痛点/本文动机<br>强化学习（RL）从零开始学习面临着样本效率低下和探索困难的问题，尤其是在稀疏奖励的环境中。这导致了模仿学习（IL）的发展，其中智能体通过模仿专家演示来学习策略，而不是通过试错过程。然而，获取演示动作可能既昂贵又不可行，例如，从大量可用的视频中获取。因此，从观察中学习（LfO）的研究兴起，它利用关于智能体行为和状态转换的观察数据来学习策略。LfO的挑战在于从原始视觉观察中提取有用特征，并使用它们来训练策略，因为缺乏明确的动作信息。<br><br>## 🚀 核心方法<br>💡 创新点1：本文提出了一种两阶段框架，利用专家演示的视觉观察来指导在线强化学习。在第一阶段，引入并离线预训练State-to-Go（STG）Transformer，以预测和区分演示的潜在转换。同时，学习时间对齐和可预测的视觉表示。在第二阶段，STG Transformer为下游强化学习任务提供内在奖励，其中智能体仅从内在奖励中学习。<br><br>💡 创新点2：同时学习判别器和时间距离回归器，以预测潜在转换的同时学习时间对齐嵌入。实验结果表明，联合学习的表示在下游RL任务中表现出增强的性能。<br><br>## 📈 实验结果<br>在Atari和Minecraft上的实验结果表明，本文提出的方法在样本效率和整体性能方面都显著优于基线方法，并在某些游戏中甚至达到了与从环境奖励中学习的策略相当的性能。这突出了利用离线视频数据来解决困难的视觉强化学习任务的潜力，而不是依赖于包含状态、动作和奖励的完整离线数据集。<br><br>## 💬 可借鉴之处<br>本文提出的两阶段框架和STG Transformer模型为从视觉观察中学习提供了新的思路和方法。同时，本文提出的判别器和时间距离回归器对于学习时间对齐嵌入和预测潜在转换具有重要作用。此外，本文的实验结果也表明，内在奖励对于指导在线强化学习任务具有重要意义。</td>
    </tr>
    <tr>
      <th>17</th>
      <td>LLaMA Rider: Spurring Large Language Models to Explore the Open World</td>
      <td>Recently, various studies have leveraged Large Language Models (LLMs) to help<br>decision-making and planning in environments, and try to align the LLMs'<br>knowledge with the world conditions. Nonetheless, the capacity of LLMs to<br>continuously acquire environmental knowledge and adapt in an open world remains<br>uncertain. In this paper, we propose an approach to spur LLMs to explore the<br>open world, gather experiences, and learn to improve their task-solving<br>capabilities. In this approach, a multi-round feedback-revision mechanism is<br>utilized to encourage LLMs to actively select appropriate revision actions<br>guided by feedback information from the environment. This facilitates<br>exploration and enhances the model's performance. Besides, we integrate<br>sub-task relabeling to assist LLMs in maintaining consistency in sub-task<br>planning and help the model learn the combinatorial nature between tasks,<br>enabling it to complete a wider range of tasks through training based on the<br>acquired exploration experiences. By evaluation in Minecraft, an open-ended<br>sandbox world, we demonstrate that our approach LLaMA-Rider enhances the<br>efficiency of the LLM in exploring the environment, and effectively improves<br>the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k<br>instances of collected data, showing minimal training costs compared to the<br>baseline using reinforcement learning.</td>
      <td>## 🌟 论文解读 | LLaMA Rider：激发大型语言模型探索开放世界<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在模拟人类智能方面取得了显著进展。许多研究开始利用LLMs的能力来帮助智能体在环境中进行决策，并发现LLMs具有一定的规划和完成任务的能力。然而，LLMs的知识来源于预训练时使用的语言语料库，可能与特定环境存在差异。为了将LLMs与实际环境相结合，一些研究通过提示工程设计特定机制，为LLMs提供环境信息。然而，LLMs在环境中并不会改进或获取新知识。此外，对于更复杂的任务，需要更复杂的机制和提示，这会导致LLMs生成成本高，并且依赖于像GPT-4这样具有足够知识的强大模型。还有一些研究通过微调来将LLMs与实际环境相结合，但这通常需要依赖于特定任务的训练数据集。强化学习（RL）方法也被研究，但这些方法将LLMs训练为特定任务的策略，并且我们发现RL方法难以扩展到更大的模型或更复杂的任务。<br><br>## 🚀 核心方法<br>本文提出了一种名为LLaMA-Rider的方法，旨在通过LLMs在开放环境中的探索来增强其能力。LLaMA-Rider是一个两阶段的学习框架，包括探索阶段和学习阶段。<br><br>### 💡 创新点1：探索阶段<br>在探索阶段，LLaMA-Rider利用反馈-修正机制来鼓励LLMs主动选择适当的修正动作，以适应环境。LLMs在环境中进行探索，收集经验，并通过反馈信息来改进其决策。此外，LLaMA-Rider还使用子任务重标记来帮助LLMs保持子任务规划的连贯性，并学习任务之间的组合性质。<br><br>### 💡 创新点2：学习阶段<br>在学习阶段，LLaMA-Rider将收集到的经验处理成数据集，并使用监督微调（SFT）来训练LLMs。除了从成功任务中获得的经验外，LLaMA-Rider还收集部分完成的子任务的经验，因为有些任务在探索阶段很难完成。开放环境中的许多任务通常具有组合性，这意味着过去任务的经验可以经常帮助完成其他任务。LLaMA-Rider使用子任务重标记来提高数据利用率，并帮助LLMs学习任务之间的组合性。<br><br>## 📈 实验结果<br>本文在Minecraft模拟器MineDojo上评估了LLaMA-Rider方法。实验结果表明，LLaMA-Rider能够有效地探索环境，并通过微调仅使用1.3k个收集到的数据实例来提高LLMs完成任务的能力，与使用强化学习的方法相比，训练成本更低。<br><br>## 💬 可借鉴之处<br>LLaMA-Rider方法为LLMs在开放环境中的探索和学习提供了一种有效的方法。其反馈-修正机制和子任务重标记技术可以帮助LLMs更好地适应环境，并提高其完成任务的能力。此外，LLaMA-Rider方法还可以扩展到其他开放环境，并具有终身探索和学习的潜力。</td>
    </tr>
    <tr>
      <th>18</th>
      <td>JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models</td>
      <td>Achieving human-like planning and control with multimodal observations in an<br>open world is a key milestone for more functional generalist agents. Existing<br>approaches can handle certain long-horizon tasks in an open world. However,<br>they still struggle when the number of open-world tasks could potentially be<br>infinite and lack the capability to progressively enhance task completion as<br>game time progresses. We introduce JARVIS-1, an open-world agent that can<br>perceive multimodal input (visual observations and human instructions),<br>generate sophisticated plans, and perform embodied control, all within the<br>popular yet challenging open-world Minecraft universe. Specifically, we develop<br>JARVIS-1 on top of pre-trained multimodal language models, which map visual<br>observations and textual instructions to plans. The plans will be ultimately<br>dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a<br>multimodal memory, which facilitates planning using both pre-trained knowledge<br>and its actual game survival experiences. JARVIS-1 is the existing most general<br>agent in Minecraft, capable of completing over 200 different tasks using<br>control and observation space similar to humans. These tasks range from<br>short-horizon tasks, e.g., "chopping trees" to long-horizon tasks, e.g.,<br>"obtaining a diamond pickaxe". JARVIS-1 performs exceptionally well in<br>short-horizon tasks, achieving nearly perfect performance. In the classic<br>long-term task of \( \texttt{ObtainDiamondPickaxe} \), JARVIS-1 surpasses the<br>reliability of current state-of-the-art agents by 5 times and can successfully<br>complete longer-horizon and more challenging tasks. The project page is<br>available at https://craftjarvis.org/JARVIS-1</td>
      <td>## 🌟 论文解读 | JARVIS-1：开放世界多任务智能体，迈向通用人工智能的关键一步<br><br>## 📌 背景痛点/本文动机<br>在开放世界中，构建能够完成各种任务的智能体一直是通用人工智能（AGI）领域的重要目标。然而，现有的方法在处理开放世界中无限数量的任务时仍然面临挑战，尤其是在任务完成能力随游戏时间推移而逐步提升方面。本文提出的JARVIS-1旨在解决这些问题，它是一个能够在开放世界中感知多模态输入（视觉观察和人类指令）、生成复杂计划并执行具身控制的智能体。<br><br>## 🚀 核心方法<br>💡 创新点1：多模态语言模型（MLM）<br>JARVIS-1基于预训练的多模态语言模型，能够将视觉观察和文本指令映射到计划中。这使得智能体能够更好地理解任务、情况和环境反馈，从而生成更准确的计划。<br><br>💡 创新点2：多模态记忆<br>JARVIS-1配备了多模态记忆，存储了过去的成功规划经验和场景。通过检索相关记忆条目，智能体的规划能力可以在上下文中得到加强，从而提高规划的正确性和一致性。<br><br>💡 创新点3：自我指导和自我改进<br>JARVIS-1能够通过自我指导机制自主生成任务，并通过探索世界来收集经验。这些经验被存储在多模态记忆中，帮助智能体在后续任务中更好地进行推理和规划。<br><br>## 📈 实验结果<br>JARVIS-1在Minecraft中表现出色，能够完成超过200个不同的任务，包括短期任务（如砍树）和长期任务（如获得钻石镐）。在经典的长任务“获得钻石镐”中，JARVIS-1的可靠性比现有最先进的智能体高出5倍，能够成功完成更长期和更具挑战性的任务。<br><br>## 💬 可借鉴之处<br>JARVIS-1的设计和实现为开放世界智能体的发展提供了重要的启示。通过结合多模态语言模型、多模态记忆和自我改进机制，JARVIS-1展示了在开放世界中实现通用人工智能的潜力。这些方法可以应用于其他开放世界环境，推动通用人工智能的发展。</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</td>
      <td>Recent studies have presented compelling evidence that large language models<br>(LLMs) can equip embodied agents with the self-driven capability to interact<br>with the world, which marks an initial step toward versatile robotics. However,<br>these efforts tend to overlook the visual richness of open worlds, rendering<br>the entire interactive process akin to "a blindfolded text-based game."<br>Consequently, LLM-based agents frequently encounter challenges in intuitively<br>comprehending their surroundings and producing responses that are easy to<br>understand. In this paper, we propose Steve-Eye, an end-to-end trained large<br>multimodal model designed to address this limitation. Steve-Eye integrates the<br>LLM with a visual encoder which enables it to process visual-text inputs and<br>generate multimodal feedback. In addition, we use a semi-automatic strategy to<br>collect an extensive dataset comprising 850K open-world instruction pairs,<br>empowering our model to encompass three essential functions for an agent:<br>multimodal perception, foundational knowledge base, and skill prediction and<br>planning. Lastly, we develop three open-world evaluation benchmarks, then carry<br>out extensive experiments from a wide range of perspectives to validate our<br>model's capability to strategically act and plan. Codes and datasets will be<br>released.</td>
      <td>## 🌟 论文解读 | Steve-Eye：为基于LLM的具身智能体赋予开放世界的视觉感知能力<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在赋予具身智能体与世界互动的能力方面取得了显著进展，这标志着通用机器人技术迈出了第一步。然而，这些研究往往忽略了开放世界的视觉丰富性，导致整个交互过程类似于“一个蒙着眼睛的基于文本的游戏”。因此，基于LLM的智能体在直观地理解周围环境和生成易于理解的响应方面经常遇到挑战。<br><br>## 🚀 核心方法<br>为了解决这一局限性，本文提出了Steve-Eye，这是一个端到端训练的大型多模态模型，旨在赋予基于LLM的具身智能体在开放世界中进行视觉感知的能力。Steve-Eye将LLM与视觉编码器相结合，使其能够处理视觉-文本输入并生成多模态反馈。此外，我们使用半自动策略收集了一个包含850K开放世界指令对的广泛数据集，使我们的模型能够涵盖智能体的三个基本功能：多模态感知、基础知识库和技能预测与规划。最后，我们开发了三个开放世界评估基准，然后从广泛的视角进行大量实验，以验证我们的模型在战略行动和规划方面的能力。<br><br>## 📈 实验结果<br>实验结果表明，Steve-Eye在开放世界场景中显著优于基于LLM的智能体。具体来说，我们在以下三个基准上进行了评估：<br>1. 环境视觉描述（ENV-VC）：评估智能体感知和描述其周围环境的能力。<br>2. 基础知识问答（FK-QA）：评估智能体掌握对决策至关重要的基本知识的熟练程度。<br>3. 技能预测与规划（SPP）：量化智能体在战略行动和规划方面的能力。<br><br>## 💬 可借鉴之处<br>Steve-Eye的研究成果为开发能够在开放世界中有效互动的具身智能体提供了重要的启示。其多模态感知、基础知识库和技能预测与规划功能为智能体在复杂环境中进行自主行动和规划提供了强大的支持。此外，Steve-Eye的开放世界评估基准为评估具身智能体的性能提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>20</th>
      <td>MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception</td>
      <td>It is a long-lasting goal to design an embodied system that can solve<br>long-horizon open-world tasks in human-like ways. However, existing approaches<br>usually struggle with compound difficulties caused by the logic-aware<br>decomposition and context-aware execution of these tasks. To this end, we<br>introduce MP5, an open-ended multimodal embodied system built upon the<br>challenging Minecraft simulator, which can decompose feasible sub-objectives,<br>design sophisticated situation-aware plans, and perform embodied action<br>control, with frequent communication with a goal-conditioned active perception<br>scheme. Specifically, MP5 is developed on top of recent advances in Multimodal<br>Large Language Models (MLLMs), and the system is modulated into functional<br>modules that can be scheduled and collaborated to ultimately solve pre-defined<br>context- and process-dependent tasks. Extensive experiments prove that MP5 can<br>achieve a 22% success rate on difficult process-dependent tasks and a 91%<br>success rate on tasks that heavily depend on the context. Moreover, MP5<br>exhibits a remarkable ability to address many open-ended tasks that are<br>entirely novel.</td>
      <td>## 🌟 论文解读 | MP5：基于主动感知的多模态开放式具身系统<br><br>## 📌 背景痛点/本文动机<br>在人工智能领域，设计一个能够以人类方式解决开放世界长时任务的具身系统一直是长期目标。然而，现有的方法通常难以应对这些任务中逻辑感知分解和上下文感知执行所带来的复合困难。为了解决这个问题，本文提出了MP5，一个基于Minecraft模拟器的开放式多模态具身系统，它能够分解可行的子目标，设计复杂的情境感知计划，并执行具身动作控制，同时与目标条件下的主动感知方案进行频繁通信。<br><br>## 🚀 核心方法<br>💡 创新点1：MP5基于最新的多模态大型语言模型（MLLMs）构建，并将系统分解为可调度和协作的功能模块，以解决预定义的上下文和过程依赖任务。<br>💡 创新点2：MP5包括一个主动感知方案，通过感知器与巡逻器之间的多轮交互，主动感知观察到的图像中的上下文信息，以解决上下文依赖任务。<br><br>## 📈 实验结果<br>MP5在困难的过程依赖任务上实现了22%的成功率，在高度依赖上下文的任务上实现了91%的成功率。此外，MP5表现出解决许多完全新颖的开放式任务的能力。<br><br>## 💬 可借鉴之处<br>MP5的设计和实现为解决开放世界长时任务提供了新的思路和方法，其主动感知方案和模块化设计对于开发更智能、更灵活的具身系统具有重要的参考价值。</td>
    </tr>
    <tr>
      <th>21</th>
      <td>OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents</td>
      <td>This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model<br>for open-world instruction-following agents in Minecraft. Compared to prior<br>works that either emit textual goals to separate controllers or produce the<br>control command directly, OmniJARVIS seeks a different path to ensure both<br>strong reasoning and efficient decision-making capabilities via unified<br>tokenization of multimodal interaction data. First, we introduce a<br>self-supervised approach to learn a behavior encoder that produces discretized<br>tokens for behavior trajectories \( \tau = \{o_0, a_0, \dots\} \) and an imitation<br>learning policy decoder conditioned on these tokens. These additional behavior<br>tokens will be augmented to the vocabulary of pretrained Multimodal Language<br>Models. With this encoder, we then pack long-term multimodal interactions<br>involving task instructions, memories, thoughts, observations, textual<br>responses, behavior trajectories, etc into unified token sequences and model<br>them with autoregressive transformers. Thanks to the semantically meaningful<br>behavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing<br>chain-of-thoughts), plan, answer questions, and act (by producing behavior<br>tokens for the imitation learning policy decoder). OmniJARVIS demonstrates<br>excellent performances on a comprehensive collection of atomic, programmatic,<br>and open-ended tasks in open-world Minecraft. Our analysis further unveils the<br>crucial design principles in interaction data formation, unified tokenization,<br>and its scaling potentials. The dataset, models, and code will be released at<br>https://craftjarvis.org/OmniJARVIS.</td>
      <td>## 🌟 论文解读 | OmniJARVIS：开启开放世界指令跟随代理的新篇章<br><br>## 📌 背景痛点/本文动机<br>随着预训练大型语言模型（LLMs）和多模态语言模型（MLMs）的成功，构建能够理解和执行指令的自主代理成为人工智能领域的重要目标。然而，现有的视觉-语言-行动（VLA）模型在开放世界环境中面临着两大挑战：复杂且高度依赖上下文的任务，以及需要长期规划的任务。现有的VLA模型要么依赖于文本指令与控制器进行通信，要么直接输出控制命令，这两种方法都存在局限性。<br><br>## 🚀 核心方法<br>OmniJARVIS提出了一种新的VLA模型，通过统一的多模态交互数据标记化方法，实现了强大的推理能力和高效的决策能力。其核心创新点包括：<br><br>💡 行为标记化：OmniJARVIS引入了一种自监督学习方法，学习一个行为编码器，将行为轨迹转换为离散的标记，并使用模仿学习策略解码器根据这些标记生成控制命令。<br><br>💡 自回归建模：通过将行为标记添加到预训练的多模态语言模型的词汇表中，OmniJARVIS将多模态交互数据打包成统一的标记序列，并使用自回归Transformer模型进行建模。<br><br>## 📈 实验结果<br>OmniJARVIS在开放世界的Minecraft环境中进行了广泛的评估，包括原子任务、程序性任务和开放性任务。结果表明，OmniJARVIS在各种任务中都表现出色，能够有效地进行推理、规划和行动。<br><br>## 💬 可借鉴之处<br>OmniJARVIS的设计为构建开放世界指令跟随代理提供了新的思路。其行为标记化和自回归建模方法可以有效地处理多模态交互数据，并实现强大的推理和决策能力。此外，OmniJARVIS的可扩展性也为未来的研究提供了广阔的空间。<br><br>## 🌟 总结<br>OmniJARVIS是一种创新的VLA模型，通过统一的多模态交互数据标记化方法，实现了强大的推理能力和高效的决策能力。它在开放世界的Minecraft环境中表现出色，为构建能够理解和执行指令的自主代理提供了新的可能性。</td>
    </tr>
    <tr>
      <th>22</th>
      <td>MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory</td>
      <td>Significant advances have been made in developing general-purpose embodied AI<br>in environments like Minecraft through the adoption of LLM-augmented<br>hierarchical approaches. While these approaches, which combine high-level<br>planners with low-level controllers, show promise, low-level controllers<br>frequently become performance bottlenecks due to repeated failures. In this<br>paper, we argue that the primary cause of failure in many low-level controllers<br>is the absence of an episodic memory system. To address this, we introduce<br>MrSteve (Memory Recall Steve-1), a novel low-level controller equipped with<br>Place Event Memory (PEM), a form of episodic memory that captures what, where,<br>and when information from episodes. This directly addresses the main limitation<br>of the popular low-level controller, Steve-1. Unlike previous models that rely<br>on short-term memory, PEM organizes spatial and event-based data, enabling<br>efficient recall and navigation in long-horizon tasks. Additionally, we propose<br>an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing<br>agents to alternate between exploration and task-solving based on recalled<br>events. Our approach significantly improves task-solving and exploration<br>efficiency compared to existing methods. We will release our code and demos on<br>the project page: https://sites.google.com/view/mr-steve.</td>
      <td>## 🌟 论文解读 | MrSteve：Minecraft中具有What-Where-When记忆的指令跟随代理<br><br>## 📌 背景痛点/本文动机<br>在Minecraft等复杂环境中，开发通用的具身AI代理面临着巨大挑战。尽管LLM增强的分层方法取得了显著进展，但低级控制器往往成为性能瓶颈，因为它们缺乏对过去事件的记忆，导致重复失败和低效探索。<br><br>## 🚀 核心方法<br>💡 创新点1：MrSteve (Memory Recall Steve-1)<br>本文提出了MrSteve，一个新型的低级控制器，它配备了Place Event Memory (PEM)，一种能够捕获What-Where-When信息的情景记忆系统。PEM超越了Steve-1等现有模型的短期记忆限制，能够有效地组织和检索空间和事件数据，从而在长期任务中实现高效的回忆和导航。<br><br>💡 创新点2：探索策略和记忆增强任务解决框架<br>为了充分利用PEM，本文还提出了探索策略和记忆增强任务解决框架。该框架允许代理根据回忆的事件在探索和任务解决之间进行切换，从而显著提高任务解决和探索效率。<br><br>## 📈 实验结果<br>实验结果表明，与现有方法相比，MrSteve在探索和长期任务解决方面都取得了显著的性能提升。例如，在ABA-Sparse任务中，MrSteve能够更快地找到任务相关的资源，并在有限的时间内完成任务。<br><br>## 💬 可借鉴之处<br>本文提出的PEM和记忆增强框架为开发更高效的低级控制器提供了新的思路。PEM的组织和检索机制可以应用于其他具身AI代理，以提高它们在复杂环境中的性能。此外，探索策略和任务解决框架的设计也可以为其他AI系统提供参考，以实现更灵活和自适应的行为。</td>
    </tr>
    <tr>
      <th>23</th>
      <td>Voyager: An Open-Ended Embodied Agent with Large Language Models</td>
      <td>We introduce Voyager, the first LLM-powered embodied lifelong learning agent<br>in Minecraft that continuously explores the world, acquires diverse skills, and<br>makes novel discoveries without human intervention. Voyager consists of three<br>key components: 1) an automatic curriculum that maximizes exploration, 2) an<br>ever-growing skill library of executable code for storing and retrieving<br>complex behaviors, and 3) a new iterative prompting mechanism that incorporates<br>environment feedback, execution errors, and self-verification for program<br>improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses<br>the need for model parameter fine-tuning. The skills developed by Voyager are<br>temporally extended, interpretable, and compositional, which compounds the<br>agent's abilities rapidly and alleviates catastrophic forgetting. Empirically,<br>Voyager shows strong in-context lifelong learning capability and exhibits<br>exceptional proficiency in playing Minecraft. It obtains 3.3x more unique<br>items, travels 2.3x longer distances, and unlocks key tech tree milestones up<br>to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill<br>library in a new Minecraft world to solve novel tasks from scratch, while other<br>techniques struggle to generalize. We open-source our full codebase and prompts<br>at https://voyager.minedojo.org/.</td>
      <td>## 🌟 论文解读 | Voyager：基于大型语言模型的开放式具身终身学习智能体<br><br>## 📌 背景痛点/本文动机<br>构建能够在开放世界中持续探索、规划和开发新技能的通用具身智能体，是人工智能领域的一大挑战。传统的强化学习和模仿学习方法在探索、可解释性和泛化方面存在局限性。近年来，基于大型语言模型（LLM）的智能体利用预训练LLM中的世界知识生成一致的行动计划或可执行策略，但它们并非终身学习者，无法在长时间跨度内逐步获取、更新、积累和转移知识。<br><br>## 🚀 核心方法<br>Voyager 是第一个由 LLM 驱动的具身终身学习智能体，能够在 Minecraft 中持续探索世界、获取多样化技能，并在没有人类干预的情况下进行新的发现。Voyager 由三个关键组件组成：<br><br>💡 创新点1：自动课程<br>Voyager 通过自动课程进行开放式探索，该课程由 GPT-4 生成，旨在“发现尽可能多的多样化事物”。课程会根据探索进度和智能体的状态提出越来越难的任务，从而推动智能体不断学习新技能。<br><br>💡 创新点2：技能库<br>Voyager 拥有一个不断增长的技能库，用于存储和检索可执行代码，以存储和检索复杂的行为。每个技能都由可执行代码表示，这些代码可以自然地表示时间扩展和组合动作，这对于 Minecraft 中的许多长期任务至关重要。<br><br>💡 创新点3：迭代提示机制<br>Voyager 通过迭代提示机制生成可执行代码，该机制利用环境反馈、执行错误和自我验证来改进程序。该机制通过执行生成的程序、获取环境反馈和执行错误，并将这些反馈纳入 GPT-4 的提示中，从而进行代码改进。这个过程会重复进行，直到自我验证模块确认任务完成，此时将程序添加到技能库中，并查询自动课程以获取下一个目标。<br><br>## 📈 实验结果<br>Voyager 在 MineDojo 框架中与其他 LLM 基于智能体技术进行了比较，结果表明 Voyager 在发现新物品、解锁 Minecraft 技术树、穿越各种地形以及将学习到的技能库应用于新世界中的未见任务方面表现出色。Voyager 获得了 3.3 倍的新物品，解锁关键技术树里程碑的速度提高了 15.3 倍，穿越的距离是基线的 2.3 倍。<br><br>## 💬 可借鉴之处<br>Voyager 的设计为开发强大的通用智能体提供了一个起点，无需调整模型参数。其自动课程、技能库和迭代提示机制为终身学习智能体的开发提供了新的思路。此外，Voyager 的技能库可以作为其他方法的即插即用资产，有效地提高性能。</td>
    </tr>
    <tr>
      <th>24</th>
      <td>Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory</td>
      <td>The captivating realm of Minecraft has attracted substantial research<br>interest in recent years, serving as a rich platform for developing intelligent<br>agents capable of functioning in open-world environments. However, the current<br>research landscape predominantly focuses on specific objectives, such as the<br>popular "ObtainDiamond" task, and has not yet shown effective generalization to<br>a broader spectrum of tasks. Furthermore, the current leading success rate for<br>the "ObtainDiamond" task stands at around 20%, highlighting the limitations of<br>Reinforcement Learning (RL) based controllers used in existing methods. To<br>tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel<br>framework integrates Large Language Models (LLMs) with text-based knowledge and<br>memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These<br>agents, equipped with the logic and common sense capabilities of LLMs, can<br>skillfully navigate complex, sparse-reward environments with text-based<br>interactions. We develop a set of structured actions and leverage LLMs to<br>generate action plans for the agents to execute. The resulting LLM-based agent<br>markedly surpasses previous methods, achieving a remarkable improvement of<br>+47.5% in success rate on the "ObtainDiamond" task, demonstrating superior<br>robustness compared to traditional RL-based controllers. Notably, our agent is<br>the first to procure all items in the Minecraft Overworld technology tree,<br>demonstrating its extensive capabilities. GITM does not need any GPU for<br>training, but a single CPU node with 32 CPU cores is enough. This research<br>shows the potential of LLMs in developing capable agents for handling<br>long-horizon, complex tasks and adapting to uncertainties in open-world<br>environments. See the project website at https://github.com/OpenGVLab/GITM.</td>
      <td>## 🌟 论文解读 | Minecraft中的幽灵：通过大型语言模型和基于文本的知识与记忆，创建开放世界环境中的通用能力智能体<br><br>## 📌 背景痛点/本文动机<br>Minecraft作为一款开放世界游戏，吸引了大量研究兴趣，成为开发能够在开放世界中运行的智能体的丰富平台。然而，目前的研究主要集中在特定目标上，如流行的“ObtainDiamond”任务，尚未在更广泛的任务上展现出有效的泛化能力。此外，现有方法在“ObtainDiamond”任务上的最高成功率仅为约20%，突显了现有基于强化学习（RL）的控制器方法的局限性。为了解决这些挑战，本文提出了Ghost in the Minecraft（GITM）框架，该框架将大型语言模型（LLMs）与基于文本的知识和记忆相结合，旨在创建能够在Minecraft中运行的通用能力智能体（GCAs）。<br><br>## 🚀 核心方法<br>💡 创新点1：LLM分解器<br>LLM分解器负责将任务目标分解为一系列更易于实现的子目标。通过解决每个子目标，可以逐步实现任务目标。LLM分解器利用从互联网收集的文本知识，将目标分解为子目标树。<br><br>💡 创新点2：LLM规划器<br>LLM规划器负责为每个子目标生成一系列结构化操作。结构化操作具有明确的语义和相应的反馈，使LLMs能够在认知层面理解周围环境并做出决策。LLM规划器还记录和总结成功的操作列表，以增强未来的规划。<br><br>💡 创新点3：LLM接口<br>LLM接口负责将结构化操作转换为键盘/鼠标操作，并与环境进行交互。它还从环境中提取观察结果，并将其转换为反馈消息。<br><br>## 📈 实验结果<br>本文的实验结果表明，基于LLM的智能体在“ObtainDiamond”任务上的成功率显著提高，达到了47.5%，超过了现有的方法。此外，该智能体是第一个在Minecraft Overworld中获取所有物品的智能体，展示了其广泛的技能。<br><br>## 💬 可借鉴之处<br>本文提出的GITM框架为开发能够在开放世界中运行的通用能力智能体提供了一种新的思路。通过利用LLMs的常识和推理能力，以及基于文本的知识和记忆，该框架能够使智能体有效地处理开放世界环境中的各种挑战。此外，该框架还具有高效的学习效率和良好的泛化能力，使其在开发通用能力智能体方面具有巨大的潜力。</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Creative Agents: Empowering Agents with Imagination for Creative Tasks</td>
      <td>We study building embodied agents for open-ended creative tasks. While<br>existing methods build instruction-following agents that can perform diverse<br>open-ended tasks, none of them demonstrates creativity -- the ability to give<br>novel and diverse task solutions implicit in the language instructions. This<br>limitation comes from their inability to convert abstract language instructions<br>into concrete task goals in the environment and perform long-horizon planning<br>for such complicated goals. Given the observation that humans perform creative<br>tasks with the help of imagination, we propose a class of solutions for<br>creative agents, where the controller is enhanced with an imaginator that<br>generates detailed imaginations of task outcomes conditioned on language<br>instructions. We introduce several approaches to implementing the components of<br>creative agents. We implement the imaginator with either a large language model<br>for textual imagination or a diffusion model for visual imagination. The<br>controller can either be a behavior-cloning policy learned from data or a<br>pre-trained foundation model generating executable codes in the environment. We<br>benchmark creative tasks with the challenging open-world game Minecraft, where<br>the agents are asked to create diverse buildings given free-form language<br>instructions. In addition, we propose novel evaluation metrics for open-ended<br>creative tasks utilizing GPT-4V, which holds many advantages over existing<br>metrics. We perform a detailed experimental analysis of creative agents,<br>showing that creative agents are the first AI agents accomplishing diverse<br>building creation in the survival mode of Minecraft. Our benchmark and models<br>are open-source for future research on creative agents<br>(https://github.com/PKU-RL/Creative-Agents).</td>
      <td>## 🌟 论文解读 | 创意智能体：赋予智能体想象力以完成创意任务<br><br>## 📌 背景痛点/本文动机<br>现有的智能体大多只能执行预定义的任务，缺乏处理开放性任务的能力，尤其是那些需要创造力的任务。例如，在Minecraft游戏中，现有的智能体可以执行简单的指令，如“收集石头”或“建造一个雪人”，但无法完成更复杂的创意任务，如“建造一个砂岩宫殿”。这是因为它们无法将抽象的语言指令转换为具体的任务目标，并执行长期规划。<br><br>## 🚀 核心方法<br>本文提出了“创意智能体”的概念，通过赋予智能体想象力来处理开放性创意任务。创意智能体由两个主要部分组成：想象器和控制器。<br><br>💡 创新点1：想象器<br>想象器负责根据语言指令生成任务结果的详细想象。本文提出了两种实现想象器的方法：<br>- **文本想象**：使用大型语言模型（LLM）如GPT-4，通过Chain-of-Thought（CoT）技术生成文本想象。<br>- **视觉想象**：使用扩散模型如Stable Diffusion，生成与文本描述相符的视觉想象。<br><br>💡 创新点2：控制器<br>控制器负责将想象转换为可执行的计划。本文提出了两种实现控制器的方法：<br>- **行为克隆控制器**：从环境中学习行为克隆策略，将图像想象转换为建筑蓝图，并执行相应的动作。<br>- **基于GPT-4(V)的控制器**：利用GPT-4(V)的视觉语言理解和代码生成能力，直接生成可执行代码来完成任务。<br><br>## 📈 实验结果<br>本文在Minecraft游戏中进行了实验，结果表明，创意智能体能够根据自由形式的语言指令创建多样化和视觉上吸引人的建筑。其中，使用扩散模型进行视觉想象，并结合GPT-4(V)进行控制的智能体表现最佳。<br><br>## 💬 可借鉴之处<br>本文提出的创意智能体框架为开放性学习和创意AI智能体研究提供了新的思路和方法。未来可以进一步探索如何提高行为克隆控制器的性能，以及如何增强智能体的创造力。</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</td>
      <td>Many reinforcement learning environments (e.g., Minecraft) provide only<br>sparse rewards that indicate task completion or failure with binary values. The<br>challenge in exploration efficiency in such environments makes it difficult for<br>reinforcement-learning-based agents to learn complex tasks. To address this,<br>this paper introduces an advanced learning system, named Auto MC-Reward, that<br>leverages Large Language Models (LLMs) to automatically design dense reward<br>functions, thereby enhancing the learning efficiency. Auto MC-Reward consists<br>of three important components: Reward Designer, Reward Critic, and Trajectory<br>Analyzer. Given the environment information and task descriptions, the Reward<br>Designer first design the reward function by coding an executable Python<br>function with predefined observation inputs. Then, our Reward Critic will be<br>responsible for verifying the code, checking whether the code is<br>self-consistent and free of syntax and semantic errors. Further, the Trajectory<br>Analyzer summarizes possible failure causes and provides refinement suggestions<br>according to collected trajectories. In the next round, Reward Designer will<br>further refine and iterate the dense reward function based on feedback.<br>Experiments demonstrate a significant improvement in the success rate and<br>learning efficiency of our agents in complex tasks in Minecraft, such as<br>obtaining diamond with the efficient ability to avoid lava, and efficiently<br>explore trees and animals that are sparse in the plains biome.</td>
      <td>## 🌟 论文解读 | Auto MC-Reward：利用大型语言模型自动设计密集奖励函数，提升Minecraft中强化学习的效率<br><br>## 📌 背景痛点/本文动机<br>Minecraft 等强化学习环境通常只提供稀疏奖励，即只有任务完成或失败时才会获得奖励。这种奖励机制使得强化学习代理在探索效率方面面临挑战，难以学习复杂任务。为了解决这个问题，本文提出了 Auto MC-Reward，一个利用大型语言模型 (LLM) 自动设计密集奖励函数的先进学习系统，从而提高学习效率。<br><br>## 🚀 核心方法<br>💡 创新点1：Auto MC-Reward 由三个关键组件组成：奖励设计器、奖励评论家和轨迹分析器。奖励设计器根据环境信息和任务描述，通过编写可执行的 Python 函数来设计奖励函数。奖励评论家负责验证代码，检查代码是否自洽且没有语法和语义错误。轨迹分析器根据收集的轨迹总结可能的失败原因，并提供改进建议。<br><br>💡 创新点2：Auto MC-Reward 利用 LLM 的任务理解和经验总结能力，为学习提供详细和即时的奖励指导。奖励设计器首先根据环境和任务的基本描述，使用 LLM 设计与任务相关的密集奖励函数。然后，奖励评论家对设计的奖励函数进行自我验证。为了解决 LLM 理解的潜在偏差或疏忽，还提出了基于 LLM 的轨迹分析器，用于分析和总结训练代理的轨迹，并帮助奖励设计器改进奖励函数。<br><br>## 📈 实验结果<br>Auto MC-Reward 在一系列代表性基准测试中进行了验证，包括地下水平探索钻石和探索平原生物群落中的树木和动物。实验结果表明，与原始稀疏奖励和现有密集奖励方法相比，Auto MC-Reward 在这些任务上取得了显著更好的结果，显示出其在稀疏奖励任务上高效学习的先进能力。通过迭代改进奖励函数的设计，Auto MC-Reward 使代理能够有效地学习对新任务有益的新行为，例如避免熔岩，从而大大提高了成功率。此外，Auto MC-Reward 仅使用原始信息就实现了高钻石获取成功率（36.5%），证明了其解决长期任务的能力。<br><br>## 💬 可借鉴之处<br>Auto MC-Reward 为解决稀疏奖励任务中的探索效率问题提供了一种新的思路。其利用 LLM 自动设计密集奖励函数的方法，可以有效地提高强化学习代理的学习效率。此外，Auto MC-Reward 的三个组件（奖励设计器、奖励评论家和轨迹分析器）可以独立运行，使得数据分析和奖励函数更新更加灵活。</td>
    </tr>
    <tr>
      <th>27</th>
      <td>Odyssey: Empowering Minecraft Agents with Open-World Skills</td>
      <td>Recent studies have delved into constructing generalist agents for open-world<br>environments like Minecraft. Despite the encouraging results, existing efforts<br>mainly focus on solving basic programmatic tasks, e.g., material collection and<br>tool-crafting following the Minecraft tech-tree, treating the ObtainDiamond<br>task as the ultimate goal. This limitation stems from the narrowly defined set<br>of actions available to agents, requiring them to learn effective long-horizon<br>strategies from scratch. Consequently, discovering diverse gameplay<br>opportunities in the open world becomes challenging. In this work, we introduce<br>Odyssey, a new framework that empowers Large Language Model (LLM)-based agents<br>with open-world skills to explore the vast Minecraft world. Odyssey comprises<br>three key parts: (1) An interactive agent with an open-world skill library that<br>consists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned<br>LLaMA-3 model trained on a large question-answering dataset with 390k+<br>instruction entries derived from the Minecraft Wiki. (3) A new agent capability<br>benchmark includes the long-term planning task, the dynamic-immediate planning<br>task, and the autonomous exploration task. Extensive experiments demonstrate<br>that the proposed Odyssey framework can effectively evaluate different<br>capabilities of LLM-based agents. All datasets, model weights, and code are<br>publicly available to motivate future research on more advanced autonomous<br>agent solutions.</td>
      <td>## 🌟 论文解读 | Odyssey：赋予Minecraft智能体开放世界技能<br><br>## 📌 背景痛点/本文动机<br>近年来，许多研究致力于构建能够在开放世界环境中（如Minecraft）执行任务的通用智能体。尽管取得了令人鼓舞的成果，但现有工作主要集中在解决基本的编程任务，例如收集材料和制作工具，并将“获得钻石”任务视为最终目标。这种局限性源于智能体可用的动作集过于狭窄，需要它们从头开始学习有效的长期策略。因此，在开放世界中探索多样化的游戏玩法变得具有挑战性。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：开放世界技能库<br>Odyssey框架开发了一个基于大型语言模型（LLM）的交互式智能体，该智能体配备了一个开放世界技能库，其中包含40个基本技能和183个组合技能。这些技能涵盖了从资源收集到工具制作，再到战斗和探索的各种任务，为智能体提供了丰富的工具来应对开放世界的挑战。<br><br>💡 创新点2：微调LLaMA-3模型<br>为了提高智能体在Minecraft中的性能，Odyssey框架使用来自Minecraft维基的大规模问答数据集对LLaMA-3模型进行了微调。通过生成包含390k+指令条目的训练数据集，并使用LoRA技术进行高效训练，Odyssey框架显著提升了LLM模型在Minecraft领域的知识储备和推理能力。<br><br>💡 创新点3：智能体能力基准<br>Odyssey框架引入了一个新的智能体能力基准，包括长期规划任务、动态即时规划任务和自主探索任务。这些任务涵盖了Minecraft中的各种复杂场景，并要求智能体展现出多样化的解决方案。通过这些基准任务，研究人员可以全面评估智能体的规划能力、资源管理能力、技能检索能力以及自主探索能力。<br><br>## 📈 实验结果<br>实验结果表明，Odyssey框架在基本编程任务和智能体能力基准任务上都取得了显著的性能提升。与现有方法相比，Odyssey框架的智能体在完成任务的速度、成功率和资源利用率方面都表现出色。此外，消融实验也证明了开放世界技能库和LLM规划器对智能体整体性能的关键作用。<br><br>## 💬 可借鉴之处<br>Odyssey框架为开发和研究开放世界智能体提供了一个全面的框架，具有以下可借鉴之处：<br><br>* **开放世界技能库**：为智能体提供丰富的工具和策略，使其能够应对各种复杂的任务和挑战。<br>* **微调LLM模型**：通过领域特定的数据集进行微调，提升LLM模型在特定领域的知识储备和推理能力。<br>* **智能体能力基准**：为评估智能体的不同能力提供标准化的框架，促进开放世界智能体研究的进展。<br><br>## 🌟 总结<br>Odyssey框架为开放世界智能体的发展开辟了新的可能性，并为研究人员提供了一个强大的工具来探索和评估智能体的能力。随着未来研究的不断深入，Odyssey框架有望推动开放世界智能体技术的进一步发展，并为人工智能的通用性研究做出贡献。</td>
    </tr>
    <tr>
      <th>28</th>
      <td>See and Think: Embodied Agent in Virtual Environment</td>
      <td>Large language models (LLMs) have achieved impressive pro-gress on several<br>open-world tasks. Recently, using LLMs to build embodied agents has been a<br>hotspot. This paper proposes STEVE, a comprehensive and visionary embodied<br>agent in the Minecraft virtual environment. STEVE comprises three key<br>components: vision perception, language instruction, and code action. Vision<br>perception involves interpreting visual information in the environment, which<br>is then integrated into the LLMs component with agent state and task<br>instruction. Language instruction is responsible for iterative reasoning and<br>decomposing complex tasks into manageable guidelines. Code action generates<br>executable skill actions based on retrieval in skill database, enabling the<br>agent to interact effectively within the Minecraft environment. We also collect<br>STEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge<br>question-answering pairs, and 200+ skill-code pairs. We conduct continuous<br>block search, knowledge question and answering, and tech tree mastery to<br>evaluate the performance. Extensive experiments show that STEVE achieves at<br>most 1.5x faster unlocking key tech trees and 2.5x quicker in block search<br>tasks.</td>
      <td>## 🌟 论文解读 | See and Think: Embodied Agent in Virtual Environment<br><br>## 📌 背景痛点/本文动机<br>在开放世界环境中，设计能够表现出智能行为和适应性的智能体一直是人工智能领域的一个长期而重要的挑战。然而，近年来，大型语言模型（LLMs）在开发方面取得了显著进展，展现出其作为多功能、通用型助手的潜力。尽管如此，在许多开放世界环境中，如Minecraft，当代智能体主要使用LLMs进行文本交互。然而，这种对文本通信的依赖限制了它们在这些世界中的交互，包括低级案例。Minecraft要求智能体具备各种技能，从制作基本物品到执行复杂任务。然而，由LLMs驱动的智能体往往产生不可预测的输出。它们交互的有效性在很大程度上取决于精心设计的提示，旨在将LLM的理解与环境的上下文和预期目标相一致。这种提示工程过程不仅费力，而且无法实现培养自主、自我驱动的智能体的目标。此外，文本通信在自然传达某些世界概念方面存在局限性，例如制作配方，这些概念通常通过视觉更有效地传达。<br><br>## 🚀 核心方法<br>💡 创新点1：提出STEVE，一个在虚拟环境中具有视觉感知、语言指令和代码动作的智能体，与之前最先进的方法相比，在解锁关键技术树方面最多快1.5倍，在块搜索任务中最多快2.3倍。<br>💡 创新点2：提出STEVE-7B/13B，一系列通过使用Llama-2-7B/13B的Minecraft知识问答对进行微调获得的大型语言模型。<br>💡 创新点3：收集STEVE-21K数据集，包括600多个视觉-环境对、20K个知识问答对和200多个技能-代码对，以证明STEVE的有效性能。<br><br>## 📈 实验结果<br>实验结果表明，STEVE在连续块搜索、知识问答和科技树掌握方面表现出色。与AutoGPT和Voyager等基线方法相比，STEVE在解锁关键技术树方面最多快1.5倍，在块搜索任务中最多快2.3倍。此外，STEVE在知识问答任务中表现出色，其性能优于Llama-2和GPT-4等更广泛的模型。<br><br>## 💬 可借鉴之处<br>本文提出的STEVE框架为构建具有视觉感知和语言指令能力的智能体提供了一个有价值的参考。此外，STEVE-21K数据集为研究人员提供了进行多模态学习研究的有用资源。</td>
    </tr>
    <tr>
      <th>29</th>
      <td>RL-GPT: Integrating Reinforcement Learning and Code-as-policy</td>
      <td>Large Language Models (LLMs) have demonstrated proficiency in utilizing<br>various tools by coding, yet they face limitations in handling intricate logic<br>and precise control. In embodied tasks, high-level planning is amenable to<br>direct coding, while low-level actions often necessitate task-specific<br>refinement, such as Reinforcement Learning (RL). To seamlessly integrate both<br>modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising<br>a slow agent and a fast agent. The slow agent analyzes actions suitable for<br>coding, while the fast agent executes coding tasks. This decomposition<br>effectively focuses each agent on specific tasks, proving highly efficient<br>within our pipeline. Our approach outperforms traditional RL methods and<br>existing GPT agents, demonstrating superior efficiency. In the Minecraft game,<br>it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it<br>achieves SOTA performance across all designated MineDojo tasks.</td>
      <td>## 🌟 论文解读 | RL-GPT：将强化学习与代码策略相结合，提升LLM在复杂环境中的表现<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在利用各种工具进行编码方面表现出色，但在处理复杂逻辑和精确控制方面存在局限性。在具身任务中，高级规划可以直接通过编码实现，而低级动作通常需要特定任务的细化，例如强化学习（RL）。为了无缝集成这两种模式，本文提出了一种两级分层框架RL-GPT，包括慢速代理和快速代理。慢速代理分析适合编码的动作，而快速代理执行编码任务。这种分解有效地使每个代理专注于特定任务，在我们的管道中表现出高效率。本文的方法优于传统的RL方法和现有的GPT代理，展示了卓越的效率。在Minecraft游戏中，它在RTX3090上的一天内迅速获得钻石。此外，它在所有指定的MineDojo任务中实现了SOTA性能。<br><br>## 🚀 核心方法<br>💡 创新点1：引入LLM代理，利用RL训练流程作为工具，以增强LLM在环境中的交互任务学习能力。<br>💡 创新点2：开发了一个两级分层框架，能够确定任务中哪些动作应该使用RL学习。<br>💡 创新点3：首次将高级GPT编码动作纳入RL动作空间，提高了RL的样本效率。<br><br>## 📈 实验结果<br>在MineDojo基准测试中，RL-GPT在所有选定的任务中实现了最高的成功率。在Minecraft游戏中，RL-GPT在RTX3090上的一天内迅速获得钻石。此外，它在所有指定的MineDojo任务中实现了SOTA性能。<br><br>## 💬 可借鉴之处<br>本文提出的RL-GPT框架为LLMs在复杂环境中的任务学习提供了一种新的思路。通过将RL和代码策略相结合，RL-GPT能够有效地处理复杂逻辑和精确控制，从而在具身任务中取得卓越的性能。此外，本文提出的两级分层框架和迭代机制也为LLMs在环境中的任务学习提供了一种新的方法。</td>
    </tr>
    <tr>
      <th>30</th>
      <td>LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence</td>
      <td>Recent embodied agents are primarily built based on reinforcement learning<br>(RL) or large language models (LLMs). Among them, RL agents are efficient for<br>deployment but only perform very few tasks. By contrast, giant LLM agents<br>(often more than 1000B parameters) present strong generalization while<br>demanding enormous computing resources. In this work, we combine their<br>advantages while avoiding the drawbacks by conducting the proposed referee RL<br>on our developed large auto-regressive model (LARM). Specifically, LARM is<br>built upon a lightweight LLM (fewer than 5B parameters) and directly outputs<br>the next action to execute rather than text. We mathematically reveal that<br>classic RL feedbacks vanish in long-horizon embodied exploration and introduce<br>a giant LLM based referee to handle this reward vanishment during training<br>LARM. In this way, LARM learns to complete diverse open-world tasks without<br>human intervention. Especially, LARM successfully harvests enchanted diamond<br>equipment in Minecraft, which demands significantly longer decision-making<br>chains than the highest achievements of prior best methods.</td>
      <td>## 🌟 论文解读 | LARM：高效且通用的具身智能模型<br><br>## 📌 背景痛点/本文动机<br>近年来，人工智能在计算机视觉和自然语言处理等领域取得了显著进展，但这些技术大多缺乏与真实世界进行物理交互的能力。为了解决这个问题，具身人工智能的概念被提出，旨在开发能够与真实世界进行交互的智能体。然而，现有的具身智能体大多局限于特定任务，缺乏通用性和开放性。本文旨在解决这个问题，提出了一种名为LARM的具身智能模型，该模型结合了强化学习和大型语言模型的优点，能够在开放世界中完成各种任务。<br><br>## 🚀 核心方法<br>💡 创新点1：LARM模型<br>LARM模型基于轻量级的大型语言模型（LLM），直接输出下一步行动，而不是文本。这使得LARM模型能够快速响应环境变化，并具有更强的通用性。<br><br>💡 创新点2：裁判强化学习<br>传统的强化学习算法在长时序探索中存在反馈消失的问题，即智能体在完成任务之前无法获得有效的奖励信号。为了解决这个问题，本文提出了裁判强化学习技术。该技术利用大型语言模型作为裁判，为智能体提供即时反馈，从而有效地指导智能体的学习过程。<br><br>## 📈 实验结果<br>本文在MineDojo和Mineflayer环境中对LARM模型进行了评估。实验结果表明，LARM模型在完成各种任务方面取得了优异的性能，包括收集资源、制作工具等。特别是在Mineflayer环境中，LARM模型成功地制作了附魔钻石装备，这是之前方法无法实现的。<br><br>## 💬 可借鉴之处<br>本文提出的LARM模型和裁判强化学习技术为开发高效且通用的具身智能体提供了新的思路。LARM模型的结构和训练方法可以应用于其他开放世界环境，而裁判强化学习技术可以解决长时序探索中的反馈消失问题。此外，本文还展示了网页数据预训练对提升模型性能的重要性，这为利用互联网数据训练具身智能体提供了启示。</td>
    </tr>
    <tr>
      <th>31</th>
      <td>Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification</td>
      <td>Building open agents has always been the ultimate goal in AI research, and<br>creative agents are the more enticing. Existing LLM agents excel at<br>long-horizon tasks with well-defined goals (e.g., `mine diamonds' in<br>Minecraft). However, they encounter difficulties on creative tasks with open<br>goals and abstract criteria due to the inability to bridge the gap between<br>them, thus lacking feedback for self-improvement in solving the task. In this<br>work, we introduce autonomous embodied verification techniques for agents to<br>fill the gap, laying the groundwork for creative tasks. Specifically, we<br>propose the Luban agent target creative building tasks in Minecraft, which<br>equips with two-level autonomous embodied verification inspired by human design<br>practices: (1) visual verification of 3D structural speculates, which comes<br>from agent synthesized CAD modeling programs; (2) pragmatic verification of the<br>creation by generating and verifying environment-relevant functionality<br>programs based on the abstract criteria. Extensive multi-dimensional human<br>studies and Elo ratings show that the Luban completes diverse creative building<br>tasks in our proposed benchmark and outperforms other baselines (\( 33\% \) to<br>\( 100\% \)) in both visualization and pragmatism. Additional demos on the<br>real-world robotic arm show the creation potential of the Luban in the physical<br>world.</td>
      <td>## 🌟 论文解读 | Luban：构建开放式的创造性智能体<br><br>## 📌 背景痛点/本文动机<br>在人工智能领域，构建能够自主解决复杂任务的开放式智能体一直是研究的目标。特别是创造性任务，由于其开放的目标和抽象的评估标准，对智能体提出了更高的要求。现有的语言模型（LLM）智能体在处理具有明确目标的长期任务方面表现出色，但在面对创造性任务时，由于缺乏明确的评估标准，难以进行自我改进。<br><br>## 🚀 核心方法<br>本文提出了Luban智能体，旨在通过自主的身体验证技术解决创造性任务中的评估难题。Luban智能体采用了两级自主身体验证机制，灵感来源于人类的设计实践：<br><br>1. **视觉验证**：Luban首先通过CAD程序合成3D结构模型，并进行视觉验证，确保模型的外观符合设计要求。<br>2. **实用验证**：在通过视觉验证后，Luban会生成与抽象标准相关的环境相关功能程序，并进行实用验证，确保模型的功能性。<br><br>## 📈 实验结果<br>通过在Minecraft中设计包含5个具有不同视觉和功能要求的建筑任务的基准测试，Luban智能体在所有任务中均成功完成，并且在可视化方面和实用性方面均优于其他基线方法（33%至100%）。此外，Luban在真实世界的机械臂上的演示也展示了其在物理世界中进行开放式创造性任务的潜力。<br><br>## 💬 可借鉴之处<br>Luban智能体的设计为构建开放式创造性智能体提供了新的思路，其两级自主身体验证机制为解决创造性任务中的评估难题提供了有效的方法。此外，Luban的设计也展示了将虚拟世界中的智能体技术应用于真实世界的潜力。</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        