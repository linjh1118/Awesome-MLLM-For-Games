
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks</td>
      <td>Building a general-purpose agent is a long-standing vision in the field of<br>artificial intelligence. Existing agents have made remarkable progress in many<br>domains, yet they still struggle to complete long-horizon tasks in an open<br>world. We attribute this to the lack of necessary world knowledge and<br>multimodal experience that can guide agents through a variety of long-horizon<br>tasks. In this paper, we propose a Hybrid Multimodal Memory module to address<br>the above challenges. It 1) transforms knowledge into Hierarchical Directed<br>Knowledge Graph that allows agents to explicitly represent and learn world<br>knowledge, and 2) summarises historical information into Abstracted Multimodal<br>Experience Pool that provide agents with rich references for in-context<br>learning. On top of the Hybrid Multimodal Memory module, a multimodal agent,<br>Optimus-1, is constructed with dedicated Knowledge-guided Planner and<br>Experience-Driven Reflector, contributing to a better planning and reflection<br>in the face of long-horizon tasks in Minecraft. Extensive experimental results<br>show that Optimus-1 significantly outperforms all existing agents on<br>challenging long-horizon task benchmarks, and exhibits near human-level<br>performance on many tasks. In addition, we introduce various Multimodal Large<br>Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show<br>that Optimus-1 exhibits strong generalization with the help of the Hybrid<br>Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Optimus-1ï¼šæ··åˆå¤šæ¨¡æ€è®°å¿†èµ‹èƒ½ä»£ç†åœ¨é•¿æ—¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æ„å»ºä¸€ä¸ªé€šç”¨ä»£ç†æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸé•¿æœŸä»¥æ¥çš„æ„¿æ™¯ã€‚å°½ç®¡ç°æœ‰çš„ä»£ç†åœ¨è®¸å¤šé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥åœ¨å¼€æ”¾ä¸–ç•Œä¸­å®Œæˆé•¿æ—¶ä»»åŠ¡ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹å¿…è¦çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¤šæ¨¡æ€ç»éªŒï¼Œè¿™äº›çŸ¥è¯†å’Œç»éªŒå¯ä»¥æŒ‡å¯¼ä»£ç†å®Œæˆå„ç§é•¿æ—¶ä»»åŠ¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ··åˆå¤šæ¨¡æ€è®°å¿†æ¨¡å—<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ··åˆå¤šæ¨¡æ€è®°å¿†æ¨¡å—ï¼Œè¯¥æ¨¡å—ç”±åˆ†å±‚æœ‰å‘çŸ¥è¯†å›¾ï¼ˆHDKGï¼‰å’ŒæŠ½è±¡å¤šæ¨¡æ€ç»éªŒæ± ï¼ˆAMEPï¼‰ç»„æˆã€‚HDKGå°†çŸ¥è¯†è½¬åŒ–ä¸ºåˆ†å±‚æœ‰å‘å›¾ç»“æ„ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ˜¾å¼åœ°è¡¨ç¤ºå’Œå­¦ä¹ ä¸–ç•ŒçŸ¥è¯†ã€‚AMEPå°†å†å²ä¿¡æ¯æ€»ç»“ä¸ºæŠ½è±¡å¤šæ¨¡æ€ç»éªŒæ± ï¼Œä¸ºä»£ç†æä¾›ä¸°å¯Œçš„å‚è€ƒï¼Œä»¥ä¾¿è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šOptimus-1ä»£ç†<br>åœ¨æ··åˆå¤šæ¨¡æ€è®°å¿†æ¨¡å—çš„åŸºç¡€ä¸Šï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€ä»£ç†Optimus-1ï¼Œè¯¥ä»£ç†ç”±çŸ¥è¯†å¼•å¯¼è§„åˆ’å™¨ã€ç»éªŒé©±åŠ¨åæ€å™¨å’ŒåŠ¨ä½œæ§åˆ¶å™¨ç»„æˆã€‚çŸ¥è¯†å¼•å¯¼è§„åˆ’å™¨åˆ©ç”¨HDKGæ¥æ•è·å®Œæˆä»»åŠ¡æ‰€éœ€çš„çŸ¥è¯†ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„å­ç›®æ ‡åºåˆ—ã€‚åŠ¨ä½œæ§åˆ¶å™¨æ ¹æ®å­ç›®æ ‡å’Œå½“å‰è§‚å¯Ÿç”Ÿæˆä½çº§åŠ¨ä½œï¼Œä¸æ¸¸æˆç¯å¢ƒäº¤äº’ä»¥æ›´æ–°ä»£ç†çš„çŠ¶æ€ã€‚ç»éªŒé©±åŠ¨åæ€å™¨å®šæœŸæ¿€æ´»ï¼Œä»AMEPä¸­æ£€ç´¢ç›¸å…³çš„å¤šæ¨¡æ€ç»éªŒï¼Œä»¥è¯„ä¼°å½“å‰å­ç›®æ ‡æ˜¯å¦å¯ä»¥æˆåŠŸæ‰§è¡Œã€‚å¦‚æœä¸è¡Œï¼Œå®ƒä¼šæŒ‡ç¤ºçŸ¥è¯†å¼•å¯¼è§„åˆ’å™¨ä¿®æ”¹å…¶è®¡åˆ’ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Minecraftä¸­è¿›è¡Œçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒOptimus-1åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿æ—¶ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæ‰€æœ‰ç°æœ‰ä»£ç†ï¼Œå¹¶åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ¥è¿‘äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†å„ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºOptimus-1çš„éª¨å¹²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ··åˆå¤šæ¨¡æ€è®°å¿†æ¨¡å—çš„å¸®åŠ©ä¸‹ï¼ŒOptimus-1åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºGPT-4VåŸºçº¿ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ··åˆå¤šæ¨¡æ€è®°å¿†æ¨¡å—å’ŒOptimus-1ä»£ç†ä¸ºæ„å»ºèƒ½å¤Ÿå®Œæˆé•¿æ—¶ä»»åŠ¡çš„é€šç”¨ä»£ç†æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„éå‚æ•°å­¦ä¹ æ–¹æ³•ä¹Ÿä¸ºä»£ç†çš„å­¦ä¹ å’Œè¿›åŒ–æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy</td>
      <td>Building an agent that can mimic human behavior patterns to accomplish<br>various open-world tasks is a long-term goal. To enable agents to effectively<br>learn behavioral patterns across diverse tasks, a key challenge lies in<br>modeling the intricate relationships among observations, actions, and language.<br>To this end, we propose Optimus-2, a novel Minecraft agent that incorporates a<br>Multimodal Large Language Model (MLLM) for high-level planning, alongside a<br>Goal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP<br>contains (1) an Action-guided Behavior Encoder that models causal relationships<br>between observations and actions at each timestep, then dynamically interacts<br>with the historical observation-action sequence, consolidating it into<br>fixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with<br>open-ended language instructions to predict actions auto-regressively.<br>Moreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}<br>dataset, which contains 25,000 videos across 8 atomic tasks, providing about<br>30M goal-observation-action pairs. The automated construction method, along<br>with the MGOA dataset, can contribute to the community's efforts to train<br>Minecraft agents. Extensive experimental results demonstrate that Optimus-2<br>exhibits superior performance across atomic tasks, long-horizon tasks, and<br>open-ended instruction tasks in Minecraft. Please see the project page at<br>https://cybertronagent.github.io/Optimus-2.github.io/.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Optimus-2ï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„Minecraftæ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œæ„å»ºèƒ½å¤Ÿæ¨¡ä»¿äººç±»è¡Œä¸ºæ¨¡å¼å¹¶å®Œæˆå„ç§ä»»åŠ¡çš„æ™ºèƒ½ä½“ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œè¦ä½¿æ™ºèƒ½ä½“æœ‰æ•ˆåœ°å­¦ä¹ è·¨ä»»åŠ¡çš„è¡Œä¸ºæ¨¡å¼ï¼Œå…³é”®æŒ‘æˆ˜åœ¨äºå»ºæ¨¡è§‚å¯Ÿã€åŠ¨ä½œå’Œè¯­è¨€ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ç°æœ‰çš„æ™ºèƒ½ä½“åœ¨å¤„ç†å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å¤šæ ·åŒ–ä»»åŠ¡æ—¶ï¼Œé€šå¸¸é‡‡ç”¨ä»»åŠ¡è§„åˆ’å™¨å’Œç›®æ ‡æ¡ä»¶ç­–ç•¥çš„æ¡†æ¶ã€‚å°½ç®¡ç°æœ‰çš„æ™ºèƒ½ä½“åœ¨åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºè§„åˆ’å™¨æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç›®æ ‡æ¡ä»¶ç­–ç•¥çš„æ€§èƒ½ç“¶é¢ˆä»ç„¶å­˜åœ¨ã€‚ç°æœ‰çš„ç­–ç•¥é€šå¸¸å¿½ç•¥äº†è§‚å¯Ÿå’ŒåŠ¨ä½œä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸”éš¾ä»¥å»ºæ¨¡å¼€æ”¾å¼çš„å­ç›®æ ‡å’Œè§‚å¯Ÿ-åŠ¨ä½œåºåˆ—ä¹‹é—´çš„å…³ç³»ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Optimus-2ï¼Œä¸€ä¸ªæ–°é¢–çš„Minecraftæ™ºèƒ½ä½“ï¼Œå®ƒç»“åˆäº†MLLMè¿›è¡Œé«˜çº§è§„åˆ’ï¼Œå¹¶é‡‡ç”¨ç›®æ ‡-è§‚å¯Ÿ-åŠ¨ä½œæ¡ä»¶ç­–ç•¥ï¼ˆGOAPï¼‰è¿›è¡Œä½çº§æ§åˆ¶ã€‚GOAPåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨ä½œå¼•å¯¼çš„è¡Œä¸ºç¼–ç å™¨<br>åŠ¨ä½œå¼•å¯¼çš„è¡Œä¸ºç¼–ç å™¨ç”¨äºå»ºæ¨¡è§‚å¯Ÿ-åŠ¨ä½œåºåˆ—ã€‚å®ƒé¦–å…ˆä½¿ç”¨å› æœæ„ŸçŸ¥å™¨å°†åŠ¨ä½œåµŒå…¥åˆ°è§‚å¯Ÿç‰¹å¾ä¸­ï¼Œåˆ©ç”¨ä»»åŠ¡ç›¸å…³çš„åŠ¨ä½œä¿¡æ¯ä½œä¸ºæŒ‡å¯¼æ¥è°ƒæ•´è§‚å¯Ÿç‰¹å¾ï¼Œä»è€Œä¸ºåŠ¨ä½œé¢„æµ‹æä¾›ç»†ç²’åº¦çš„è§‚å¯Ÿ-åŠ¨ä½œä¿¡æ¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†åœ¨ä¸è¶…å‡ºè¾“å…¥é•¿åº¦é™åˆ¶çš„æƒ…å†µä¸‹å¯¹é•¿æœŸè§‚å¯Ÿ-åŠ¨ä½œåºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œå¼•å…¥äº†å†å²èšåˆå™¨ï¼Œå°†å½“å‰è§‚å¯Ÿ-åŠ¨ä½œä¿¡æ¯ä¸å†å²åºåˆ—åŠ¨æ€åœ°æ•´åˆæˆå›ºå®šé•¿åº¦çš„è¡Œä¸ºæ ‡è®°ã€‚è¡Œä¸ºæ ‡è®°å¯ä»¥ä»¥å›ºå®šä¸”é€‚å½“çš„é•¿åº¦æ•è·è§‚å¯Ÿ-åŠ¨ä½œåºåˆ—çš„é•¿æœŸä¾èµ–å…³ç³»ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé¢„æµ‹ä¸è§‚å¯Ÿ-åŠ¨ä½œåºåˆ—é€»è¾‘ä¸€è‡´çš„åŠ¨ä½œï¼Œè€Œä¸æ˜¯ä»…åŸºäºå½“å‰è§‚å¯Ÿè¿›è¡Œå­¤ç«‹çš„åŠ¨ä½œé¢„æµ‹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹<br>ä¸ºäº†æ˜ç¡®ç¼–ç å­ç›®æ ‡çš„è¯­ä¹‰ï¼Œå¼•å…¥äº†MLLMä½œä¸ºGOAPçš„éª¨å¹²ç½‘ç»œã€‚å®ƒå°†å­ç›®æ ‡ä¸è¡Œä¸ºæ ‡è®°å¯¹é½ï¼Œä»¥è‡ªå›å½’æ–¹å¼é¢„æµ‹åç»­åŠ¨ä½œã€‚åˆ©ç”¨MLLMçš„è¯­è¨€ç†è§£å’Œå¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ï¼Œå®ƒå¯ä»¥æ›´å¥½åœ°æ•´åˆå¼€æ”¾å¼å­ç›®æ ‡å’Œè§‚å¯Ÿ-åŠ¨ä½œåºåˆ—çš„ç‰¹å¾ï¼Œä»è€Œå¢å¼ºç­–ç•¥çš„åŠ¨ä½œé¢„æµ‹èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Minecraftçš„å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜Optimus-2åœ¨åŸå­ä»»åŠ¡ã€é•¿æœŸä»»åŠ¡å’Œå¼€æ”¾å¼æŒ‡ä»¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚ä¸ä¹‹å‰çš„SOTAç›¸æ¯”ï¼ŒOptimus-2åœ¨åŸå­ä»»åŠ¡ã€é•¿æœŸä»»åŠ¡å’Œå¼€æ”¾å¼å­ç›®æ ‡ä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†å¹³å‡27%ã€10%å’Œ18%çš„æå‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„Optimus-2æ™ºèƒ½ä½“åŠå…¶GOAPç­–ç•¥ä¸ºå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„æ™ºèƒ½ä½“è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯ã€‚åŠ¨ä½œå¼•å¯¼çš„è¡Œä¸ºç¼–ç å™¨å’ŒMLLMçš„å¼•å…¥æœ‰æ•ˆåœ°è§£å†³äº†è§‚å¯Ÿã€åŠ¨ä½œå’Œè¯­è¨€ä¹‹é—´çš„å¤æ‚å…³ç³»å»ºæ¨¡é—®é¢˜ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œå¼€æ”¾å¼æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„MGOAæ•°æ®é›†ä¸ºè®­ç»ƒMinecraftæ™ºèƒ½ä½“æä¾›äº†é«˜è´¨é‡çš„æ•°æ®èµ„æºï¼Œæœ‰åŠ©äºæ¨åŠ¨ç›¸å…³ç ”ç©¶çš„å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos</td>
      <td>Pretraining on noisy, internet-scale datasets has been heavily studied as a<br>technique for training models with broad, general capabilities for text,<br>images, and other modalities. However, for many sequential decision domains<br>such as robotics, video games, and computer use, publicly available data does<br>not contain the labels required to train behavioral priors in the same way. We<br>extend the internet-scale pretraining paradigm to sequential decision domains<br>through semi-supervised imitation learning wherein agents learn to act by<br>watching online unlabeled videos. Specifically, we show that with a small<br>amount of labeled data we can train an inverse dynamics model accurate enough<br>to label a huge unlabeled source of online data -- here, online videos of<br>people playing Minecraft -- from which we can then train a general behavioral<br>prior. Despite using the native human interface (mouse and keyboard at 20Hz),<br>we show that this behavioral prior has nontrivial zero-shot capabilities and<br>that it can be fine-tuned, with both imitation learning and reinforcement<br>learning, to hard-exploration tasks that are impossible to learn from scratch<br>via reinforcement learning. For many tasks our models exhibit human-level<br>performance, and we are the first to report computer agents that can craft<br>diamond tools, which can take proficient humans upwards of 20 minutes (24,000<br>environment actions) of gameplay to accomplish.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è§†é¢‘é¢„è®­ç»ƒï¼ˆVPTï¼‰ï¼šé€šè¿‡è§‚çœ‹æ— æ ‡ç­¾åœ¨çº¿è§†é¢‘å­¦ä¹ è¡ŒåŠ¨<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸï¼Œé€šè¿‡åœ¨å¤§å‹äº’è”ç½‘æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå·²ç»è¯æ˜äº†è®­ç»ƒå¤§å‹é€šç”¨åŸºç¡€æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå¯¹äºè®¸å¤šåºåˆ—å†³ç­–é¢†åŸŸï¼Œå¦‚æœºå™¨äººã€è§†é¢‘æ¸¸æˆå’Œè®¡ç®—æœºä½¿ç”¨ï¼Œå…¬å¼€å¯ç”¨çš„æ•°æ®å¹¶ä¸åŒ…å«è®­ç»ƒè¡Œä¸ºå…ˆéªŒæ‰€éœ€çš„æ ‡ç­¾ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡åˆ©ç”¨äº’è”ç½‘ä¸Šå¤§é‡æœªæ ‡è®°çš„è§†é¢‘æ•°æ®ï¼Œå°†è¿™äº›é¢„è®­ç»ƒèŒƒå¼æ‰©å±•åˆ°åºåˆ—å†³ç­–é¢†åŸŸã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠç›‘ç£æ¨¡ä»¿å­¦ä¹ <br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠç›‘ç£æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è§‚çœ‹åœ¨çº¿æœªæ ‡è®°çš„è§†é¢‘ï¼Œä½¿æ™ºèƒ½ä½“å­¦ä¼šè¡ŒåŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨å°‘é‡æ ‡è®°æ•°æ®è®­ç»ƒä¸€ä¸ªé€†åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è¶³å¤Ÿå‡†ç¡®ï¼Œå¯ä»¥æ ‡è®°å¤§é‡æœªæ ‡è®°çš„åœ¨çº¿æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œäººä»¬ç©Minecraftçš„è§†é¢‘ï¼‰ï¼Œç„¶åä»ä¸­è®­ç»ƒä¸€ä¸ªé€šç”¨çš„è¡Œä¸ºå…ˆéªŒã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€†åŠ¨åŠ›å­¦æ¨¡å‹<br>ä¸è¡Œä¸ºå…‹éš†ç›¸æ¯”ï¼Œé€†åŠ¨åŠ›å­¦å»ºæ¨¡ä»»åŠ¡æ›´ç®€å•ï¼Œå› ä¸ºå®ƒæ˜¯éå› æœçš„ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥æŸ¥çœ‹è¿‡å»å’Œæœªæ¥çš„å¸§æ¥æ¨æ–­åŠ¨ä½œã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œç¯å¢ƒæœºåˆ¶æ¯”ç¯å¢ƒä¸­å¯èƒ½å‘ç”Ÿçš„äººç±»è¡Œä¸ºçš„å¹¿åº¦è¦ç®€å•å¾—å¤šï¼Œè¿™è¡¨æ˜éå› æœé€†åŠ¨åŠ›å­¦æ¨¡å‹å¯èƒ½éœ€è¦æ¯”å› æœè¡Œä¸ºå…‹éš†æ¨¡å‹å°‘å¾—å¤šçš„æ•°æ®æ¥è®­ç»ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ä½¿ç”¨äº†åŸç”Ÿäººç±»ç•Œé¢ï¼ˆ20Hzçš„é¼ æ ‡å’Œé”®ç›˜ï¼‰ï¼Œä½†è¿™ç§æ–¹æ³•çš„è¡Œä¸ºå…ˆéªŒå…·æœ‰éå¹³å‡¡çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œä»¥è§£å†³å¼ºåŒ–å­¦ä¹ æ— æ³•ä»å¤´å­¦ä¹ çš„å›°éš¾æ¢ç´¢ä»»åŠ¡ã€‚å¯¹äºè®¸å¤šä»»åŠ¡ï¼Œæ¨¡å‹è¡¨ç°å‡ºäººç±»æ°´å¹³çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¯ç¬¬ä¸€ä¸ªæŠ¥å‘Šèƒ½å¤Ÿåˆ¶ä½œé’»çŸ³å·¥å…·çš„è®¡ç®—æœºä»£ç†ï¼Œè¿™éœ€è¦ç†Ÿç»ƒçš„äººç±»ç©å®¶è¶…è¿‡20åˆ†é’Ÿï¼ˆ24,000ä¸ªç¯å¢ƒåŠ¨ä½œï¼‰çš„æ¸¸æˆæ—¶é—´æ‰èƒ½å®Œæˆã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„è§†é¢‘é¢„è®­ç»ƒï¼ˆVPTï¼‰æ–¹æ³•ä¸ºåˆ©ç”¨äº’è”ç½‘ä¸Šå¤§é‡æœªæ ‡è®°çš„æ•°æ®è¿›è¡Œåºåˆ—å†³ç­–é¢†åŸŸçš„é¢„è®­ç»ƒæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å°‘é‡æ ‡è®°æ•°æ®ï¼Œè€Œä¸”èƒ½å¤Ÿé€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œä»¥è§£å†³å¼ºåŒ–å­¦ä¹ æ— æ³•ä»å¤´å­¦ä¹ çš„å›°éš¾æ¢ç´¢ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºä»»ä½•å…·æœ‰å¤§é‡æœªæ ‡è®°æ•°æ®çš„é¢†åŸŸï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</td>
    </tr>
    <tr>
      <th>3</th>
      <td>GROOT: Learning to Follow Instructions by Watching Gameplay Videos</td>
      <td>We study the problem of building a controller that can follow open-ended<br>instructions in open-world environments. We propose to follow reference videos<br>as instructions, which offer expressive goal specifications while eliminating<br>the need for expensive text-gameplay annotations. A new learning framework is<br>derived to allow learning such instruction-following controllers from gameplay<br>videos while producing a video instruction encoder that induces a structured<br>goal space. We implement our agent GROOT in a simple yet effective<br>encoder-decoder architecture based on causal transformers. We evaluate GROOT<br>against open-world counterparts and human players on a proposed Minecraft<br>SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the<br>human-machine gap as well as exhibiting a 70% winning rate over the best<br>generalist agent baseline. Qualitative analysis of the induced goal space<br>further demonstrates some interesting emergent properties, including the goal<br>composition and complex gameplay behavior synthesis. The project page is<br>available at https://craftjarvis-groot.github.io.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GROOTï¼šé€šè¿‡è§‚çœ‹æ¸¸æˆè§†é¢‘å­¦ä¹ æŒ‡ä»¤éµå¾ª<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œæ„å»ºèƒ½å¤Ÿéµå¾ªå¼€æ”¾æŒ‡ä»¤çš„æ§åˆ¶å™¨ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ§åˆ¶å™¨é€šå¸¸åªèƒ½å®Œæˆé¢„å®šä¹‰çš„ã€æœ‰é™çš„ç¨‹åºæ€§ä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è§‚çœ‹æ¸¸æˆè§†é¢‘æ¥å­¦ä¹ æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†ç›®æ ‡æŒ‡å®šä¸ºå‚è€ƒæ¸¸æˆè§†é¢‘ç‰‡æ®µï¼Œä»è€Œæä¾›ä¸°å¯Œçš„ç›®æ ‡è§„èŒƒï¼ŒåŒæ—¶æ¶ˆé™¤å¯¹æ˜‚è´µçš„æ–‡æœ¬-æ¸¸æˆæ³¨é‡Šçš„éœ€æ±‚ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥äº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒæ—¶äº§ç”Ÿä¸€ä¸ªç›®æ ‡ç©ºé—´å’Œä¸€ä¸ªè§†é¢‘æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ï¼Œä»è€Œå®ç°ä»æ¸¸æˆè§†é¢‘ä¸­å­¦ä¹ æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Minecraft SkillForgeåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGROOTåœ¨æ•´ä½“Eloè¯„åˆ†æ¯”è¾ƒä¸­è¶…è¿‡äº†æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œå¹¶ä¸”åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„è·å–é’»çŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„å­¦ä¹ æ¡†æ¶å’ŒGROOTä»£ç†çš„æ¶æ„è®¾è®¡ä¸ºæ„å»ºèƒ½å¤Ÿéµå¾ªå¼€æ”¾æŒ‡ä»¤çš„æ§åˆ¶å™¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†ç›®æ ‡ç©ºé—´å’Œæ§åˆ¶å™¨ç­–ç•¥çš„æ½œåœ¨åº”ç”¨ï¼Œä¸ºè§£å†³å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å¤æ‚ä»»åŠ¡æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</td>
    </tr>
    <tr>
      <th>4</th>
      <td>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</td>
      <td>Autonomous agents have made great strides in specialist domains like Atari<br>games and Go. However, they typically learn tabula rasa in isolated<br>environments with limited and manually conceived objectives, thus failing to<br>generalize across a wide spectrum of tasks and capabilities. Inspired by how<br>humans continually learn and adapt in the open world, we advocate a trinity of<br>ingredients for building generalist agents: 1) an environment that supports a<br>multitude of tasks and goals, 2) a large-scale database of multimodal<br>knowledge, and 3) a flexible and scalable agent architecture. We introduce<br>MineDojo, a new framework built on the popular Minecraft game that features a<br>simulation suite with thousands of diverse open-ended tasks and an<br>internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and<br>forum discussions. Using MineDojo's data, we propose a novel agent learning<br>algorithm that leverages large pre-trained video-language models as a learned<br>reward function. Our agent is able to solve a variety of open-ended tasks<br>specified in free-form language without any manually designed dense shaping<br>reward. We open-source the simulation suite, knowledge bases, algorithm<br>implementation, and pretrained models (https://minedojo.org) to promote<br>research towards the goal of generally capable embodied agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MineDojoï¼šæ„å»ºå…·æœ‰äº’è”ç½‘è§„æ¨¡çŸ¥è¯†çš„å¼€æ”¾å¼å…·èº«æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ä¼ ç»Ÿçš„è‡ªä¸»æ™ºèƒ½ä½“åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚Atariæ¸¸æˆå’Œå›´æ£‹ï¼‰å–å¾—äº†å·¨å¤§è¿›æ­¥ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨å­¤ç«‹çš„ç¯å¢ƒä¸­å­¦ä¹ ï¼Œç›®æ ‡æœ‰é™ä¸”æ‰‹åŠ¨è®¾è®¡ï¼Œå› æ­¤æ— æ³•åœ¨å¹¿æ³›çš„ä»»åŠ¡å’Œèƒ½åŠ›ä¹‹é—´è¿›è¡Œæ³›åŒ–ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»åœ¨å¼€æ”¾ä¸–ç•Œä¸­ä¸æ–­å­¦ä¹ å’Œé€‚åº”ï¼Œèƒ½å¤Ÿåˆ©ç”¨å¤§é‡æ¥è‡ªè‡ªèº«ç»éªŒå’Œä»–äººçš„å…ˆéªŒçŸ¥è¯†ã€‚æœ¬æ–‡æ—¨åœ¨æ„å»ºèƒ½å¤Ÿåƒäººç±»ä¸€æ ·åœ¨å¼€æ”¾ä¸–ç•Œä¸­å­¦ä¹ å’Œé€‚åº”çš„é€šç”¨æ™ºèƒ½ä½“ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼€æ”¾å¼ç¯å¢ƒ<br>MineDojoåŸºäºæµè¡Œçš„Minecraftæ¸¸æˆï¼Œæä¾›äº†ä¸€ä¸ªå…·æœ‰æ•°åƒä¸ªå¤šæ ·åŒ–å¼€æ”¾å¼ä»»åŠ¡çš„æ¨¡æ‹Ÿå¥—ä»¶ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬ç”Ÿå­˜ã€é‡‡é›†ã€æŠ€æœ¯æ ‘å’Œæˆ˜æ–—ç­‰ç±»åˆ«ï¼Œæ¶µç›–äº†ä»ç®€å•åˆ°å¤æ‚çš„å„ç§éš¾åº¦çº§åˆ«ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šäº’è”ç½‘è§„æ¨¡çš„å¤šæ¨¡æ€çŸ¥è¯†åº“<br>MineDojoæ”¶é›†äº†å¤§é‡çš„Minecraftç›¸å…³æ•°æ®ï¼ŒåŒ…æ‹¬YouTubeè§†é¢‘ã€Wikié¡µé¢å’ŒRedditè®¨è®ºã€‚è¿™äº›æ•°æ®æ¶µç›–äº†æ¸¸æˆçš„æ‰€æœ‰æ–¹é¢ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›äº†ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šçµæ´»å¯æ‰©å±•çš„æ™ºèƒ½ä½“æ¶æ„<br>MineDojoæå‡ºäº†ä¸€ä¸ªåŸºäºTransformeré¢„è®­ç»ƒèŒƒå¼çš„æ™ºèƒ½ä½“å­¦ä¹ ç®—æ³•ã€‚è¯¥ç®—æ³•åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒçš„è§†é¢‘è¯­è¨€æ¨¡å‹ä½œä¸ºå­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œèƒ½å¤Ÿè§£å†³å„ç§å¼€æ”¾å¼ä»»åŠ¡ï¼Œè€Œæ— éœ€æ‰‹åŠ¨è®¾è®¡å¯†é›†çš„å¥–åŠ±å‡½æ•°ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>MineDojoçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶æå‡ºçš„MINECLIPå¥–åŠ±æ¨¡å‹åœ¨ç¨‹åºæ€§ä»»åŠ¡å’Œåˆ›é€ æ€§ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚ä¸æ‰‹åŠ¨è®¾è®¡çš„å¯†é›†å¥–åŠ±å‡½æ•°ç›¸æ¯”ï¼ŒMINECLIPåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šå–å¾—äº†ç«äº‰æ€§çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†å®ƒä»¬ã€‚æ­¤å¤–ï¼ŒMINECLIPè¿˜èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°åˆ›é€ æ€§ä»»åŠ¡ï¼Œå…¶è¯„ä¼°ç»“æœä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MineDojoä¸ºå¼€å‘é€šç”¨æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„æ¡†æ¶ã€‚å…¶å¼€æ”¾å¼ç¯å¢ƒã€äº’è”ç½‘è§„æ¨¡çš„çŸ¥è¯†åº“å’Œçµæ´»å¯æ‰©å±•çš„æ™ºèƒ½ä½“æ¶æ„ä¸ºç ”ç©¶äººå‘˜æä¾›äº†æ¢ç´¢å¼€æ”¾å¼æ™ºèƒ½ä½“å­¦ä¹ çš„å¼ºå¤§å·¥å…·ã€‚æ­¤å¤–ï¼ŒMineDojoçš„å¼€æºä»£ç å’Œæ•°æ®é›†å°†ä¿ƒè¿›ç¤¾åŒºå¯¹é€šç”¨æ™ºèƒ½ä½“ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Mastering Diverse Domains through World Models</td>
      <td>Developing a general algorithm that learns to solve tasks across a wide range<br>of applications has been a fundamental challenge in artificial intelligence.<br>Although current reinforcement learning algorithms can be readily applied to<br>tasks similar to what they have been developed for, configuring them for new<br>application domains requires significant human expertise and experimentation.<br>We present DreamerV3, a general algorithm that outperforms specialized methods<br>across over 150 diverse tasks, with a single configuration. Dreamer learns a<br>model of the environment and improves its behavior by imagining future<br>scenarios. Robustness techniques based on normalization, balancing, and<br>transformations enable stable learning across domains. Applied out of the box,<br>Dreamer is the first algorithm to collect diamonds in Minecraft from scratch<br>without human data or curricula. This achievement has been posed as a<br>significant challenge in artificial intelligence that requires exploring<br>farsighted strategies from pixels and sparse rewards in an open world. Our work<br>allows solving challenging control problems without extensive experimentation,<br>making reinforcement learning broadly applicable.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | DreamerV3ï¼šé€šè¿‡ä¸–ç•Œæ¨¡å‹æŒæ¡å¤šæ ·åŒ–é¢†åŸŸ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¼ºåŒ–å­¦ä¹ åœ¨è§£å†³ç‰¹å®šä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†ç®—æ³•åº”ç”¨äºæ–°é¢†åŸŸé€šå¸¸éœ€è¦å¤§é‡çš„äººå·¥è°ƒæ•´å’Œå®éªŒã€‚è¿™é™åˆ¶äº†å¼ºåŒ–å­¦ä¹ çš„é€šç”¨æ€§å’Œå®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº† DreamerV3ï¼Œä¸€ä¸ªé€šç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä¸åŒçš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œé‡æ–°é…ç½®ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸–ç•Œæ¨¡å‹å­¦ä¹ <br>DreamerV3 é€šè¿‡å­¦ä¹ ä¸€ä¸ªä¸–ç•Œæ¨¡å‹æ¥é¢„æµ‹æ½œåœ¨è¡ŒåŠ¨çš„ç»“æœï¼Œä»è€Œè®©æ™ºèƒ½ä½“èƒ½å¤Ÿæƒ³è±¡æœªæ¥çš„åœºæ™¯å¹¶åšå‡ºæ›´å¥½çš„å†³ç­–ã€‚ä¸–ç•Œæ¨¡å‹ä½¿ç”¨å¾ªç¯çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆRSSMï¼‰æ¥é¢„æµ‹æœªæ¥çš„çŠ¶æ€å’Œå¥–åŠ±ï¼Œå¹¶é€šè¿‡é‡å»ºè¾“å…¥æ¥ç¡®ä¿è¡¨ç¤ºä¿¡æ¯ä¸°å¯Œã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé²æ£’æ€§æŠ€æœ¯<br>DreamerV3 é‡‡ç”¨äº†ä¸€ç³»åˆ—é²æ£’æ€§æŠ€æœ¯ï¼ŒåŒ…æ‹¬å½’ä¸€åŒ–ã€å¹³è¡¡å’Œè½¬æ¢ï¼Œä»¥ç¡®ä¿ç®—æ³•èƒ½å¤Ÿåœ¨ä¸åŒçš„é¢†åŸŸç¨³å®šå­¦ä¹ ã€‚è¿™äº›æŠ€æœ¯å¸®åŠ© DreamerV3 åœ¨å„ç§ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬è¿ç»­æ§åˆ¶ã€ç¦»æ•£åŠ¨ä½œã€ç¨€ç–å¥–åŠ±ã€å›¾åƒè¾“å…¥ã€ç©ºé—´ç¯å¢ƒå’Œæ£‹ç›˜æ¸¸æˆç­‰ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>DreamerV3 åœ¨è¶…è¿‡ 150 ä¸ªå¤šæ ·åŒ–çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†ä¸“é—¨ä¸ºç‰¹å®šé¢†åŸŸè®¾è®¡çš„ç®—æ³•ã€‚åœ¨ Atariã€ProcGenã€DMLabã€Atari100kã€Proprio Controlã€Visual Control å’Œ BSuite ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDreamerV3 éƒ½å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒDreamerV3 è¿˜æˆåŠŸåœ°åœ¨ Minecraft æ¸¸æˆä¸­æ”¶é›†é’»çŸ³ï¼Œè¿™æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>DreamerV3 çš„æˆåŠŸè¡¨æ˜ï¼Œé€šè¿‡å­¦ä¹ ä¸–ç•Œæ¨¡å‹å’Œé‡‡ç”¨é²æ£’æ€§æŠ€æœ¯ï¼Œå¯ä»¥å¼€å‘å‡ºé€šç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»è€Œè§£å†³å„ç§ä¸åŒçš„ä»»åŠ¡ã€‚è¿™é¡¹å·¥ä½œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ï¼ŒåŒ…æ‹¬ä»äº’è”ç½‘è§†é¢‘ä¸­æ•™æˆæ™ºèƒ½ä½“ä¸–ç•ŒçŸ¥è¯†ï¼Œä»¥åŠå­¦ä¹ è·¨é¢†åŸŸçš„å•ä¸€ä¸–ç•Œæ¨¡å‹ï¼Œä»¥ä½¿äººå·¥æ™ºèƒ½ä½“èƒ½å¤Ÿç§¯ç´¯æ›´å¹¿æ³›çš„çŸ¥è¯†å’Œèƒ½åŠ›ã€‚</td>
    </tr>
    <tr>
      <th>6</th>
      <td>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft</td>
      <td>Constructing AI models that respond to text instructions is challenging,<br>especially for sequential decision-making tasks. This work introduces a<br>methodology, inspired by unCLIP, for instruction-tuning generative models of<br>behavior without relying on a large dataset of instruction-labeled<br>trajectories. Using this methodology, we create an instruction-tuned Video<br>Pretraining (VPT) model called STEVE-1, which can follow short-horizon<br>open-ended text and visual instructions in Minecraft. STEVE-1 is trained in two<br>steps: adapting the pretrained VPT model to follow commands in MineCLIP's<br>latent space, then training a prior to predict latent codes from text. This<br>allows us to finetune VPT through self-supervised behavioral cloning and<br>hindsight relabeling, reducing the need for costly human text annotations, and<br>all for only $60 of compute. By leveraging pretrained models like VPT and<br>MineCLIP and employing best practices from text-conditioned image generation,<br>STEVE-1 sets a new bar for open-ended instruction-following in Minecraft with<br>low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming<br>previous baselines and robustly completing 12 of 13 tasks in our early-game<br>evaluation suite. We provide experimental evidence highlighting key factors for<br>downstream performance, including pretraining, classifier-free guidance, and<br>data scaling. All resources, including our model weights, training scripts, and<br>evaluation tools are made available for further research.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Minecraft ä¸­çš„æ–‡æœ¬åˆ°è¡Œä¸ºç”Ÿæˆæ¨¡å‹ï¼šSTEVE-1<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æ„å»ºèƒ½å¤Ÿå“åº”æ–‡æœ¬æŒ‡ä»¤çš„ AI æ¨¡å‹æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é¡ºåºå†³ç­–çš„ä»»åŠ¡ä¸­ã€‚ç°æœ‰çš„æ¨¡å‹å¾€å¾€éœ€è¦å¤§é‡å¸¦æœ‰æŒ‡ä»¤æ ‡ç­¾çš„è½¨è¿¹æ•°æ®é›†ï¼Œè¿™æ—¢æ˜‚è´µåˆéš¾ä»¥è·å–ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€ä¾èµ–å¤§é‡æŒ‡ä»¤æ ‡ç­¾è½¨è¿¹æ•°æ®é›†çš„æ–¹æ³•ï¼Œç”¨äºæ„å»ºèƒ½å¤Ÿå“åº”æ–‡æœ¬æŒ‡ä»¤çš„è¡Œä¸ºç”Ÿæˆæ¨¡å‹ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå— unCLIP å¯å‘çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§å— unCLIP å¯å‘çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œç”¨äºæ„å»ºè¡Œä¸ºç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ–¹æ³•å°†é—®é¢˜åˆ†è§£ä¸ºä¸¤ä¸ªæ¨¡å‹ï¼šä¸€ä¸ªç”¨äºç”Ÿæˆè¡Œä¸ºè½¨è¿¹çš„ç­–ç•¥æ¨¡å‹ï¼Œå¦ä¸€ä¸ªç”¨äºå°†æ–‡æœ¬æŒ‡ä»¤è½¬æ¢ä¸ºè§†è§‰åµŒå…¥çš„å…ˆéªŒæ¨¡å‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥é¿å…ä½¿ç”¨æ˜‚è´µçš„æ–‡æœ¬æŒ‡ä»¤æ ‡ç­¾ï¼Œè€Œæ˜¯åˆ©ç”¨è§†è§‰åµŒå…¥è¿›è¡Œè®­ç»ƒã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäº VPT å’Œ MineCLIP çš„æ¨¡å‹æ„å»º<br>æœ¬æ–‡åˆ©ç”¨äº†ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼šVPT å’Œ MineCLIPã€‚VPT æ˜¯ä¸€ä¸ªåŸºäº Minecraft æ¸¸æˆè§†é¢‘é¢„è®­ç»ƒçš„è¡Œä¸ºæ¨¡å‹ï¼Œè€Œ MineCLIP æ˜¯ä¸€ä¸ªå°†æ–‡æœ¬å’Œè§†é¢‘ç‰‡æ®µå¯¹é½çš„æ¨¡å‹ã€‚é€šè¿‡å°†è¿™ä¸¤ä¸ªæ¨¡å‹ç»“åˆèµ·æ¥ï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªèƒ½å¤Ÿå“åº”æ–‡æœ¬æŒ‡ä»¤çš„è¡Œä¸ºç”Ÿæˆæ¨¡å‹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºè‡ªç›‘ç£å­¦ä¹ å’Œå›æº¯é‡æ ‡è®°çš„å¾®è°ƒ<br>æœ¬æ–‡ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ å’Œå›æº¯é‡æ ‡è®°æŠ€æœ¯å¯¹ VPT æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å‡å°‘å¯¹æ˜‚è´µçš„äººç±»æ–‡æœ¬æ³¨é‡Šçš„éœ€æ±‚ï¼Œå¹¶åˆ©ç”¨ç°æœ‰çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒSTEVE-1 åœ¨ Minecraft ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°å“åº”æ–‡æœ¬æŒ‡ä»¤ï¼Œå¹¶å®Œæˆå„ç§ä»»åŠ¡ã€‚ä¸ä¹‹å‰çš„åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒSTEVE-1 åœ¨ä½çº§æ§åˆ¶ï¼ˆé¼ æ ‡å’Œé”®ç›˜ï¼‰å’ŒåŸå§‹åƒç´ è¾“å…¥æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œé¢„è®­ç»ƒã€åˆ†ç±»å™¨æ— å…³å¼•å¯¼å’Œæ•°æ®ç¼©æ”¾ç­‰å› ç´ å¯¹ä¸‹æ¸¸æ€§èƒ½è‡³å…³é‡è¦ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸå’Œä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è™šæ‹Ÿç°å®ç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†é¢„è®­ç»ƒã€åˆ†ç±»å™¨æ— å…³å¼•å¯¼å’Œæ•°æ®ç¼©æ”¾ç­‰å› ç´ å¯¹ä¸‹æ¸¸æ€§èƒ½çš„é‡è¦æ€§ï¼Œä¸ºæ„å»ºæ›´å¼ºå¤§çš„ AI æ¨¡å‹æä¾›äº†å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction</td>
      <td>We study the problem of learning goal-conditioned policies in Minecraft, a<br>popular, widely accessible yet challenging open-ended environment for<br>developing human-level multi-task agents. We first identify two main challenges<br>of learning such policies: 1) the indistinguishability of tasks from the state<br>distribution, due to the vast scene diversity, and 2) the non-stationary nature<br>of environment dynamics caused by partial observability. To tackle the first<br>challenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage<br>the emergence of goal-relevant visual state representations. To tackle the<br>second challenge, the policy is further fueled by an adaptive horizon<br>prediction module that helps alleviate the learning uncertainty brought by the<br>non-stationary dynamics. Experiments on 20 Minecraft tasks show that our method<br>significantly outperforms the best baseline so far; in many of them, we double<br>the performance. Our ablation and exploratory studies then explain how our<br>approach beat the counterparts and also unveil the surprising bonus of<br>zero-shot generalization to new scenes (biomes). We hope our agent could help<br>shed some light on learning goal-conditioned, multi-task agents in challenging,<br>open-ended environments like Minecraft.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åœ¨å¼€æ”¾ä¸–ç•Œä¸­å®ç°å¤šä»»åŠ¡æ§åˆ¶ï¼šç›®æ ‡æ„ŸçŸ¥è¡¨ç¤ºå­¦ä¹ å’Œè‡ªé€‚åº”é¢„æµ‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼Œå¦‚Minecraftï¼Œä¸ºå¼€å‘èƒ½å¤Ÿæ‰§è¡Œå„ç§ä»»åŠ¡çš„æ™ºèƒ½ä½“æä¾›äº†ä¸°å¯Œçš„å¹³å°ã€‚ç„¶è€Œï¼Œè¿™äº›ç¯å¢ƒä¹Ÿå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ï¼š<br>1. **çŠ¶æ€åˆ†å¸ƒçš„å¤šæ ·æ€§**ï¼šç”±äºåœºæ™¯çš„å¤šæ ·æ€§ï¼Œä¸åŒä»»åŠ¡çš„çŠ¶æ€éš¾ä»¥åŒºåˆ†ï¼Œè¿™ä½¿å¾—å­¦ä¹ ç›®æ ‡æ¡ä»¶ç­–ç•¥å˜å¾—å›°éš¾ã€‚<br>2. **ç¯å¢ƒåŠ¨æ€çš„éå¹³ç¨³æ€§**ï¼šç”±äºéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ï¼Œç¯å¢ƒåŠ¨æ€å…·æœ‰éå¹³ç¨³æ€§ï¼Œå¯¼è‡´å­¦ä¹ çš„ä¸ç¡®å®šæ€§å¢åŠ ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä»¥ä¸‹åˆ›æ–°æ–¹æ³•ï¼š<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼š**ç›®æ ‡æ„ŸçŸ¥éª¨å¹²ç½‘ç»œï¼ˆGSB**ï¼‰<br>   - GSBé€šè¿‡åœ¨å¤šä¸ªå±‚æ¬¡ä¸Šèåˆç›®æ ‡ä¿¡æ¯ï¼Œé¼“åŠ±å‡ºç°ä¸ç›®æ ‡ç›¸å…³çš„è§†è§‰çŠ¶æ€è¡¨ç¤ºï¼Œä»è€Œè§£å†³çŠ¶æ€åˆ†å¸ƒå¤šæ ·æ€§çš„é—®é¢˜ã€‚<br>   - GSBç”±å¤šä¸ªç›®æ ‡å·ç§¯å—ï¼ˆg-conv blockï¼‰ç»„æˆï¼Œè¿™äº›å—é€šè¿‡é€šé“è°ƒåˆ¶å°†ç›®æ ‡ä¿¡æ¯ä¸è§†è§‰ç‰¹å¾èåˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼š**è‡ªé€‚åº”é¢„æµ‹æ¨¡å—**<br>   - ä¸ºäº†åº”å¯¹ç¯å¢ƒåŠ¨æ€çš„éå¹³ç¨³æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†è‡ªé€‚åº”é¢„æµ‹æ¨¡å—ï¼Œè¯¥æ¨¡å—é¢„æµ‹ä»å½“å‰çŠ¶æ€åˆ°ç›®æ ‡çš„å‰©ä½™æ—¶é—´æ­¥æ•°ï¼ˆå³è·ç¦»åˆ°ç›®æ ‡çš„è·ç¦»ï¼‰ã€‚<br>   - è‡ªé€‚åº”é¢„æµ‹æ¨¡å—é€šè¿‡é¢„æµ‹å‰©ä½™æ—¶é—´æ­¥æ•°ï¼Œå¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£ç›®æ ‡çš„å®Œæˆç¨‹åº¦ï¼Œä»è€Œæé«˜å†³ç­–çš„å‡†ç¡®æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Minecraftçš„20ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œåœ¨è®¸å¤šä»»åŠ¡ä¸­æ€§èƒ½ç¿»å€ã€‚æ¶ˆèç ”ç©¶å’Œæ¢ç´¢æ€§ç ”ç©¶è§£é‡Šäº†æœ¬æ–‡æ–¹æ³•å¦‚ä½•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æ­ç¤ºäº†ä»¤äººæƒŠè®¶çš„é›¶æ ·æœ¬æ³›åŒ–åˆ°æ–°åœºæ™¯ï¼ˆç”Ÿç‰©ç¾¤è½ï¼‰çš„é¢å¤–ä¼˜åŠ¿ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„GSBå’Œè‡ªé€‚åº”é¢„æµ‹æ¨¡å—ä¸ºåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å­¦ä¹ ç›®æ ‡æ¡ä»¶ç­–ç•¥æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œå¤šä»»åŠ¡çš„æ™ºèƒ½ä½“æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>8</th>
      <td>MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control</td>
      <td>It is a long-lasting goal to design a generalist-embodied agent that can<br>follow diverse instructions in human-like ways. However, existing approaches<br>often fail to steadily follow instructions due to difficulties in understanding<br>abstract and sequential natural language instructions. To this end, we<br>introduce MineDreamer, an open-ended embodied agent built upon the challenging<br>Minecraft simulator with an innovative paradigm that enhances<br>instruction-following ability in low-level control signal generation.<br>Specifically, MineDreamer is developed on top of recent advances in Multimodal<br>Large Language Models (MLLMs) and diffusion models, and we employ a<br>Chain-of-Imagination (CoI) mechanism to envision the step-by-step process of<br>executing instructions and translating imaginations into more precise visual<br>prompts tailored to the current state; subsequently, the agent generates<br>keyboard-and-mouse actions to efficiently achieve these imaginations, steadily<br>following the instructions at each step. Extensive experiments demonstrate that<br>MineDreamer follows single and multi-step instructions steadily, significantly<br>outperforming the best generalist agent baseline and nearly doubling its<br>performance. Moreover, qualitative analysis of the agent's imaginative ability<br>reveals its generalization and comprehension of the open world.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MineDreamerï¼šåŸºäºæƒ³è±¡é“¾çš„æ¨¡æ‹Ÿä¸–ç•Œæ§åˆ¶æŒ‡ä»¤è·Ÿéš<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œè®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿä»¥äººç±»æ–¹å¼ç†è§£å’Œæ‰§è¡Œå¤šæ ·åŒ–æŒ‡ä»¤çš„é€šç”¨å‹å…·èº«æ™ºèƒ½ä½“ä¸€ç›´æ˜¯é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¾€å¾€éš¾ä»¥ç¨³å®šåœ°éµå¾ªæŒ‡ä»¤ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£å’Œæ‰§è¡ŒæŠ½è±¡å’Œé¡ºåºçš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ–¹é¢å­˜åœ¨å›°éš¾ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥â€œæƒ³è±¡é“¾â€ï¼ˆChain-of-Imagination, CoIï¼‰æœºåˆ¶<br>MineDreamer é€šè¿‡ CoI æœºåˆ¶ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®æŒ‡ä»¤å’Œå½“å‰çŠ¶æ€é€æ­¥æƒ³è±¡å¹¶æ‰§è¡ŒæŒ‡ä»¤ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»åœ¨è§£å†³é—®é¢˜æ—¶ï¼Œæ ¹æ®å½“å‰çŠ¶æ€é€æ­¥æƒ³è±¡ä¸‹ä¸€æ­¥ç›®æ ‡çš„è¿‡ç¨‹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¢å¼ºçš„æ‰©æ•£æ¨¡å‹<br>MineDreamer ä½¿ç”¨ MLLM å¢å¼ºçš„æ‰©æ•£æ¨¡å‹æ¥ç”ŸæˆåŒ…å«ç‰©ç†è§„åˆ™å’Œç¯å¢ƒç†è§£çš„æƒ³è±¡å›¾åƒï¼Œè¿™äº›å›¾åƒä½œä¸ºæ›´ç²¾ç¡®çš„è§†è§‰æç¤ºï¼Œå¼•å¯¼æ™ºèƒ½ä½“ç”Ÿæˆä½çº§æ§åˆ¶ä¿¡å·ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç›®æ ‡æ¼‚ç§»æ”¶é›†æ–¹æ³•<br>ä¸ºäº†è®­ç»ƒ Imaginatorï¼ŒMineDreamer ä½¿ç”¨äº†ç›®æ ‡æ¼‚ç§»æ”¶é›†æ–¹æ³•æ¥æ”¶é›†å¤§é‡å…·èº«æ•°æ®ï¼Œå¸®åŠ© Imaginator ç†è§£å¦‚ä½•é€æ­¥å®ŒæˆæŒ‡ä»¤ä»¥åŠå¦‚ä½•é‡å¤å®ŒæˆæŒ‡ä»¤ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>MineDreamer åœ¨æ‰§è¡Œå•æ­¥å’Œå¤šæ­¥æŒ‡ä»¤æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºæœ€ä½³é€šç”¨å‹æ™ºèƒ½ä½“åŸºçº¿ï¼Œæ€§èƒ½å‡ ä¹ç¿»å€ã€‚æ­¤å¤–ï¼Œå¯¹æ™ºèƒ½ä½“æƒ³è±¡èƒ½åŠ›çš„å®šæ€§åˆ†æè¡¨æ˜ï¼Œå®ƒèƒ½å¤Ÿç†è§£å’Œé€‚åº”å¼€æ”¾ä¸–ç•Œçš„ç¯å¢ƒã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MineDreamer çš„ CoI æœºåˆ¶ä¸ºè§£å†³æŒ‡ä»¤è·Ÿéšé—®é¢˜æä¾›äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå…¶ MLLM å¢å¼ºçš„æ‰©æ•£æ¨¡å‹å’Œç›®æ ‡æ¼‚ç§»æ”¶é›†æ–¹æ³•ä¹Ÿä¸ºå…·èº«æ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒMineDreamer çš„æˆåŠŸä¹Ÿè¡¨æ˜ï¼Œå…·èº«æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations</td>
      <td>Recently, both Contrastive Learning (CL) and Mask Image Modeling (MIM)<br>demonstrate that self-supervision is powerful to learn good representations.<br>However, naively combining them is far from success. In this paper, we start by<br>making the empirical observation that a naive joint optimization of CL and MIM<br>losses leads to conflicting gradient directions - more severe as the layers go<br>deeper. This motivates us to shift the paradigm from combining loss at the end,<br>to choosing the proper learning method per network layer. Inspired by<br>experimental observations, we find that MIM and CL are suitable to lower and<br>higher layers, respectively. We hence propose to combine them in a surprisingly<br>simple, "sequential cascade" fashion: early layers are first trained under one<br>MIM loss, on top of which latter layers continue to be trained under another CL<br>loss. The proposed Layer Grafted Pre-training learns good visual<br>representations that demonstrate superior label efficiency in downstream<br>applications, in particular yielding strong few-shot performance besides linear<br>evaluation. For instance, on ImageNet-1k, Layer Grafted Pre-training yields<br>65.5% Top-1 accuracy in terms of 1% few-shot learning with ViT-B/16, which<br>improves MIM and CL baselines by 14.4% and 2.1% with no bells and whistles. The<br>code is available at<br>https://github.com/VITA-Group/layerGraftedPretraining_ICLR23.git.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Layer Grafted Pre-trainingï¼šç»“åˆå¯¹æ¯”å­¦ä¹ å’Œæ©ç å›¾åƒå»ºæ¨¡ï¼Œæå‡æ ‡ç­¾æ•ˆç‡<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰å’Œæ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰éƒ½è¯æ˜äº†è‡ªç›‘ç£å­¦ä¹ åœ¨è§†è§‰è¡¨å¾å­¦ä¹ ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç®€å•åœ°ç»“åˆè¿™ä¸¤ç§æ–¹æ³•å¹¶æ²¡æœ‰å–å¾—ç†æƒ³çš„æ•ˆæœã€‚æœ¬æ–‡ä½œè€…é€šè¿‡å®éªŒå‘ç°ï¼Œå°†CLå’ŒMIMçš„æŸå¤±å‡½æ•°åŒæ—¶ä¼˜åŒ–ä¼šå¯¼è‡´æ¢¯åº¦æ–¹å‘å†²çªï¼Œå¹¶ä¸”éšç€ç½‘ç»œå±‚æ•°çš„åŠ æ·±ï¼Œå†²çªå˜å¾—æ›´åŠ ä¸¥é‡ã€‚è¿™ä¿ƒä½¿ä½œè€…é‡æ–°æ€è€ƒå¦‚ä½•æ›´å¥½åœ°ç»“åˆè¿™ä¸¤ç§æ–¹æ³•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå±‚çº§å«æ¥é¢„è®­ç»ƒ<br>æœ¬æ–‡æå‡ºäº†å±‚çº§å«æ¥é¢„è®­ç»ƒï¼ˆLayer Grafted Pre-trainingï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†MIMå’ŒCLåˆ†åˆ«åº”ç”¨äºç½‘ç»œçš„ä¸åŒå±‚çº§ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆä½¿ç”¨MIMæŸå¤±å‡½æ•°è®­ç»ƒç½‘ç»œçš„ä½å±‚ï¼Œç„¶ååœ¨è¿™äº›å±‚çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨CLæŸå¤±å‡½æ•°ç»§ç»­è®­ç»ƒç½‘ç»œçš„é«˜å±‚ã€‚è¿™ç§â€œé¡ºåºçº§è”â€çš„æ–¹å¼æœ‰æ•ˆåœ°é¿å…äº†MIMå’ŒCLæŸå¤±å‡½æ•°ä¹‹é—´çš„å†²çªï¼Œå¹¶ä½¿æ¯ä¸ªæŸå¤±å‡½æ•°éƒ½èƒ½åœ¨å…¶æœ€åˆé€‚çš„å±‚çº§ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¹³æ»‘å«æ¥<br>ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–å±‚çº§å«æ¥é¢„è®­ç»ƒï¼Œæœ¬æ–‡æå‡ºäº†â€œå¹³æ»‘å«æ¥â€çš„ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œä¸ä»…è®­ç»ƒé«˜å±‚ï¼Œè¿˜å…è®¸ä½å±‚ä»¥è¾ƒå°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒã€‚è¿™ç§ç­–ç•¥å¯ä»¥é¿å…ç‰¹å¾ç©ºé—´ä¸­çªç„¶çš„å˜åŒ–ï¼Œå¹¶ä½¿ä½å±‚èƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™MIMå­¦ä¹ åˆ°çš„ç‰¹å¾ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨ImageNet-1kæ•°æ®é›†ä¸Šï¼Œå±‚çº§å«æ¥é¢„è®­ç»ƒåœ¨1%å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­å–å¾—äº†65.5%çš„Top-1å‡†ç¡®ç‡ï¼Œæ¯”MIMå’ŒCLåŸºçº¿åˆ†åˆ«æé«˜äº†14.4%å’Œ2.1%ã€‚æ­¤å¤–ï¼Œå±‚çº§å«æ¥é¢„è®­ç»ƒåœ¨10%å°‘æ ·æœ¬å­¦ä¹ å’Œçº¿æ€§è¯„ä¼°ä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>å±‚çº§å«æ¥é¢„è®­ç»ƒæ¡†æ¶ä¸ºç»“åˆå¯¹æ¯”å­¦ä¹ å’Œæ©ç å›¾åƒå»ºæ¨¡æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§è§†è§‰è¡¨å¾å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶æœ‰æœ›æå‡æ ‡ç­¾æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„â€œå¹³æ»‘å«æ¥â€ç­–ç•¥ä¹Ÿä¸ºå…¶ä»–å¤šé˜¶æ®µé¢„è®­ç»ƒä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Vision-Language Models Provide Promptable Representations for Reinforcement Learning</td>
      <td>Humans can quickly learn new behaviors by leveraging background world<br>knowledge. In contrast, agents trained with reinforcement learning (RL)<br>typically learn behaviors from scratch. We thus propose a novel approach that<br>uses the vast amounts of general and indexable world knowledge encoded in<br>vision-language models (VLMs) pre-trained on Internet-scale data for embodied<br>RL. We initialize policies with VLMs by using them as promptable<br>representations: embeddings that encode semantic features of visual<br>observations based on the VLM's internal knowledge and reasoning capabilities,<br>as elicited through prompts that provide task context and auxiliary<br>information. We evaluate our approach on visually-complex, long horizon RL<br>tasks in Minecraft and robot navigation in Habitat. We find that our policies<br>trained on embeddings from off-the-shelf, general-purpose VLMs outperform<br>equivalent policies trained on generic, non-promptable image embeddings. We<br>also find our approach outperforms instruction-following methods and performs<br>comparably to domain-specific embeddings. Finally, we show that our approach<br>can use chain-of-thought prompting to produce representations of common-sense<br>semantic reasoning, improving policy performance in novel scenes by 1.5 times.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›å¯æç¤ºçš„è¡¨ç¤º<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>äººç±»èƒ½å¤Ÿå¿«é€Ÿå­¦ä¹ æ–°è¡Œä¸ºï¼Œè¿™å¾—ç›Šäºä»–ä»¬ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„æ™ºèƒ½ä½“é€šå¸¸éœ€è¦ä»å¤´å¼€å§‹å­¦ä¹ ï¼Œç¼ºä¹åˆ©ç”¨èƒŒæ™¯çŸ¥è¯†çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨åœ¨äº’è”ç½‘è§„æ¨¡æ•°æ®ä¸Šé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥ä¸ºå…·èº«RLæä¾›ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œå¯æç¤ºè¡¨ç¤ºçš„å¼ºåŒ–å­¦ä¹ â€ï¼ˆPR2Lï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡å‘VLMæä¾›ä»»åŠ¡ç›¸å…³çš„æç¤ºï¼Œä½¿å…¶ç”ŸæˆåŒ…å«è¯­ä¹‰ä¿¡æ¯çš„è¡¨ç¤ºï¼Œå¹¶å°†å…¶ç”¨äºè®­ç»ƒRLç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼ŒPR2LåŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ­¥éª¤ï¼š<br><br>1. **æç¤ºè®¾è®¡**ï¼šè®¾è®¡ä»»åŠ¡ç›¸å…³çš„æç¤ºï¼Œå¼•å¯¼VLMå…³æ³¨å¹¶ç¼–ç å›¾åƒä¸­ä¸ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰ç‰¹å¾ã€‚<br>2. **è¡¨ç¤ºæå–**ï¼šä½¿ç”¨VLMå¯¹æç¤ºå’Œå›¾åƒè¿›è¡Œç¼–ç ï¼Œå¹¶æå–ä¸­é—´å±‚çš„è¡¨ç¤ºä½œä¸ºçŠ¶æ€è¡¨ç¤ºã€‚<br>3. **ç­–ç•¥è®­ç»ƒ**ï¼šä½¿ç”¨RLç®—æ³•è®­ç»ƒç­–ç•¥ï¼Œå°†æå–çš„è¡¨ç¤ºè½¬æ¢ä¸ºä½çº§åŠ¨ä½œã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨Minecraftå’ŒHabitatä¸¤ä¸ªé¢†åŸŸè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜PR2Låœ¨ä»¥ä¸‹æ–¹é¢å–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ï¼š<br><br>* **æ€§èƒ½æå‡**ï¼šä¸ä½¿ç”¨éæç¤ºå›¾åƒåµŒå…¥æˆ–æŒ‡ä»¤æ¡ä»¶çš„æ–¹æ³•ç›¸æ¯”ï¼ŒPR2Lè®­ç»ƒçš„ç­–ç•¥åœ¨Minecraftå’ŒHabitatä»»åŠ¡ä¸­å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚<br>* **æ ·æœ¬æ•ˆç‡**ï¼šPR2Lèƒ½å¤Ÿåˆ©ç”¨VLMçš„å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œå‡å°‘è®­ç»ƒæ‰€éœ€çš„æ ·æœ¬æ•°é‡ã€‚<br>* **æ³›åŒ–èƒ½åŠ›**ï¼šPR2Lèƒ½å¤Ÿåˆ©ç”¨VLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿ç­–ç•¥åœ¨æ–°çš„åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>PR2Lä¸ºåˆ©ç”¨VLMè¿›è¡Œå¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br><br>* **æç¤ºè®¾è®¡**ï¼šè®¾è®¡æœ‰æ•ˆçš„æç¤ºæ˜¯PR2LæˆåŠŸçš„å…³é”®ï¼Œéœ€è¦æ ¹æ®ä»»åŠ¡ç‰¹ç‚¹è¿›è¡Œç²¾å¿ƒè®¾è®¡ã€‚<br>* **è¡¨ç¤ºé€‰æ‹©**ï¼šé€‰æ‹©åˆé€‚çš„VLMå±‚å’Œè¡¨ç¤ºæ–¹æ³•ï¼Œä»¥æå–å¯¹ä»»åŠ¡æœ‰ç”¨çš„è¯­ä¹‰ä¿¡æ¯ã€‚<br>* **ç­–ç•¥è®­ç»ƒ**ï¼šä½¿ç”¨åˆé€‚çš„RLç®—æ³•å’Œç­–ç•¥ç½‘ç»œç»“æ„ï¼Œä»¥å……åˆ†åˆ©ç”¨VLMæä¾›çš„è¡¨ç¤ºã€‚<br><br>## ğŸŒŸ æœªæ¥å±•æœ›<br>éšç€VLMèƒ½åŠ›çš„ä¸æ–­æå‡ï¼ŒPR2Læœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œå¹¶ä¸ºå¼ºåŒ–å­¦ä¹ å¸¦æ¥æ–°çš„çªç ´ã€‚</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Reinforcement Learning Friendly Vision-Language Model for Minecraft</td>
      <td>One of the essential missions in the AI research community is to build an<br>autonomous embodied agent that can achieve high-level performance across a wide<br>spectrum of tasks. However, acquiring or manually designing rewards for all<br>open-ended tasks is unrealistic. In this paper, we propose a novel cross-modal<br>contrastive learning framework architecture, CLIP4MC, aiming to learn a<br>reinforcement learning (RL) friendly vision-language model (VLM) that serves as<br>an intrinsic reward function for open-ended tasks. Simply utilizing the<br>similarity between the video snippet and the language prompt is not RL-friendly<br>since standard VLMs may only capture the similarity at a coarse level. To<br>achieve RL-friendliness, we incorporate the task completion degree into the VLM<br>training objective, as this information can assist agents in distinguishing the<br>importance between different states. Moreover, we provide neat YouTube datasets<br>based on the large-scale YouTube database provided by MineDojo. Specifically,<br>two rounds of filtering operations guarantee that the dataset covers enough<br>essential information and that the video-text pair is highly correlated.<br>Empirically, we demonstrate that the proposed method achieves better<br>performance on RL tasks compared with baselines. The code and datasets are<br>available at https://github.com/PKU-RL/CLIP4MC.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºCLIP4MCçš„å¼ºåŒ–å­¦ä¹ å‹å¥½å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨Minecraftä¸­çš„åº”ç”¨<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨äººå·¥æ™ºèƒ½ç ”ç©¶é¢†åŸŸï¼Œæ„å»ºä¸€ä¸ªèƒ½å¤Ÿåœ¨å„ç§ä»»åŠ¡ä¸­å®ç°é«˜çº§æ€§èƒ½çš„è‡ªä¸»å…·èº«æ™ºèƒ½ä½“æ˜¯æ ¸å¿ƒç›®æ ‡ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œä¸ºæ‰€æœ‰å¼€æ”¾æ€§ä»»åŠ¡æ‰‹åŠ¨è·å–æˆ–è®¾è®¡å¥–åŠ±å‡½æ•°æ˜¯ä¸åˆ‡å®é™…çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æ¶æ¶æ„CLIP4MCï¼Œæ—¨åœ¨å­¦ä¹ ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å‹å¥½çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä½œä¸ºå¼€æ”¾æ€§ä»»åŠ¡çš„å†…åœ¨å¥–åŠ±å‡½æ•°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºäº†ä¸¤ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†<br>- ç¬¬ä¸€ä¸ªæ•°æ®é›†ç»è¿‡æ•°æ®æ¸…æ´—å’Œå…¨å±€çº§ç›¸å…³æ€§è¿‡æ»¤ï¼Œè®­ç»ƒå‡ºçš„VLMæ€§èƒ½ä¸å®˜æ–¹å‘å¸ƒçš„MineCLIPç›¸å½“ã€‚<br>- ç¬¬äºŒä¸ªæ•°æ®é›†è¿›ä¸€æ­¥ç»“åˆäº†å±€éƒ¨çº§ç›¸å…³æ€§è¿‡æ»¤ï¼Œæ›´é€‚åˆRLä»»åŠ¡ï¼Œè®­ç»ƒå‡ºçš„VLMæ€§èƒ½ä¼˜äºç¬¬ä¸€ä¸ªæ•°æ®é›†ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºCLIP4MCæ¨¡å‹<br>- CLIP4MCæ¨¡å‹é€šè¿‡å°†ä»»åŠ¡å®Œæˆç¨‹åº¦çº³å…¥VLMè®­ç»ƒç›®æ ‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ æ¯ä¸ªè§†é¢‘ç‰‡æ®µä¸ä»»åŠ¡å®Œæˆç¨‹åº¦ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œä¸ºRLè®­ç»ƒè¿‡ç¨‹æä¾›æ›´å‹å¥½çš„å¥–åŠ±ä¿¡å·ã€‚<br>- CLIP4MCæ¨¡å‹åœ¨MineDojoçš„ç¼–ç¨‹ä»»åŠ¡ä¸Šå–å¾—äº†æ›´å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨RLä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>- åœ¨MineDojoçš„ç¼–ç¨‹ä»»åŠ¡ä¸­ï¼ŒCLIP4MCæ¨¡å‹åœ¨ç‹©çŒä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ›´é«˜çš„æˆåŠŸç‡ï¼Œè¯æ˜äº†å…¶åœ¨RLä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚<br>- é€šè¿‡ç›¸å…³æ€§åˆ†æï¼ŒCLIP4MCæ¨¡å‹çš„å¥–åŠ±ä¿¡å·ä¸ç›®æ ‡å®ä½“çš„å°ºå¯¸å…·æœ‰æ›´é«˜çš„ç›¸å…³æ€§ï¼Œè¿™å¯¹äºRLè®­ç»ƒè¿‡ç¨‹è‡³å…³é‡è¦ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>- æœ¬æ–‡æå‡ºçš„CLIP4MCæ¨¡å‹å’Œæ„å»ºçš„æ•°æ®é›†ä¸ºRLä»»åŠ¡ä¸­çš„å¥–åŠ±å‡½æ•°è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚<br>- æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†ä»»åŠ¡å®Œæˆç¨‹åº¦çº³å…¥VLMè®­ç»ƒç›®æ ‡å¯ä»¥æœ‰æ•ˆåœ°æé«˜RLè®­ç»ƒæ€§èƒ½ã€‚<br>- æœ¬æ–‡çš„ç ”ç©¶æˆæœå¯ä»¥åº”ç”¨äºå…¶ä»–å¼€æ”¾æ€§ä»»åŠ¡ä¸­çš„RLè®­ç»ƒï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶ç­‰ã€‚</td>
    </tr>
    <tr>
      <th>12</th>
      <td>ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting</td>
      <td>Vision-language models (VLMs) have excelled in multimodal tasks, but adapting<br>them to embodied decision-making in open-world environments presents<br>challenges. One critical issue is bridging the gap between discrete entities in<br>low-level observations and the abstract concepts required for effective<br>planning. A common solution is building hierarchical agents, where VLMs serve<br>as high-level reasoners that break down tasks into executable sub-tasks,<br>typically specified using language. However, language suffers from the<br>inability to communicate detailed spatial information. We propose<br>visual-temporal context prompting, a novel communication protocol between VLMs<br>and policy models. This protocol leverages object segmentation from past<br>observations to guide policy-environment interactions. Using this approach, we<br>train ROCKET-1, a low-level policy that predicts actions based on concatenated<br>visual observations and segmentation masks, supported by real-time object<br>tracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to<br>tackle complex tasks that demand spatial reasoning. Experiments in Minecraft<br>show that our approach enables agents to achieve previously unattainable tasks,<br>with a \( \mathbf{76}\% \) absolute improvement in open-world interaction<br>performance. Codes and demos are now available on the project page:<br>https://craftjarvis.github.io/ROCKET-1.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ROCKET-1ï¼šæŒæ¡å¼€æ”¾ä¸–ç•Œäº¤äº’çš„è§†è§‰-æ—¶é—´ä¸Šä¸‹æ–‡æç¤º<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œå…·èº«å†³ç­–æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸€ä¸ªå…³é”®é—®é¢˜æ˜¯å¼¥åˆä½çº§è§‚å¯Ÿä¸­ç¦»æ•£å®ä½“ä¸æœ‰æ•ˆè§„åˆ’æ‰€éœ€çš„æŠ½è±¡æ¦‚å¿µä¹‹é—´çš„å·®è·ã€‚ä¸€ç§å¸¸è§çš„è§£å†³æ–¹æ¡ˆæ˜¯æ„å»ºåˆ†å±‚ä»£ç†ï¼Œå…¶ä¸­VLMsä½œä¸ºé«˜çº§æ¨ç†å™¨ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„å­ä»»åŠ¡ï¼Œé€šå¸¸ä½¿ç”¨è¯­è¨€æŒ‡å®šã€‚ç„¶è€Œï¼Œè¯­è¨€åœ¨ä¼ è¾¾è¯¦ç»†çš„ç©ºé—´ä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§†è§‰-æ—¶é—´ä¸Šä¸‹æ–‡æç¤º<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é€šä¿¡åè®®ï¼Œç§°ä¸ºè§†è§‰-æ—¶é—´ä¸Šä¸‹æ–‡æç¤ºï¼Œç”¨äºVLMså’Œæ”¿ç­–æ¨¡å‹ä¹‹é—´çš„é€šä¿¡ã€‚è¯¥åè®®åˆ©ç”¨è¿‡å»è§‚å¯Ÿåˆ°çš„å¯¹è±¡åˆ†å‰²æ¥æŒ‡å¯¼ç­–ç•¥-ç¯å¢ƒäº¤äº’ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šROCKET-1<br>ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ROCKET-1ï¼Œè¿™æ˜¯ä¸€ä¸ªä½çº§ç­–ç•¥ï¼Œå®ƒæ ¹æ®è¿æ¥çš„è§†è§‰è§‚å¯Ÿå’Œåˆ†å‰²æ©ç é¢„æµ‹åŠ¨ä½œï¼Œå¹¶ç”±SAM-2çš„å®æ—¶å¯¹è±¡è·Ÿè¸ªæ”¯æŒã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡Šæ”¾äº†VLMsçš„æ½œåŠ›ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿå¤„ç†éœ€è¦ç©ºé—´æ¨ç†çš„å¤æ‚ä»»åŠ¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Minecraftä¸­çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ä»£ç†èƒ½å¤Ÿå®Œæˆä»¥å‰æ— æ³•å®Œæˆçš„ä»»åŠ¡ï¼Œå¼€æ”¾ä¸–ç•Œäº¤äº’æ€§èƒ½æé«˜äº†76%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„è§†è§‰-æ—¶é—´ä¸Šä¸‹æ–‡æç¤ºåè®®ä¸ºVLMså’Œæ”¿ç­–æ¨¡å‹ä¹‹é—´çš„é€šä¿¡æä¾›äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œæœ‰åŠ©äºè§£å†³å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å…·èº«å†³ç­–é—®é¢˜ã€‚ROCKET-1ä½œä¸ºä¸€ç§ä½çº§ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†éœ€è¦ç©ºé—´æ¨ç†çš„å¤æ‚ä»»åŠ¡ï¼Œå¹¶å…·æœ‰å®æ—¶å¯¹è±¡è·Ÿè¸ªèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å‘åè½¨è¿¹é‡æ–°æ ‡è®°æ–¹æ³•å¯ä»¥è‡ªåŠ¨æ£€æµ‹å’Œåˆ†å‰²æ”¶é›†çš„è½¨è¿¹ä¸­çš„æ‰€éœ€å¯¹è±¡ï¼Œä¸ºROCKET-1çš„è®­ç»ƒæä¾›äº†ä¾¿åˆ©ã€‚</td>
    </tr>
    <tr>
      <th>13</th>
      <td>GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents</td>
      <td>Developing agents that can follow multimodal instructions remains a<br>fundamental challenge in robotics and AI. Although large-scale pre-training on<br>unlabeled datasets (no language instruction) has enabled agents to learn<br>diverse behaviors, these agents often struggle with following instructions.<br>While augmenting the dataset with instruction labels can mitigate this issue,<br>acquiring such high-quality annotations at scale is impractical. To address<br>this issue, we frame the problem as a semi-supervised learning task and<br>introduce GROOT-2, a multimodal instructable agent trained using a novel<br>approach that combines weak supervision with latent variable models. Our method<br>consists of two key components: constrained self-imitating, which utilizes<br>large amounts of unlabeled demonstrations to enable the policy to learn diverse<br>behaviors, and human intention alignment, which uses a smaller set of labeled<br>demonstrations to ensure the latent space reflects human intentions. GROOT-2's<br>effectiveness is validated across four diverse environments, ranging from video<br>games to robotic manipulation, demonstrating its robust multimodal<br>instruction-following capabilities.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GROOT-2ï¼šå¼±ç›‘ç£å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšä»£ç†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨æœºå™¨äººå­¦å’Œäººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¼€å‘èƒ½å¤Ÿéµå¾ªå¤šæ¨¡æ€æŒ‡ä»¤çš„ä»£ç†ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚å°½ç®¡åœ¨å¤§è§„æ¨¡æ— æ ‡ç­¾æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ˆæ²¡æœ‰è¯­è¨€æŒ‡ä»¤ï¼‰å·²ç»ä½¿ä»£ç†èƒ½å¤Ÿå­¦ä¹ å¤šæ ·åŒ–çš„è¡Œä¸ºï¼Œä½†è¿™äº›ä»£ç†åœ¨éµå¾ªæŒ‡ä»¤æ—¶å¾€å¾€é‡åˆ°å›°éš¾ã€‚è™½ç„¶é€šè¿‡å¢åŠ æ•°æ®é›†çš„æŒ‡ä»¤æ ‡ç­¾å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†åœ¨å¤§è§„æ¨¡ä¸Šè·å–é«˜è´¨é‡æ³¨é‡Šæ˜¯ä¸åˆ‡å®é™…çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†é—®é¢˜æ¡†æ¶åŒ–ä¸ºåŠç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†GROOT-2ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¯æŒ‡ä»¤ä»£ç†ï¼Œä½¿ç”¨ä¸€ç§ç»“åˆå¼±ç›‘ç£å’Œæ½œåœ¨å˜é‡æ¨¡å‹çš„æ–°æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šçº¦æŸè‡ªæˆ‘æ¨¡ä»¿<br>åˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„æ¼”ç¤ºæ¥ä½¿ç­–ç•¥èƒ½å¤Ÿå­¦ä¹ å¤šæ ·åŒ–çš„è¡Œä¸ºã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šäººç±»æ„å›¾å¯¹é½<br>ä½¿ç”¨è¾ƒå°çš„ä¸€ç»„æ ‡è®°æ¼”ç¤ºæ¥ç¡®ä¿æ½œåœ¨ç©ºé—´åæ˜ äººç±»æ„å›¾ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>GROOT-2çš„æœ‰æ•ˆæ€§åœ¨å››ä¸ªä¸åŒçš„ç¯å¢ƒä¸­å¾—åˆ°éªŒè¯ï¼Œä»è§†é¢‘æ¸¸æˆåˆ°æœºå™¨äººæ“ä½œï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>GROOT-2åœ¨å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšæ–¹é¢å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œå¯ä»¥åº”ç”¨äºå„ç§ç¯å¢ƒï¼ŒåŒ…æ‹¬è§†é¢‘æ¸¸æˆå’Œæœºå™¨äººæ“ä½œã€‚æ­¤å¤–ï¼ŒGROOT-2çš„è®­ç»ƒæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦å¼±ç›‘ç£å­¦ä¹ çš„ä»»åŠ¡ã€‚</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents</td>
      <td>We investigate the challenge of task planning for multi-task embodied agents<br>in open-world environments. Two main difficulties are identified: 1) executing<br>plans in an open-world environment (e.g., Minecraft) necessitates accurate and<br>multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla<br>planners do not consider how easy the current agent can achieve a given<br>sub-task when ordering parallel sub-goals within a complicated plan, the<br>resulting plan could be inefficient or even infeasible. To this end, we propose<br>"\( \underline{D} \)escribe, \( \underline{E} \)xplain, \( \underline{P} \)lan and<br>\( \underline{S} \)elect" (\( \textbf{DEPS} \)), an interactive planning approach based<br>on Large Language Models (LLMs). DEPS facilitates better error correction on<br>initial LLM-generated \( \textit{plan} \) by integrating \( \textit{description} \) of<br>the plan execution process and providing self-\( \textit{explanation} \) of<br>feedback when encountering failures during the extended planning phases.<br>Furthermore, it includes a goal \( \textit{selector} \), which is a trainable<br>module that ranks parallel candidate sub-goals based on the estimated steps of<br>completion, consequently refining the initial plan. Our experiments mark the<br>milestone of the first zero-shot multi-task agent that can robustly accomplish<br>70+ Minecraft tasks and nearly double the overall performances. Further testing<br>reveals our method's general effectiveness in popularly adopted non-open-ended<br>domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and<br>exploratory studies detail how our design beats the counterparts and provide a<br>promising update on the \( \texttt{ObtainDiamond} \) grand challenge with our<br>approach. The code is released at https://github.com/CraftJarvis/MC-Planner.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤äº’å¼è§„åˆ’ï¼ŒåŠ©åŠ›å¼€æ”¾ä¸–ç•Œå¤šä»»åŠ¡æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå¤šä»»åŠ¡æ™ºèƒ½ä½“é¢ä¸´ç€ä¸¤å¤§æŒ‘æˆ˜ï¼š1ï¼‰æ‰§è¡Œè®¡åˆ’éœ€è¦ç²¾ç¡®çš„å¤šæ­¥æ¨ç†ï¼Œå› ä¸ºä»»åŠ¡å…·æœ‰é•¿æœŸæ€§ï¼›2ï¼‰ä¼ ç»Ÿçš„è§„åˆ’å™¨åœ¨æ’åºå¤æ‚çš„è®¡åˆ’ä¸­çš„å¹¶è¡Œå­ç›®æ ‡æ—¶ï¼Œæ²¡æœ‰è€ƒè™‘å½“å‰æ™ºèƒ½ä½“å®Œæˆç»™å®šå­ä»»åŠ¡çš„éš¾æ˜“ç¨‹åº¦ï¼Œå¯¼è‡´ç”Ÿæˆçš„è®¡åˆ’å¯èƒ½æ•ˆç‡ä½ä¸‹ç”šè‡³ä¸å¯è¡Œã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†â€œæè¿°ã€è§£é‡Šã€è§„åˆ’å’Œé€‰æ‹©â€ï¼ˆDEPSï¼‰çš„äº¤äº’å¼è§„åˆ’æ–¹æ³•ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæè¿°ã€è§£é‡Šå’Œè§„åˆ’<br>DEPS é€šè¿‡é›†æˆè®¡åˆ’æ‰§è¡Œè¿‡ç¨‹çš„æè¿°å’Œæä¾›è‡ªæˆ‘è§£é‡Šçš„åé¦ˆï¼Œæ›´å¥½åœ°çº æ­£åˆå§‹ LLM ç”Ÿæˆçš„è®¡åˆ’ä¸­çš„é”™è¯¯ã€‚å½“é‡åˆ°å¤±è´¥æ—¶ï¼Œæè¿°å™¨ä¼šæ€»ç»“å½“å‰æƒ…å†µå¹¶å‘é€ç»™ LLMï¼ŒLLM ä½œä¸ºè§£é‡Šå™¨å®šä½é”™è¯¯ï¼Œç„¶åæ ¹æ®æè¿°å™¨å’Œè§£é‡Šå™¨çš„ä¿¡æ¯æ›´æ–°è®¡åˆ’ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç›®æ ‡é€‰æ‹©å™¨<br>DEPS åŒ…å«ä¸€ä¸ªå¯è®­ç»ƒçš„ç›®æ ‡é€‰æ‹©å™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—æ ¹æ®å®Œæˆæ¯ä¸ªå¹¶è¡Œå€™é€‰å­ç›®æ ‡çš„ä¼°è®¡æ­¥éª¤å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œä»è€Œç»†åŒ–åˆå§‹è®¡åˆ’ã€‚é€‰æ‹©å™¨ä½¿ç”¨é¢„æµ‹å‰©ä½™æ—¶é—´æ­¥æ•°æ¥å®Œæˆæ¯ä¸ªç›®æ ‡ï¼Œå¹¶æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©æœ€æ¥è¿‘çš„ç›®æ ‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒDEPS åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼ˆå¦‚ Minecraftï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œèƒ½å¤Ÿç¨³å¥åœ°å®Œæˆ 70 å¤šä¸ªä»»åŠ¡ï¼Œå¹¶ä¸”æ•´ä½“æ€§èƒ½å‡ ä¹ç¿»å€ã€‚æ­¤å¤–ï¼ŒDEPS åœ¨éå¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼ˆå¦‚ ALFWorld å’Œæ¡Œé¢æ“ä½œï¼‰ä¸­ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>DEPS çš„äº¤äº’å¼è§„åˆ’æ–¹æ³•ä¸ºå¼€æ”¾ä¸–ç•Œå¤šä»»åŠ¡æ™ºèƒ½ä½“çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡é›†æˆæè¿°ã€è§£é‡Šå’Œè§„åˆ’ï¼Œä»¥åŠä½¿ç”¨ç›®æ ‡é€‰æ‹©å™¨ï¼ŒDEPS èƒ½å¤Ÿç”Ÿæˆæ›´å¯é å’Œé«˜æ•ˆçš„è®¡åˆ’ï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„ä»»åŠ¡å®Œæˆèƒ½åŠ›ã€‚</td>
    </tr>
    <tr>
      <th>15</th>
      <td>LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation</td>
      <td>To assist with everyday human activities, robots must solve complex<br>long-horizon tasks and generalize to new settings. Recent deep reinforcement<br>learning (RL) methods show promise in fully autonomous learning, but they<br>struggle to reach long-term goals in large environments. On the other hand,<br>Task and Motion Planning (TAMP) approaches excel at solving and generalizing<br>across long-horizon tasks, thanks to their powerful state and action<br>abstractions. But they assume predefined skill sets, which limits their<br>real-world applications. In this work, we combine the benefits of these two<br>paradigms and propose an integrated task planning and skill learning framework<br>named LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages the<br>symbolic interface of a task planner to guide RL-based skill learning and<br>creates abstract state space to enable skill reuse. More importantly, LEAGUE<br>learns manipulation skills in-situ of the task planning system, continuously<br>growing its capability and the set of tasks that it can solve. We evaluate<br>LEAGUE on four challenging simulated task domains and show that LEAGUE<br>outperforms baselines by large margins. We also show that the learned skills<br>can be reused to accelerate learning in new tasks domains and transfer to a<br>physical robot platform.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | LEAGUEï¼šåŸºäºå¼•å¯¼çš„æŠ€èƒ½å­¦ä¹ å’ŒæŠ½è±¡ï¼ŒåŠ©åŠ›æœºå™¨äººè§£å†³é•¿æœŸæ“ä½œä»»åŠ¡<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæœºå™¨äººå·²ç»é€æ¸èµ°è¿›æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œå¹¶åœ¨å„ç§åœºæ™¯ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œè¦è®©æœºå™¨äººçœŸæ­£å®ç°è‡ªä¸»å­¦ä¹ å’Œæ“ä½œï¼Œä»ç„¶é¢ä¸´ç€è®¸å¤šæŒ‘æˆ˜ã€‚å…¶ä¸­ï¼Œé•¿æœŸæ“ä½œä»»åŠ¡ï¼ˆlong-horizon tasksï¼‰çš„è§£å†³å’Œæ³›åŒ–èƒ½åŠ›æ˜¯æœºå™¨äººé¢†åŸŸçš„ä¸€å¤§éš¾é¢˜ã€‚ç°æœ‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ–¹æ³•åœ¨è‡ªä¸»å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤§å‹ç¯å¢ƒä¸­å®ç°é•¿æœŸç›®æ ‡ä»ç„¶å­˜åœ¨å›°éš¾ã€‚å¦ä¸€æ–¹é¢ï¼Œä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’ï¼ˆTAMPï¼‰æ–¹æ³•æ“…é•¿è§£å†³å’Œæ³›åŒ–é•¿æœŸä»»åŠ¡ï¼Œä½†ç”±äºå…¶ä¾èµ–äºé¢„å®šä¹‰çš„æŠ€èƒ½é›†ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†å…‹æœä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†LEAGUEï¼ˆLearning and Abstraction with Guidanceï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†DRLå’ŒTAMPçš„ä¼˜åŠ¿ï¼Œå®ç°äº†é•¿æœŸæ“ä½œä»»åŠ¡çš„è§£å†³å’Œæ³›åŒ–ã€‚LEAGUEçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨ä»»åŠ¡è§„åˆ’å™¨çš„ç¬¦å·æ¥å£æŒ‡å¯¼åŸºäºå¼ºåŒ–å­¦ä¹ çš„æŠ€èƒ½å­¦ä¹ ï¼Œå¹¶åˆ›å»ºæŠ½è±¡çŠ¶æ€ç©ºé—´ä»¥å®ç°æŠ€èƒ½å¤ç”¨ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨ä»»åŠ¡è§„åˆ’ç³»ç»Ÿä¸­å­¦ä¹ æ“ä½œæŠ€èƒ½ï¼Œä¸æ–­æ‰©å±•å…¶èƒ½åŠ›å’Œå¯è§£å†³çš„ä»»åŠ¡é›†ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨¡æ‹Ÿä»»åŠ¡é¢†åŸŸå¯¹LEAGUEè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜LEAGUEåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†å­¦ä¹ åˆ°çš„æŠ€èƒ½å¯ä»¥å¤ç”¨äºåŠ é€Ÿæ–°ä»»åŠ¡é¢†åŸŸçš„å­¦ä¹ ï¼Œå¹¶è¿ç§»åˆ°ç‰©ç†æœºå™¨äººå¹³å°ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>LEAGUEæ¡†æ¶ä¸ºæœºå™¨äººè§£å†³é•¿æœŸæ“ä½œä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ã€‚æ­¤å¤–ï¼ŒLEAGUEæ¡†æ¶ä¸­çš„æŠ€èƒ½å­¦ä¹ å’ŒæŠ½è±¡æ–¹æ³•ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–å¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›å€Ÿé‰´ã€‚</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Learning from Visual Observation via Offline Pretrained State-to-Go Transformer</td>
      <td>Learning from visual observation (LfVO), aiming at recovering policies from<br>only visual observation data, is promising yet a challenging problem. Existing<br>LfVO approaches either only adopt inefficient online learning schemes or<br>require additional task-specific information like goal states, making them not<br>suited for open-ended tasks. To address these issues, we propose a two-stage<br>framework for learning from visual observation. In the first stage, we<br>introduce and pretrain State-to-Go (STG) Transformer offline to predict and<br>differentiate latent transitions of demonstrations. Subsequently, in the second<br>stage, the STG Transformer provides intrinsic rewards for downstream<br>reinforcement learning tasks where an agent learns merely from intrinsic<br>rewards. Empirical results on Atari and Minecraft show that our proposed method<br>outperforms baselines and in some tasks even achieves performance comparable to<br>the policy learned from environmental rewards. These results shed light on the<br>potential of utilizing video-only data to solve difficult visual reinforcement<br>learning tasks rather than relying on complete offline datasets containing<br>states, actions, and rewards. The project's website and code can be found at<br>https://sites.google.com/view/stgtransformer.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨ç¦»çº¿é¢„è®­ç»ƒçš„State-to-Go Transformerä»è§†è§‰è§‚å¯Ÿä¸­å­¦ä¹ <br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»é›¶å¼€å§‹å­¦ä¹ é¢ä¸´ç€æ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œæ¢ç´¢å›°éš¾çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¨€ç–å¥–åŠ±çš„ç¯å¢ƒä¸­ã€‚è¿™å¯¼è‡´äº†æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰çš„å‘å±•ï¼Œå…¶ä¸­æ™ºèƒ½ä½“é€šè¿‡æ¨¡ä»¿ä¸“å®¶æ¼”ç¤ºæ¥å­¦ä¹ ç­–ç•¥ï¼Œè€Œä¸æ˜¯é€šè¿‡è¯•é”™è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè·å–æ¼”ç¤ºåŠ¨ä½œå¯èƒ½æ—¢æ˜‚è´µåˆä¸å¯è¡Œï¼Œä¾‹å¦‚ï¼Œä»å¤§é‡å¯ç”¨çš„è§†é¢‘ä¸­è·å–ã€‚å› æ­¤ï¼Œä»è§‚å¯Ÿä¸­å­¦ä¹ ï¼ˆLfOï¼‰çš„ç ”ç©¶å…´èµ·ï¼Œå®ƒåˆ©ç”¨å…³äºæ™ºèƒ½ä½“è¡Œä¸ºå’ŒçŠ¶æ€è½¬æ¢çš„è§‚å¯Ÿæ•°æ®æ¥å­¦ä¹ ç­–ç•¥ã€‚LfOçš„æŒ‘æˆ˜åœ¨äºä»åŸå§‹è§†è§‰è§‚å¯Ÿä¸­æå–æœ‰ç”¨ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥è®­ç»ƒç­–ç•¥ï¼Œå› ä¸ºç¼ºä¹æ˜ç¡®çš„åŠ¨ä½œä¿¡æ¯ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œåˆ©ç”¨ä¸“å®¶æ¼”ç¤ºçš„è§†è§‰è§‚å¯Ÿæ¥æŒ‡å¯¼åœ¨çº¿å¼ºåŒ–å­¦ä¹ ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œå¼•å…¥å¹¶ç¦»çº¿é¢„è®­ç»ƒState-to-Goï¼ˆSTGï¼‰Transformerï¼Œä»¥é¢„æµ‹å’ŒåŒºåˆ†æ¼”ç¤ºçš„æ½œåœ¨è½¬æ¢ã€‚åŒæ—¶ï¼Œå­¦ä¹ æ—¶é—´å¯¹é½å’Œå¯é¢„æµ‹çš„è§†è§‰è¡¨ç¤ºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒSTG Transformerä¸ºä¸‹æ¸¸å¼ºåŒ–å­¦ä¹ ä»»åŠ¡æä¾›å†…åœ¨å¥–åŠ±ï¼Œå…¶ä¸­æ™ºèƒ½ä½“ä»…ä»å†…åœ¨å¥–åŠ±ä¸­å­¦ä¹ ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒæ—¶å­¦ä¹ åˆ¤åˆ«å™¨å’Œæ—¶é—´è·ç¦»å›å½’å™¨ï¼Œä»¥é¢„æµ‹æ½œåœ¨è½¬æ¢çš„åŒæ—¶å­¦ä¹ æ—¶é—´å¯¹é½åµŒå…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè”åˆå­¦ä¹ çš„è¡¨ç¤ºåœ¨ä¸‹æ¸¸RLä»»åŠ¡ä¸­è¡¨ç°å‡ºå¢å¼ºçš„æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Atariå’ŒMinecraftä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œæ•´ä½“æ€§èƒ½æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨æŸäº›æ¸¸æˆä¸­ç”šè‡³è¾¾åˆ°äº†ä¸ä»ç¯å¢ƒå¥–åŠ±ä¸­å­¦ä¹ çš„ç­–ç•¥ç›¸å½“çš„æ€§èƒ½ã€‚è¿™çªå‡ºäº†åˆ©ç”¨ç¦»çº¿è§†é¢‘æ•°æ®æ¥è§£å†³å›°éš¾çš„è§†è§‰å¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„æ½œåŠ›ï¼Œè€Œä¸æ˜¯ä¾èµ–äºåŒ…å«çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±çš„å®Œæ•´ç¦»çº¿æ•°æ®é›†ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ä¸¤é˜¶æ®µæ¡†æ¶å’ŒSTG Transformeræ¨¡å‹ä¸ºä»è§†è§‰è§‚å¯Ÿä¸­å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æå‡ºçš„åˆ¤åˆ«å™¨å’Œæ—¶é—´è·ç¦»å›å½’å™¨å¯¹äºå­¦ä¹ æ—¶é—´å¯¹é½åµŒå…¥å’Œé¢„æµ‹æ½œåœ¨è½¬æ¢å…·æœ‰é‡è¦ä½œç”¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„å®éªŒç»“æœä¹Ÿè¡¨æ˜ï¼Œå†…åœ¨å¥–åŠ±å¯¹äºæŒ‡å¯¼åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä»»åŠ¡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</td>
    </tr>
    <tr>
      <th>17</th>
      <td>LLaMA Rider: Spurring Large Language Models to Explore the Open World</td>
      <td>Recently, various studies have leveraged Large Language Models (LLMs) to help<br>decision-making and planning in environments, and try to align the LLMs'<br>knowledge with the world conditions. Nonetheless, the capacity of LLMs to<br>continuously acquire environmental knowledge and adapt in an open world remains<br>uncertain. In this paper, we propose an approach to spur LLMs to explore the<br>open world, gather experiences, and learn to improve their task-solving<br>capabilities. In this approach, a multi-round feedback-revision mechanism is<br>utilized to encourage LLMs to actively select appropriate revision actions<br>guided by feedback information from the environment. This facilitates<br>exploration and enhances the model's performance. Besides, we integrate<br>sub-task relabeling to assist LLMs in maintaining consistency in sub-task<br>planning and help the model learn the combinatorial nature between tasks,<br>enabling it to complete a wider range of tasks through training based on the<br>acquired exploration experiences. By evaluation in Minecraft, an open-ended<br>sandbox world, we demonstrate that our approach LLaMA-Rider enhances the<br>efficiency of the LLM in exploring the environment, and effectively improves<br>the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k<br>instances of collected data, showing minimal training costs compared to the<br>baseline using reinforcement learning.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | LLaMA Riderï¼šæ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹æ¢ç´¢å¼€æ”¾ä¸–ç•Œ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è®¸å¤šç ”ç©¶å¼€å§‹åˆ©ç”¨LLMsçš„èƒ½åŠ›æ¥å¸®åŠ©æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–ï¼Œå¹¶å‘ç°LLMså…·æœ‰ä¸€å®šçš„è§„åˆ’å’Œå®Œæˆä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMsçš„çŸ¥è¯†æ¥æºäºé¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„è¯­è¨€è¯­æ–™åº“ï¼Œå¯èƒ½ä¸ç‰¹å®šç¯å¢ƒå­˜åœ¨å·®å¼‚ã€‚ä¸ºäº†å°†LLMsä¸å®é™…ç¯å¢ƒç›¸ç»“åˆï¼Œä¸€äº›ç ”ç©¶é€šè¿‡æç¤ºå·¥ç¨‹è®¾è®¡ç‰¹å®šæœºåˆ¶ï¼Œä¸ºLLMsæä¾›ç¯å¢ƒä¿¡æ¯ã€‚ç„¶è€Œï¼ŒLLMsåœ¨ç¯å¢ƒä¸­å¹¶ä¸ä¼šæ”¹è¿›æˆ–è·å–æ–°çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå¯¹äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦æ›´å¤æ‚çš„æœºåˆ¶å’Œæç¤ºï¼Œè¿™ä¼šå¯¼è‡´LLMsç”Ÿæˆæˆæœ¬é«˜ï¼Œå¹¶ä¸”ä¾èµ–äºåƒGPT-4è¿™æ ·å…·æœ‰è¶³å¤ŸçŸ¥è¯†çš„å¼ºå¤§æ¨¡å‹ã€‚è¿˜æœ‰ä¸€äº›ç ”ç©¶é€šè¿‡å¾®è°ƒæ¥å°†LLMsä¸å®é™…ç¯å¢ƒç›¸ç»“åˆï¼Œä½†è¿™é€šå¸¸éœ€è¦ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæ•°æ®é›†ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¹Ÿè¢«ç ”ç©¶ï¼Œä½†è¿™äº›æ–¹æ³•å°†LLMsè®­ç»ƒä¸ºç‰¹å®šä»»åŠ¡çš„ç­–ç•¥ï¼Œå¹¶ä¸”æˆ‘ä»¬å‘ç°RLæ–¹æ³•éš¾ä»¥æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹æˆ–æ›´å¤æ‚çš„ä»»åŠ¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLLaMA-Riderçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡LLMsåœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„æ¢ç´¢æ¥å¢å¼ºå…¶èƒ½åŠ›ã€‚LLaMA-Rideræ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬æ¢ç´¢é˜¶æ®µå’Œå­¦ä¹ é˜¶æ®µã€‚<br><br>### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¢ç´¢é˜¶æ®µ<br>åœ¨æ¢ç´¢é˜¶æ®µï¼ŒLLaMA-Rideråˆ©ç”¨åé¦ˆ-ä¿®æ­£æœºåˆ¶æ¥é¼“åŠ±LLMsä¸»åŠ¨é€‰æ‹©é€‚å½“çš„ä¿®æ­£åŠ¨ä½œï¼Œä»¥é€‚åº”ç¯å¢ƒã€‚LLMsåœ¨ç¯å¢ƒä¸­è¿›è¡Œæ¢ç´¢ï¼Œæ”¶é›†ç»éªŒï¼Œå¹¶é€šè¿‡åé¦ˆä¿¡æ¯æ¥æ”¹è¿›å…¶å†³ç­–ã€‚æ­¤å¤–ï¼ŒLLaMA-Riderè¿˜ä½¿ç”¨å­ä»»åŠ¡é‡æ ‡è®°æ¥å¸®åŠ©LLMsä¿æŒå­ä»»åŠ¡è§„åˆ’çš„è¿è´¯æ€§ï¼Œå¹¶å­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„ç»„åˆæ€§è´¨ã€‚<br><br>### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå­¦ä¹ é˜¶æ®µ<br>åœ¨å­¦ä¹ é˜¶æ®µï¼ŒLLaMA-Riderå°†æ”¶é›†åˆ°çš„ç»éªŒå¤„ç†æˆæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è®­ç»ƒLLMsã€‚é™¤äº†ä»æˆåŠŸä»»åŠ¡ä¸­è·å¾—çš„ç»éªŒå¤–ï¼ŒLLaMA-Riderè¿˜æ”¶é›†éƒ¨åˆ†å®Œæˆçš„å­ä»»åŠ¡çš„ç»éªŒï¼Œå› ä¸ºæœ‰äº›ä»»åŠ¡åœ¨æ¢ç´¢é˜¶æ®µå¾ˆéš¾å®Œæˆã€‚å¼€æ”¾ç¯å¢ƒä¸­çš„è®¸å¤šä»»åŠ¡é€šå¸¸å…·æœ‰ç»„åˆæ€§ï¼Œè¿™æ„å‘³ç€è¿‡å»ä»»åŠ¡çš„ç»éªŒå¯ä»¥ç»å¸¸å¸®åŠ©å®Œæˆå…¶ä»–ä»»åŠ¡ã€‚LLaMA-Riderä½¿ç”¨å­ä»»åŠ¡é‡æ ‡è®°æ¥æé«˜æ•°æ®åˆ©ç”¨ç‡ï¼Œå¹¶å¸®åŠ©LLMså­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„ç»„åˆæ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨Minecraftæ¨¡æ‹Ÿå™¨MineDojoä¸Šè¯„ä¼°äº†LLaMA-Rideræ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaMA-Riderèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢ç¯å¢ƒï¼Œå¹¶é€šè¿‡å¾®è°ƒä»…ä½¿ç”¨1.3kä¸ªæ”¶é›†åˆ°çš„æ•°æ®å®ä¾‹æ¥æé«˜LLMså®Œæˆä»»åŠ¡çš„èƒ½åŠ›ï¼Œä¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ç›¸æ¯”ï¼Œè®­ç»ƒæˆæœ¬æ›´ä½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>LLaMA-Rideræ–¹æ³•ä¸ºLLMsåœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„æ¢ç´¢å’Œå­¦ä¹ æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚å…¶åé¦ˆ-ä¿®æ­£æœºåˆ¶å’Œå­ä»»åŠ¡é‡æ ‡è®°æŠ€æœ¯å¯ä»¥å¸®åŠ©LLMsæ›´å¥½åœ°é€‚åº”ç¯å¢ƒï¼Œå¹¶æé«˜å…¶å®Œæˆä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLLaMA-Rideræ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¼€æ”¾ç¯å¢ƒï¼Œå¹¶å…·æœ‰ç»ˆèº«æ¢ç´¢å’Œå­¦ä¹ çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>18</th>
      <td>JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models</td>
      <td>Achieving human-like planning and control with multimodal observations in an<br>open world is a key milestone for more functional generalist agents. Existing<br>approaches can handle certain long-horizon tasks in an open world. However,<br>they still struggle when the number of open-world tasks could potentially be<br>infinite and lack the capability to progressively enhance task completion as<br>game time progresses. We introduce JARVIS-1, an open-world agent that can<br>perceive multimodal input (visual observations and human instructions),<br>generate sophisticated plans, and perform embodied control, all within the<br>popular yet challenging open-world Minecraft universe. Specifically, we develop<br>JARVIS-1 on top of pre-trained multimodal language models, which map visual<br>observations and textual instructions to plans. The plans will be ultimately<br>dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a<br>multimodal memory, which facilitates planning using both pre-trained knowledge<br>and its actual game survival experiences. JARVIS-1 is the existing most general<br>agent in Minecraft, capable of completing over 200 different tasks using<br>control and observation space similar to humans. These tasks range from<br>short-horizon tasks, e.g., "chopping trees" to long-horizon tasks, e.g.,<br>"obtaining a diamond pickaxe". JARVIS-1 performs exceptionally well in<br>short-horizon tasks, achieving nearly perfect performance. In the classic<br>long-term task of \( \texttt{ObtainDiamondPickaxe} \), JARVIS-1 surpasses the<br>reliability of current state-of-the-art agents by 5 times and can successfully<br>complete longer-horizon and more challenging tasks. The project page is<br>available at https://craftjarvis.org/JARVIS-1</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | JARVIS-1ï¼šå¼€æ”¾ä¸–ç•Œå¤šä»»åŠ¡æ™ºèƒ½ä½“ï¼Œè¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½çš„å…³é”®ä¸€æ­¥<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¼€æ”¾ä¸–ç•Œä¸­ï¼Œæ„å»ºèƒ½å¤Ÿå®Œæˆå„ç§ä»»åŠ¡çš„æ™ºèƒ½ä½“ä¸€ç›´æ˜¯é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰é¢†åŸŸçš„é‡è¦ç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨å¤„ç†å¼€æ”¾ä¸–ç•Œä¸­æ— é™æ•°é‡çš„ä»»åŠ¡æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä»»åŠ¡å®Œæˆèƒ½åŠ›éšæ¸¸æˆæ—¶é—´æ¨ç§»è€Œé€æ­¥æå‡æ–¹é¢ã€‚æœ¬æ–‡æå‡ºçš„JARVIS-1æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ƒæ˜¯ä¸€ä¸ªèƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­æ„ŸçŸ¥å¤šæ¨¡æ€è¾“å…¥ï¼ˆè§†è§‰è§‚å¯Ÿå’Œäººç±»æŒ‡ä»¤ï¼‰ã€ç”Ÿæˆå¤æ‚è®¡åˆ’å¹¶æ‰§è¡Œå…·èº«æ§åˆ¶çš„æ™ºèƒ½ä½“ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰<br>JARVIS-1åŸºäºé¢„è®­ç»ƒçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†è§†è§‰è§‚å¯Ÿå’Œæ–‡æœ¬æŒ‡ä»¤æ˜ å°„åˆ°è®¡åˆ’ä¸­ã€‚è¿™ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ä»»åŠ¡ã€æƒ…å†µå’Œç¯å¢ƒåé¦ˆï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®çš„è®¡åˆ’ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ¨¡æ€è®°å¿†<br>JARVIS-1é…å¤‡äº†å¤šæ¨¡æ€è®°å¿†ï¼Œå­˜å‚¨äº†è¿‡å»çš„æˆåŠŸè§„åˆ’ç»éªŒå’Œåœºæ™¯ã€‚é€šè¿‡æ£€ç´¢ç›¸å…³è®°å¿†æ¡ç›®ï¼Œæ™ºèƒ½ä½“çš„è§„åˆ’èƒ½åŠ›å¯ä»¥åœ¨ä¸Šä¸‹æ–‡ä¸­å¾—åˆ°åŠ å¼ºï¼Œä»è€Œæé«˜è§„åˆ’çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè‡ªæˆ‘æŒ‡å¯¼å’Œè‡ªæˆ‘æ”¹è¿›<br>JARVIS-1èƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘æŒ‡å¯¼æœºåˆ¶è‡ªä¸»ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶é€šè¿‡æ¢ç´¢ä¸–ç•Œæ¥æ”¶é›†ç»éªŒã€‚è¿™äº›ç»éªŒè¢«å­˜å‚¨åœ¨å¤šæ¨¡æ€è®°å¿†ä¸­ï¼Œå¸®åŠ©æ™ºèƒ½ä½“åœ¨åç»­ä»»åŠ¡ä¸­æ›´å¥½åœ°è¿›è¡Œæ¨ç†å’Œè§„åˆ’ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>JARVIS-1åœ¨Minecraftä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå®Œæˆè¶…è¿‡200ä¸ªä¸åŒçš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬çŸ­æœŸä»»åŠ¡ï¼ˆå¦‚ç æ ‘ï¼‰å’Œé•¿æœŸä»»åŠ¡ï¼ˆå¦‚è·å¾—é’»çŸ³é•ï¼‰ã€‚åœ¨ç»å…¸çš„é•¿ä»»åŠ¡â€œè·å¾—é’»çŸ³é•â€ä¸­ï¼ŒJARVIS-1çš„å¯é æ€§æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ™ºèƒ½ä½“é«˜å‡º5å€ï¼Œèƒ½å¤ŸæˆåŠŸå®Œæˆæ›´é•¿æœŸå’Œæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>JARVIS-1çš„è®¾è®¡å’Œå®ç°ä¸ºå¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†é‡è¦çš„å¯ç¤ºã€‚é€šè¿‡ç»“åˆå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€è®°å¿†å’Œè‡ªæˆ‘æ”¹è¿›æœºåˆ¶ï¼ŒJARVIS-1å±•ç¤ºäº†åœ¨å¼€æ”¾ä¸–ç•Œä¸­å®ç°é€šç”¨äººå·¥æ™ºèƒ½çš„æ½œåŠ›ã€‚è¿™äº›æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–å¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼Œæ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</td>
      <td>Recent studies have presented compelling evidence that large language models<br>(LLMs) can equip embodied agents with the self-driven capability to interact<br>with the world, which marks an initial step toward versatile robotics. However,<br>these efforts tend to overlook the visual richness of open worlds, rendering<br>the entire interactive process akin to "a blindfolded text-based game."<br>Consequently, LLM-based agents frequently encounter challenges in intuitively<br>comprehending their surroundings and producing responses that are easy to<br>understand. In this paper, we propose Steve-Eye, an end-to-end trained large<br>multimodal model designed to address this limitation. Steve-Eye integrates the<br>LLM with a visual encoder which enables it to process visual-text inputs and<br>generate multimodal feedback. In addition, we use a semi-automatic strategy to<br>collect an extensive dataset comprising 850K open-world instruction pairs,<br>empowering our model to encompass three essential functions for an agent:<br>multimodal perception, foundational knowledge base, and skill prediction and<br>planning. Lastly, we develop three open-world evaluation benchmarks, then carry<br>out extensive experiments from a wide range of perspectives to validate our<br>model's capability to strategically act and plan. Codes and datasets will be<br>released.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Steve-Eyeï¼šä¸ºåŸºäºLLMçš„å…·èº«æ™ºèƒ½ä½“èµ‹äºˆå¼€æ”¾ä¸–ç•Œçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èµ‹äºˆå…·èº«æ™ºèƒ½ä½“ä¸ä¸–ç•Œäº’åŠ¨çš„èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™æ ‡å¿—ç€é€šç”¨æœºå™¨äººæŠ€æœ¯è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶å¾€å¾€å¿½ç•¥äº†å¼€æ”¾ä¸–ç•Œçš„è§†è§‰ä¸°å¯Œæ€§ï¼Œå¯¼è‡´æ•´ä¸ªäº¤äº’è¿‡ç¨‹ç±»ä¼¼äºâ€œä¸€ä¸ªè’™ç€çœ¼ç›çš„åŸºäºæ–‡æœ¬çš„æ¸¸æˆâ€ã€‚å› æ­¤ï¼ŒåŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨ç›´è§‚åœ°ç†è§£å‘¨å›´ç¯å¢ƒå’Œç”Ÿæˆæ˜“äºç†è§£çš„å“åº”æ–¹é¢ç»å¸¸é‡åˆ°æŒ‘æˆ˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Steve-Eyeï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨èµ‹äºˆåŸºäºLLMçš„å…·èº«æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿›è¡Œè§†è§‰æ„ŸçŸ¥çš„èƒ½åŠ›ã€‚Steve-Eyeå°†LLMä¸è§†è§‰ç¼–ç å™¨ç›¸ç»“åˆï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è§†è§‰-æ–‡æœ¬è¾“å…¥å¹¶ç”Ÿæˆå¤šæ¨¡æ€åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨åŠè‡ªåŠ¨ç­–ç•¥æ”¶é›†äº†ä¸€ä¸ªåŒ…å«850Kå¼€æ”¾ä¸–ç•ŒæŒ‡ä»¤å¯¹çš„å¹¿æ³›æ•°æ®é›†ï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæ¶µç›–æ™ºèƒ½ä½“çš„ä¸‰ä¸ªåŸºæœ¬åŠŸèƒ½ï¼šå¤šæ¨¡æ€æ„ŸçŸ¥ã€åŸºç¡€çŸ¥è¯†åº“å’ŒæŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸‰ä¸ªå¼€æ”¾ä¸–ç•Œè¯„ä¼°åŸºå‡†ï¼Œç„¶åä»å¹¿æ³›çš„è§†è§’è¿›è¡Œå¤§é‡å®éªŒï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„æ¨¡å‹åœ¨æˆ˜ç•¥è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒSteve-Eyeåœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºåŸºäºLLMçš„æ™ºèƒ½ä½“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹ä¸‰ä¸ªåŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼š<br>1. ç¯å¢ƒè§†è§‰æè¿°ï¼ˆENV-VCï¼‰ï¼šè¯„ä¼°æ™ºèƒ½ä½“æ„ŸçŸ¥å’Œæè¿°å…¶å‘¨å›´ç¯å¢ƒçš„èƒ½åŠ›ã€‚<br>2. åŸºç¡€çŸ¥è¯†é—®ç­”ï¼ˆFK-QAï¼‰ï¼šè¯„ä¼°æ™ºèƒ½ä½“æŒæ¡å¯¹å†³ç­–è‡³å…³é‡è¦çš„åŸºæœ¬çŸ¥è¯†çš„ç†Ÿç»ƒç¨‹åº¦ã€‚<br>3. æŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’ï¼ˆSPPï¼‰ï¼šé‡åŒ–æ™ºèƒ½ä½“åœ¨æˆ˜ç•¥è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Steve-Eyeçš„ç ”ç©¶æˆæœä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­æœ‰æ•ˆäº’åŠ¨çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„å¯ç¤ºã€‚å…¶å¤šæ¨¡æ€æ„ŸçŸ¥ã€åŸºç¡€çŸ¥è¯†åº“å’ŒæŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’åŠŸèƒ½ä¸ºæ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œè‡ªä¸»è¡ŒåŠ¨å’Œè§„åˆ’æä¾›äº†å¼ºå¤§çš„æ”¯æŒã€‚æ­¤å¤–ï¼ŒSteve-Eyeçš„å¼€æ”¾ä¸–ç•Œè¯„ä¼°åŸºå‡†ä¸ºè¯„ä¼°å…·èº«æ™ºèƒ½ä½“çš„æ€§èƒ½æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>20</th>
      <td>MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception</td>
      <td>It is a long-lasting goal to design an embodied system that can solve<br>long-horizon open-world tasks in human-like ways. However, existing approaches<br>usually struggle with compound difficulties caused by the logic-aware<br>decomposition and context-aware execution of these tasks. To this end, we<br>introduce MP5, an open-ended multimodal embodied system built upon the<br>challenging Minecraft simulator, which can decompose feasible sub-objectives,<br>design sophisticated situation-aware plans, and perform embodied action<br>control, with frequent communication with a goal-conditioned active perception<br>scheme. Specifically, MP5 is developed on top of recent advances in Multimodal<br>Large Language Models (MLLMs), and the system is modulated into functional<br>modules that can be scheduled and collaborated to ultimately solve pre-defined<br>context- and process-dependent tasks. Extensive experiments prove that MP5 can<br>achieve a 22% success rate on difficult process-dependent tasks and a 91%<br>success rate on tasks that heavily depend on the context. Moreover, MP5<br>exhibits a remarkable ability to address many open-ended tasks that are<br>entirely novel.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MP5ï¼šåŸºäºä¸»åŠ¨æ„ŸçŸ¥çš„å¤šæ¨¡æ€å¼€æ”¾å¼å…·èº«ç³»ç»Ÿ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œè®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿä»¥äººç±»æ–¹å¼è§£å†³å¼€æ”¾ä¸–ç•Œé•¿æ—¶ä»»åŠ¡çš„å…·èº«ç³»ç»Ÿä¸€ç›´æ˜¯é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸éš¾ä»¥åº”å¯¹è¿™äº›ä»»åŠ¡ä¸­é€»è¾‘æ„ŸçŸ¥åˆ†è§£å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ‰§è¡Œæ‰€å¸¦æ¥çš„å¤åˆå›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MP5ï¼Œä¸€ä¸ªåŸºäºMinecraftæ¨¡æ‹Ÿå™¨çš„å¼€æ”¾å¼å¤šæ¨¡æ€å…·èº«ç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿåˆ†è§£å¯è¡Œçš„å­ç›®æ ‡ï¼Œè®¾è®¡å¤æ‚çš„æƒ…å¢ƒæ„ŸçŸ¥è®¡åˆ’ï¼Œå¹¶æ‰§è¡Œå…·èº«åŠ¨ä½œæ§åˆ¶ï¼ŒåŒæ—¶ä¸ç›®æ ‡æ¡ä»¶ä¸‹çš„ä¸»åŠ¨æ„ŸçŸ¥æ–¹æ¡ˆè¿›è¡Œé¢‘ç¹é€šä¿¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMP5åŸºäºæœ€æ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ„å»ºï¼Œå¹¶å°†ç³»ç»Ÿåˆ†è§£ä¸ºå¯è°ƒåº¦å’Œåä½œçš„åŠŸèƒ½æ¨¡å—ï¼Œä»¥è§£å†³é¢„å®šä¹‰çš„ä¸Šä¸‹æ–‡å’Œè¿‡ç¨‹ä¾èµ–ä»»åŠ¡ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMP5åŒ…æ‹¬ä¸€ä¸ªä¸»åŠ¨æ„ŸçŸ¥æ–¹æ¡ˆï¼Œé€šè¿‡æ„ŸçŸ¥å™¨ä¸å·¡é€»å™¨ä¹‹é—´çš„å¤šè½®äº¤äº’ï¼Œä¸»åŠ¨æ„ŸçŸ¥è§‚å¯Ÿåˆ°çš„å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥è§£å†³ä¸Šä¸‹æ–‡ä¾èµ–ä»»åŠ¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>MP5åœ¨å›°éš¾çš„è¿‡ç¨‹ä¾èµ–ä»»åŠ¡ä¸Šå®ç°äº†22%çš„æˆåŠŸç‡ï¼Œåœ¨é«˜åº¦ä¾èµ–ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ä¸Šå®ç°äº†91%çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒMP5è¡¨ç°å‡ºè§£å†³è®¸å¤šå®Œå…¨æ–°é¢–çš„å¼€æ”¾å¼ä»»åŠ¡çš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MP5çš„è®¾è®¡å’Œå®ç°ä¸ºè§£å†³å¼€æ”¾ä¸–ç•Œé•¿æ—¶ä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…¶ä¸»åŠ¨æ„ŸçŸ¥æ–¹æ¡ˆå’Œæ¨¡å—åŒ–è®¾è®¡å¯¹äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´çµæ´»çš„å…·èº«ç³»ç»Ÿå…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚</td>
    </tr>
    <tr>
      <th>21</th>
      <td>OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents</td>
      <td>This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model<br>for open-world instruction-following agents in Minecraft. Compared to prior<br>works that either emit textual goals to separate controllers or produce the<br>control command directly, OmniJARVIS seeks a different path to ensure both<br>strong reasoning and efficient decision-making capabilities via unified<br>tokenization of multimodal interaction data. First, we introduce a<br>self-supervised approach to learn a behavior encoder that produces discretized<br>tokens for behavior trajectories \( \tau = \{o_0, a_0, \dots\} \) and an imitation<br>learning policy decoder conditioned on these tokens. These additional behavior<br>tokens will be augmented to the vocabulary of pretrained Multimodal Language<br>Models. With this encoder, we then pack long-term multimodal interactions<br>involving task instructions, memories, thoughts, observations, textual<br>responses, behavior trajectories, etc into unified token sequences and model<br>them with autoregressive transformers. Thanks to the semantically meaningful<br>behavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing<br>chain-of-thoughts), plan, answer questions, and act (by producing behavior<br>tokens for the imitation learning policy decoder). OmniJARVIS demonstrates<br>excellent performances on a comprehensive collection of atomic, programmatic,<br>and open-ended tasks in open-world Minecraft. Our analysis further unveils the<br>crucial design principles in interaction data formation, unified tokenization,<br>and its scaling potentials. The dataset, models, and code will be released at<br>https://craftjarvis.org/OmniJARVIS.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | OmniJARVISï¼šå¼€å¯å¼€æ”¾ä¸–ç•ŒæŒ‡ä»¤è·Ÿéšä»£ç†çš„æ–°ç¯‡ç« <br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰çš„æˆåŠŸï¼Œæ„å»ºèƒ½å¤Ÿç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤çš„è‡ªä¸»ä»£ç†æˆä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦ç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­é¢ä¸´ç€ä¸¤å¤§æŒ‘æˆ˜ï¼šå¤æ‚ä¸”é«˜åº¦ä¾èµ–ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ï¼Œä»¥åŠéœ€è¦é•¿æœŸè§„åˆ’çš„ä»»åŠ¡ã€‚ç°æœ‰çš„VLAæ¨¡å‹è¦ä¹ˆä¾èµ–äºæ–‡æœ¬æŒ‡ä»¤ä¸æ§åˆ¶å™¨è¿›è¡Œé€šä¿¡ï¼Œè¦ä¹ˆç›´æ¥è¾“å‡ºæ§åˆ¶å‘½ä»¤ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½å­˜åœ¨å±€é™æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>OmniJARVISæå‡ºäº†ä¸€ç§æ–°çš„VLAæ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€äº¤äº’æ•°æ®æ ‡è®°åŒ–æ–¹æ³•ï¼Œå®ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å’Œé«˜æ•ˆçš„å†³ç­–èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ è¡Œä¸ºæ ‡è®°åŒ–ï¼šOmniJARVISå¼•å…¥äº†ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå­¦ä¹ ä¸€ä¸ªè¡Œä¸ºç¼–ç å™¨ï¼Œå°†è¡Œä¸ºè½¨è¿¹è½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°ï¼Œå¹¶ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ ç­–ç•¥è§£ç å™¨æ ¹æ®è¿™äº›æ ‡è®°ç”Ÿæˆæ§åˆ¶å‘½ä»¤ã€‚<br><br>ğŸ’¡ è‡ªå›å½’å»ºæ¨¡ï¼šé€šè¿‡å°†è¡Œä¸ºæ ‡è®°æ·»åŠ åˆ°é¢„è®­ç»ƒçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„è¯æ±‡è¡¨ä¸­ï¼ŒOmniJARVISå°†å¤šæ¨¡æ€äº¤äº’æ•°æ®æ‰“åŒ…æˆç»Ÿä¸€çš„æ ‡è®°åºåˆ—ï¼Œå¹¶ä½¿ç”¨è‡ªå›å½’Transformeræ¨¡å‹è¿›è¡Œå»ºæ¨¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>OmniJARVISåœ¨å¼€æ”¾ä¸–ç•Œçš„Minecraftç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬åŸå­ä»»åŠ¡ã€ç¨‹åºæ€§ä»»åŠ¡å’Œå¼€æ”¾æ€§ä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼ŒOmniJARVISåœ¨å„ç§ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œæ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>OmniJARVISçš„è®¾è®¡ä¸ºæ„å»ºå¼€æ”¾ä¸–ç•ŒæŒ‡ä»¤è·Ÿéšä»£ç†æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶è¡Œä¸ºæ ‡è®°åŒ–å’Œè‡ªå›å½’å»ºæ¨¡æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å¤šæ¨¡æ€äº¤äº’æ•°æ®ï¼Œå¹¶å®ç°å¼ºå¤§çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒOmniJARVISçš„å¯æ‰©å±•æ€§ä¹Ÿä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å¹¿é˜”çš„ç©ºé—´ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>OmniJARVISæ˜¯ä¸€ç§åˆ›æ–°çš„VLAæ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€äº¤äº’æ•°æ®æ ‡è®°åŒ–æ–¹æ³•ï¼Œå®ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å’Œé«˜æ•ˆçš„å†³ç­–èƒ½åŠ›ã€‚å®ƒåœ¨å¼€æ”¾ä¸–ç•Œçš„Minecraftç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ºæ„å»ºèƒ½å¤Ÿç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤çš„è‡ªä¸»ä»£ç†æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</td>
    </tr>
    <tr>
      <th>22</th>
      <td>MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory</td>
      <td>Significant advances have been made in developing general-purpose embodied AI<br>in environments like Minecraft through the adoption of LLM-augmented<br>hierarchical approaches. While these approaches, which combine high-level<br>planners with low-level controllers, show promise, low-level controllers<br>frequently become performance bottlenecks due to repeated failures. In this<br>paper, we argue that the primary cause of failure in many low-level controllers<br>is the absence of an episodic memory system. To address this, we introduce<br>MrSteve (Memory Recall Steve-1), a novel low-level controller equipped with<br>Place Event Memory (PEM), a form of episodic memory that captures what, where,<br>and when information from episodes. This directly addresses the main limitation<br>of the popular low-level controller, Steve-1. Unlike previous models that rely<br>on short-term memory, PEM organizes spatial and event-based data, enabling<br>efficient recall and navigation in long-horizon tasks. Additionally, we propose<br>an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing<br>agents to alternate between exploration and task-solving based on recalled<br>events. Our approach significantly improves task-solving and exploration<br>efficiency compared to existing methods. We will release our code and demos on<br>the project page: https://sites.google.com/view/mr-steve.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MrSteveï¼šMinecraftä¸­å…·æœ‰What-Where-Whenè®°å¿†çš„æŒ‡ä»¤è·Ÿéšä»£ç†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨Minecraftç­‰å¤æ‚ç¯å¢ƒä¸­ï¼Œå¼€å‘é€šç”¨çš„å…·èº«AIä»£ç†é¢ä¸´ç€å·¨å¤§æŒ‘æˆ˜ã€‚å°½ç®¡LLMå¢å¼ºçš„åˆ†å±‚æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä½çº§æ§åˆ¶å™¨å¾€å¾€æˆä¸ºæ€§èƒ½ç“¶é¢ˆï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹å¯¹è¿‡å»äº‹ä»¶çš„è®°å¿†ï¼Œå¯¼è‡´é‡å¤å¤±è´¥å’Œä½æ•ˆæ¢ç´¢ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMrSteve (Memory Recall Steve-1)<br>æœ¬æ–‡æå‡ºäº†MrSteveï¼Œä¸€ä¸ªæ–°å‹çš„ä½çº§æ§åˆ¶å™¨ï¼Œå®ƒé…å¤‡äº†Place Event Memory (PEM)ï¼Œä¸€ç§èƒ½å¤Ÿæ•è·What-Where-Whenä¿¡æ¯çš„æƒ…æ™¯è®°å¿†ç³»ç»Ÿã€‚PEMè¶…è¶Šäº†Steve-1ç­‰ç°æœ‰æ¨¡å‹çš„çŸ­æœŸè®°å¿†é™åˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ç»„ç»‡å’Œæ£€ç´¢ç©ºé—´å’Œäº‹ä»¶æ•°æ®ï¼Œä»è€Œåœ¨é•¿æœŸä»»åŠ¡ä¸­å®ç°é«˜æ•ˆçš„å›å¿†å’Œå¯¼èˆªã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¢ç´¢ç­–ç•¥å’Œè®°å¿†å¢å¼ºä»»åŠ¡è§£å†³æ¡†æ¶<br>ä¸ºäº†å……åˆ†åˆ©ç”¨PEMï¼Œæœ¬æ–‡è¿˜æå‡ºäº†æ¢ç´¢ç­–ç•¥å’Œè®°å¿†å¢å¼ºä»»åŠ¡è§£å†³æ¡†æ¶ã€‚è¯¥æ¡†æ¶å…è®¸ä»£ç†æ ¹æ®å›å¿†çš„äº‹ä»¶åœ¨æ¢ç´¢å’Œä»»åŠ¡è§£å†³ä¹‹é—´è¿›è¡Œåˆ‡æ¢ï¼Œä»è€Œæ˜¾è‘—æé«˜ä»»åŠ¡è§£å†³å’Œæ¢ç´¢æ•ˆç‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMrSteveåœ¨æ¢ç´¢å’Œé•¿æœŸä»»åŠ¡è§£å†³æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ABA-Sparseä»»åŠ¡ä¸­ï¼ŒMrSteveèƒ½å¤Ÿæ›´å¿«åœ°æ‰¾åˆ°ä»»åŠ¡ç›¸å…³çš„èµ„æºï¼Œå¹¶åœ¨æœ‰é™çš„æ—¶é—´å†…å®Œæˆä»»åŠ¡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„PEMå’Œè®°å¿†å¢å¼ºæ¡†æ¶ä¸ºå¼€å‘æ›´é«˜æ•ˆçš„ä½çº§æ§åˆ¶å™¨æä¾›äº†æ–°çš„æ€è·¯ã€‚PEMçš„ç»„ç»‡å’Œæ£€ç´¢æœºåˆ¶å¯ä»¥åº”ç”¨äºå…¶ä»–å…·èº«AIä»£ç†ï¼Œä»¥æé«˜å®ƒä»¬åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ¢ç´¢ç­–ç•¥å’Œä»»åŠ¡è§£å†³æ¡†æ¶çš„è®¾è®¡ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–AIç³»ç»Ÿæä¾›å‚è€ƒï¼Œä»¥å®ç°æ›´çµæ´»å’Œè‡ªé€‚åº”çš„è¡Œä¸ºã€‚</td>
    </tr>
    <tr>
      <th>23</th>
      <td>Voyager: An Open-Ended Embodied Agent with Large Language Models</td>
      <td>We introduce Voyager, the first LLM-powered embodied lifelong learning agent<br>in Minecraft that continuously explores the world, acquires diverse skills, and<br>makes novel discoveries without human intervention. Voyager consists of three<br>key components: 1) an automatic curriculum that maximizes exploration, 2) an<br>ever-growing skill library of executable code for storing and retrieving<br>complex behaviors, and 3) a new iterative prompting mechanism that incorporates<br>environment feedback, execution errors, and self-verification for program<br>improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses<br>the need for model parameter fine-tuning. The skills developed by Voyager are<br>temporally extended, interpretable, and compositional, which compounds the<br>agent's abilities rapidly and alleviates catastrophic forgetting. Empirically,<br>Voyager shows strong in-context lifelong learning capability and exhibits<br>exceptional proficiency in playing Minecraft. It obtains 3.3x more unique<br>items, travels 2.3x longer distances, and unlocks key tech tree milestones up<br>to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill<br>library in a new Minecraft world to solve novel tasks from scratch, while other<br>techniques struggle to generalize. We open-source our full codebase and prompts<br>at https://voyager.minedojo.org/.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Voyagerï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾å¼å…·èº«ç»ˆèº«å­¦ä¹ æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æ„å»ºèƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­æŒç»­æ¢ç´¢ã€è§„åˆ’å’Œå¼€å‘æ–°æŠ€èƒ½çš„é€šç”¨å…·èº«æ™ºèƒ½ä½“ï¼Œæ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨æ¢ç´¢ã€å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“åˆ©ç”¨é¢„è®­ç»ƒLLMä¸­çš„ä¸–ç•ŒçŸ¥è¯†ç”Ÿæˆä¸€è‡´çš„è¡ŒåŠ¨è®¡åˆ’æˆ–å¯æ‰§è¡Œç­–ç•¥ï¼Œä½†å®ƒä»¬å¹¶éç»ˆèº«å­¦ä¹ è€…ï¼Œæ— æ³•åœ¨é•¿æ—¶é—´è·¨åº¦å†…é€æ­¥è·å–ã€æ›´æ–°ã€ç§¯ç´¯å’Œè½¬ç§»çŸ¥è¯†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>Voyager æ˜¯ç¬¬ä¸€ä¸ªç”± LLM é©±åŠ¨çš„å…·èº«ç»ˆèº«å­¦ä¹ æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿåœ¨ Minecraft ä¸­æŒç»­æ¢ç´¢ä¸–ç•Œã€è·å–å¤šæ ·åŒ–æŠ€èƒ½ï¼Œå¹¶åœ¨æ²¡æœ‰äººç±»å¹²é¢„çš„æƒ…å†µä¸‹è¿›è¡Œæ–°çš„å‘ç°ã€‚Voyager ç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªåŠ¨è¯¾ç¨‹<br>Voyager é€šè¿‡è‡ªåŠ¨è¯¾ç¨‹è¿›è¡Œå¼€æ”¾å¼æ¢ç´¢ï¼Œè¯¥è¯¾ç¨‹ç”± GPT-4 ç”Ÿæˆï¼Œæ—¨åœ¨â€œå‘ç°å°½å¯èƒ½å¤šçš„å¤šæ ·åŒ–äº‹ç‰©â€ã€‚è¯¾ç¨‹ä¼šæ ¹æ®æ¢ç´¢è¿›åº¦å’Œæ™ºèƒ½ä½“çš„çŠ¶æ€æå‡ºè¶Šæ¥è¶Šéš¾çš„ä»»åŠ¡ï¼Œä»è€Œæ¨åŠ¨æ™ºèƒ½ä½“ä¸æ–­å­¦ä¹ æ–°æŠ€èƒ½ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæŠ€èƒ½åº“<br>Voyager æ‹¥æœ‰ä¸€ä¸ªä¸æ–­å¢é•¿çš„æŠ€èƒ½åº“ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢å¯æ‰§è¡Œä»£ç ï¼Œä»¥å­˜å‚¨å’Œæ£€ç´¢å¤æ‚çš„è¡Œä¸ºã€‚æ¯ä¸ªæŠ€èƒ½éƒ½ç”±å¯æ‰§è¡Œä»£ç è¡¨ç¤ºï¼Œè¿™äº›ä»£ç å¯ä»¥è‡ªç„¶åœ°è¡¨ç¤ºæ—¶é—´æ‰©å±•å’Œç»„åˆåŠ¨ä½œï¼Œè¿™å¯¹äº Minecraft ä¸­çš„è®¸å¤šé•¿æœŸä»»åŠ¡è‡³å…³é‡è¦ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¿­ä»£æç¤ºæœºåˆ¶<br>Voyager é€šè¿‡è¿­ä»£æç¤ºæœºåˆ¶ç”Ÿæˆå¯æ‰§è¡Œä»£ç ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨ç¯å¢ƒåé¦ˆã€æ‰§è¡Œé”™è¯¯å’Œè‡ªæˆ‘éªŒè¯æ¥æ”¹è¿›ç¨‹åºã€‚è¯¥æœºåˆ¶é€šè¿‡æ‰§è¡Œç”Ÿæˆçš„ç¨‹åºã€è·å–ç¯å¢ƒåé¦ˆå’Œæ‰§è¡Œé”™è¯¯ï¼Œå¹¶å°†è¿™äº›åé¦ˆçº³å…¥ GPT-4 çš„æç¤ºä¸­ï¼Œä»è€Œè¿›è¡Œä»£ç æ”¹è¿›ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤è¿›è¡Œï¼Œç›´åˆ°è‡ªæˆ‘éªŒè¯æ¨¡å—ç¡®è®¤ä»»åŠ¡å®Œæˆï¼Œæ­¤æ—¶å°†ç¨‹åºæ·»åŠ åˆ°æŠ€èƒ½åº“ä¸­ï¼Œå¹¶æŸ¥è¯¢è‡ªåŠ¨è¯¾ç¨‹ä»¥è·å–ä¸‹ä¸€ä¸ªç›®æ ‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>Voyager åœ¨ MineDojo æ¡†æ¶ä¸­ä¸å…¶ä»– LLM åŸºäºæ™ºèƒ½ä½“æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒï¼Œç»“æœè¡¨æ˜ Voyager åœ¨å‘ç°æ–°ç‰©å“ã€è§£é” Minecraft æŠ€æœ¯æ ‘ã€ç©¿è¶Šå„ç§åœ°å½¢ä»¥åŠå°†å­¦ä¹ åˆ°çš„æŠ€èƒ½åº“åº”ç”¨äºæ–°ä¸–ç•Œä¸­çš„æœªè§ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚Voyager è·å¾—äº† 3.3 å€çš„æ–°ç‰©å“ï¼Œè§£é”å…³é”®æŠ€æœ¯æ ‘é‡Œç¨‹ç¢‘çš„é€Ÿåº¦æé«˜äº† 15.3 å€ï¼Œç©¿è¶Šçš„è·ç¦»æ˜¯åŸºçº¿çš„ 2.3 å€ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Voyager çš„è®¾è®¡ä¸ºå¼€å‘å¼ºå¤§çš„é€šç”¨æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªèµ·ç‚¹ï¼Œæ— éœ€è°ƒæ•´æ¨¡å‹å‚æ•°ã€‚å…¶è‡ªåŠ¨è¯¾ç¨‹ã€æŠ€èƒ½åº“å’Œè¿­ä»£æç¤ºæœºåˆ¶ä¸ºç»ˆèº«å­¦ä¹ æ™ºèƒ½ä½“çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒVoyager çš„æŠ€èƒ½åº“å¯ä»¥ä½œä¸ºå…¶ä»–æ–¹æ³•çš„å³æ’å³ç”¨èµ„äº§ï¼Œæœ‰æ•ˆåœ°æé«˜æ€§èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>24</th>
      <td>Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory</td>
      <td>The captivating realm of Minecraft has attracted substantial research<br>interest in recent years, serving as a rich platform for developing intelligent<br>agents capable of functioning in open-world environments. However, the current<br>research landscape predominantly focuses on specific objectives, such as the<br>popular "ObtainDiamond" task, and has not yet shown effective generalization to<br>a broader spectrum of tasks. Furthermore, the current leading success rate for<br>the "ObtainDiamond" task stands at around 20%, highlighting the limitations of<br>Reinforcement Learning (RL) based controllers used in existing methods. To<br>tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel<br>framework integrates Large Language Models (LLMs) with text-based knowledge and<br>memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These<br>agents, equipped with the logic and common sense capabilities of LLMs, can<br>skillfully navigate complex, sparse-reward environments with text-based<br>interactions. We develop a set of structured actions and leverage LLMs to<br>generate action plans for the agents to execute. The resulting LLM-based agent<br>markedly surpasses previous methods, achieving a remarkable improvement of<br>+47.5% in success rate on the "ObtainDiamond" task, demonstrating superior<br>robustness compared to traditional RL-based controllers. Notably, our agent is<br>the first to procure all items in the Minecraft Overworld technology tree,<br>demonstrating its extensive capabilities. GITM does not need any GPU for<br>training, but a single CPU node with 32 CPU cores is enough. This research<br>shows the potential of LLMs in developing capable agents for handling<br>long-horizon, complex tasks and adapting to uncertainties in open-world<br>environments. See the project website at https://github.com/OpenGVLab/GITM.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Minecraftä¸­çš„å¹½çµï¼šé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºæ–‡æœ¬çš„çŸ¥è¯†ä¸è®°å¿†ï¼Œåˆ›å»ºå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>Minecraftä½œä¸ºä¸€æ¬¾å¼€æ”¾ä¸–ç•Œæ¸¸æˆï¼Œå¸å¼•äº†å¤§é‡ç ”ç©¶å…´è¶£ï¼Œæˆä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿è¡Œçš„æ™ºèƒ½ä½“çš„ä¸°å¯Œå¹³å°ã€‚ç„¶è€Œï¼Œç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šç›®æ ‡ä¸Šï¼Œå¦‚æµè¡Œçš„â€œObtainDiamondâ€ä»»åŠ¡ï¼Œå°šæœªåœ¨æ›´å¹¿æ³›çš„ä»»åŠ¡ä¸Šå±•ç°å‡ºæœ‰æ•ˆçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨â€œObtainDiamondâ€ä»»åŠ¡ä¸Šçš„æœ€é«˜æˆåŠŸç‡ä»…ä¸ºçº¦20%ï¼Œçªæ˜¾äº†ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ§åˆ¶å™¨æ–¹æ³•çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Ghost in the Minecraftï¼ˆGITMï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸åŸºäºæ–‡æœ¬çš„çŸ¥è¯†å’Œè®°å¿†ç›¸ç»“åˆï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿåœ¨Minecraftä¸­è¿è¡Œçš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“ï¼ˆGCAsï¼‰ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLLMåˆ†è§£å™¨<br>LLMåˆ†è§£å™¨è´Ÿè´£å°†ä»»åŠ¡ç›®æ ‡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´æ˜“äºå®ç°çš„å­ç›®æ ‡ã€‚é€šè¿‡è§£å†³æ¯ä¸ªå­ç›®æ ‡ï¼Œå¯ä»¥é€æ­¥å®ç°ä»»åŠ¡ç›®æ ‡ã€‚LLMåˆ†è§£å™¨åˆ©ç”¨ä»äº’è”ç½‘æ”¶é›†çš„æ–‡æœ¬çŸ¥è¯†ï¼Œå°†ç›®æ ‡åˆ†è§£ä¸ºå­ç›®æ ‡æ ‘ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMè§„åˆ’å™¨<br>LLMè§„åˆ’å™¨è´Ÿè´£ä¸ºæ¯ä¸ªå­ç›®æ ‡ç”Ÿæˆä¸€ç³»åˆ—ç»“æ„åŒ–æ“ä½œã€‚ç»“æ„åŒ–æ“ä½œå…·æœ‰æ˜ç¡®çš„è¯­ä¹‰å’Œç›¸åº”çš„åé¦ˆï¼Œä½¿LLMsèƒ½å¤Ÿåœ¨è®¤çŸ¥å±‚é¢ç†è§£å‘¨å›´ç¯å¢ƒå¹¶åšå‡ºå†³ç­–ã€‚LLMè§„åˆ’å™¨è¿˜è®°å½•å’Œæ€»ç»“æˆåŠŸçš„æ“ä½œåˆ—è¡¨ï¼Œä»¥å¢å¼ºæœªæ¥çš„è§„åˆ’ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šLLMæ¥å£<br>LLMæ¥å£è´Ÿè´£å°†ç»“æ„åŒ–æ“ä½œè½¬æ¢ä¸ºé”®ç›˜/é¼ æ ‡æ“ä½œï¼Œå¹¶ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚å®ƒè¿˜ä»ç¯å¢ƒä¸­æå–è§‚å¯Ÿç»“æœï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºåé¦ˆæ¶ˆæ¯ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨â€œObtainDiamondâ€ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°äº†47.5%ï¼Œè¶…è¿‡äº†ç°æœ‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ™ºèƒ½ä½“æ˜¯ç¬¬ä¸€ä¸ªåœ¨Minecraft Overworldä¸­è·å–æ‰€æœ‰ç‰©å“çš„æ™ºèƒ½ä½“ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„æŠ€èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„GITMæ¡†æ¶ä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿è¡Œçš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡åˆ©ç”¨LLMsçš„å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥åŠåŸºäºæ–‡æœ¬çš„çŸ¥è¯†å’Œè®°å¿†ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä½¿æ™ºèƒ½ä½“æœ‰æ•ˆåœ°å¤„ç†å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å„ç§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å…·æœ‰é«˜æ•ˆçš„å­¦ä¹ æ•ˆç‡å’Œè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å¼€å‘é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Creative Agents: Empowering Agents with Imagination for Creative Tasks</td>
      <td>We study building embodied agents for open-ended creative tasks. While<br>existing methods build instruction-following agents that can perform diverse<br>open-ended tasks, none of them demonstrates creativity -- the ability to give<br>novel and diverse task solutions implicit in the language instructions. This<br>limitation comes from their inability to convert abstract language instructions<br>into concrete task goals in the environment and perform long-horizon planning<br>for such complicated goals. Given the observation that humans perform creative<br>tasks with the help of imagination, we propose a class of solutions for<br>creative agents, where the controller is enhanced with an imaginator that<br>generates detailed imaginations of task outcomes conditioned on language<br>instructions. We introduce several approaches to implementing the components of<br>creative agents. We implement the imaginator with either a large language model<br>for textual imagination or a diffusion model for visual imagination. The<br>controller can either be a behavior-cloning policy learned from data or a<br>pre-trained foundation model generating executable codes in the environment. We<br>benchmark creative tasks with the challenging open-world game Minecraft, where<br>the agents are asked to create diverse buildings given free-form language<br>instructions. In addition, we propose novel evaluation metrics for open-ended<br>creative tasks utilizing GPT-4V, which holds many advantages over existing<br>metrics. We perform a detailed experimental analysis of creative agents,<br>showing that creative agents are the first AI agents accomplishing diverse<br>building creation in the survival mode of Minecraft. Our benchmark and models<br>are open-source for future research on creative agents<br>(https://github.com/PKU-RL/Creative-Agents).</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ›æ„æ™ºèƒ½ä½“ï¼šèµ‹äºˆæ™ºèƒ½ä½“æƒ³è±¡åŠ›ä»¥å®Œæˆåˆ›æ„ä»»åŠ¡<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç°æœ‰çš„æ™ºèƒ½ä½“å¤§å¤šåªèƒ½æ‰§è¡Œé¢„å®šä¹‰çš„ä»»åŠ¡ï¼Œç¼ºä¹å¤„ç†å¼€æ”¾æ€§ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é‚£äº›éœ€è¦åˆ›é€ åŠ›çš„ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨Minecraftæ¸¸æˆä¸­ï¼Œç°æœ‰çš„æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œç®€å•çš„æŒ‡ä»¤ï¼Œå¦‚â€œæ”¶é›†çŸ³å¤´â€æˆ–â€œå»ºé€ ä¸€ä¸ªé›ªäººâ€ï¼Œä½†æ— æ³•å®Œæˆæ›´å¤æ‚çš„åˆ›æ„ä»»åŠ¡ï¼Œå¦‚â€œå»ºé€ ä¸€ä¸ªç ‚å²©å®«æ®¿â€ã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬æ— æ³•å°†æŠ½è±¡çš„è¯­è¨€æŒ‡ä»¤è½¬æ¢ä¸ºå…·ä½“çš„ä»»åŠ¡ç›®æ ‡ï¼Œå¹¶æ‰§è¡Œé•¿æœŸè§„åˆ’ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†â€œåˆ›æ„æ™ºèƒ½ä½“â€çš„æ¦‚å¿µï¼Œé€šè¿‡èµ‹äºˆæ™ºèƒ½ä½“æƒ³è±¡åŠ›æ¥å¤„ç†å¼€æ”¾æ€§åˆ›æ„ä»»åŠ¡ã€‚åˆ›æ„æ™ºèƒ½ä½“ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šæƒ³è±¡å™¨å’Œæ§åˆ¶å™¨ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæƒ³è±¡å™¨<br>æƒ³è±¡å™¨è´Ÿè´£æ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆä»»åŠ¡ç»“æœçš„è¯¦ç»†æƒ³è±¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç°æƒ³è±¡å™¨çš„æ–¹æ³•ï¼š<br>- **æ–‡æœ¬æƒ³è±¡**ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4ï¼Œé€šè¿‡Chain-of-Thoughtï¼ˆCoTï¼‰æŠ€æœ¯ç”Ÿæˆæ–‡æœ¬æƒ³è±¡ã€‚<br>- **è§†è§‰æƒ³è±¡**ï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionï¼Œç”Ÿæˆä¸æ–‡æœ¬æè¿°ç›¸ç¬¦çš„è§†è§‰æƒ³è±¡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ§åˆ¶å™¨<br>æ§åˆ¶å™¨è´Ÿè´£å°†æƒ³è±¡è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„è®¡åˆ’ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç°æ§åˆ¶å™¨çš„æ–¹æ³•ï¼š<br>- **è¡Œä¸ºå…‹éš†æ§åˆ¶å™¨**ï¼šä»ç¯å¢ƒä¸­å­¦ä¹ è¡Œä¸ºå…‹éš†ç­–ç•¥ï¼Œå°†å›¾åƒæƒ³è±¡è½¬æ¢ä¸ºå»ºç­‘è“å›¾ï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œã€‚<br>- **åŸºäºGPT-4(V)çš„æ§åˆ¶å™¨**ï¼šåˆ©ç”¨GPT-4(V)çš„è§†è§‰è¯­è¨€ç†è§£å’Œä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œç›´æ¥ç”Ÿæˆå¯æ‰§è¡Œä»£ç æ¥å®Œæˆä»»åŠ¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨Minecraftæ¸¸æˆä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåˆ›æ„æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®è‡ªç”±å½¢å¼çš„è¯­è¨€æŒ‡ä»¤åˆ›å»ºå¤šæ ·åŒ–å’Œè§†è§‰ä¸Šå¸å¼•äººçš„å»ºç­‘ã€‚å…¶ä¸­ï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†è§‰æƒ³è±¡ï¼Œå¹¶ç»“åˆGPT-4(V)è¿›è¡Œæ§åˆ¶çš„æ™ºèƒ½ä½“è¡¨ç°æœ€ä½³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åˆ›æ„æ™ºèƒ½ä½“æ¡†æ¶ä¸ºå¼€æ”¾æ€§å­¦ä¹ å’Œåˆ›æ„AIæ™ºèƒ½ä½“ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•æé«˜è¡Œä¸ºå…‹éš†æ§åˆ¶å™¨çš„æ€§èƒ½ï¼Œä»¥åŠå¦‚ä½•å¢å¼ºæ™ºèƒ½ä½“çš„åˆ›é€ åŠ›ã€‚</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</td>
      <td>Many reinforcement learning environments (e.g., Minecraft) provide only<br>sparse rewards that indicate task completion or failure with binary values. The<br>challenge in exploration efficiency in such environments makes it difficult for<br>reinforcement-learning-based agents to learn complex tasks. To address this,<br>this paper introduces an advanced learning system, named Auto MC-Reward, that<br>leverages Large Language Models (LLMs) to automatically design dense reward<br>functions, thereby enhancing the learning efficiency. Auto MC-Reward consists<br>of three important components: Reward Designer, Reward Critic, and Trajectory<br>Analyzer. Given the environment information and task descriptions, the Reward<br>Designer first design the reward function by coding an executable Python<br>function with predefined observation inputs. Then, our Reward Critic will be<br>responsible for verifying the code, checking whether the code is<br>self-consistent and free of syntax and semantic errors. Further, the Trajectory<br>Analyzer summarizes possible failure causes and provides refinement suggestions<br>according to collected trajectories. In the next round, Reward Designer will<br>further refine and iterate the dense reward function based on feedback.<br>Experiments demonstrate a significant improvement in the success rate and<br>learning efficiency of our agents in complex tasks in Minecraft, such as<br>obtaining diamond with the efficient ability to avoid lava, and efficiently<br>explore trees and animals that are sparse in the plains biome.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Auto MC-Rewardï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°ï¼Œæå‡Minecraftä¸­å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>Minecraft ç­‰å¼ºåŒ–å­¦ä¹ ç¯å¢ƒé€šå¸¸åªæä¾›ç¨€ç–å¥–åŠ±ï¼Œå³åªæœ‰ä»»åŠ¡å®Œæˆæˆ–å¤±è´¥æ—¶æ‰ä¼šè·å¾—å¥–åŠ±ã€‚è¿™ç§å¥–åŠ±æœºåˆ¶ä½¿å¾—å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨æ¢ç´¢æ•ˆç‡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å­¦ä¹ å¤æ‚ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† Auto MC-Rewardï¼Œä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°çš„å…ˆè¿›å­¦ä¹ ç³»ç»Ÿï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAuto MC-Reward ç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šå¥–åŠ±è®¾è®¡å™¨ã€å¥–åŠ±è¯„è®ºå®¶å’Œè½¨è¿¹åˆ†æå™¨ã€‚å¥–åŠ±è®¾è®¡å™¨æ ¹æ®ç¯å¢ƒä¿¡æ¯å’Œä»»åŠ¡æè¿°ï¼Œé€šè¿‡ç¼–å†™å¯æ‰§è¡Œçš„ Python å‡½æ•°æ¥è®¾è®¡å¥–åŠ±å‡½æ•°ã€‚å¥–åŠ±è¯„è®ºå®¶è´Ÿè´£éªŒè¯ä»£ç ï¼Œæ£€æŸ¥ä»£ç æ˜¯å¦è‡ªæ´½ä¸”æ²¡æœ‰è¯­æ³•å’Œè¯­ä¹‰é”™è¯¯ã€‚è½¨è¿¹åˆ†æå™¨æ ¹æ®æ”¶é›†çš„è½¨è¿¹æ€»ç»“å¯èƒ½çš„å¤±è´¥åŸå› ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šAuto MC-Reward åˆ©ç”¨ LLM çš„ä»»åŠ¡ç†è§£å’Œç»éªŒæ€»ç»“èƒ½åŠ›ï¼Œä¸ºå­¦ä¹ æä¾›è¯¦ç»†å’Œå³æ—¶çš„å¥–åŠ±æŒ‡å¯¼ã€‚å¥–åŠ±è®¾è®¡å™¨é¦–å…ˆæ ¹æ®ç¯å¢ƒå’Œä»»åŠ¡çš„åŸºæœ¬æè¿°ï¼Œä½¿ç”¨ LLM è®¾è®¡ä¸ä»»åŠ¡ç›¸å…³çš„å¯†é›†å¥–åŠ±å‡½æ•°ã€‚ç„¶åï¼Œå¥–åŠ±è¯„è®ºå®¶å¯¹è®¾è®¡çš„å¥–åŠ±å‡½æ•°è¿›è¡Œè‡ªæˆ‘éªŒè¯ã€‚ä¸ºäº†è§£å†³ LLM ç†è§£çš„æ½œåœ¨åå·®æˆ–ç–å¿½ï¼Œè¿˜æå‡ºäº†åŸºäº LLM çš„è½¨è¿¹åˆ†æå™¨ï¼Œç”¨äºåˆ†æå’Œæ€»ç»“è®­ç»ƒä»£ç†çš„è½¨è¿¹ï¼Œå¹¶å¸®åŠ©å¥–åŠ±è®¾è®¡å™¨æ”¹è¿›å¥–åŠ±å‡½æ•°ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>Auto MC-Reward åœ¨ä¸€ç³»åˆ—ä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬åœ°ä¸‹æ°´å¹³æ¢ç´¢é’»çŸ³å’Œæ¢ç´¢å¹³åŸç”Ÿç‰©ç¾¤è½ä¸­çš„æ ‘æœ¨å’ŒåŠ¨ç‰©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸå§‹ç¨€ç–å¥–åŠ±å’Œç°æœ‰å¯†é›†å¥–åŠ±æ–¹æ³•ç›¸æ¯”ï¼ŒAuto MC-Reward åœ¨è¿™äº›ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ›´å¥½çš„ç»“æœï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸Šé«˜æ•ˆå­¦ä¹ çš„å…ˆè¿›èƒ½åŠ›ã€‚é€šè¿‡è¿­ä»£æ”¹è¿›å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼ŒAuto MC-Reward ä½¿ä»£ç†èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å¯¹æ–°ä»»åŠ¡æœ‰ç›Šçš„æ–°è¡Œä¸ºï¼Œä¾‹å¦‚é¿å…ç†”å²©ï¼Œä»è€Œå¤§å¤§æé«˜äº†æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒAuto MC-Reward ä»…ä½¿ç”¨åŸå§‹ä¿¡æ¯å°±å®ç°äº†é«˜é’»çŸ³è·å–æˆåŠŸç‡ï¼ˆ36.5%ï¼‰ï¼Œè¯æ˜äº†å…¶è§£å†³é•¿æœŸä»»åŠ¡çš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Auto MC-Reward ä¸ºè§£å†³ç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸­çš„æ¢ç´¢æ•ˆç‡é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚å…¶åˆ©ç”¨ LLM è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æé«˜å¼ºåŒ–å­¦ä¹ ä»£ç†çš„å­¦ä¹ æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒAuto MC-Reward çš„ä¸‰ä¸ªç»„ä»¶ï¼ˆå¥–åŠ±è®¾è®¡å™¨ã€å¥–åŠ±è¯„è®ºå®¶å’Œè½¨è¿¹åˆ†æå™¨ï¼‰å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œä½¿å¾—æ•°æ®åˆ†æå’Œå¥–åŠ±å‡½æ•°æ›´æ–°æ›´åŠ çµæ´»ã€‚</td>
    </tr>
    <tr>
      <th>27</th>
      <td>Odyssey: Empowering Minecraft Agents with Open-World Skills</td>
      <td>Recent studies have delved into constructing generalist agents for open-world<br>environments like Minecraft. Despite the encouraging results, existing efforts<br>mainly focus on solving basic programmatic tasks, e.g., material collection and<br>tool-crafting following the Minecraft tech-tree, treating the ObtainDiamond<br>task as the ultimate goal. This limitation stems from the narrowly defined set<br>of actions available to agents, requiring them to learn effective long-horizon<br>strategies from scratch. Consequently, discovering diverse gameplay<br>opportunities in the open world becomes challenging. In this work, we introduce<br>Odyssey, a new framework that empowers Large Language Model (LLM)-based agents<br>with open-world skills to explore the vast Minecraft world. Odyssey comprises<br>three key parts: (1) An interactive agent with an open-world skill library that<br>consists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned<br>LLaMA-3 model trained on a large question-answering dataset with 390k+<br>instruction entries derived from the Minecraft Wiki. (3) A new agent capability<br>benchmark includes the long-term planning task, the dynamic-immediate planning<br>task, and the autonomous exploration task. Extensive experiments demonstrate<br>that the proposed Odyssey framework can effectively evaluate different<br>capabilities of LLM-based agents. All datasets, model weights, and code are<br>publicly available to motivate future research on more advanced autonomous<br>agent solutions.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Odysseyï¼šèµ‹äºˆMinecraftæ™ºèƒ½ä½“å¼€æ”¾ä¸–ç•ŒæŠ€èƒ½<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œè®¸å¤šç ”ç©¶è‡´åŠ›äºæ„å»ºèƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼ˆå¦‚Minecraftï¼‰æ‰§è¡Œä»»åŠ¡çš„é€šç”¨æ™ºèƒ½ä½“ã€‚å°½ç®¡å–å¾—äº†ä»¤äººé¼“èˆçš„æˆæœï¼Œä½†ç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨è§£å†³åŸºæœ¬çš„ç¼–ç¨‹ä»»åŠ¡ï¼Œä¾‹å¦‚æ”¶é›†ææ–™å’Œåˆ¶ä½œå·¥å…·ï¼Œå¹¶å°†â€œè·å¾—é’»çŸ³â€ä»»åŠ¡è§†ä¸ºæœ€ç»ˆç›®æ ‡ã€‚è¿™ç§å±€é™æ€§æºäºæ™ºèƒ½ä½“å¯ç”¨çš„åŠ¨ä½œé›†è¿‡äºç‹­çª„ï¼Œéœ€è¦å®ƒä»¬ä»å¤´å¼€å§‹å­¦ä¹ æœ‰æ•ˆçš„é•¿æœŸç­–ç•¥ã€‚å› æ­¤ï¼Œåœ¨å¼€æ”¾ä¸–ç•Œä¸­æ¢ç´¢å¤šæ ·åŒ–çš„æ¸¸æˆç©æ³•å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼€æ”¾ä¸–ç•ŒæŠ€èƒ½åº“<br>Odysseyæ¡†æ¶å¼€å‘äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äº¤äº’å¼æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“é…å¤‡äº†ä¸€ä¸ªå¼€æ”¾ä¸–ç•ŒæŠ€èƒ½åº“ï¼Œå…¶ä¸­åŒ…å«40ä¸ªåŸºæœ¬æŠ€èƒ½å’Œ183ä¸ªç»„åˆæŠ€èƒ½ã€‚è¿™äº›æŠ€èƒ½æ¶µç›–äº†ä»èµ„æºæ”¶é›†åˆ°å·¥å…·åˆ¶ä½œï¼Œå†åˆ°æˆ˜æ–—å’Œæ¢ç´¢çš„å„ç§ä»»åŠ¡ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›äº†ä¸°å¯Œçš„å·¥å…·æ¥åº”å¯¹å¼€æ”¾ä¸–ç•Œçš„æŒ‘æˆ˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¾®è°ƒLLaMA-3æ¨¡å‹<br>ä¸ºäº†æé«˜æ™ºèƒ½ä½“åœ¨Minecraftä¸­çš„æ€§èƒ½ï¼ŒOdysseyæ¡†æ¶ä½¿ç”¨æ¥è‡ªMinecraftç»´åŸºçš„å¤§è§„æ¨¡é—®ç­”æ•°æ®é›†å¯¹LLaMA-3æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚é€šè¿‡ç”ŸæˆåŒ…å«390k+æŒ‡ä»¤æ¡ç›®çš„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨LoRAæŠ€æœ¯è¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼ŒOdysseyæ¡†æ¶æ˜¾è‘—æå‡äº†LLMæ¨¡å‹åœ¨Minecrafté¢†åŸŸçš„çŸ¥è¯†å‚¨å¤‡å’Œæ¨ç†èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ™ºèƒ½ä½“èƒ½åŠ›åŸºå‡†<br>Odysseyæ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ™ºèƒ½ä½“èƒ½åŠ›åŸºå‡†ï¼ŒåŒ…æ‹¬é•¿æœŸè§„åˆ’ä»»åŠ¡ã€åŠ¨æ€å³æ—¶è§„åˆ’ä»»åŠ¡å’Œè‡ªä¸»æ¢ç´¢ä»»åŠ¡ã€‚è¿™äº›ä»»åŠ¡æ¶µç›–äº†Minecraftä¸­çš„å„ç§å¤æ‚åœºæ™¯ï¼Œå¹¶è¦æ±‚æ™ºèƒ½ä½“å±•ç°å‡ºå¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡è¿™äº›åŸºå‡†ä»»åŠ¡ï¼Œç ”ç©¶äººå‘˜å¯ä»¥å…¨é¢è¯„ä¼°æ™ºèƒ½ä½“çš„è§„åˆ’èƒ½åŠ›ã€èµ„æºç®¡ç†èƒ½åŠ›ã€æŠ€èƒ½æ£€ç´¢èƒ½åŠ›ä»¥åŠè‡ªä¸»æ¢ç´¢èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒOdysseyæ¡†æ¶åœ¨åŸºæœ¬ç¼–ç¨‹ä»»åŠ¡å’Œæ™ºèƒ½ä½“èƒ½åŠ›åŸºå‡†ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOdysseyæ¡†æ¶çš„æ™ºèƒ½ä½“åœ¨å®Œæˆä»»åŠ¡çš„é€Ÿåº¦ã€æˆåŠŸç‡å’Œèµ„æºåˆ©ç”¨ç‡æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒä¹Ÿè¯æ˜äº†å¼€æ”¾ä¸–ç•ŒæŠ€èƒ½åº“å’ŒLLMè§„åˆ’å™¨å¯¹æ™ºèƒ½ä½“æ•´ä½“æ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Odysseyæ¡†æ¶ä¸ºå¼€å‘å’Œç ”ç©¶å¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br><br>* **å¼€æ”¾ä¸–ç•ŒæŠ€èƒ½åº“**ï¼šä¸ºæ™ºèƒ½ä½“æä¾›ä¸°å¯Œçš„å·¥å…·å’Œç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿåº”å¯¹å„ç§å¤æ‚çš„ä»»åŠ¡å’ŒæŒ‘æˆ˜ã€‚<br>* **å¾®è°ƒLLMæ¨¡å‹**ï¼šé€šè¿‡é¢†åŸŸç‰¹å®šçš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œæå‡LLMæ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å‚¨å¤‡å’Œæ¨ç†èƒ½åŠ›ã€‚<br>* **æ™ºèƒ½ä½“èƒ½åŠ›åŸºå‡†**ï¼šä¸ºè¯„ä¼°æ™ºèƒ½ä½“çš„ä¸åŒèƒ½åŠ›æä¾›æ ‡å‡†åŒ–çš„æ¡†æ¶ï¼Œä¿ƒè¿›å¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä½“ç ”ç©¶çš„è¿›å±•ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>Odysseyæ¡†æ¶ä¸ºå¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä½“çš„å‘å±•å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œå¹¶ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·æ¥æ¢ç´¢å’Œè¯„ä¼°æ™ºèƒ½ä½“çš„èƒ½åŠ›ã€‚éšç€æœªæ¥ç ”ç©¶çš„ä¸æ–­æ·±å…¥ï¼ŒOdysseyæ¡†æ¶æœ‰æœ›æ¨åŠ¨å¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä½“æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½çš„é€šç”¨æ€§ç ”ç©¶åšå‡ºè´¡çŒ®ã€‚</td>
    </tr>
    <tr>
      <th>28</th>
      <td>See and Think: Embodied Agent in Virtual Environment</td>
      <td>Large language models (LLMs) have achieved impressive pro-gress on several<br>open-world tasks. Recently, using LLMs to build embodied agents has been a<br>hotspot. This paper proposes STEVE, a comprehensive and visionary embodied<br>agent in the Minecraft virtual environment. STEVE comprises three key<br>components: vision perception, language instruction, and code action. Vision<br>perception involves interpreting visual information in the environment, which<br>is then integrated into the LLMs component with agent state and task<br>instruction. Language instruction is responsible for iterative reasoning and<br>decomposing complex tasks into manageable guidelines. Code action generates<br>executable skill actions based on retrieval in skill database, enabling the<br>agent to interact effectively within the Minecraft environment. We also collect<br>STEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge<br>question-answering pairs, and 200+ skill-code pairs. We conduct continuous<br>block search, knowledge question and answering, and tech tree mastery to<br>evaluate the performance. Extensive experiments show that STEVE achieves at<br>most 1.5x faster unlocking key tech trees and 2.5x quicker in block search<br>tasks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | See and Think: Embodied Agent in Virtual Environment<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œè®¾è®¡èƒ½å¤Ÿè¡¨ç°å‡ºæ™ºèƒ½è¡Œä¸ºå’Œé€‚åº”æ€§çš„æ™ºèƒ½ä½“ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé•¿æœŸè€Œé‡è¦çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼€å‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå±•ç°å‡ºå…¶ä½œä¸ºå¤šåŠŸèƒ½ã€é€šç”¨å‹åŠ©æ‰‹çš„æ½œåŠ›ã€‚å°½ç®¡å¦‚æ­¤ï¼Œåœ¨è®¸å¤šå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå¦‚Minecraftï¼Œå½“ä»£æ™ºèƒ½ä½“ä¸»è¦ä½¿ç”¨LLMsè¿›è¡Œæ–‡æœ¬äº¤äº’ã€‚ç„¶è€Œï¼Œè¿™ç§å¯¹æ–‡æœ¬é€šä¿¡çš„ä¾èµ–é™åˆ¶äº†å®ƒä»¬åœ¨è¿™äº›ä¸–ç•Œä¸­çš„äº¤äº’ï¼ŒåŒ…æ‹¬ä½çº§æ¡ˆä¾‹ã€‚Minecraftè¦æ±‚æ™ºèƒ½ä½“å…·å¤‡å„ç§æŠ€èƒ½ï¼Œä»åˆ¶ä½œåŸºæœ¬ç‰©å“åˆ°æ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±LLMsé©±åŠ¨çš„æ™ºèƒ½ä½“å¾€å¾€äº§ç”Ÿä¸å¯é¢„æµ‹çš„è¾“å‡ºã€‚å®ƒä»¬äº¤äº’çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œæ—¨åœ¨å°†LLMçš„ç†è§£ä¸ç¯å¢ƒçš„ä¸Šä¸‹æ–‡å’Œé¢„æœŸç›®æ ‡ç›¸ä¸€è‡´ã€‚è¿™ç§æç¤ºå·¥ç¨‹è¿‡ç¨‹ä¸ä»…è´¹åŠ›ï¼Œè€Œä¸”æ— æ³•å®ç°åŸ¹å…»è‡ªä¸»ã€è‡ªæˆ‘é©±åŠ¨çš„æ™ºèƒ½ä½“çš„ç›®æ ‡ã€‚æ­¤å¤–ï¼Œæ–‡æœ¬é€šä¿¡åœ¨è‡ªç„¶ä¼ è¾¾æŸäº›ä¸–ç•Œæ¦‚å¿µæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚åˆ¶ä½œé…æ–¹ï¼Œè¿™äº›æ¦‚å¿µé€šå¸¸é€šè¿‡è§†è§‰æ›´æœ‰æ•ˆåœ°ä¼ è¾¾ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSTEVEï¼Œä¸€ä¸ªåœ¨è™šæ‹Ÿç¯å¢ƒä¸­å…·æœ‰è§†è§‰æ„ŸçŸ¥ã€è¯­è¨€æŒ‡ä»¤å’Œä»£ç åŠ¨ä½œçš„æ™ºèƒ½ä½“ï¼Œä¸ä¹‹å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨è§£é”å…³é”®æŠ€æœ¯æ ‘æ–¹é¢æœ€å¤šå¿«1.5å€ï¼Œåœ¨å—æœç´¢ä»»åŠ¡ä¸­æœ€å¤šå¿«2.3å€ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSTEVE-7B/13Bï¼Œä¸€ç³»åˆ—é€šè¿‡ä½¿ç”¨Llama-2-7B/13Bçš„MinecraftçŸ¥è¯†é—®ç­”å¯¹è¿›è¡Œå¾®è°ƒè·å¾—çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ”¶é›†STEVE-21Kæ•°æ®é›†ï¼ŒåŒ…æ‹¬600å¤šä¸ªè§†è§‰-ç¯å¢ƒå¯¹ã€20Kä¸ªçŸ¥è¯†é—®ç­”å¯¹å’Œ200å¤šä¸ªæŠ€èƒ½-ä»£ç å¯¹ï¼Œä»¥è¯æ˜STEVEçš„æœ‰æ•ˆæ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒSTEVEåœ¨è¿ç»­å—æœç´¢ã€çŸ¥è¯†é—®ç­”å’Œç§‘æŠ€æ ‘æŒæ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸AutoGPTå’ŒVoyagerç­‰åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒSTEVEåœ¨è§£é”å…³é”®æŠ€æœ¯æ ‘æ–¹é¢æœ€å¤šå¿«1.5å€ï¼Œåœ¨å—æœç´¢ä»»åŠ¡ä¸­æœ€å¤šå¿«2.3å€ã€‚æ­¤å¤–ï¼ŒSTEVEåœ¨çŸ¥è¯†é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½ä¼˜äºLlama-2å’ŒGPT-4ç­‰æ›´å¹¿æ³›çš„æ¨¡å‹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„STEVEæ¡†æ¶ä¸ºæ„å»ºå…·æœ‰è§†è§‰æ„ŸçŸ¥å’Œè¯­è¨€æŒ‡ä»¤èƒ½åŠ›çš„æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å‚è€ƒã€‚æ­¤å¤–ï¼ŒSTEVE-21Kæ•°æ®é›†ä¸ºç ”ç©¶äººå‘˜æä¾›äº†è¿›è¡Œå¤šæ¨¡æ€å­¦ä¹ ç ”ç©¶çš„æœ‰ç”¨èµ„æºã€‚</td>
    </tr>
    <tr>
      <th>29</th>
      <td>RL-GPT: Integrating Reinforcement Learning and Code-as-policy</td>
      <td>Large Language Models (LLMs) have demonstrated proficiency in utilizing<br>various tools by coding, yet they face limitations in handling intricate logic<br>and precise control. In embodied tasks, high-level planning is amenable to<br>direct coding, while low-level actions often necessitate task-specific<br>refinement, such as Reinforcement Learning (RL). To seamlessly integrate both<br>modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising<br>a slow agent and a fast agent. The slow agent analyzes actions suitable for<br>coding, while the fast agent executes coding tasks. This decomposition<br>effectively focuses each agent on specific tasks, proving highly efficient<br>within our pipeline. Our approach outperforms traditional RL methods and<br>existing GPT agents, demonstrating superior efficiency. In the Minecraft game,<br>it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it<br>achieves SOTA performance across all designated MineDojo tasks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | RL-GPTï¼šå°†å¼ºåŒ–å­¦ä¹ ä¸ä»£ç ç­–ç•¥ç›¸ç»“åˆï¼Œæå‡LLMåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ©ç”¨å„ç§å·¥å…·è¿›è¡Œç¼–ç æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†å¤æ‚é€»è¾‘å’Œç²¾ç¡®æ§åˆ¶æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨å…·èº«ä»»åŠ¡ä¸­ï¼Œé«˜çº§è§„åˆ’å¯ä»¥ç›´æ¥é€šè¿‡ç¼–ç å®ç°ï¼Œè€Œä½çº§åŠ¨ä½œé€šå¸¸éœ€è¦ç‰¹å®šä»»åŠ¡çš„ç»†åŒ–ï¼Œä¾‹å¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚ä¸ºäº†æ— ç¼é›†æˆè¿™ä¸¤ç§æ¨¡å¼ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤çº§åˆ†å±‚æ¡†æ¶RL-GPTï¼ŒåŒ…æ‹¬æ…¢é€Ÿä»£ç†å’Œå¿«é€Ÿä»£ç†ã€‚æ…¢é€Ÿä»£ç†åˆ†æé€‚åˆç¼–ç çš„åŠ¨ä½œï¼Œè€Œå¿«é€Ÿä»£ç†æ‰§è¡Œç¼–ç ä»»åŠ¡ã€‚è¿™ç§åˆ†è§£æœ‰æ•ˆåœ°ä½¿æ¯ä¸ªä»£ç†ä¸“æ³¨äºç‰¹å®šä»»åŠ¡ï¼Œåœ¨æˆ‘ä»¬çš„ç®¡é“ä¸­è¡¨ç°å‡ºé«˜æ•ˆç‡ã€‚æœ¬æ–‡çš„æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„RLæ–¹æ³•å’Œç°æœ‰çš„GPTä»£ç†ï¼Œå±•ç¤ºäº†å“è¶Šçš„æ•ˆç‡ã€‚åœ¨Minecraftæ¸¸æˆä¸­ï¼Œå®ƒåœ¨RTX3090ä¸Šçš„ä¸€å¤©å†…è¿…é€Ÿè·å¾—é’»çŸ³ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ‰€æœ‰æŒ‡å®šçš„MineDojoä»»åŠ¡ä¸­å®ç°äº†SOTAæ€§èƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥LLMä»£ç†ï¼Œåˆ©ç”¨RLè®­ç»ƒæµç¨‹ä½œä¸ºå·¥å…·ï¼Œä»¥å¢å¼ºLLMåœ¨ç¯å¢ƒä¸­çš„äº¤äº’ä»»åŠ¡å­¦ä¹ èƒ½åŠ›ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘äº†ä¸€ä¸ªä¸¤çº§åˆ†å±‚æ¡†æ¶ï¼Œèƒ½å¤Ÿç¡®å®šä»»åŠ¡ä¸­å“ªäº›åŠ¨ä½œåº”è¯¥ä½¿ç”¨RLå­¦ä¹ ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé¦–æ¬¡å°†é«˜çº§GPTç¼–ç åŠ¨ä½œçº³å…¥RLåŠ¨ä½œç©ºé—´ï¼Œæé«˜äº†RLçš„æ ·æœ¬æ•ˆç‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨MineDojoåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRL-GPTåœ¨æ‰€æœ‰é€‰å®šçš„ä»»åŠ¡ä¸­å®ç°äº†æœ€é«˜çš„æˆåŠŸç‡ã€‚åœ¨Minecraftæ¸¸æˆä¸­ï¼ŒRL-GPTåœ¨RTX3090ä¸Šçš„ä¸€å¤©å†…è¿…é€Ÿè·å¾—é’»çŸ³ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ‰€æœ‰æŒ‡å®šçš„MineDojoä»»åŠ¡ä¸­å®ç°äº†SOTAæ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„RL-GPTæ¡†æ¶ä¸ºLLMsåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ä»»åŠ¡å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡å°†RLå’Œä»£ç ç­–ç•¥ç›¸ç»“åˆï¼ŒRL-GPTèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤æ‚é€»è¾‘å’Œç²¾ç¡®æ§åˆ¶ï¼Œä»è€Œåœ¨å…·èº«ä»»åŠ¡ä¸­å–å¾—å“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ä¸¤çº§åˆ†å±‚æ¡†æ¶å’Œè¿­ä»£æœºåˆ¶ä¹Ÿä¸ºLLMsåœ¨ç¯å¢ƒä¸­çš„ä»»åŠ¡å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚</td>
    </tr>
    <tr>
      <th>30</th>
      <td>LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence</td>
      <td>Recent embodied agents are primarily built based on reinforcement learning<br>(RL) or large language models (LLMs). Among them, RL agents are efficient for<br>deployment but only perform very few tasks. By contrast, giant LLM agents<br>(often more than 1000B parameters) present strong generalization while<br>demanding enormous computing resources. In this work, we combine their<br>advantages while avoiding the drawbacks by conducting the proposed referee RL<br>on our developed large auto-regressive model (LARM). Specifically, LARM is<br>built upon a lightweight LLM (fewer than 5B parameters) and directly outputs<br>the next action to execute rather than text. We mathematically reveal that<br>classic RL feedbacks vanish in long-horizon embodied exploration and introduce<br>a giant LLM based referee to handle this reward vanishment during training<br>LARM. In this way, LARM learns to complete diverse open-world tasks without<br>human intervention. Especially, LARM successfully harvests enchanted diamond<br>equipment in Minecraft, which demands significantly longer decision-making<br>chains than the highest achievements of prior best methods.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | LARMï¼šé«˜æ•ˆä¸”é€šç”¨çš„å…·èº«æ™ºèƒ½æ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™äº›æŠ€æœ¯å¤§å¤šç¼ºä¹ä¸çœŸå®ä¸–ç•Œè¿›è¡Œç‰©ç†äº¤äº’çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå…·èº«äººå·¥æ™ºèƒ½çš„æ¦‚å¿µè¢«æå‡ºï¼Œæ—¨åœ¨å¼€å‘èƒ½å¤Ÿä¸çœŸå®ä¸–ç•Œè¿›è¡Œäº¤äº’çš„æ™ºèƒ½ä½“ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å…·èº«æ™ºèƒ½ä½“å¤§å¤šå±€é™äºç‰¹å®šä»»åŠ¡ï¼Œç¼ºä¹é€šç”¨æ€§å’Œå¼€æ”¾æ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLARMçš„å…·èº«æ™ºèƒ½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­å®Œæˆå„ç§ä»»åŠ¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLARMæ¨¡å‹<br>LARMæ¨¡å‹åŸºäºè½»é‡çº§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç›´æ¥è¾“å‡ºä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œè€Œä¸æ˜¯æ–‡æœ¬ã€‚è¿™ä½¿å¾—LARMæ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿå“åº”ç¯å¢ƒå˜åŒ–ï¼Œå¹¶å…·æœ‰æ›´å¼ºçš„é€šç”¨æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè£åˆ¤å¼ºåŒ–å­¦ä¹ <br>ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨é•¿æ—¶åºæ¢ç´¢ä¸­å­˜åœ¨åé¦ˆæ¶ˆå¤±çš„é—®é¢˜ï¼Œå³æ™ºèƒ½ä½“åœ¨å®Œæˆä»»åŠ¡ä¹‹å‰æ— æ³•è·å¾—æœ‰æ•ˆçš„å¥–åŠ±ä¿¡å·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è£åˆ¤å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè£åˆ¤ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›å³æ—¶åé¦ˆï¼Œä»è€Œæœ‰æ•ˆåœ°æŒ‡å¯¼æ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨MineDojoå’ŒMineflayerç¯å¢ƒä¸­å¯¹LARMæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLARMæ¨¡å‹åœ¨å®Œæˆå„ç§ä»»åŠ¡æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ”¶é›†èµ„æºã€åˆ¶ä½œå·¥å…·ç­‰ã€‚ç‰¹åˆ«æ˜¯åœ¨Mineflayerç¯å¢ƒä¸­ï¼ŒLARMæ¨¡å‹æˆåŠŸåœ°åˆ¶ä½œäº†é™„é­”é’»çŸ³è£…å¤‡ï¼Œè¿™æ˜¯ä¹‹å‰æ–¹æ³•æ— æ³•å®ç°çš„ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„LARMæ¨¡å‹å’Œè£åˆ¤å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ä¸ºå¼€å‘é«˜æ•ˆä¸”é€šç”¨çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚LARMæ¨¡å‹çš„ç»“æ„å’Œè®­ç»ƒæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–å¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼Œè€Œè£åˆ¤å¼ºåŒ–å­¦ä¹ æŠ€æœ¯å¯ä»¥è§£å†³é•¿æ—¶åºæ¢ç´¢ä¸­çš„åé¦ˆæ¶ˆå¤±é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†ç½‘é¡µæ•°æ®é¢„è®­ç»ƒå¯¹æå‡æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ï¼Œè¿™ä¸ºåˆ©ç”¨äº’è”ç½‘æ•°æ®è®­ç»ƒå…·èº«æ™ºèƒ½ä½“æä¾›äº†å¯ç¤ºã€‚</td>
    </tr>
    <tr>
      <th>31</th>
      <td>Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification</td>
      <td>Building open agents has always been the ultimate goal in AI research, and<br>creative agents are the more enticing. Existing LLM agents excel at<br>long-horizon tasks with well-defined goals (e.g., `mine diamonds' in<br>Minecraft). However, they encounter difficulties on creative tasks with open<br>goals and abstract criteria due to the inability to bridge the gap between<br>them, thus lacking feedback for self-improvement in solving the task. In this<br>work, we introduce autonomous embodied verification techniques for agents to<br>fill the gap, laying the groundwork for creative tasks. Specifically, we<br>propose the Luban agent target creative building tasks in Minecraft, which<br>equips with two-level autonomous embodied verification inspired by human design<br>practices: (1) visual verification of 3D structural speculates, which comes<br>from agent synthesized CAD modeling programs; (2) pragmatic verification of the<br>creation by generating and verifying environment-relevant functionality<br>programs based on the abstract criteria. Extensive multi-dimensional human<br>studies and Elo ratings show that the Luban completes diverse creative building<br>tasks in our proposed benchmark and outperforms other baselines (\( 33\% \) to<br>\( 100\% \)) in both visualization and pragmatism. Additional demos on the<br>real-world robotic arm show the creation potential of the Luban in the physical<br>world.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Lubanï¼šæ„å»ºå¼€æ”¾å¼çš„åˆ›é€ æ€§æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œæ„å»ºèƒ½å¤Ÿè‡ªä¸»è§£å†³å¤æ‚ä»»åŠ¡çš„å¼€æ”¾å¼æ™ºèƒ½ä½“ä¸€ç›´æ˜¯ç ”ç©¶çš„ç›®æ ‡ã€‚ç‰¹åˆ«æ˜¯åˆ›é€ æ€§ä»»åŠ¡ï¼Œç”±äºå…¶å¼€æ”¾çš„ç›®æ ‡å’ŒæŠ½è±¡çš„è¯„ä¼°æ ‡å‡†ï¼Œå¯¹æ™ºèƒ½ä½“æå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚ç°æœ‰çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“åœ¨å¤„ç†å…·æœ‰æ˜ç¡®ç›®æ ‡çš„é•¿æœŸä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é¢å¯¹åˆ›é€ æ€§ä»»åŠ¡æ—¶ï¼Œç”±äºç¼ºä¹æ˜ç¡®çš„è¯„ä¼°æ ‡å‡†ï¼Œéš¾ä»¥è¿›è¡Œè‡ªæˆ‘æ”¹è¿›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†Lubanæ™ºèƒ½ä½“ï¼Œæ—¨åœ¨é€šè¿‡è‡ªä¸»çš„èº«ä½“éªŒè¯æŠ€æœ¯è§£å†³åˆ›é€ æ€§ä»»åŠ¡ä¸­çš„è¯„ä¼°éš¾é¢˜ã€‚Lubanæ™ºèƒ½ä½“é‡‡ç”¨äº†ä¸¤çº§è‡ªä¸»èº«ä½“éªŒè¯æœºåˆ¶ï¼Œçµæ„Ÿæ¥æºäºäººç±»çš„è®¾è®¡å®è·µï¼š<br><br>1. **è§†è§‰éªŒè¯**ï¼šLubané¦–å…ˆé€šè¿‡CADç¨‹åºåˆæˆ3Dç»“æ„æ¨¡å‹ï¼Œå¹¶è¿›è¡Œè§†è§‰éªŒè¯ï¼Œç¡®ä¿æ¨¡å‹çš„å¤–è§‚ç¬¦åˆè®¾è®¡è¦æ±‚ã€‚<br>2. **å®ç”¨éªŒè¯**ï¼šåœ¨é€šè¿‡è§†è§‰éªŒè¯åï¼ŒLubanä¼šç”Ÿæˆä¸æŠ½è±¡æ ‡å‡†ç›¸å…³çš„ç¯å¢ƒç›¸å…³åŠŸèƒ½ç¨‹åºï¼Œå¹¶è¿›è¡Œå®ç”¨éªŒè¯ï¼Œç¡®ä¿æ¨¡å‹çš„åŠŸèƒ½æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>é€šè¿‡åœ¨Minecraftä¸­è®¾è®¡åŒ…å«5ä¸ªå…·æœ‰ä¸åŒè§†è§‰å’ŒåŠŸèƒ½è¦æ±‚çš„å»ºç­‘ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼ŒLubanæ™ºèƒ½ä½“åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å‡æˆåŠŸå®Œæˆï¼Œå¹¶ä¸”åœ¨å¯è§†åŒ–æ–¹é¢å’Œå®ç”¨æ€§æ–¹é¢å‡ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼ˆ33%è‡³100%ï¼‰ã€‚æ­¤å¤–ï¼ŒLubanåœ¨çœŸå®ä¸–ç•Œçš„æœºæ¢°è‡‚ä¸Šçš„æ¼”ç¤ºä¹Ÿå±•ç¤ºäº†å…¶åœ¨ç‰©ç†ä¸–ç•Œä¸­è¿›è¡Œå¼€æ”¾å¼åˆ›é€ æ€§ä»»åŠ¡çš„æ½œåŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Lubanæ™ºèƒ½ä½“çš„è®¾è®¡ä¸ºæ„å»ºå¼€æ”¾å¼åˆ›é€ æ€§æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶ä¸¤çº§è‡ªä¸»èº«ä½“éªŒè¯æœºåˆ¶ä¸ºè§£å†³åˆ›é€ æ€§ä»»åŠ¡ä¸­çš„è¯„ä¼°éš¾é¢˜æä¾›äº†æœ‰æ•ˆçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒLubançš„è®¾è®¡ä¹Ÿå±•ç¤ºäº†å°†è™šæ‹Ÿä¸–ç•Œä¸­çš„æ™ºèƒ½ä½“æŠ€æœ¯åº”ç”¨äºçœŸå®ä¸–ç•Œçš„æ½œåŠ›ã€‚</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        